<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GCNDepth: Self-supervised Monocular Depth Estimation based on Graph Convolutional Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Masoumian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hatem</forename><forename type="middle">A</forename><surname>Rashwan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saddam</forename><surname>Abdulwahab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juli?n</forename><surname>Cristiano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenec</forename><surname>Puig</surname></persName>
						</author>
						<title level="a" type="main">GCNDepth: Self-supervised Monocular Depth Estimation based on Graph Convolutional Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>Graph convolutional network</term>
					<term>Monocular depth estimation</term>
					<term>Self-supervision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation is a challenging task of 3D reconstruction to enhance the accuracy sensing of environment awareness. This work brings a new solution with a set of improvements, which increase the quantitative and qualitative understanding of depth maps compared to existing methods. Recently, convolutional neural network (CNN) has demonstrated its extraordinary ability in estimating depth maps from monocular videos. However, traditional CNN does not support topological structure and they can work only on regular image regions with determined size and weights. On the other hand, graph convolutional networks (GCN) can handle the convolution on non-Euclidean data and it can be applied to irregular image regions within a topological structure. Therefore, in this work in order to preserve object geometric appearances and distributions, we aim at exploiting GCN for a self-supervised depth estimation model. Our model consists of two parallel auto-encoder networks: the first is an auto-encoder that will depend on ResNet-50 and extract the feature from the input image and on multiscale GCN to estimate the depth map. In turn, the second network will be used to estimate the ego-motion vector (i.e., 3D pose) between two consecutive frames based on ResNet-18. Both the estimated 3D pose and depth map will be used for constructing a target image. A combination of loss functions related to photometric, reprojection and smoothness is used to cope with bad depth prediction and preserve the discontinuities of the objects. In particular, our method provided comparable and promising results with a high prediction accuracy of 89% on the publicly KITTI and Make3D datasets along with a reduction of 40% in the number of trainable parameters compared to the state of the art solutions. The source code is publicly available at https://github.com/ArminMasoumian/GCNDepth.git</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N the Artificial Intelligence (AI) field, especially deep learning (DL) networks have accomplished high performance in various tasks of depth estimation and ego-motion prediction and nowadays it is steeply expanding. The importance of depth estimating, as a pull factor for the entry of newfangled technologies into self-driving vehicles <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and object distance prediction <ref type="bibr" target="#b2">[3]</ref> helping impaired/blind persons, is targeting the improvement of the quality and productivity in the day-to-day life of humankind. The stereo vision system is one of the common computer vision techniques is used for depth estimation. However, in order to save cost and computational resources, many methods This research has been possible with the support of the Secretariad Universitatsi Recercadel Departamentd Empresai Coneixement de la Generalitat de Catalunya. A. <ref type="bibr">Masoumian</ref> have been presented to perform monocular depth estimation. However, the primitive works focused on studying the extent of the depth prediction with supervised deep networks. Nevertheless, gathering the large and accurate datasets and ground truth depth for training supervised models is a difficult task <ref type="bibr" target="#b3">[4]</ref>, especially for developing a high resolution and quality of ground truth. Besides, extremely expensive components such as 2D/3D LIDAR sensors are needed. Regarding the unsupervised monocular depth estimation, using monocular video is a noteworthy alternative to stereo datasets. For using a monocular video dataset to predict the depth of a single image, any DL model needs to include a pose estimator, which receives a sequence of frames as an input and its output provide the corresponding camera transformation <ref type="bibr" target="#b4">[5]</ref>. Both the depth (disparity) map and camera transformation can be used for constructing the target frame that can be exploited to minimize the model error during the optimization process.</p><p>Most of the new existing DL monocular depth estimation networks use convolutional neural networks (CNN) to extract the feature information. However, CNN is limited because it does not consider the characteristics of the geometric depth information and the distribution of depth maps. Besides, there is recently a need to extend deep neural models from Euclidean domain achieved by CNNs to non-Euclidean domains, which is generally referred to as a geometric DL a growing research area <ref type="bibr" target="#b5">[6]</ref>. Therefore, the research community started to observe the importance of DL networks based on graphs. The effectiveness of the graph convolution network (GCN) has been proved in processing graph data on the tasks of graph node classification, such as depth prediction. Thus, in this work, we propose an architectural DL network, the socalled GCNDepth, that can help to advance monocular depth estimation.</p><p>In general, our contributions are summarized as follow:</p><p>? A graph convolutional network (GCN) is proposed for self-supervised depth prediction to improve the accuracy of depth maps by learning the nodes (i.e., pixels) representation through constructing the depth maps via propagating neighbor's information in an iterative manner until reaching a stable point. ? To widely exploit the diverse spatial correlations between the pixels at multiple scales, we also propose multiple GCN networks in the layers of the decoder network with different neighbourhood scales. ? A combination of different loss functions, related to photometric, reprojection and smoothness, is proposed to improve the quality of predicted depth maps. The reprojection loss is used to cope with objects occlusion, and the reconstruction loss is proposed for feature reconstruction to reduce the losses between target and reconstructed images. In turn, smoothness loss is used to preserve the edges and boundaries of the objects and reduce the effect of texture regions on the estimated depth. This article is organized as follows, Section II reviews the background and related works on monocular depth estimation, the detailed explanation of the proposed model is described in Section III. The validation of our system through experimental results is given in Section IV and Section V represents the conclusion of this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND RELATED WORK</head><p>Depth estimation can be widely categorized into supervised and self-supervised DL. In this section, we present a brief review of both supervised-and self-supervised-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervised Depth Estimation</head><p>Single image depth estimation is an intrinsically ill-posed dilemma that a single input image can project multiple feasible depth maps. Supervised methods proved that this problem can be solved by fitting the relation between color images and their comparable depths by learning with ground truth. Diversified approaches have been explored for solving this problem such as end-to-end supervised learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, combining local predictions <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref> and non-parametric scene sampling <ref type="bibr" target="#b10">[10]</ref>. All fully supervised training methods require both RGB images and the ground truth depth of each image for training. These ground truth depths can be delicately collected either from LIDAR sensors or be rendered from simulation engines <ref type="bibr" target="#b12">[11]</ref>. However, the LIDAR sensors limit allocating to new vision sensors and rendering real scenes <ref type="bibr" target="#b13">[12]</ref>. It is a difficult task to create or collect datasets with ground truth for training supervised models. Therefore, finding the original ground truths for supervised training is one of the limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-supervised Depth Estimation</head><p>Self-supervised learning consolidates and unifies these parts into a single framework. As an alternative for the absence of ground truth, self-supervised models can be trained by comparing a target image to a reconstructed image as the supervisory signal. Image reconstruction can be done either by stereo training or monocular training. Stereo training uses synchronized stereo pairs of images and predicts the disparity pixel between the pairs <ref type="bibr" target="#b14">[13]</ref>. There are various approaches based on stereo pairs such as generative adversarial networks <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>, temporal information <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b18">[17]</ref> and predicting continuous disparity <ref type="bibr" target="#b19">[18]</ref>. Regarding monocular video training, a sequence of frames provides the training signal along with the network needs to predict the camera pose between each consecutive frame, which is only needed during training to improve constrain of the depth prediction. Recent monocular depth estimation approaches, such as using enforced edge consistency <ref type="bibr" target="#b20">[19]</ref> and adding a depth normalization layer as smoothness term <ref type="bibr" target="#b21">[20]</ref>, have achieved high performance compared to the stereo pair training. Some self-supervised training works also on making presumptions about material properties and appearance, such as the brightness constancy of object surfaces between each frame. Lina Liu et al. <ref type="bibr" target="#b22">[21]</ref> used domain separation to relieve the variation of illumination between day and night images. Also, Michael et al. <ref type="bibr" target="#b23">[22]</ref> used Wavelet Decomposition to achieve a high-efficiency depth map. Regarding boosting depth estimation to high resolution, a content-adaptive multi-resolution merging model was proposed in <ref type="bibr" target="#b24">[23]</ref>. Most of the aforementioned models start to tackle the problem in a self-supervised way by learning the depth map based on the photometric error and adopting differentiable interpolation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[24]</ref>- <ref type="bibr" target="#b29">[28]</ref> as loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph Neural Network</head><p>TThe graph convolutional network proposed by Kipf et al. <ref type="bibr" target="#b30">[29]</ref> aims at the semi-supervised node classification task on graphs and represents a learning method for target nodes to propagate the neighboring information through recurrent neural networks (RNN). Their GCN model employed a propagation rule based on the first-order approximation of spectral convolutions on graphs. However, this method requires high consumption of computational resources depending on the size of input data. Thus, recent works have proposed to use CNNs for neighbour information propagation, and at the same time, they directly depend on graphs and take the advantage of their organizational information <ref type="bibr" target="#b31">[30]</ref>. Another example of a graph-based model based on CNNs was proposed in <ref type="bibr" target="#b32">[31]</ref> by arranging the adjacent nodes information with convolutionbased on spectral graph theory. However, this causes losing many nodes of the image when 3D objects are mapped in 2D planes. Fu et al. <ref type="bibr" target="#b33">[32]</ref>, created the depth topological graph from a coarse depth map and they used this graph as a depth clue in their model to avoid depth nodes losses. Although this technique generates a depth topological graph from a coarse depth map obtained from pre-trained models. Therefore, a pretrained model is required in their method. Thus, we propose a self-supervised CNN-GCN auto-encoder for monocular depth estimation to solve the aforementioned problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we firstly describe the architecture of the proposed model introducing our graph convolutional network and the whole structure of our novel self-supervised model (DepthNet and PoseNet) with a detailed description of the network. Besides, we present the loss functions used for training the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Defination</head><p>The GCNDepth is a multi-task DL-based system that consists of two parallel networks, DepthNet and PoseNet. If I ? A represent a monocular RGB image, the problem of generating its corresponding depth image, D ? B, can be formally define as a function ? D : A ?? B that maps elements from domain A to elements in its corresponding domain B, as follows:</p><formula xml:id="formula_0">D(p) = ? D (I s (p))<label>(1)</label></formula><p>where the proposed model, DepthNet, approximates the prediction of a depth map, D, as a function, ? D , which is fed by a source RGB frame, I s (p) as an input, where p represents a pixel in the corresponding image.</p><p>Similarly, the problem of estimating the viewpoint between two consequent RGB images can be formally define as a function ? E : A ?? R 3 , which is fed by two consequent frames, I s (p) and I t (p) as an input and predicts an ego-motion vector, as follows: E Is??It = [r T , t T ], where r = [??, ??, ??] T is is a rotation vector, and t = [?x, ?y, ?z] T is a translation vector. The mapping process can be approximated as follows:</p><formula xml:id="formula_1">E Is??It = ? E (I s (p), I t (p))<label>(2)</label></formula><p>Both the depth and ego-motion vector along with the I s source frame are used for reconstructing an image, I rec that has to be close to the target image, I t . Thus, our model, GCNDepth, aims at approximating the total process as follows:</p><formula xml:id="formula_2">?(I s (p), I t (p)) = (D(p), E Is??It , I rec (p))<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Convolutional Network</head><p>One of the main problems with CNN models is that they are not able to compute the data of non-Euclidean domains. The use of DL models based on CNN on complex 3D scenes, such as depth maps, can yield a significant loss in the details of the objects in the scene, or even break the topological structure of the scene <ref type="bibr" target="#b5">[6]</ref>. Thus, the use of GCN networks that introduce topological structure and node features can increase the feature representation of hidden layers. This helps the model to learn how to map the depth information from low-dimensional features. Besides, they can represent the topological structure of the scene by representing the relations between nodes allowed. Generally, the graph convolution is defined as:</p><formula xml:id="formula_3">Z = ?(AXW )<label>(4)</label></formula><p>In equation 4, ?(.) defines a non-linear activation function, A is an adjacency matrix, which measures the relation between the nodes in the graph and W is the trainable weight matrix. X represents the input nodes into the graph structure, which in our case is an extracted feature from the CNN encoder. The adjacency matrix is an N ? N sparse matrix with N being the number of nodes. To avoid adjacency matrix change the scale of the feature vector, we added an identity matrix to obtain self-loop as:</p><formula xml:id="formula_4">? = A + I<label>(5)</label></formula><p>In our case and regarding the non-linear activation function, for the first layer of GCN, ReLU activation is used to reduce the dependency of the parameters and avoid over-fitting. For the second layer of GCN, Log-Softmax is used to normalize the output of the graph. The first adjacency matrix of the first graph was initialized randomly with the same size of the nodes in the first layer of the depth decoder. In the end, in order to boost and increase the quality of predicted depth maps, a multi-scale GCN based is used. This technique combines the feature information of each scale with the topology graph of depth. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the architecture of our GCN model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-supervised CNN-GCN Autoencoder</head><p>To predict the depth map of a single image, the selfsupervised training depth estimation network of our model, DepthNet, is an auto-encoder based on the well-known architecture of UNet <ref type="bibr" target="#b34">[33]</ref>. The auto-encoder network consists of two successive sub-networks: the first one is an encoder that maps the input into high-level feature representation, and a decoder that maps the feature representation to a reconstruction of the depth. In this paper, we proposed to use a CNNs-based encoder and a GCN-based decoder.</p><p>1) DepthNet Encoder: For the encoder network, the input is an image represented as a grid-like data which is regular and its pixels have the same amount of neighbours. CNNs are capable of exploiting the local connectivity, and global structure of image data by extracting local meaningful features that are shared within the input images used during the training stage. Therefore, in our case, CNNs are suitable for extracting global-based visual features from the whole scene shown in the input image. Our encoder network consists of 5 deep layers, the last four layers are standard Resnet-50 <ref type="bibr" target="#b35">[34]</ref> blocks. The first layer before the ResNet blocks is a fast convolutional layer, Conv1x1, which consists of a convolution + batch normalization + max-pooling operation. <ref type="table" target="#tab_1">Table I</ref> represents the network details of the encoder network. 2) DepthNet Decoder: Regarding the depth decoder and for large-scale depth estimation, we aim at using a geometric DL network that can help in extracting object-based location features as well as keeping the relationships between nodes in the depth maps through generating a depth topological graph in multi-scale. Therefore, we used multi-scale GCN as shown Our approach is to use four levels of GCN in constructing the depth images. The main components of the decoder network are 'upconvolution' layers, consisting of unpooling (up-sampling the feature maps, as opposed to pooling) and a transpose convolution that performs an inverse convolution operation. In order to accurately estimate the depth images, we apply the 'upconvolution' to feature maps and concatenate it with corresponding feature maps from the corresponding layers of the encoder network, and an up-sampled coarser depth prediction using GCN of the previous layer. This approach helps the proposed model to preserve the highlevel information passed from coarser feature maps as well as the fine local information provided in lower layer feature maps. Each step increases the resolution twice. This process is repeated 4 times, providing a predicted depth map, which resolution is half of the input image. This loop cycle is called multi-scale because in each layer of our decoder network, the GCN is updated and up-sampled and is sent to the next layer. The parameters of each layer used in our depth decoder are described in <ref type="table" target="#tab_1">Table II.</ref> 3) PoseNet Estimator: The pose estimation network is a regression network with encoder and decoder parts. The pose encoder receives a concatenated pair of images, I s and I t . Our encoder network consists of 5 deep layers; the first layer is a fast convolutional layer that consists of a 1 ? 1 convolution that is fed by a concatenation of a pair of images, I s and I p , followed by batch normalization and max-pooling. The last four layers are standard ResNet-18 blocks <ref type="bibr" target="#b36">[35]</ref>, which is similar to our depth encoder with fewer hidden layers. The output of the last layer (i.e., ResNet-18-L4) from the pose encoder is a 512 feature map. In turn, our pose decoder contains four convolution layers. The input of the pose decoder is the output of ResNet-18-L4. Besides, the pose decoder has a convolutional weight in the first layer similar to proposed in <ref type="bibr" target="#b4">[5]</ref>. The decoder layer parameters are shown in <ref type="table" target="#tab_1">Table III.   TABLE II:</ref> The network architecture of depth decoder. K is the kernel size, S the stride, Chn the number of output channel, Input corresponds to the input channel of each layer and ? represents upsampling by 2x</p><formula xml:id="formula_5">Layer K S Chn Input Activation iL4 GC4-1 GC4-2 Disp4 3 3 3 3 1 1 1 1 512 1 1 1 L4 Adj4, iL4 Adj4, GC4-1 GC4-2 Leaky-ReLU ReLU Log-SoftMax Sigmoid iL3 Adj3 Disp4 GC3-1 GC3-2 Disp3 1 1 1 1 1 L3 ?Adj4 ?Disp4 Adj3, iL3, Disp4</formula><p>Adj3, GC3-1 GC3-2</p><formula xml:id="formula_6">Leaky-ReLU - - ReLU Log-SoftMax Sigmoid iL2 Adj2 Disp3 GC2-1 GC2-2 Disp2 1 1 1 1 1 L2 ?Adj3 ?Disp3 Adj2, iL2, Disp3</formula><p>Adj2, GC2-1 GC2-2 </p><formula xml:id="formula_7">Leaky-ReLU - - ReLU Log-SoftMax Sigmoid iL1 Adj1 Disp2 GC1-1 GC1-2 Disp1 1 1 1 1 1 L1 ?Adj2 ?Disp2 Adj1, iL1, Disp2 Adj1, GC1-1 GC1-2 Leaky-ReLU - - ReLU Log-SoftMax Sigmoid</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Overall Pipelines</head><p>The proposed method consists of two main networks. The first network called DepthNet was explained in the previous subsection. The source image is an input of the DepthNet and the output is the depth map. The second network is PoseNet, which is a pose estimator to estimate the egomotion of the source and the target images (in our case the consecutive image). The output of PoseNet is the relative pose between the source and target images. These two main networks provide a geometry information to provide point-topoint correspondences of the reconstructed image. The whole architecture of our model is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Geometry Models and Losses</head><p>In monocular video datasets, based on the source frame I s , and the target frame I t , the reconstructed image I rec , can be reconstructed using the resulting depth and the 3D pose. The total loss for the whole network contains three main losses, which penalizes the losses between reconstructed and target images on one side, and the resulting depth and the source image on the other side.</p><p>The first loss function called reconstruction loss L Rec constrains the quality of the learnt features to reconstruct the target image, as proposed in <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_8">L Rec = p |I rec (p) ? I t (p)|<label>(6)</label></formula><p>Regarding achieving a better performance and to cope with occlusions between frames in a monocular video, the reconstruction loss L Rec is combined with the reprojection loss, L P l , which combines the L1-norm and SSIM losses as defined in <ref type="bibr" target="#b21">[20]</ref>.</p><formula xml:id="formula_9">L P l = 0.15 ? p |I rec (p) ? I t (p)| +0.85 ? 1 ? SSIM (I rec , I t ) 2<label>(7)</label></formula><p>In addition, if we consider that the image intensity function obeys the Lambertian shading function, the network should extract gradient-based features corresponding to the object's shapes in the input color image. To handle the depth discontinuity being usually problematic due to occlusion, over smoothing and textured regions, the resulting depth map requires a loss function to preserve edges and boundaries of the objects and degrade the texture effects. Thus, the first and the second derivative of depth images can highlight geometric characteristics of the objects and homogeneous regions in the image <ref type="bibr" target="#b37">[36]</ref>.</p><p>Consequently, to ensure that the learnt features of the input image yields edge-preserving depth maps, a discriminative loss function, L Dis , can be defined to give a large weight to the low texture regions.</p><formula xml:id="formula_10">L Dis = p e ??? 1 Is(p) |? 1 D(p)|<label>(8)</label></formula><p>Where, D represents the predicted depth maps at each pixel p, ? 1 represents the first order derivative at each pixel p and ?, a weight factor, is empirically set by 0.5 in this work that yielded the highest accuracy.</p><p>In addition, the second-order behaviour of the surface in a scene is compatible with the curvature measurements of the depth surface that is relative to the normal at one of its points in the immediate vicinity of this point. Thus, a curvature loss L Cvt can be defined based on the second-order derivative of gradients as proposed in <ref type="bibr" target="#b13">[12]</ref>. L Cvt also keeps geometric characteristics of the objects and gives a low weight for textured regions:</p><formula xml:id="formula_11">L Cvt = p e ??? 2 Is(p) |? 2 D(p)|<label>(9)</label></formula><p>The combination of discriminative and curvature losses is used as a smoothness loss function which can be defined as:</p><formula xml:id="formula_12">L Smooth = ?L Dis + ?L Cvt<label>(10)</label></formula><p>The ? and ? are set to 1e ? 3 via cross validation. The final loss can be used for the optimization process of the whole network and a penalty for a bad depth prediction is defined as:</p><formula xml:id="formula_13">L F inal = L P l + L Rec + L Smooth<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details</head><p>We implemented our method by using the Pytorch framework <ref type="bibr" target="#b38">[37]</ref> and the proposed model was trained for 20 epochs with a batch size of 10 with one GTX 1080TI GPU. The Adam optimizer <ref type="bibr" target="#b39">[38]</ref> has been utilized with an initial learning rate of 0.0001 and reduced by half after 75% of the total iterations. The pre-trained Resnet-18 and ResNet-50 layers are used for the PoseNet and DepthNet encoders, respectively <ref type="bibr" target="#b40">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we demonstrate the evaluating performance of our proposed model. To evaluate our approach, we carry out comprehensive experiments on public benchmark datasets such as KITTI dataset <ref type="bibr" target="#b41">[40]</ref> and Make3D dataset <ref type="bibr" target="#b9">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4: Comparison of disparity results on KITTI dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth Evaluation on the KITTI Dataset</head><p>KITTI dataset is a vision dataset for depth and pose estimation. The dataset contains 200 videos of street scenes in the daylight captured by RGB cameras and the depth maps captured by the Velodyne laser scanner. The synchronized single images from a monocular camera were used and Eigen split <ref type="bibr" target="#b42">[41]</ref> with 39810 images for training, 4424 for validation and 697 images for testing. The image pre-processing method proposed in <ref type="bibr" target="#b25">[24]</ref> has been used for removing static frames. The resolution of the images is 1024 ? 320 pixels.</p><p>Regarding the evaluation, the standard metrics of depth evaluation, such as Absolute and Relative Error (Abs-Rel), Squared Relative Error (Sq-Rel), Root Mean Squared Error (RMSE) and Root Mean Squared Log Error (RMSE-Log), are computed as shown in <ref type="table" target="#tab_1">Table IV</ref>. Besides, we used ?t to calculate the accuracy of the estimated depth with different thresholds. The same original input image size is used for evaluation and depth is capped at 80 meters based on the information of the KITTI dataset. Both input size and output size of images are 1024 ? 320 pixels.</p><p>The median scaling introduced by <ref type="bibr" target="#b13">[12]</ref> is used for predicted depths to match the ground-truth scale. The median scaling subtracts the variable's distribution in the data sample and normalize it by the median deviation. The proposed framework is compared with the state-of-the-art of self-supervision based monocular depth estimation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[12]</ref>, <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b25">[24]</ref>- <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b43">[42]</ref>- <ref type="bibr" target="#b49">[48]</ref>. The performances of our model compared with the stateof-the-art solutions is summarized in <ref type="table" target="#tab_4">Table V</ref>. As shown in <ref type="table" target="#tab_4">Table V</ref>, the GCNDepth method achieved the highest performance in terms of Abs-Rel, Sq-Rel, second and third accuracy of (? 2 ,? 3 ) evaluation metrics. In addition, the proposed method also achieved second best results in RMSE, RMSE-Log and first accuracy of (? 1 ) with a slight difference of 0.003 with RMSE-log, and 0.5% with ? 1 compared to the highest results achieved by <ref type="bibr" target="#b13">[12]</ref>. In general, the model of Featdepth <ref type="bibr" target="#b13">[12]</ref> and our model, GCNDepth, provided comparable results and they outperform the other tested methods.</p><p>Although, the Featdepth model achieved similar results to our model, the GCNDepth model yields a 40% reduction in the number of trainable parameters compared to the Featdepth model. Where the GCNDepth model has trainable parameters of 48, 220, 954, in turn the Featdepth model has 79, 681, 406. Since the Featdepth model has an extra deep feature network for feature representation learning to cope with the geometry problem of self-supervision depth estimation. The comparable results show that the use of GCN in reconstructing the depth images can improve the photometric error that appeared in the self-supervision problem without using the feature network as proposed in <ref type="bibr" target="#b13">[12]</ref>.</p><p>In addition, our model achieved high performance on the KITTI benchmark evaluation in the SILog and iRMSE metrics and achieved comparable results in the Sq-Rel and Abs-Rel metrics compared to other state-of-the-art of self-supervised  Qualitatively, the comparison of predicted depth results of the proposed model can be seen in  <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[12]</ref>. In the second row of <ref type="figure" target="#fig_3">Fig 4,</ref> our method estimates the depth between the consecutive cars and correctly detects the boundaries of the two cars. In the third and fourth row, our method properly preserves the discontinuities of the objects without any distortion like what happens with the two other methods. In the last row of <ref type="figure" target="#fig_3">Fig 4,</ref> our model can be able to detect the human body in a full shape showing the depth of the key points of body parts, such as the head, neck, shoulder, etc. However, the other models proposed in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[12]</ref>, can not be able to detect the head of the human and there are no homogeneous depth values for other body parts. The qualitative results support that GCNDepth can extract precise depth maps and recover the depth of objects with higher precision compared to the baselines <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[12]</ref>. The depth maps generated by GCNDepth maintain the boundaries and details of objects that can be clearly realised. In contrast, depth maps resulting from baselines have crumbled boundaries and the objects can not be recognized. The preserving of the objects discontinuities can help in building more accurate semantic maps and visual-inertial odometry for autonomous vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>To get a better understanding of the performance of the proposed method, in <ref type="table" target="#tab_1">Table VII</ref>, we showed an ablation study by changing different components of our model, GCNDepth. Firstly, we tested different variations of the GCNDepth model as follows:</p><p>? single-scale GCN, (SS), where the single scale was tested on the first layer of depth decoder after the encoder. ? a single scale with different values for the percentage similarity of each node (i.e., vertices) or pixel with their neighbor nodes, (i.e., P). ? multi-scale GCN layers (MS) with different values of P. ? changing the activation function of the GCN layer after the second hidden layer by ReLU or Log-softmax. ? optimizing the dropout of GCN decoder As shown in <ref type="table" target="#tab_1">Table VII</ref>, it is obvious that multi-scale GCN with a P value of 0.7 (70 percent of similarity) achieved higher results compared to the other variations of the proposed model. Regarding the activation functions, the used final model has achieved the highest score is multi-scale GCN with P value of 0.7 and Log-softmax activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth Evaluation on the Make3D Dataset</head><p>Additionally, we tested the performance of the GCNDepth model on the Make3D dataset using our trained model based on the KITTI dataset. In other words, we used the Make3D dataset just for validation and testing. The Make3D dataset contains 400 RGB images for training and 134 images for a test set. The results in <ref type="table" target="#tab_1">Table VIII</ref> show that we outperformed the state-of-the-art of self-supervised methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b27">[26]</ref> evaluated on the Make3D dataset in terms of Sq-Rel, RMSE and RMSE-log metrics of 3.075, 6.757 and 0.107, respectively without fine-tuning the GCNDepth model with the training set of Make3D. In turn, the Monodepth2 model <ref type="bibr" target="#b4">[5]</ref> yielded the best Abs-Rel error among the four self-supervised approaches with a value of 0.322. While GCNDepth provided the second best Abs-Rel error of 0.424. Besides, the GCNDepth model  yielded the second-best results after the supervised-based model proposed in <ref type="bibr" target="#b54">[53]</ref>, which provided the best results with differences of 0.22, 1.235, 1.075 and 0.023 of the four metrics: Abs-Rel, Sq-Rel, RMSE and RMSE-log, respectively. This can be considered promising results compared to the supervisedbased approaches.</p><p>Qualitative results with the Make3D dataset are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. GCNDepth is able to estimate depth values even in low texture regions and with different illumination, changes compared to the two other self-supervision models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[12]</ref>. For instance, in the first row of <ref type="figure" target="#fig_4">Fig. 5</ref>, compared to the two other models, the depth map resulting from our model showed that the column of the light in the input image is more visible and with homogeneous depth values and closer to the camera than the other objects (e.g., trees). In turn, the second row of <ref type="figure" target="#fig_4">Fig. 5</ref> shows that the green view in the image is faded into the background in the depth maps from the baselines, but with our model, the green view in the depth image can be clearly recognized and with boundaries distinguished from the background. In contrast to the other methods in the last row of <ref type="figure" target="#fig_4">Fig 5,</ref> the house can be easily identified in the depth map resulting with GCNDepth. In the graph network, the relationships between nodes are of importance that constitutes the path of information transmission in GCN. Thus, we believe that the features extracted from GCNs maintain the weights of different objects in the scenes and these features help deal with reconstructing depth maps preserving the discontinuities of the objects. It is obvious that this can possibly improve the performance of reconstructing geometric information for more accurate depth map prediction. V. CONCLUSION This paper presents a self-supervised DL model based on a multi-scale graph convolutional network (GCN) for monocular depth estimation. The proposed model consists of two networks: 1) depth estimation and 2) pose estimation. The use of GCN in the decoder of the depth estimation autoencoder can map the depth information from low-dimensional features and it can represent the topological structure of the scene by representing the relations between the scene pixels. Besides, to improve the depth estimation, a combination of different loss functions is used i) absolute mean error between the target image and the reconstruction image, ii) perceptional loss to minimizing the photometric reprojection error, and iii) a combination between discriminative and curvature losses to highlight geometric characteristics of the objects and textured regions in the image. The proposed method achieved a comparable depth estimation from monocular video single image to the existing methods for both KITTI and Make3D datasets. The generated depth maps with GCNDepth clearly depict object edges and boundaries which is useful for semantic map and visual odometry. The ongoing work is to address how to improve the network that is able to predict depth maps for night-time images. In turn, future work aims at developing a complete model for pose, depth and motion estimation from monocular videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An illustration of the proposed GCN module containing two hidden layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of DepthNet network architecture in Fig. 2. The adjacency matrix of the initial graph is built based on the number of nodes of the features generated by the last layer of the encoder network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Schematic illustration of the whole framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 4 .</head><label>4</label><figDesc>The first row of Fig 4 represents a clear depth estimation of far and small objects with our GCNDepth model compared to the two methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of disparity results on Make3D dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, H. A. Rashwan , S. Abdulwahab, J. Cristiano and D. Puig are with IRCV Group, Rovira i Virgili University, Tarragona 43007, Spain (Corresponding E-mail: masoumian.armin@gmail.com).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>The network architecture of depth encoder. K is the kernel size, S the stride, Chn the number of output channel, Input corresponds to the input channel of each layer</figDesc><table><row><cell>Layer</cell><cell>K</cell><cell>S</cell><cell>Chn</cell><cell>Input</cell><cell>Activation</cell></row><row><cell>Conv1x1</cell><cell>1</cell><cell>1</cell><cell>64</cell><cell>Img (1024?320?3)</cell><cell>ReLU</cell></row><row><cell>ResNet-50 L1</cell><cell>3</cell><cell>1</cell><cell>256</cell><cell>Conv1x1 (512?160?64)</cell><cell>ReLU</cell></row><row><cell>ResNet-50 L2</cell><cell>4</cell><cell>1</cell><cell>512</cell><cell>ResNet L1 (256?80?256)</cell><cell>-</cell></row><row><cell>ResNet-50 L3</cell><cell>6</cell><cell>1</cell><cell>1024</cell><cell>ResNet L2 (128?40?512)</cell><cell>-</cell></row><row><cell>ResNet-50 L4</cell><cell>3</cell><cell>1</cell><cell>2048</cell><cell>ResNet L3 (64?20?1024</cell><cell>SoftMax</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>The network architecture of pose decoder. K is the kernel size, S the stride, Chn the number of output channel and Input corresponds to the input channel of each layer.</figDesc><table><row><cell>Layer</cell><cell>K</cell><cell>S</cell><cell>Chn</cell><cell>Input</cell><cell>Activation</cell></row><row><cell>Out1</cell><cell>1</cell><cell>1</cell><cell>256</cell><cell>ResNet-18 L4</cell><cell>ReLU</cell></row><row><cell>Out2</cell><cell>3</cell><cell>1</cell><cell>256</cell><cell>Out1</cell><cell>ReLU</cell></row><row><cell>Out3</cell><cell>3</cell><cell>1</cell><cell>256</cell><cell>Out2</cell><cell>ReLU</cell></row><row><cell>Out4</cell><cell>1</cell><cell>1</cell><cell>6</cell><cell>Out3</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Standard depth evaluation metrics. The pred and gt denotes predicted depth and ground truth, respectively. D represents the set of all predicted depths value for a single image and | . | returns the number of the elements in each input set</figDesc><table><row><cell>Abs-Rel</cell><cell>1 |D|</cell><cell>pred ? D |gt ? pred|/gt</cell></row><row><cell>Sq-Rel</cell><cell>1 |D|</cell><cell>pred ? D ||gt ? pred|| 2 /gt</cell></row><row><cell>RMSE</cell><cell>1 |D|</cell><cell>pred ? D ||gt ? pred|| 2</cell></row><row><cell>RMSE-Log</cell><cell></cell><cell></cell></row></table><note>1 |D| pred ? D ||log(gt) ? log(pred)|| 2 ?t 1 |D| |{pred ? D |max( gt pred , pred gt ) &lt; 1.25 t }|?100%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Comparison of different methods on KITTI dataset. Best results are in bold blue and second best results are in bold red color.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Lower Better</cell><cell></cell><cell cols="3">Higher Better</cell></row><row><cell>Method</cell><cell>Abs-Rel</cell><cell>Sq-Rel</cell><cell>RMSE</cell><cell>RMSE-Log</cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell></row><row><cell>SfMLearner [24]</cell><cell>0.208</cell><cell>1.768</cell><cell>6.958</cell><cell>0.283</cell><cell cols="3">0.678 0.885 0.957</cell></row><row><cell>DNC [42]</cell><cell>0.182</cell><cell>1.481</cell><cell>6.501</cell><cell>0.283</cell><cell cols="3">0.725 0.906 0.963</cell></row><row><cell>Vid2Depth [27]</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220</cell><cell>0.250</cell><cell cols="3">0.762 0.916 0.968</cell></row><row><cell>LEGO [19]</cell><cell>0.162</cell><cell>1.352</cell><cell>6.276</cell><cell>0.252</cell><cell cols="3">0.783 0.921 0.969</cell></row><row><cell>GeoNet [25]</cell><cell>0.155</cell><cell>1.296</cell><cell>5.857</cell><cell>0.233</cell><cell cols="3">0.793 0.931 0.973</cell></row><row><cell>DF-Net [43]</cell><cell>0.150</cell><cell>1.124</cell><cell>5.507</cell><cell>0.223</cell><cell cols="3">0.806 0.933 0.973</cell></row><row><cell>DDVO [26]</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583</cell><cell>0.228</cell><cell cols="3">0.810 0.936 0.974</cell></row><row><cell>EPC++ [44]</cell><cell>0.141</cell><cell>1.029</cell><cell>5.350</cell><cell>0.228</cell><cell cols="3">0.816 0.941 0.976</cell></row><row><cell>Struct2Depth [45]</cell><cell>0.141</cell><cell>1.036</cell><cell>5.291</cell><cell>0.215</cell><cell cols="3">0.816 0.945 0.979</cell></row><row><cell>SIGNet [46]</cell><cell>0.133</cell><cell>0.905</cell><cell>5.181</cell><cell>0.208</cell><cell cols="3">0.825 0.947 0.981</cell></row><row><cell>CC [47]</cell><cell>0.140</cell><cell>1.070</cell><cell>5.326</cell><cell>0.217</cell><cell cols="3">0.826 0.941 0.975</cell></row><row><cell>LearnK [28]</cell><cell>0.128</cell><cell>0.959</cell><cell>5.232</cell><cell>0.212</cell><cell cols="3">0.845 0.947 0.976</cell></row><row><cell>DualNet [48]</cell><cell>0.121</cell><cell>0.837</cell><cell>4.945</cell><cell>0.197</cell><cell cols="2">0.853 0.955</cell><cell>0.982</cell></row><row><cell>SimVODIS [49]</cell><cell>0.123</cell><cell>0.797</cell><cell>4.727</cell><cell>0.193</cell><cell cols="2">0.854 0.960</cell><cell>0.984</cell></row><row><cell>Monodepth2 [5]</cell><cell>0.115</cell><cell>0.882</cell><cell>4.701</cell><cell>0.190</cell><cell>0.879</cell><cell cols="2">0.961 0.982</cell></row><row><cell>FeatDepth [12]</cell><cell>0.104</cell><cell>0.729</cell><cell>4.481</cell><cell>0.179</cell><cell cols="3">0.893 0.965 0.984</cell></row><row><cell>GCNDepth</cell><cell>0.104</cell><cell>0.720</cell><cell>4.494</cell><cell>0.181</cell><cell cols="3">0.888 0.965 0.984</cell></row><row><cell cols="4">methods as shown in Table VI. The results have shown</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">in Table VI supported that the use of GCN in estimating</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">depth maps from a monocular video can yield depth maps</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">outperforming or matching the state of the art on the KITTI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Performance of our model on KITTI public benchmark.</figDesc><table><row><cell>Method</cell><cell>SILog</cell><cell>Sq-Rel</cell><cell>Abs-Rel</cell><cell>iRMSE</cell></row><row><cell>GCNDepth</cell><cell>15.54</cell><cell>4.26</cell><cell>12.75</cell><cell>15.99</cell></row><row><cell>packnSFMHR [50]</cell><cell>15.80</cell><cell>4.75</cell><cell>12.28</cell><cell>17.96</cell></row><row><cell>MultiDepth [51]</cell><cell>16.05</cell><cell>3.89</cell><cell>13.82</cell><cell>18.21</cell></row><row><cell>LSIM [52]</cell><cell>17.92</cell><cell>6.88</cell><cell>14.04</cell><cell>17.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>Ablation results for different components. SS represents the single scale GCN and MS represent the multi scale GCN.</figDesc><table><row><cell>Method</cell><cell>Asb-Rel</cell><cell>Sq-Rel</cell><cell>RMSE</cell><cell>RMSE-Log</cell><cell>?&lt;1.25</cell><cell>?&lt;1.25 2</cell><cell>?&lt;1.25 3</cell></row><row><cell>SS, P=0.1</cell><cell>0.1399</cell><cell>1.0857</cell><cell>5.7320</cell><cell>0.2356</cell><cell>0.7862</cell><cell>0.9221</cell><cell>0.9699</cell></row><row><cell>SS, P=0.3</cell><cell>0.1371</cell><cell>1.0563</cell><cell>5.6353</cell><cell>0.2327</cell><cell>0.7920</cell><cell>0.9252</cell><cell>0.9699</cell></row><row><cell>SS, P=0.5</cell><cell>0.1520</cell><cell>1.1781</cell><cell>6.0753</cell><cell>0.2596</cell><cell>0.7468</cell><cell>0.9104</cell><cell>0.9640</cell></row><row><cell>SS, P=0.7</cell><cell>0.1353</cell><cell>0.9919</cell><cell>5.1480</cell><cell>0.2135</cell><cell>0.8141</cell><cell>0.9394</cell><cell>0.9775</cell></row><row><cell>SS, P=0.9</cell><cell>0.1420</cell><cell>1.1172</cell><cell>5.8659</cell><cell>0.2448</cell><cell>0.7749</cell><cell>0.9184</cell><cell>0.9684</cell></row><row><cell>MS, P=0.1</cell><cell>0.2416</cell><cell>2.3555</cell><cell>7.0855</cell><cell>0.3123</cell><cell>0.6781</cell><cell>0.8539</cell><cell>0.9348</cell></row><row><cell>MS, P=0.3</cell><cell>0.1469</cell><cell>1.1514</cell><cell>5.9808</cell><cell>0.2501</cell><cell>0.7668</cell><cell>0.9142</cell><cell>0.9659</cell></row><row><cell>MS, P=0.5</cell><cell>0.1621</cell><cell>1.3294</cell><cell>6.4627</cell><cell>0.2763</cell><cell>0.7336</cell><cell>0.8860</cell><cell>0.9520</cell></row><row><cell>MS, P=0.7</cell><cell>0.1665</cell><cell>1.4121</cell><cell>6.7864</cell><cell>0.2833</cell><cell>0.7207</cell><cell>0.8809</cell><cell>0.9501</cell></row><row><cell>MS, P=.09</cell><cell>0.1389</cell><cell>1.0520</cell><cell>5.8736</cell><cell>0.2316</cell><cell>0.7745</cell><cell>0.9237</cell><cell>0.9746</cell></row><row><cell>MS, P=0.7, ReLU</cell><cell>0.2308</cell><cell>2.1185</cell><cell>7.6269</cell><cell>0.3285</cell><cell>0.6453</cell><cell>0.8360</cell><cell>0.9276</cell></row><row><cell>MS, P=0.7, Log SoftMax</cell><cell>0.1040</cell><cell>0.7201</cell><cell>4.4942</cell><cell>0.1810</cell><cell>0.8881</cell><cell>0.9651</cell><cell>0.9841</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>Maked3D results. Type D represents depth supervision methods and type M represents self-supervised mono supervision</figDesc><table><row><cell>Method</cell><cell>Type</cell><cell>Abs Rel</cell><cell>Sq Rel</cell><cell>RMSE</cell><cell>log 10</cell></row><row><cell>Karsch [54]</cell><cell>D</cell><cell>0.428</cell><cell>5.079</cell><cell>8.389</cell><cell>0.149</cell></row><row><cell>Liu [55]</cell><cell>D</cell><cell>0.475</cell><cell>6.562</cell><cell>10.05</cell><cell>0.165</cell></row><row><cell>Laina [53]</cell><cell>D</cell><cell>0.204</cell><cell>1.840</cell><cell>5.683</cell><cell>0.084</cell></row><row><cell>Zhou [24]</cell><cell>M</cell><cell>0.383</cell><cell>5.321</cell><cell>10.47</cell><cell>0.478</cell></row><row><cell>DDVO [26]</cell><cell>M</cell><cell>0.387</cell><cell>4.720</cell><cell>8.090</cell><cell>0.204</cell></row><row><cell>Monodepth2 [5]</cell><cell>M</cell><cell>0.322</cell><cell>3.589</cell><cell>7.417</cell><cell>0.201</cell></row><row><cell>GCNDepth</cell><cell>M</cell><cell>0.424</cell><cell>3.075</cell><cell>6.757</cell><cell>0.107</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3 3 3 3 3 1 1 1 1 1 1 256</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3 3 3 3 3 1 1 1 1 1 1128</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3 3 3 3 3 1 1 1 1 1 1 64</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This research has been possible with the support of the Secretariad Universitatsi Recercadel Departamentd Empresai Coneixement de la Generalitat de Catalunya (2020 FISDU 00405). We are thankfully acknowledging the use of the University of Rovira I Virgili (URV) facility in carrying out this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F R</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paix?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F D</forename><surname>Souza</surname></persName>
		</author>
		<title level="m">Self -Driving Cars A Survey -2017</title>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Self-Driving Cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R L</forename><surname>Laboratories</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Medasani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Absolute distance prediction based on deep learning object detection and monocular depth estimation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Masoumian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Marei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdulwahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cristiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rashwan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1406.2283" />
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2366" to="2374" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Digging Into Self-Supervised Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1806.01260" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3827" to="3837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Geometric deep learning : going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>May</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Ordinal Regression Network for Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<ptr target="http://arxiv.org/abs/1806.02446" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ACM</publisher>
			<biblScope unit="page">577</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3D scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="volume">7576</biblScope>
		</imprint>
	</monogr>
	<note>PART 5</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<ptr target="http://cs.nyu.edu/" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="775" to="788" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
	<note>2016-Decem</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature-metric Loss for Self-supervised Learning of Depth and Egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2007.10603" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="page" from="572" to="588" />
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="842" to="857" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised Adversarial Depth Estimation using Cycled Generative Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1807.10915" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -2018 International Conference on 3D Vision</title>
		<meeting>-2018 International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="587" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Reid</surname></persName>
		</author>
		<ptr target="https://github.com/Huangying-Zhan/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-12" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2018-12" />
			<biblScope unit="page" from="1082" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LEGO: Learning Edge with Geometry all at Once by Watching Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-supervised monocular depth estimation for all day images using domain separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2108.07628</idno>
		<ptr target="https://arxiv.org/abs/2108.07628" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single image depth estimation using wavelet decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<idno>abs/2106.02022</idno>
		<ptr target="https://arxiv.org/abs/2106.02022" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Boosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M H</forename><surname>Miangoleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<idno>abs/2105.14021</idno>
		<ptr target="https://arxiv.org/abs/2105.14021" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Depth and Ego-Motion from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.07813" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-01" />
			<biblScope unit="page" from="6612" to="6621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning Depth from Monocular Videos using Direct Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Depth and Ego-Motion from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04998v1[cs.CV]10</idno>
		<title level="m">Depth from Videos in the Wild : Unsupervised Monocular Depth Learning from Unknown Cameras</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dual graph convolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno>abs/1909.06121</idno>
		<ptr target="http://arxiv.org/abs/1909.06121" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Spectral Networks and Deep Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monocular Depth Estimation Based on Multi-Scale Graph Convolution Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="997" to="1009" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/LSVRC/2015/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using Curvilinear Features in Focus for Registering a Single Image to a 3D Object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gurdjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Charvillat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4429" to="4443" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Facebook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Srl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980v9" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings. International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th International Conference on Machine Learning, ICML 2019</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Depth and Flow using Cross-Task Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Separate</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Every Pixel Counts ++: Joint Learning of Geometry and Motion with 3D Holistic Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.06125v2" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2624" to="2641" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Depth Prediction Without the Sensors : Leveraging Structure for Unsupervised Learning from Monocular Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bharadia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Flow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segmentation</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simvodis: Simultaneous visual odometry, object detection, and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">3D Packing for Self-Supervised Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Mar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">MultiDepth : Single-Image Depth Estimation via Multi-Task Regression and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="32" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learn Stereo , Infer Mono : Siamese Networks for Self-Supervised , Monocular , Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deeper Depth Prediction with Fully Convolutional Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.00373" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -2016 4th International Conference on 3D Vision</title>
		<meeting>-2016 4th International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">DepthTransfer: Depth Extraction from Video Using Non-parametric Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">http:/arxiv.org/abs/2001.00987http:/dx.doi.org/10.1109/TPAMI.2014.2316835</idno>
		<ptr target="http://arxiv.org/abs/2001.00987http://dx.doi.org/10.1109/TPAMI.2014.2316835" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Discrete-Continuous Depth Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">He is currently pursuing the Ph.D. degree with the IRCV Group at URV. His current research interests include machine learning</title>
	</analytic>
	<monogr>
		<title level="m">2017 and the M.Sc. degree in mechatronics systems from Kingston University</title>
		<meeting><address><addrLine>Debrecen, Hungary; London, U.K</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Armin Masoumian received the B.Sc. degree in mechatronics engineering from the University of Debrecen</orgName>
		</respStmt>
	</monogr>
	<note>deep learning, computer vision, robotics and mechatronics</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">respectively, and the Ph.D. degree in computer vision from Universitat Rovira i Virgili, Spain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hatem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Irit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cnrs</forename><surname>Inp-Toulouse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">France</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>South Valley University ; South Valley University ; IRCV Group, Department of Computer Engineering and Mathematics, Universitat Rovira i Virgili ; University of Toulouse</orgName>
		</respStmt>
	</monogr>
	<note>he joined the Electrical Engineering Department. he was a Post-Doctoral Researcher with the VORTEX Group. he has been a Beatriu de Pin?s Researcher with URV. His research interests include image processing, computer vision, machine learning, and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">he joined the Intelligent Technologies for Advanced Knowledge Acquisition ITAKA Group, DEIM, URV. His research interests include image processing, computer vision, machine learning, and pattern recognition</title>
	</analytic>
	<monogr>
		<title level="m">2012, and the M.Sc. degree in computer security and artificial intelligence from URV</title>
		<meeting><address><addrLine>Yemen; Tarragona, Spain; Bucaramanga, Colombia; Tarragona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Saddam Abdulwahab received the B.S. degree in computer science from Hodeidah University, Hodeidah ; Department of Computer Science and Engineering, Hodeidah University ; Computer Science from Rovira i Virgili University</orgName>
		</respStmt>
	</monogr>
	<note>, respectively. He is currently senior postdoctoral researcher at the intelligent robotics and computer vision group (IRCV). His research interests include artificial intelligence, robotics, biologically inspired control and evolutionary computation</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">His research interests include image processing, texture analysis, perceptual models for image analysis</title>
	</analytic>
	<monogr>
		<title level="m">Domenec Puig received the M.S. and Ph.D. degrees in computer science from the Polytechnic University of Catalonia</title>
		<meeting><address><addrLine>Barcelona, Spain; Tarragona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-07" />
		</imprint>
		<respStmt>
			<orgName>Universitat Rovira i Virgili ; Intelligent Robotics and Computer Vision Group ; Universitat Rovira i Virgili</orgName>
		</respStmt>
	</monogr>
	<note>where he is currently a Professor. scene analysis, and mobile robotics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
