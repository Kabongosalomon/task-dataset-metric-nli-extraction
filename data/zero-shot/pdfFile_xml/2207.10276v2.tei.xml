<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProMix: Combating Label Noise via Maximizing Clean Sample Utility *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haobo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Chongqing University</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Chongqing University</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Chongqing University</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
							<email>lfeng@cqu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Chongqing University</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
							<email>j.zhao@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Chongqing University</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ProMix: Combating Label Noise via Maximizing Clean Sample Utility *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to train deep neural networks under label noise is appealing, as imperfectly annotated data are relatively cheaper to obtain. State-of-theart approaches are based on semi-supervised learning (SSL), which selects small loss examples as clean and then applies SSL techniques for boosted performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. In this work, we propose a novel noisylabel learning framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high-confidence selection technique that selects those examples having high confidence and matched prediction with its given labels. Combining with the small-loss selection, our method is able to achieve a precision of 99.27 and a recall of 98.22 in detecting clean samples on the CIFAR-10N dataset. Based on such a large set of clean data, ProMix improves the best baseline method by +2.67% on CIFAR-10N and +1.61% on CIFAR-100N datasets. The code and data are available at https://github.com/Justherozen/ProMix. * Winner of the 1st Learning and Mining with Noisy Labels Challenge in IJCAI-ECAI 2022. This is an informal technical report. ? Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The great success of deep neural networks (DNNs) attributes to massive and accurately-labeled data, which is notoriously expensive and time-consuming to obtain. As an alternative, online queries <ref type="bibr" target="#b1">[Blum et al., 2003]</ref> and crowdsourcing <ref type="bibr" target="#b12">[Yan et al., 2014;</ref><ref type="bibr" target="#b14">Yu et al., 2018]</ref> have been widely used to enhance labeling efficiency. However, due to the lack of expertise, such annotation processes inevitably introduce wrong labels to models and result in performance degradation. To this end, noisy-label learning (NLL) , which aims to mitigate the side-effect of label noise, has attracted huge attention from the community.</p><p>A plethora of works has been developed for the NLL problem. Some early approaches <ref type="bibr" target="#b1">[Ghosh et al., 2017;</ref><ref type="bibr" target="#b9">Wang et al., 2019b;</ref><ref type="bibr" target="#b9">Wang et al., 2019b]</ref> design noise-robust loss to correct losses according to network prediction. These methods disregard specific information about label noise and tend to underperform under a high noise ratio. Some other methods <ref type="bibr" target="#b7">[Patrini et al., 2017;</ref><ref type="bibr" target="#b1">Goldberger and Ben-Reuven, 2017;</ref><ref type="bibr" target="#b3">Hendrycks et al., 2018]</ref> utilize the latent noise transition matrix to calibrate the loss function. However, estimating the noise transition matrix typically requires strong statistical assumptions or collecting an extra set of clean samples and is intractable in practice. Although these methods are theoretically grounded, their performance typically lags behind the fully-supervised counterpart.</p><p>Another active line of research <ref type="bibr" target="#b3">Jiang et al., 2018;</ref><ref type="bibr">Shen and Sanghavi, 2019]</ref> in NLL targets to select a set of clean samples from noisy ones. Amongst them, the small-loss selection is a popular criterion as DNNs are known to learn easy patterns before fitting noisy labels <ref type="bibr" target="#b15">[Zhang et al., 2017]</ref>. For instance, Co-teaching  trains two peer networks simultaneously, where each network selects small-loss examples of the other. The most recent semi-supervised learning (SSL) methods <ref type="bibr" target="#b3">[Jiang et al., 2018;</ref><ref type="bibr">Li et al., 2020a]</ref> are also built upon the sample selection methods and achieve state-of-the-art performance in NLL. The pioneering method DivideMix <ref type="bibr">[Li et al., 2020a]</ref> separates the dataset into a labeled set with clean examples and an unlabeled set with noisy ones, and resolves the NLL method in an SSL fashion. Despite the promise, current selection-based methods solely ask for a clean-enough clean set, i.e., ensuring a high precision of the correctly-labeled data. However, there remains a rich set of valuable clean examples being overlooked, properly utilizing which can enlarge the labeled set for SSL algorithms and further boost the performance.</p><p>Motivated by this, we propose a novel NLL approach ProMix that progressively selects clean examples to realize maximized utility. Our selection procedure comprises of two basic mechanisms. The first one is a modified small-loss selection method that performs class-wise selection to enforce a class-balanced prior. Then, we enlarge the clean set by selecting those having high prediction scores on the given labels, which automatically explores new examples to reach maximized utilization of clean samples. On the CIFAR-10N dataset, our selection strategy is able to reach a recall of 98.22 regarding the cleaness, while preserving a high precision of 99.27. After that, we improve FixMatch method for SSL training which corrects noisy samples by high-confidence and agreement of two peer networks. Experiments on CIFAR-10N and CIFAR-100N datasets demonstrate that ProMix outperforms state-of-the-art NLL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Noisy-Label Learning</head><p>Denote the training dataset by D = {(x i , y i )} n i=1 containing n examples, where each tuple comprises of an image x i and a noisy label vector y i ? [0, 1] C . In contrast to the supervised learning setup, the ground-truth labels are not observable. The goal of NLL is to obtain a functional mapping f : X ? [0, 1] C that predicts the true label for unseen testing data. Here f is the softmax output of a deep network. We adopt the cross-entropy loss for classifier training,</p><formula xml:id="formula_0">l CE (f (x i ), y i ) = ? C j=1 y ij log f j (x i )<label>(1)</label></formula><p>where f j is the j-th entry of classifier prediction and y ij is the j-th entry of y i . In this work, we investigate a new selectionbased NLL method under the guidance of the following principles: 1) Selecting a clean sample set as good as possible;</p><p>2) Properly correct the labels of the remaining examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Small Loss Selection</head><p>The deep neural networks are known to have the memorization effect <ref type="bibr" target="#b15">[Zhang et al., 2017]</ref> and tend to learn easy patterns before fitting noisy labels. Thus, existing NLL methods typically adopt the small loss selection criterion to filter out a clean subset for robust model training. Given the training dataset D, we first calculate the cross-entropy loss l i = l CE (f (x i ), y i ) for each example. Next, ? proportion of examples with the smallest loss values l i are identified as clean. After that, one may train the classifier by the clean set merely , or train in a semi-supervised learning (SSL) fashion by regarding the remaining examples as unlabeled data <ref type="bibr">[Li et al., 2020a]</ref>. Despite of its simplicity, the small loss selection strategy has two main drawbacks. 1) Imbalanced Selection. It is known that some labels are typically harder to fit than the remaining and the loss values are typically not comparable directly across different labels. Thus, some labels can dominate the selection procedure and cause the class imbalance issue. It also leads to poorly-calibrated classifier prediction on the basis of the memory effect, which then hinders the subsequent semi-supervised learning procedure. 2) Fixed Budget. It typically selects a fixed budget of clean samples and resort to SSL techniques for complementary learning. A too-small budget leads to low clean sample utility, while a too-large budget enrolls many unexpected noisy examples. Therefore, it is usually annoying to tune the ratio parameter ?.</p><p>3 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Progressive Sample Selection</head><p>Our sample selection algorithm comprises two main mechanisms, which we describe as follows: Class-wise Small-loss Selection (CSS). At each epoch, we first split the whole training data according to model predic-</p><formula xml:id="formula_1">tions D to C sets S j = {(x i , y i )|j = arg max j ?Si f j (x i )}.</formula><p>Given the j-th set, we calculate the cross-entropy loss values l i for each example and select k = min( 1 C ?n , |S j |) examples with smallest l i as clean ones. Compared to the original small loss selection method, we relate k to the average number of examples n/C such that produces a roughly balanced small loss set. Matched High-Confidence Selection (MHCS). In parallel, we introduce another selection strategy that is inspired by pseudo-labeling schemes in semi-supervised learning <ref type="bibr" target="#b8">[Sohn et al., 2020]</ref>. Specifically, we calculate the confidence score e i = C j=1 f j (x i )I(j = y i ) for each example. That is, we select those examples with high confidence e i ? ? , while their predictions should also match the given labels. In practice, we set an high threshold such that the selected samples have a high probability of being correct.</p><p>Samples that satisfy either criterion are selected as clean. Despite of the simplicity, we show our selection algorithm has two appealing properties. First, the balanced clean set gives rise to a well-calibrated classifier, which can promote accurate label guessing on unlabeled data. Second, we admit a dynamically expanded clean set via the MHCS mechanism, which automatically explores new clean data by trusting the high confidential examples. Remark Our MHCS selection strategy is motivated by Fix-Match <ref type="bibr" target="#b8">[Sohn et al., 2020]</ref>. But, in contrast to the SSL setup, our clean set is dynamically generated. With the naive highconfidence selection, the DNNs can assign an arbitrary label with high confidence and select them as clean, which results in a vicious cycle. Therefore, we anchor the high confidence to the given labels. According to the classical learning theory [Shalev-Shwartz and Ben-David, 2014], we have the following theorem. Theorem 1. Let H be the hypothesis set of f . Denote the population risk by R(f ) = E[l CE ] and the corresponding empirical risk byR(f ). We define the true hypothesis by f * = arg min R(f ) and empirical risk minimizer b? f = arg minR(f ). Given a dataset containing n examples, with probability at least 1 ? ?, the following inequality holds:</p><formula xml:id="formula_2">R(f ) ? R(f * ) + 2R(H) + 5 2 log(8/?) n<label>(2)</label></formula><p>whereR(H) is the empirical Rademacher of H.</p><p>Considering a simple case in which f consists of multiple ReLU-activated fully-connected layers, we can apply the techniques outlined in [Allen-  to derive an upper bound of the Rademacher complexity O( 1 n ). If we assume a perfectly clean set with n c samples, the model has larger than a probability of 1 ? O(e ?nc 2 /25 ) to give anwrong prediction on the dirty remaining training data. Therefore, the classifier trained on clean data has a much larger probability of matching the given labels on correctly-labeled data than noisy ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semi-Supervised Training</head><p>Denote the clean set by D c and the set of the remaining (potentially) dirty data by D d . We regard the noisy data as unlabeled ones and perform SSL training on these two sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency Regularization.</head><p>Following previous works <ref type="bibr">[Li et al., 2020a]</ref>, we incorporate consistency regularization for boosted performance, which assumes that a classifier should produce similar prediction on a local neighbor of each data point. Given an image x, we adopt two data augmentation methods SimAugment <ref type="bibr" target="#b4">[Khosla et al., 2020]</ref> and <ref type="bibr">RandAugment [Cubuk et al., 2019]</ref> to obtain a weaklyaugmented image x w and strongly-augmented counterpart x s . Then, we define the the CR loss by l cr = l CE (x s i , y i ) which is the cross-entropy loss on strongly-augmented example and the given labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixup.</head><p>We further incorporate mixup training for improved performance. Given a pair of weakly-augmented examples x w i and x w j in the D c , we create a virtual training example by linearly interpolating both,</p><formula xml:id="formula_3">x m i = ?x w i + (1 ? ?)x w j , y m i = ?y w i + (1 ? ?)y w j<label>(3)</label></formula><p>where ? ? Beta(?, ?) and we simply set ? = 4 without further tuning. Similarly, we define the mixup loss l mix as the crossentropy loss on x m i and y m i . Besides interpolation-based data augmentation, we also employ a masking-based mixed sample data augmentation approach named FMix [Harris et al., 2020] for exploiting local consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guessing Labels on Unlabeled Data.</head><p>We perform label guessing on the dirty set D d . While we still anticipate selecting high confidence instances, there is no free labels to be matched on falsely-labeled data. Following previous works, we train two peer networks to produce potential labels for each other. Formally, we denote the networks by f 1 and f 2 , and select a new subset D c by the following condition,</p><formula xml:id="formula_4">(max j f 1 j (x) ? ? ) ? (max j f 2 j (x) ? ? )? (arg max j f 1 j (x) == arg max j f 2 j (x))<label>(4)</label></formula><p>In other words, the two network should hold high confidence on the same label. However, the two networks are trained in similar environments. Thus, they are likely to agree on false labels and the guessed labels should be carefully treated. In practice, we enable the label guessing at the last K epochs such that the networks are accurate enough. Moreover, we set a maximal ratio of total selected samples by ? (including the clean set) and those examples with smaller mean confidence values</p><formula xml:id="formula_5">maxj f 1 j (x)+maxj f 2 j (x) 2</formula><p>are discarded. After that, we take the shared prediction arg max j f 1 j (x) as the guessed labels for D c . This procedure is similar as FixMatch, which also trains the classifier on a combination of labeled data and those unlabeled data with high confidence.</p><p>Training Loss. The overall loss is given by, l total = l cls + ?(l cr + l mix )</p><p>where ? is a weighting factor. We optimize the loss on all selected examples D c ? D c and take the mean outputs of the two networks as prediction. The whole learning procedure is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.   , JoCoR <ref type="bibr" target="#b11">[Wei et al., 2020]</ref>, Co-Teaching , GCE  and naive cross-entropy (CE) method. For their performance, we directly adopt the reported results from the leaderboard 1 of CIFAR-10N and CIFAR-100N datasets.</p><p>Implementation details. Two simple 18-layer ResNet are applied as the backbone, trained for 420 epochs using a standard SGD optimizer with a momentum of 0.9 and weight decay of 5e ?4 . The batch size is fixed as 256. The initial learning rate is 0.05, which decays by the cosine learning rate schedule. We set the ratio parameters as ? = 0.5 and ? = 0.9. We fix the threshold parameter ? = 0.99 for CIFAR-10N and ? = 0.95 for CIFAR-100N. ? is set as 4 for mixup training. We linearly ramp up ? from 0 to 1 during the first 50 epochs that avoids the consistency regularizer and mixup training overfitting false labels at the beginning. The label guessing is enabled at the last 150 (K) epochs. Following DivideMix, we warm up the two networks by fitting noisy labels with an entropy penalty. The total warm up epochs are set as 10 for CIFAR-10N and 30 for CIFAR-100N. Empirically, we also find that training more epochs can further improve performance. Henceforth, we train a ProMix* </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>ProMix achieves SOTA results. As shown in <ref type="table">Table  1</ref>, our proposed method ProMix achieves the best performance compared with other existing NLL methods under various noise settings. Specifically, on the CIFAR-10N dataset, ProMix exceeds the best baseline by 1.47%, 1.71%, and 2.67% under Aggregate, Rand1, and Worst label annotations. Moreover, ProMix achieves SOTA results on the even more challenging CIFAR-100N dataset and improves the best baseline DivideMix by 1.61% on accuracy. Finally, the ProMix* model further enhances the default ProMix model when trained for a longer time. The results clearly verify the effectiveness of ProMix.</p><p>Detection ability of ProMix.</p><p>Besides, we report the detection ability of ProMix in <ref type="table" target="#tab_1">Table 2</ref>, where ProMix achieves a high F1-score in terms either detecting clean or noisy samples. In particular, when detecting clean samples, ProMix obtains a high recall value while maintaining a good precision score. We conclude that ProMix does indeed attempt to select clean sampels as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clean samples are the real treasures.</head><p>We further test a variant of ProMix (ProMix-Naive) that comprises of a single network. In this design, the clean data dominates the training procedure. On the CIFAR-10N datasets, ProMix-Naive still beats all the baselines by a large margin. These observations indicate that clean samples are the real treasures in NLL, well-utilizing which is the key of successful learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Existing NLL methods can be roughly categorized into two types. The former line of works aims at correcting the loss functions. Since the vanilla cross-entropy (CE) loss has been proved to easily overfit corrupted data <ref type="bibr" target="#b15">[Zhang et al., 2017]</ref>, robust loss has been designed to achieve a small and unbiased risk for unobservable clean data in the presence of noisy labels. <ref type="bibr" target="#b1">[Ghosh et al., 2017]</ref> first analyze that Mean Absolute Error (MAE) is more noise-tolerant than traditional Mean Square Error (MSE) and Categorical Cross Entropy (CCE). Many variants of MAE are devised following that, including Generalized Cross Entropy (GCE) <ref type="bibr" target="#b14">[Zhang and Sabuncu, 2018]</ref>, Symmetric Cross Entropy <ref type="bibr" target="#b9">[Wang et al., 2019b]</ref> and Improved MAE <ref type="bibr" target="#b9">[Wang et al., 2019a]</ref>. <ref type="bibr">DMI [Xu et al., 2019]</ref> takes Shannon's mutual information into account and proposes an information-based loss function. <ref type="bibr" target="#b5">[Liu and Guo, 2020]</ref> design a new family of peer losses to punish overly agreements between classifiers and noisy data. Some methods <ref type="bibr" target="#b7">[Patrini et al., 2017;</ref><ref type="bibr" target="#b9">Vahdat, 2017;</ref><ref type="bibr" target="#b12">Xia et al., 2019;</ref>) consider the noise distribution and depend on the estimated noise transition matrix to explicitly correct the loss function, but the noise transition matrix could be hard to estimate in the presence of heavy noise and a large number of classes. Rather than correcting the loss function, some works <ref type="bibr" target="#b9">[Veit et al., 2017;</ref> concentrate on directly correcting labels while extra noise might be introduced by false corrections.</p><p>The latter strand tries to exploit the memorization effect of DNN and filter out a clean subset from the origin noisy examples based on the small-loss criterion <ref type="bibr">[Li et al., 2020a]</ref> to alleviate the overfitting problem. MentorNet <ref type="bibr" target="#b3">[Jiang et al., 2018]</ref> generates pseudo-labels using a pre-trained mentor network to facilitate the training of the student network. Coteaching  simultaneously trains two networks with different initialization such that the peer network could teach each other by selecting small-loss examples. Co-teaching+ <ref type="bibr" target="#b14">[Yu et al., 2019]</ref> improves Co-teaching by maintaining the disagreement between the two networks. JoCoR <ref type="bibr" target="#b11">[Wei et al., 2020]</ref>   <ref type="bibr">[Li et al., 2020b;</ref><ref type="bibr" target="#b1">Bai et al., 2021]</ref> focus on early learning phenomenon mentioned in <ref type="bibr" target="#b1">[Arpit et al., 2017]</ref>. Recently, <ref type="bibr">MOIT [Ortego et al., 2021]</ref> and Jo-SRC <ref type="bibr" target="#b13">[Yao et al., 2021]</ref> propose a joint contrastive manner to alleviate the negative effect of noisy labels. The main limitation of existing selection-based methods is the low utility of clean samples, which further restricts the later SSL training for higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we explore a new confidence-based selection mechanism for maximizing the utility of clean examples in noisy-label learning. The proposed framework is simple but achieves significant superior performance to the state-of-theart NLL techniques. We believe our work can inspire researchers to promote the NLL methods by well utilizing clean samples, which are the true treasure of our data. In the future, we anticipate more powerful semi-supervised learning techniques to further improve our method, e.g. incorporating the contrastive learning technique  which has been successfully applied in the weakly-supervised learning scenario.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of ProMix. The left is the selection process of one network. The right shows the label guessing process on the noisy set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detection ability of ProMix on CIFAR-10N datasets. Noise detection indicates the wrongly-labeled data are regarded as positive, and clean detection regards correctly-labeled ones as positive.</figDesc><table><row><cell>Baselines. We compare ProMix with multiple NLL meth-</cell></row><row><cell>ods on the Leaderboard. Specifically, they are SOP [Liu et al.,</cell></row><row><cell>2022], CORES* [Cheng et al., 2021], DivideMix [Li et al.,</cell></row><row><cell>2020a], ELR+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>highlights the agreement between networks and combines Co-teaching with Co-regularization technique. DivideMix [Li et al., 2020a] employs two Beta Mixture Model to separate samples and utilizes MixMatch [Berthelot et al., 2019] strategy to leverage unselected examples. What's more, there are some other works</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen-Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Noise-tolerant learning, the parity problem, and the statistical query model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<editor>Miao Xu, Weihua Hu, Ivor W. Tsang, and</editor>
		<meeting><address><addrLine>Bo Han, Quanming Yao, Xingrui Yu, Gang Niu</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1909" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1919" to="1925" />
		</imprint>
	</monogr>
	<note>ICLR. OpenReview.net. Han et al.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<meeting><address><addrLine>Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>ICLR. OpenReview.net, 2020. [Li et al., 2020b] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="1928" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="4313" to="4324" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Peer loss functions: Learning from noisy labels without knowing noise rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="6226" to="6236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed loss minimization</title>
		<idno>abs/2202.14026</idno>
	</analytic>
	<monogr>
		<title level="m">Understanding Machine Learning -From Theory to Algorithms</title>
		<editor>Shalev-Shwartz and Ben-David, 2014] Shai Shalev-Shwartz and Shai Ben-David</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5739" to="5748" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Toward robustness against label noise in training deep discriminative neural networks. In NeurIPS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1903" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pico: Contrastive label disambiguation for partial label learning</title>
		<idno>abs/2201.08984</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13723" to="13732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">L dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="291" to="327" />
		</imprint>
	</monogr>
	<note>Learning from multiple annotators with varying expertise</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Josrc: A contrastive approach for combating noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5192" to="5201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="8792" to="8802" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Clusterability as an alternative to anchor points when learning with noisy labels</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<imprint>
			<date type="published" when="2021" />
			<publisher>PMLR</publisher>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12912" to="12923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Beyond images: Label noise transition matrix estimation for tasks with lower-quality features</title>
		<idno>abs/2202.01273</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
