<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating 6D Pose From Localizing Designated Surface Keypoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
						</author>
						<title level="a" type="main">Estimating 6D Pose From Localizing Designated Surface Keypoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an accurate yet effective solution for 6D pose estimation from an RGB image. The core of our approach is that we first designate a set of surface points on target object model as keypoints and then train a keypoint detector (KPD) to localize them. Finally a PnP algorithm can recover the 6D pose according to the 2D-3D relationship of keypoints. Different from recent stateof-the-art CNN-based approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> that rely on a time-consuming post-processing procedure, our method can achieve competitive accuracy without any refinement after pose prediction. Meanwhile, we obtain a 30% relative improvement in terms of ADD accuracy [9] among methods without using refinement. Moreover, we succeed in handling heavy occlusion by selecting the most confident keypoints to recover the 6D pose. For the sake of reproducibility, we will make our code and models publicly available soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of 6D pose estimation is now grabbing more and more attention because of its significance in robotics, virtual reality and augmented reality. Recently, the CNNbased methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref> are providing terrific results in 6D pose estimation area without relying on rich texture information. Since the depth cameras are vulnerable in the wild or on specular objects and they consume too much power being an active sensor on mobile devices, some recent literature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref> proposes to estimate 6D pose directly from a single RGB image without using depth information. Methods proposed in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> can get convincing results which are competitive with those leveraging RGB-D data. Yet they both rely on an effective but time-consuming refinement procedure in order to achieve accurate pose estimation. <ref type="bibr" target="#b26">[27]</ref> proposes to eliminate the refinement to obtain a huge speed gain. However, they yield worse accuracy re- * contribute equally ? corresponding author sults in terms of ADD metric <ref type="bibr" target="#b8">[9]</ref>. In this paper we argue for a fast and accurate approach. We reckon that recovering 6D pose from estimating surface keypoints is more natural and easier than from predicting the viewpoints because the surface keypoints are more directly and closely associated with features of the target object. Another benefit of predicting a number of surface keypoints is the robustness against occlusion since even if some keypoints are invisible due to partial occlusion, three remaining ones are sufficient to estimating 6D pose by Perspective-n-point (PnP) algorithm <ref type="bibr" target="#b14">[15]</ref>. Therefore we propose to predict 6D pose from localizing surface keypoints.</p><p>We first select k 3D keypoints in the model offline by 3D SIFT algorithm <ref type="bibr" target="#b23">[24]</ref> and generate a dataset with bounding boxes and keypoints annotated. After that, we separately train the object detector YOLOv3 <ref type="bibr" target="#b22">[23]</ref> and a keypoint detector (KPD). As shown in human pose estimation literature, localizing keypoints by predicting heatmaps <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> is better at regressing keypoints coordinates directly <ref type="bibr" target="#b29">[30]</ref>. Therefore in our KPD we adopt a heatmap localization strategy to estimate the locations of keypoints.</p><p>During testing stage, the target object is detected by YOLOv3 and then its 2D keypoints are localized by the KPD. Thanks to the Perspective-n-point (PnP) <ref type="bibr" target="#b14">[15]</ref>, we can estimate the object's 6d pose via the 2D-3D correspondences of keypoints. In order to deal with partial occlusion, we propose to select the most confident keypoints before running PnP algorithm. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates our proposed pipeline.</p><p>We evaluate our architecture on two benchmark datasets: LineMod <ref type="bibr" target="#b8">[9]</ref> and Occlusion <ref type="bibr" target="#b0">[1]</ref>. Concerning accuracy, we not only outperform the state-of-the-art not-usingrefinement method <ref type="bibr" target="#b26">[27]</ref> by a large margin (30% relatively) but also achieve competitive results with using-refinement methods. Meanwhile, we are much faster than usingrefinement techniques due to the elimination of the timeconsuming refinement procedure.</p><p>Our key contribution is that we present a novel method of 6D pose estimation based on localizing designated surface keypoints. We show the great potential of RGB-only methods without refinement by achieving state-of-the-art results. Moreover, we conduct several ablation studies to verify the effectiveness of key components in our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is a large number of literature on 6D pose estimation. We first review them from different perspectives and then have a short glance at human pose estimation work where we get inspiration.</p><p>Classical Feature Mapping Methods Many classical approaches aim to find suitable feature descriptors and predicting pose by feature matching.</p><p>Scale-invariant approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref> use depth information to extract or learn features which are robust to different lighting conditions and even partial occlusions. Other approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28]</ref> attempt to find local keypoints and then vote for the orientations. They either require certain object textural property or are not robust enough for handling occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-based Methods</head><p>Deep learning in 6D pose estimation have demonstrated its great power and potential. <ref type="bibr" target="#b31">[32]</ref> points out that one can recover 6D pose from figures by estimating viewpoints or keypoints. The viewpoint based method <ref type="bibr" target="#b10">[11]</ref> extends SSD <ref type="bibr" target="#b15">[16]</ref> and constructs an end-to-end architecture outputting viewpoints and inplane-rotations. Then it uses the projection relationship to lift the object and further refine estimated poses. <ref type="bibr" target="#b24">[25]</ref> creatively learns implicit orientation by Augmented Autoencoder. The keypoints based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> attempt to predict corner points of the bounding boxes and then use PnP algorithm to estimate 6D pose. <ref type="bibr" target="#b19">[20]</ref> estimates locations of semantic keypoints instead. Other methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref> make an effort to train translation and rotation predictors then combine them to get the 6D pose.</p><p>RGB Only Without Refinement Since it is not always possible to use the depth cameras which consume much power on mobile devices and are easy to fail in the open air, methods only using RGB images to recover 6D pose have been demonstrated to be competitive <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref> with a natural advantage of cheap data requirement. Furthermore, our baseline <ref type="bibr" target="#b26">[27]</ref> eliminates the refinement procedure which is time-consuming and thus not suitable for real-time applications such as virtual reality and robots grasping. Moreover, the Iterative Closest Points (ICP) refinement needs tricky tuning procedures to work well, especially when only RGB images are given. Therefore, we also take an RGB-only method and not use the post-processing procedure.</p><p>Person Pose Estimation Deep learning methods have dominated person pose estimation tasks in recent years, not only in single person pose estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref> but also in multiple person pose estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. <ref type="bibr" target="#b5">[6]</ref> proposes a two-step accurate and fast framework for multiple pose estimation. Given input images, they first use a human detector to draw bounding boxes and then utilize a single person pose estimator (SPPE) to predict keypoints. We are inspired by them and adopt their two-step techniques followed by the PnP algorithm to get the final 6D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We can transform the 6D pose estimation problem into detecting 2D image coordinates of keypoints whose 3D coordinates in model space are previously known by us. Given the 2D coordinate predictions and their associated 3D coordinates in model space, we can recover 6D pose via a Perspective-n-point (PnP) <ref type="bibr" target="#b14">[15]</ref>. Different from <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, our keypoints are designated by us on the model surface via a 3D SIFT <ref type="bibr" target="#b23">[24]</ref> algorithm. We now present our pipeline and describe the critical procedures in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Our Pipeline</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the input to our architecture is an RGB image I. Our goal is to estimate 6D pose P = R t of the target object, where R is the rotation matrix and t is the translation matrix.</p><p>Utilizing the accurate and efficient detector YOLOv3 <ref type="bibr" target="#b22">[23]</ref>, our first stage aims at detecting bounding box of the target object. Then the image I is cropped by this bounding box and then resized to a new image I * with only one proposed object. Then I * is sent to the keypoint detector (KPD).</p><p>In the second stage, the KPD is designed for localizing 2D keypoints which are previously designated in 3D model space. (Keypoint designation will be discussed in Section 3.2.) We use ResNet-101 <ref type="bibr" target="#b7">[8]</ref> as the backbone of Keypoint Detector. The KPD takes I * as input and outputs k heatmaps corresponding to k 2D points. For the p-th keypoint, we consider the pixel with the maximum value in the p-th heatmap as its estimated location. That is,</p><formula xml:id="formula_0">H p x,y = max i (max j H p i,j )<label>(1)</label></formula><p>where (x, y) is the estimated coordinates of the p-th keypoint in I * . We futher denote H p x,y as the confidence of the p-th estimated keypoint. With detected bounding box, we can derive the estimated location of this p-th keypoint in the original image I. In this way, we store 2D coordinates of all keypoints in matrix M k2D .</p><p>In cases where the objects are under occlusion, the keypoints are ranked by their confidence values and only the l most confident keypoints are preserved and utilized in the third stage because they are normally more accurate. Theoretically, l 3 is sufficient to estimate 3D pose while l can be larger than three in practice to boost the predicting precision. Our further experiment (See Section 5.2) shows that this selecting procedure is effective when handling occlusion.</p><p>During the third stage, we use the Perspective-n-point (PnP) <ref type="bibr" target="#b14">[15]</ref> algorithm to recover 6D pose via the relationship between 2D and 3D keypoints. Let M k2D and M k3D be the 2D and 3D coordintes of keypoints in image space and model space respectively, the transformation relationship is:</p><formula xml:id="formula_1">M k2D = K R t M k3D<label>(2)</label></formula><p>where both M k3D and M k2D are represented in homogeneous form, and K is the known camera intrinsic matrix. Given M k3D and M k2D , the PnP algorithm can figure out the rotation matrix R and translation matrix t through the rigid relationship of target object. Mathmatically, the PnP algorithm solve the matrix [R t by simple least square optimaztion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Keypoint Designation and Annotation</head><p>Formally, let M 3D be the point cloud of the object model, keypoint designation means that we select a set of points in model space as the keypoints M k3D . In our proposed pipeline, neither the number nor the type of keypoints is restricted, but it would be better to designate keypoints with strong representative ability. We reckon that surface points are a better choice than the vertices alongside bounding boxes because the surface points are closely related with the model features. Since 3D Scale-invariant feature transform (SIFT) algorithm <ref type="bibr" target="#b23">[24]</ref> can extract local features which are invariant to rotation, scale and robust to different illumination conditions, we adopt SIFT keypoints as our designated keypoints.</p><p>We need to annotate those designated keypoints in images for training keypoint detector (KPD). The designated 3D keypoints are projected to image space applying Equation 2, deriving the ground truth coordinates of k keypoints in image space. During generating synthetic images via rendering and pasting as used in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref>, we can obtain the virtual camera intrinsic matrix K and assigned rotationtranslation matrix R t which are needed by Equation 2. As for the dataset with real pose annotated, K, R and t are given directly.</p><p>Since human pose estimation literature has shown that localizing keypoints by predicting heatmaps <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> is better at regressing keypoints coordinates directly <ref type="bibr" target="#b29">[30]</ref>, we employ the Gaussian function on the 2D coordinates and generate k ground truth heatmaps for the k keypoints. Formally, let x p be the ground truth location of p-th keypoint and its corresponding heatmap is H p . The value at location p ? R 2 in H p is defined as</p><formula xml:id="formula_2">H p (p) = exp ? p ? x p 2 ? 2<label>(3)</label></formula><p>where ? is a parameter controlling the spread of the peak. The ground truth heatmaps are used for training KPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Testing</head><p>Different from the end-to-end training in <ref type="bibr" target="#b26">[27]</ref>, we train the YOLO detector and the KPD separately. Training in two stages can help decouple the problem of object detection and keypoint localization so that we can address them one by one, leading to better overall results.</p><p>Feeding our annotated dataset, we train YOLOv3 using common settings as described in <ref type="bibr" target="#b22">[23]</ref> and KPD by minimizing the MSE loss between the ground truth and predicted heatmaps. Moreover, to deal with the bias in bounding box detection which would draw to an error in keypoint localization, we use the developed Pose-guided Proposal Generator (PGPG) proposed by <ref type="bibr" target="#b5">[6]</ref> to augment the training data. The whole procedure of generating dataset and training KPD is shown in <ref type="figure">Figure 2</ref>.</p><p>During testing, we load the trained weights of both YOLO and KPD to the overall architecture. As described in Section 3.1, our pipeline first detects the bounding box, <ref type="figure">Figure 2</ref>. Generating dataset with keypoints annotations and training KPD to localize them. Upper row: We render the model and paste it onto RGB image. PGPG can augment input data dealing with inaccurate bounding box <ref type="bibr" target="#b5">[6]</ref>. The followed ResNet101 is trained to generate heatmaps corresponding to k designated keypoints. Lower row: We designate keypoints from 3D model and transfer them into RGB images via equation 2. Then k ground truth heatmaps are generated according to the 2D locations of k keypoints via a Gaussian Function. We train the KPD by minimizing the MSE Loss between predicted and ground truth heatmaps. More examples of designated keypoints can be found in supplementary. then localizes the keypoints. Finally, the PnP algorithm will help recover the 6D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Handle Symmetry Cases</head><p>Estimating a keypoint p 0 located on a symmetric object may run into problem for there may be several points appearing the same as p 0 . We need to handle different types of symmetry carefully (See <ref type="figure" target="#fig_1">Figure 3)</ref>.</p><p>Mirror Symmetry A pair of points with mirror symmetry are in fact distinguishable for one of them is on the left and the other is on the right. Our KPD can learn to distinguish the left side from the right natually from end-to-end training and thus the mirror symmetry can be handled without special care which is verified by our experiments.</p><p>?-rotational Symmetry We consider that an object has ?-rotational symmetry if it looks the same after ? angle rotation. For a designated keypoint p on an object with ?rotation symmetry, there are one or more points in symmetry with p and they can be mistaken as p, resulting in divergence in training. To address such ambiguity, the training images should be projected from the 3D space that is within the range [0, ?] in cylindrical coordinates system around the symmetric axis (see lower part of <ref type="figure" target="#fig_1">Figure 3</ref>). On the other side, as used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27]</ref>, we take symmetry into account when evaluating the results. Specifically, our predicted pose of an ?-rotational object may vary due to the rotational symmetry, but the various poses should all be considered as correct as long as they are symmetric with ground truth pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed pipeline in two challenging benchmark datasets: LineMod <ref type="bibr" target="#b8">[9]</ref> and Occlusion <ref type="bibr" target="#b0">[1]</ref>. We first describe two datasets and evaluation metrics in short and give the implementation details. Then both qualitative and quantitative results are presented.  <ref type="table">Table 1</ref>. Accuracy comparison of methods with refinement or without refinement in terms of ADD metric on the LineMod dataset. The overall best numbers are represented in bold and the best numbers in methods without refinement are represented in bold and italic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ape Bvise Cam Can Cat Drill Duck Box Glue Holep Iron Lamp Phone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Datasets</head><p>LineMod <ref type="bibr" target="#b8">[9]</ref> is a standard benchmark dataset for 6D pose estimation. The LineMod dataset contains 18273 test images for 15 objects. The central object in the RGB image is considered as the target object whose bounding box, rotation and translation matrices are annotated. The 3D models with surface color are also provided and thus we can generate synthetic images for training. As others, we will skip the third and seventh objects which lack a meshed model.</p><p>Occlusion <ref type="bibr" target="#b0">[1]</ref> is a dataset for multi-object detection and 6D pose estimation. It is created from LineMod dataset by denoting extra bounding boxes and poses of all the seven kinds of other objects appearing in the Benchvise sequence. As its name implies, most objects in the Occlusion dataset are under partial occlusion, making the multi-object detection and pose recovery quite difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We use two metrics to evaluate the 6D pose estimation results. The 2D reprojection error <ref type="bibr" target="#b1">[2]</ref> is the mean distance between the 2D projection of the object's 3D mesh vertices applying the predicted and the ground truth pose, and the predicted pose is correct if the error is less than 5 pixels. It mainly measures if the estimated pose is visually acceptable, so they are suitable for applications in virtual or augmented reality. The ADD error <ref type="bibr" target="#b8">[9]</ref> is the mean 3D distance between model vertices transformed by the ground truth pose and by the predicted pose. Formally,</p><formula xml:id="formula_3">? ADD = 1 |M | x?M (Rx + t) ? (Rx +t)<label>(4)</label></formula><p>where M is the set of model vertices, R and t are the predicted rotation and translation matrices whileR andt are the ground truth ones. The estimated pose is considered to be correct if the ADD error is smaller than 10% of the object's diameter. The ADD metric is stricter because it requires the predicted pose to be accurate enough to limit the ADD error within around 1 ? 2 cm. We use the terminologies 2D reprojection accuracy and ADD accuracy to rep-resent the percentage of correct poses among all predicted poses using the above two metrics correspondingly. For evaluating the symmetric objects, the 2D reprojection error can naturally accept symmetric poses as long as the objects with pose applied are visualized the same in image space. However, the ADD metric must be changed slightly to deal with symmetry cases. As in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27]</ref>, we change Equation 4 to</p><formula xml:id="formula_4">? ADD = 1 |M | x1?M min x2?M (Rx 1 + t) ? (Rx 2 +t)</formula><p>(5) which is looser than the normal form. We only use this form when the object has rotational symmetry such as the eggbox in LineMod.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>The overall architecture is implemented with PyTroch 0.4.1 and run on an i9-7900X CPU @3.30GHz with an NVIDIA Geforce GTX 1080Ti.</p><p>Generating Training Dataset Besides the training images provided by <ref type="bibr" target="#b26">[27]</ref>, we also render about 30000 synthetic images using OpenGL and annotate keypoints on them for training as aforementioned in Section 3.2. We tune the parameters of SIFT to set the total number of designated keypoints to be 50 for all objects except for the eggbox where we choose 17 unsymmetric keypoints.</p><p>Training Setting For the first stage, we train YOLO using stochastic gradient descent for optimization, with learning rate initially set as 0.001 and divided by 10 at every 100 epochs. We randomly change the hue, saturation and exposure of images by up to a factor of 1.5 while we also randomly scale the image by up to a factor of 20% of the image size. For the second stage, we train the KPD using the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> and the learning rate is set as a constant 0.001. To increase the inference speed, we use multi-process during training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average w/o BB8 <ref type="bibr" target="#b21">[22]</ref> 83.9 Tekin <ref type="bibr" target="#b26">[27]</ref> 90.4 OURS 94.5</p><p>w/Ref.</p><p>Brachmann <ref type="bibr" target="#b1">[2]</ref> 73.7 BB8 <ref type="bibr" target="#b21">[22]</ref> 89.3  <ref type="bibr" target="#b26">[27]</ref> 45 -OURS 25 - <ref type="table">Table 3</ref>. Speed results of state-of-the-art methods. <ref type="bibr" target="#b26">[27]</ref> and OURS don't use the refinement procedure. (We retest the speed of <ref type="bibr" target="#b26">[27]</ref> using their published codes on our server for a fair comparison.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Accuracy and Speed on LineMod Dataset</head><p>2D reprojection accuracy results are shown in table 2. Our method outperforms other state-of-the-art methods. The state-of-the-art not-using-refinement method <ref type="bibr" target="#b26">[27]</ref> has achieved a slightly better results than the using-refinement methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. We take a step forward and show the advantage of not-using-refinement methods in terms of 2D reprojection accuracy.</p><p>ADD accuracy results are shown in table 1. We not only overtake the state-of-the-art not-using-refinement methods by a large margin (30% relatively) but also break the domination of using-refinement methods by achieving competitive results. We can see those using-refinement methods work poorly if the refinement procedure is eliminated. For objects which have rich information in their outline (such as Benchvise, Iron and Lamp) or objects whose surface curvature varies greatly (such as Cat and Camera), our method performs very well. Nevertheless, our method is not that satisfactory when dealing with symmetric objects (such as eggbox) or objects which have little surface information (such as phone and glue) in comparison with <ref type="bibr" target="#b10">[11]</ref>, the stateof-the-art using-refinement method. In general, we beat <ref type="bibr" target="#b10">[11]</ref> at 6 out of 13 sequences, showing the great potential of not-using-refinement methods. Some qualitative results can be found in <ref type="figure">Figure 5</ref>.</p><p>Our speed results are shown in table 3. We can see the post-refinement procedures are time-consuming. Since we can achieve state-of-the-art accuracy even without refinement, we can feel free to eliminate the refinement procedure. <ref type="bibr" target="#b26">[27]</ref> is faster than us because they use a one-shot design. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on the Occlusion Dataset</head><p>For experiments on the LineMod dataset, we use all the predicted keypoints to estimate 6D pose and do not abandon any predicted keypoints. But when on the Occlusion dataset we only keep l = 10 most confident keypoints and abandon other low-confident keypoints. The results are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. The state-of-the-art not-using-refinement method <ref type="bibr" target="#b26">[27]</ref> chooses 183 images in the Occlusion dataset to train and others to test so there are very similar occlusions between the training and testing images. We follow the strict experiment condition with <ref type="bibr" target="#b21">[22]</ref> that we do not use any images in the test sequence. Although the training process is more difficult, we still achieve much better results than <ref type="bibr" target="#b26">[27]</ref>. For a 15px threshold in 2D reprojection accuracy, <ref type="bibr" target="#b26">[27]</ref> can estimate about 50% frames correctly while we can estimate around 65% frames correctly, closer to 80% in state-of-theart using-refinement method <ref type="bibr" target="#b21">[22]</ref>. Some qualitative results are in <ref type="figure">Figure 5</ref>. <ref type="figure">Figure 5</ref>. Qualitative results on LineMod and Occlusion datasets. First two rows: Results on the LineMod dataset, our method can estimate 6D pose correctly in challenging scenes with extreme lighting conditions, heavy clutter and motion blur. Third row: Result on the Occlusion dataset. We can still recover pose correctly when partial occlusion exists. Last row: Failure cases on the Occlusion dataset due to severe blur or overly deficient feature information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis and Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Inferring Invisible Back-face Keypoints</head><p>When estimating surface keypoints from a single RGB image, some keypoints can be invisible because they are on the back of the 3D object. However, this seemingly frustrating problem can be handled naturally by our architecture. Since the investigated objects are rigid with 6 degrees of freedom (DoF), our CNN can learn to infer back-face keypoints from end-to-end training without special treatment. In other words, although only a few keypoints on the front of the 3D object can be seen directly from the RGB image, our trained pipeline can still predict all the designated keypoints including invisible ones at high enough accuracy (See <ref type="figure" target="#fig_3">Figure 6</ref>). This portrait is worth noticing and very beneficial for estimating 6D pose from localizing surface keypoints. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Selecting Most Confident Keypoints</head><p>In theory, the PnP algorithm needs at least three pairs of keypoints to recover an object's 6D pose. We can des- <ref type="figure">Figure 7</ref>. Visualize the selecting procedure. Left: The ground truth keypoints (yellow) and predicted keypoints (blue). The top-10 confident points are plotted with crosses while others are randomly sampled and plotted with dots. We can see that the top-10 confident points are predicted accurately, but those unselected unconfident keypoints are often biased due to occlusion. Right: Ground truth poses (green) and predicted poses (blue). ignate much more than three keypoints and thus being free to select the most confident keypoints to overcome occlusion. As we mentioned in Section 4.5, among 50 keypoints, we select top-10 confident estimated keypoints and sacrifice others. We visualize this procedure in <ref type="figure">Figure 7</ref>. We also conduct a quantitative experiment to show the relation between the pose estimation accuracy and the number of selected most confident keypoints. The results are in <ref type="figure">Figure  8</ref>. We can see that selecting the top-10 confident keypoints can lead to prominently more accurate poses in comparison with preserving all 50 predicted keypoints. Generally selecting fewer most-confident keypoints can lead to better 6D pose estimation because the selected keypoints are relatively more accurate. However, few selected keypoints would do harm to the effectiveness of the PnP algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Surface Keypoints Designation</head><p>We designate the 3D SIFT surface points as keypoints instead of using 8 corners and the center of 3D bounding boxes as <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> do. And we conduct a comparative experiment to validate that SIFT keypoints are better than corners points. We only change M k3D , the set of keypoints to conduct all the comparative experiments based on our pipeline. (Note that in our pipeline design, points in M k3D aren't necessary to be on the model surface.) For the sake of fairness, we restrict the number of keypoints to 9, and we keep all the training settings the same. We train each of them for <ref type="figure">Figure 8</ref>. The relation between 2D reprojection accuracy of predicted poses and number of selected most confident keypoints which are used in PnP algorithm. <ref type="figure">Figure 9</ref>. Comparison between two ways of keypoint designation: SIFT points and the corners plus center of bounding boxes. We test on three objects on LineMod dataset in terms of 2D projection accuracy. enough epochs to ensure they both converge well.</p><p>The overall results are in <ref type="figure">Figure 9</ref>. We can see that choosing surface points as keypoints works much better than corners and the center of the 3D bounding box. This is unsurprising because the surface keypoints are more related to the features of the target results than the bounding box vertices in the air and thus the network can estimate the keypoints more accurately. This comparison can partially explain why our pipeline achieves better results on LineMod than <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a novel and natural method to estimate 6D pose mainly by localizing the designated surface keypoints. Several experiments are conducted to validate the accuracy, speed and robustness of our method. Concerning accuracy, our approach surpasses the state-of-the-art notusing-refinement method by a large margin and is competitive with the state-of-art using-refinement method. Meanwhile, our method is much faster than using-refinement methods. Additionally, our approach can handle symmetry naturally and is robust to occlusion. We are looking forward to future work about better ways of keypoint designation to recover 6D pose more accurately.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of our proposed pipeline. We first (a) detect bounding box then (b) localize the designated keypoints using keypoint detector (KPD). Finally (c) we use a PnP algorithm to recover the 6D pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Dealing with symmetry. Top: Our pipeline can handle two ducks in mirror symmetry without special care. Lower Left: the ?-rotational (? = ?) symmetric object eggbox has pairs of indistinguishable keypoints in 3D space. (One pair in green and the other in yellow.) Lower Right: We can handle the ?-rotational symmetry by restricting training images in half of the 3D space, just as if we only look at the eggbox from the front.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Results on the Occlusion dataset. Top: Percentage of correctly estimated poses as a function of the pixel threshold in 2D reprojection accuracy. We can correctly estimate about 65% frames for a 15px threshold and about 80 % frames for a 30 px threshold on average. Bottom: Two frames with a 15px and 30px error respectively. The green and blue rendered bounding boxes denote the ground truth and predicted pose respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Infering back-face points. Our pipeline can predict keypoints not only on the front (a) but also on the back (b) of the model. The keypoints are all estimated precisely without distinction. (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>81.8 36.6 68.8 41.8 63.5 27.2 69.6 80.0 42.6 75.0 71.1 47.7 56.0 OURS 41.2 85.7 78.9 85.2 73.9 77.0 42.7 78.9 72.5 63.9 94.4 98.1 51.0 72.6</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg</cell></row><row><cell>BB8[22]</cell><cell cols="12">27.9 62.0 40.1 48.1 45.2 58.6 32.8 40.0 27.0 42.4 67.0 39.9 35.2 43.6</cell></row><row><cell cols="13">SSD-6D [11] Tekin [27] Brachmann [2] 33.2 64.8 38.4 62.9 42.7 61.9 30.2 49.9 31.2 52.8 80.0 67.0 38.1 50.2 0 0.2 0.4 1.4 0.5 2.6 0 8.9 0 0.3 8.9 8.2 0.2 2.4 21.6 w/Ref. w/o BB8 [22] 40.4 91.8 55.7 64.1 62.6 74.4 44.3 57.8 41.2 67.2 84.7 76.5 54.0 62.7</cell></row><row><cell>SSD-6D [11]</cell><cell>65</cell><cell>80</cell><cell>78</cell><cell>86</cell><cell>70</cell><cell>73</cell><cell>66 100 100</cell><cell>49</cell><cell>78</cell><cell>73</cell><cell>79</cell><cell>79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of 2D reprojection Accuracy on LineMod.</figDesc><table><row><cell>The pixel threshold is 5.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Speed (fps) Ref. Time (ms)</cell></row><row><cell>Branchmann [2]</cell><cell>2</cell><cell>100</cell></row><row><cell>BB8 [22]</cell><cell>3</cell><cell>21</cell></row><row><cell>SSD-6D [11]</cell><cell>10</cell><cell>24</cell></row><row><cell>Tekin</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recovering 6d object pose and predicting next-bestview in the crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning human pose estimation features with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Epnp: An accurate o(n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Derpanis, and K. Daniilidis. 6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast single shot detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ammirato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Multimedia, MM &apos;07</title>
		<meeting>the 15th ACM International Conference on Multimedia, MM &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Latentclass hough forests for 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
