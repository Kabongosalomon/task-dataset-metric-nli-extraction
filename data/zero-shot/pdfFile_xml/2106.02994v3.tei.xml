<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Topology from Synthetic Data for Unsupervised Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safa</forename><surname>Cicek</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
						</author>
						<title level="a" type="main">Learning Topology from Synthetic Data for Unsupervised Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for inferring dense depth maps from images and sparse depth measurements by leveraging synthetic data to learn the association of sparse point clouds with dense natural shapes, and using the image as evidence to validate the predicted depth map. Our learned prior for natural shapes uses only sparse depth as input, not images, so the method is not affected by the covariate shift when attempting to transfer learned models from synthetic data to real ones. This allows us to use abundant synthetic data with ground truth to learn the most difficult component of the reconstruction process, which is topology estimation, and use the image to refine the prediction based on photometric evidence. Our approach uses fewer parameters than previous methods, yet, achieves the state of the art on both indoor and outdoor benchmark datasets. Code available at: https://github.com/alexklwong/learning-topology-synthetic-data</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Images are "dense" in the sense of providing a color value (irradiance) at every pixel, but they contain only sparse information about the geometry of the scene, both because (i) the pre-image of a pixel is an arbitrarily large subset of the scene with no single depth value, and (ii) large portions of the image do not allow establishing unique correspondence due to occlusions or the aperture problem. We focus on the second problem (ii), aiming to use higherlevel information to impute a depth value at every pixel even when correspondence is not defined (occlusion), or where it yields a continuum of possible depth values (aperture problem). Where the given images do not provide direct evidence on the geometry of the underlying scene, we have to use priors learned from different scenes: The fact that walls tend to be flat, surfaces piecewise smooth, objects mostly convex etc. can be evinced from data about scenes other than the one at hand.</p><p>The key challenge in this process is to determine the topology of the scene; that is, what point is "close" to which in the sense of being part of the same surface or object. Once that is determined, dense depth estimation is just a matter of piecewise smooth interpolation. Errors in the former cannot be compensated by however clever processing in the latter. So, we focus on learning scene topology from images, in This work was supported by ONR N00014-19-1-2229 and ARO W911NF-17-1-0304. <ref type="bibr" target="#b0">1</ref> Alex Wong and Stefano Soatto are with Department of Computer Science, University of California, Los Angeles. Email: alexw@cs.ucla.edu, soatto@cs.ucla.edu <ref type="figure">Fig. 1</ref>. Is it possible learn topology only from sparse points? There exists an abundance of synthetic data (e.g. SceneNet <ref type="bibr" target="#b22">[22]</ref>) with ground-truth depth. We aim to infer the topology of objects from only sparse points (i.e. without images), so that we can leverage synthetic data without the need to adapt for the large domain gap between real and synthetic images. such a way that can be used to complete the depth map where current images do not provide sufficient evidence.</p><p>In some cases, there may be independent mechanisms to associate measurements of light (irradiance at the pixels) to geometric properties of a scene, from tactile perception to controlled illumination or other sensory devices. In a passive setting, absent any side information, it is difficult to obtain ground truth association, so synthetic data is a natural choice but for the covariate shift when transferring models to the real world. We bypass this challenge altogether by learning the association not from photometry to geometry (images to shapes), which requires a high-level understanding of the semantics of objects, but from sparse geometry (point cloud) to topology (connectivity and dense geometry), using abundant synthetic data, without having to face concerns about covariate shift and domain adaptation.</p><p>Recent approaches to depth completion that are amenable to utilize our model include <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b40">[40]</ref>. <ref type="bibr" target="#b40">[40]</ref> utilizes both image and depth from synthetic data and is plagued by the domain gap when transferring to real data. <ref type="bibr" target="#b37">[37]</ref> uses a piecewise planar "scaffolding" of the scene to transfer the supervisory signal from sparse points to their neighbors. However, the piecewise planar assumption is too coarse and initial errors can have catastrophic consequences. We learn the topology of the objects only from sparse points (i.e. no RGB images). Of course, the learned topology is only a "guess," or prior, which needs to be reconciled with the images, but we posit that it is better than generic priors like smoothness or proximity, as it enables drawing from properties of the distribution of natural shapes. Accordingly, we first learn to predict an approximate topology from sparse points with a lightweight network that we call ScaffNet. In a second stage, our fusion network (FusionNet) leverages photometric evidence from real images to correct the prediction.</p><p>More specifically, our contributions include (i) repurposing Spatial Pyramid Pooling (SPP) <ref type="bibr" target="#b13">[14]</ref> to densify the sparse depth measurements while balancing the trade-off between sparsity (pooling with small kernels) and levels of detail (large kernels); (ii) we train an SPP-augmented, light-weight network (ScaffNet) on synthetic data to learn connectivity and dense geometry from sparse inputs and demonstrate that the shapes learned can generalize well to datasets with different scene geometry (i.e. from synthetic scenes of randomly arranged household rooms in SceneNet <ref type="bibr" target="#b22">[22]</ref> to real scenes of laboratories, classrooms and gardens in VOID <ref type="bibr" target="#b37">[37]</ref>); (iii) we propose to learn additive and multiplicative residuals with FusionNet to alleviate the network from the burden of having to re-learn depth; (iv) we treat the topology learned by ScaffNet as a prior and design an adaptive loss function that selectively regularizes the predictions of FusionNet by conditioning on the fitness of the each model to data.</p><p>II. RELATED WORK Supervised depth completion methods learn a direct map from image and sparse depth to dense depth by minimizing the difference to ground-truth. <ref type="bibr" target="#b4">[5]</ref> cast depth completion as compressive sensing by learning a dictionary and <ref type="bibr" target="#b5">[6]</ref> morphological operators. Recent innovations include network operations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref> and architectures <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b40">[40]</ref> for processing sparse depth. <ref type="bibr" target="#b21">[21]</ref> handled sparse depth and images separately and fused them after a single convolution (early fusion), while <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b40">[40]</ref> used two separate encoders (late fusion); <ref type="bibr" target="#b2">[3]</ref> used a 2D-3D fusion network. To propagate sparse depth through the network, <ref type="bibr" target="#b6">[7]</ref> used normalized convolutions with a binary map while <ref type="bibr" target="#b14">[15]</ref> performed joint concatenation and convolution to upsample the sparse depth. Additionally, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b33">[33]</ref> learned confidence maps and <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b41">[41]</ref> exploited surface normals for guidance. Like us, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b42">[42]</ref> proposed light-weight networks that can be deployed onto SLAM/VIO systems.</p><p>Training these methods requires per-pixel ground-truth, often unavailable or prohibitively expensive. We instead learn to infer topology from synthetic data with ground truth and abundant un-annotated real data. Unsupervised depth completion assumes additional (stereo, temporally consecutive frames) data available during training. Both stereo <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b40">[40]</ref> and monocular <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref> paradigms learn dense depth from an image and sparse depth measurements by minimizing the photometric error between the input image and its reconstruction from other views along with the difference between prediction and sparse depth input (sparse depth reconstruction). <ref type="bibr" target="#b21">[21]</ref> used Perspective-n-Point <ref type="bibr" target="#b19">[19]</ref> and RANSAC <ref type="bibr" target="#b8">[9]</ref> to align consecutive frames and <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b36">[36]</ref> proposed an adaptive weighting framework. <ref type="bibr" target="#b40">[40]</ref> also used synthetic data but require image, sparse and ground-truth depth. They do not address the domain gap between the additional dataset and the target dataset; hence, their learned prior may at times hurt performance. Unlike <ref type="bibr" target="#b40">[40]</ref>, we seek to learn topology from sparse points from a synthetic dataset and do not require images, thus bypassing the domain gap. <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b40">[40]</ref> learn end-to-end without supervision, with sparse depth input. However, convolutions are ineffective in processing sparse input because most of the receptive fields are not activated in Overview. Sparse points (z) are first densified by SPP (see <ref type="figure">Fig. 3</ref>), and fed to ScaffNet (trained with synthetic data) to produce an approximate topologyd 0 . FusionNet, trained with an unsupervised loss (Eqn. 3), refine? d 0 with ? and ? by fusing thed 0 with the information from the image It to produce the final predictiond(x) = ?(x)d 0 (x) + ?(x) for x ? ?. Only ScaffNet (red) and FusionNet (green) are required for inference. the early layers. Instead, we leverage spatial pyramid pooling <ref type="bibr" target="#b13">[14]</ref> to increase the receptive field and "densify" the sparse input before feeding it into a topology estimation network.</p><p>[37] used a two-stage approach to first approximate the mesh and later fuse with image information. The "scaffolding" is prone to errors in regions that lack depth input or contain complex structures. Our approach is also two-staged, but we seek to learn topology from synthetic data. To alleviate the fusion network from having to re-learn the approximate geometry, we learn the residual from the image to refine the approximation using a fusion network. Lastly, we also regularize our predictions using the approximated topology conditioned on the fitness of the model to data.</p><p>Domain adaptation for depth prediction <ref type="bibr" target="#b24">[24]</ref> applied ordinary domain adaptation to single-image depth prediction. We do not attempt to reduce the domain gap between synthetic and real images, but leverage the sparse depth to learn dense topology from synthetic shapes. We use RGB images only as evidence to refine the estimates. <ref type="bibr" target="#b0">[1]</ref> leveraged synthetic data for in-painting depth. Our problem is more challenging as the sparse points cover ?5% of the image space <ref type="bibr" target="#b32">[32]</ref> and as few as 0.5% indoors <ref type="bibr" target="#b37">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD FORMULATION</head><p>Our goal is to recover a 3D scene from a real RGB image I t : ? ? R 2 ? R 3 + and the associated set of sparse depth measurements z : ? z ? ? ? R + , without access to ground-truth depth annotations. We follow the unsupervised monocular training paradigm <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b37">[37]</ref> and assume there exists temporally adjacent frames, I ? for ? ? T . = {t ? 1, t + 1} denoting the previous and the next time stamp relative to I t , available during training. Additionally, we assume there also exists a synthetic dataset, containing sparse depth z : ? z ? R + and associated ground-truth dense depth d gt : ? ? R + , available.</p><p>Our approach is separated into two stages: (i) we train a topology estimator f ? (z) with a synthetic dataset D to approximate the scened 0 := f ? (z). By exploiting the statistics of large synthetic datasets (where one can obtain ground-truth depth for free), our topology estimator learns to extract patterns of connectivity between sparse points to model scene structures too complex for hand-crafted priors (e.g. nearest-neighbor). However, as the topology is only informed by the sparse points, one cannot hope to recover regions with very few (or no) points with high precision. This is where the image comes back into the picture; (ii) we refine the initial estimated 0 by incorporating information from the image belonging to the target (real) domain for which we do not have any ground-truth depth. We propose to learn a multiplicative scale ? ? (I t , z,d 0 ) and an additive residual</p><formula xml:id="formula_0">? ? (I t , z,d 0 ) aroundd 0 where [? ? (I t , z,d 0 ), ? ? (I t , z,d 0 )] = f ? (I t , z,d 0 ) 1 .</formula><p>In other words, our network f ? fuses the image information (I t ) with target sparse points (z) and initial estimate (d 0 ) to produce the final dense depthd (see <ref type="figure" target="#fig_0">Fig. 2</ref>). Hence, the name FusionNet. By learning ?(x) and ?(x) aroundd 0 (x) for x ? ?, instead of directly mapping from the initial estimate and image to the final prediction, we alleviate the network from having to re-learn depth from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning Topology using Synthetic Data</head><p>Can we learn to infer the dense topology of the scene given only sparse points? This is an ill-posed problem as there exists infinitely many possible scenes compatible with the missing depth measurements. We propose to learn a topology estimator (ScaffNet) to produce dense depth from sparse depth measurements without the use of an RGB image. To accomplish this, we leverage synthetic datasets with accurate dense depth to capture the patterns of complex geometry, present in everyday objects, that hand-crafted priors (e.g. nearest neighbor, piece-wise smoothness <ref type="bibr" target="#b37">[37]</ref>) cannot.</p><p>For this, we train a small encoder-decoder network f ? (?), comprised of only ?1.4M parameters, by minimizing the normalized L1 difference between the estimated 0 and the ground-truth dense depth d gt for each sample:</p><formula xml:id="formula_1">l 0 = 1 |?| x?? | f ? (z(x)) ? d gt (x) d gt (x) |<label>(1)</label></formula><p>to learn a mapping from sparse points z to dense topolog? d 0 = f ? (z(x)). We note that it is impossible for f ? (?) to recover regions of the scene that lack sparse depth measurements. Hence,d 0 serves as an initial estimate of the scene. Therefore, we propose to refined 0 using information from the image (see Sec. III-B). Spatial Pyramid Pooling (SPP). Since most of the input values for sparse depth measurements are zeroes, the activations of early convolutional layers tend to be zeroes as well. To process the sparse input, we augment our network (f ? (?)) with an SPP module (see <ref type="figure">Fig. 3</ref>) where each layer in SPP is a max-pool of different kernel sizes. For max-pool layers with large kernel sizes, the sparse input is densified, leading to more neurons being activated in the subsequent layers. However, the finer details (e.g. small or thin objects close to the camera) are corrupted. For max-pool layers with small kernel sizes, details of the sparse input are preserved, but as a result, fewer neurons are activated. Therefore, we weight <ref type="figure">Fig. 3</ref>. Spatial Pyramid Pooling (SPP) for depth completion. The SPP module balances details versus density for sparse depth inputs max-pooled at different scales. When pooled with small kernels, the details of the sparse inputs are preserved, but the subsequent layers will produce very few nonzero activations. When pooled with large kernels, inputs are densified, but details are lost. By weighting the output of the pooling with several stacked 1 ? 1 convolutions, the network learns to optimize for this trade-off. the output of the max-pool layers, with different kernelsizes, using several stacked 1 ? 1 convolutional layers such that the network can optimize for this trade-off. We note that SPP also increases the receptive field of the network. The output of SPP is fed into our encoder-decoder network and the weights belonging to the 1 ? 1 convolutions are jointly optimized. We demonstrate the effectiveness of our SPP module in an ablation study in <ref type="table" target="#tab_0">Table IV</ref> of Sec. VI. To validate our claim that the representation obtained without SPP is much sparser than that obtained with SPP, we provide additional discussion, ablation studies, and visualizations of features extracted with and without SPP in Sec. I of Supp. Mat. We also discuss differences between our variant of SPP and those employed in classification <ref type="bibr" target="#b1">[2]</ref> and stereo <ref type="bibr" target="#b13">[14]</ref> (both operating on already dense inputs) in Sec. I-E of Supp. Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning to Refine (Bringing the Image Back)</head><p>ScaffNet, our topology estimator, f ? (?) learns to predict the coarse scene structured 0 from sparse points z where available. However, regions where sparse points are unavailable (due to scan pattern or range of sensor), predictions of f ? (?) can be misleading. This is where we bring the image back. We want to learn a function f ? (I t , z,d 0 ) that produces the scale ?(x) and the residual ?(x) for every element in the image domain, x ? ?. ?(x) and ?(x) refine eachd 0 (x) based on the image to produce the final depth prediction:</p><formula xml:id="formula_2">d(x) = ?(x)d 0 (x) + ?(x)<label>(2)</label></formula><p>where ? is encouraged to be close to 1 and ? to be close to 0 (see Eqn. 9). To learn ?(x) and ?(x), we leverage the geometric relations between the image I t and its temporally adjacent frames I ? , which provide a set of constraints on depth values, up to an unknown scale. These constraints are realized by (i) reconstructing I t with I ? to construct a photometric consistency loss (Eqn. 5). To ground the depth values to metric scale, we (ii) reconstruct the sparse depth measurements z (where available) withd (Eqn. 6). As depth completion is ill-posed, we assume (iii) generic (not informed by data) local smoothness and connectivity (Eqn. 7) on the surfaces populating the scene. Lastly, to leverage the prior learned from synthetic data (side information), we propose to (iv) selectively regularized usingd 0 conditioned on the fitness of model to the data (Eqn. 9). Our unsupervised objective function is the linear combination of four terms:</p><formula xml:id="formula_3">L = w ph l ph + w sz l sz + w sm l sm + w pz l pz<label>(3)</label></formula><p>where l ph denotes photometric consistency, l sz sparse depth consistency, l sm local smoothness and l pz the topology prior. Each term is weighted by their associated w (see Sec. V). Photometric Consistency. We leverage epipolar constraints as a supervisory signal by reconstructing I t from I ? for ? ? T . = {t ? 1, t + 1} to yield:</p><formula xml:id="formula_4">I ? (x,d) = I ? ?g ? t K ?1xd (x) (4) wherex = [x 1] are the homogeneous coordinates of x ? ?, g ? t ? SE(3)</formula><p>is the relative pose of the camera from time t to ? , K denotes the camera intrinsics, and ? refers to the perspective projection. To realize the geometric constraints as a loss, we minimize the average photometric reprojection error measured by a combination of L1 penalty and SSIM <ref type="bibr" target="#b34">[34]</ref>, a perceptual metric, to penalize dissimilarities:</p><formula xml:id="formula_5">l ph = 1 |?| ? ?T x?? w co |? ? (x,d) ? I t (x)|+ w st 1 ? SSIM(? ? (x,d), I t (x))<label>(5)</label></formula><p>w co and w st are weights for each term and will be discussed in Sec. V. Note that we also jointly learn pose g ? t as a by product of minimizing Eqn. 5. Sparse Depth Consistency. Photometric reconstruction recovers the scene structure up to a scale; to ground the scene to metric scale, we minimize the L1 difference between our predictionsd and the sparse depth measurements over the sparse depth domain (? z ):</p><formula xml:id="formula_6">l sz = 1 |? z | x??z |d(x) ? z(x)|.<label>(6)</label></formula><p>Local Smoothness. While sparse depth measurements removes ambiguity, there still exists infinitely many scenes compatible with the image in regions not in the sparse depth domain (?\? z ). Hence, we assume local smoothness and connectivity overd with an L1 penalty on the gradients in the x? (? X ) and y? (? X ) directions. To allow discontinuities along object boundaries, we weight each term using its respective image gradients, ? X = e ?|? X It(x)| and ? Y = e ?|? Y It(x)| , where strong gradients produce lower weight:</p><formula xml:id="formula_7">l sm = 1 |?| x?? ? X (x)|? Xd (x)| + ? Y (x)|? Yd (x)|.<label>(7)</label></formula><p>Topology Prior. Learning depth from motion is essentially a correspondence problem (2D search space over the image domain). Rather than exploring the entire solution or hypothesis space, we look to leverage the initial topologyd 0 predicted by f ? (?) as a prior to limit the scope or to bias our predictions towards the set of hypotheses compatible with what we have learned from a synthetic dataset, D. However, the quality ofd 0 degrades in regions with very few or no sparse depth measurements. Hence, one should </p><formula xml:id="formula_8">x?? |d(x) ? dgt(x)| RMSE 1 |?| x?? |d(x) ? dgt(x)| 2 1/2 iMAE 1 |?| x?? |1/d(x) ? 1/dgt(x)| iRMSE 1 |?| x?? |1/d(x) ? 1/dgt(x)| 2 1/2</formula><p>Evaluation metrics for KITTI and VOID. dgt denotes the ground truth.</p><p>selectively regularize based on the compatibility between the priord 0 and the image I t . To elaborate, we should bias our predictionsd towards the priord 0 only ifd 0 is more compatible with the image thand. Following this intuition, we present a simple, yet effective, per-pixel regularizer conditioned on the fitness ofd 0 andd to the image as measured by the discrepancies between their respective reconstructions (Eqn. 4),? ? (x,d) and? ? (x,d 0 ), and the image I t (x).</p><p>To optimize for this trade-off, we construct W (x), an indicator function that is 1 if the photometric discrepancy</p><formula xml:id="formula_9">? = |I t (x)?? ? (x,d)| is greater than ? 0 = |I t (x)?? ? (x,d 0 )| and 0 otherwise, for every x ? ?, W (x) = 1 if ? &gt; ? 0 0 otherwise.<label>(8)</label></formula><p>We then use W (x) as a pixel-wise mask to selectively impose the topology priord 0 (x) on the predictionsd(x):</p><formula xml:id="formula_10">l tp = 1 x?? W (x) x?? W (x)|d(x) ?d 0 (x)|.<label>(9)</label></formula><p>Note that Eqn. 9 is flexible and encourages ?(x) to be close to 1 and ?(x) to 0 only at locations where the initial topology predictiond 0 is already a good fit for the image. The network is free to modifyd 0 where the topology is incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS</head><p>SceneNet <ref type="bibr" target="#b22">[22]</ref> consists of 5 million RGB image and depth map pairs of size 320?240 rendered from indoor trajectories of randomly arranged rooms. Out of 17 splits provided, we only choose one due to computational restrictions. In a single split, there are 1000 subsequences, each containing 300 images of the same scene, recorded over a trajectory. We constructed sparse points for each ground-truth depth sample by running Harris corner detector <ref type="bibr" target="#b12">[13]</ref> over the RGB images. The resulting points are subsampled using kmeans to produce the final 375 corners which corresponds to 0.49% of all the pixels. We train ScaffNet on SceneNet for indoor target dataset (VOID <ref type="bibr" target="#b37">[37]</ref>). Despite SceneNet (indoor, household rooms) having different scene structures from VOID (indoor and outdoor, laboratories and gardens), ScaffNet trained on SceneNet generalizes to VOID.</p><p>Virtual KITTI (VKITTI) <ref type="bibr" target="#b9">[10]</ref> consists of 35 synthetic videos (5 cloned from the KITTI <ref type="bibr" target="#b32">[32]</ref>, each with 7 variations in weather, lighting or camera angle) for a total of ?17K 1242 ? 375 frames. <ref type="bibr" target="#b9">[10]</ref> use the commercial computer <ref type="bibr">Fig. 4</ref>. KITTI depth completion testing set. Visualizations are taken directly from KITTI online benchmark. Our method better captures smooth surfaces (car, highlighted in orange) and complex objects (trees, highlighted in yellow). We also perform better in recovering buildings (also in yellow). The overall improvement is apparent in the error map. Many red regions (high error) in <ref type="bibr" target="#b37">[37]</ref> are marked blue (low error) in our results. graphics engine Unity to create virtual worlds that similar to scenes in KITTI. However, there is still a large domain gap between RGB images of both domains. To bypass the domain gap in photometric variations, we only use the dense depth maps of VKITTI. To acquire the sparse points, we imitate the sparse depth measurement of KITTI (produced by a lidar) so that the marginal distributions of sparse points are close across domains. We use VKITTI to train the topology estimator for outdoor target dataset (KITTI).</p><p>KITTI. We evaluate our approach on the KITTI depth completion benchmark <ref type="bibr" target="#b32">[32]</ref>. The dataset provides ?80,000 raw image frames and associated sparse depth maps. The sparse depth maps are the raw output from the Velodyne lidar sensor, each with a density of ?5%. The ground-truth depth map is created by accumulating the neighbouring 11 raw lidar scans, with dense depth corresponding to the bottom 30% of the images. We use the official 1,000 samples for validation and test on 1,000 designated samples, which we submit to the KITTI online benchmark for evaluation.</p><p>VOID <ref type="bibr" target="#b37">[37]</ref> provides synchronized RGB image frames and sparse depth maps of ? 640 ? 480 resolution of indoor (laboratories, classrooms) and outdoor (gardens) scenes. ? 1500 sparse depth points (covering ? 0.5% of the image) are the set of features tracked by XIVO [8], a VIO system. The ground-truth depth maps are dense and are acquired by active stereo. The entire dataset contains 56 sequences with challenging motion. Of the 56 sequences, 48 sequences (? 40, 000) are designated for training and 8 for testing. The testing set contains 800 frames. We follow the evaluation protocol of <ref type="bibr" target="#b37">[37]</ref> and cap the depths between 0.2 and 5 meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTATION DETAILS</head><p>To train our system, we: (i) train ScaffNet on synthetic data, (ii) freeze ScaffNet weights, (iii) train FusionNet with frozen ScaffNet on real data. Training ScaffNet on VKITTI <ref type="bibr" target="#b9">[10]</ref> took ?12 hours (30 epochs) while training FusionNet on KITTI <ref type="bibr" target="#b32">[32]</ref> requires ?27 hours (30 epochs) on an Nvidia GTX 1080Ti. Training ScaffNet on SceneNet <ref type="bibr" target="#b22">[22]</ref> requires ?6 hours (10 epochs). FusionNet also requires ?6 hours (10 epochs) for VOID <ref type="bibr" target="#b37">[37]</ref>. End to end inference takes ?32 ms per image. We used Adam <ref type="bibr" target="#b17">[17]</ref> with ? 1 = 0.9 and ? 2 = 0.999 to optimize our network with a base learning rates Results of <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b40">[40]</ref> are directly taken from their papers. Our method (FusionNet) performs the best across all metrics for the KITTI validation set while using fewer parameters than the state of the art (VGG11 <ref type="bibr" target="#b37">[37]</ref>). Our topology estimator (ScaffNet) alone outperforms <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref> on MAE and <ref type="bibr" target="#b21">[21]</ref> on iMAE ( <ref type="bibr" target="#b40">[40]</ref> did not report their iMAE on the validation set). Note that Scaffolding and Our ScaffNet are using sparse-depth only and do not use RGB images. ScaffNet is trained on synthetic data and evaluated on real data. of 1.5 ? 10 ?4 for VKITTI and KITTI and 5 ? 10 ?5 for SceneNet and VOID. We decrease the learning rate by half after 18 epochs for VKITTI and KITTI and 6 epochs for SceneNet and VOID, and again after 24 epochs and 8 epochs, respectively. We train our network with a batch size of 8 using 768 ? 320 crops for VKITTI and KITTI and 640 ? 480 for SceneNet and VOID. To replicate our results on KITTI, we set the weights for each term in our loss function as: w ph = 1.00, w co = 0.20, w st = 0.40, w sz = 0.10, w sm = 0.01 and w tp = 0.10. For VOID, we increased w sz to 1.00 and w sm to 0.40. We only apply l tp after 60K steps (for 271K total steps) for KITTI and 20K steps (for 51K total steps) for VOID to allow pose to stabilize. We perform horizontal shifts as data augmentation on KITTI and VKITTI. We do not use any data augmentation for SceneNet and VOID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. KITTI Depth Completion Benchmark</head><p>We evaluate our method on the unsupervised KITTI depth completion benchmark (outdoor driving scenarios), using metrics defined in <ref type="table" target="#tab_0">Table I. In Table II</ref>, we compare ScaffNet, which uses only sparse depth and trained in synthetic data,  Results are directly taken from the KITTI online benchmark. Key comparisons: (i) <ref type="bibr" target="#b40">[40]</ref> uses both synthetic images and depth maps to train their model and is plagued by the domain gap between real and synthetic images. We bypass the need to adapt to the covariate shift by learning topology from only sparse depth. (ii) <ref type="bibr" target="#b37">[37]</ref> uses hand-crafted scaffolding and learns depth from scratch. Instead, we propose to exploit the distribution of natural shapes from synthetic datasets and learn multiplicative and additive residuals to alleviate the network from needed to re-learn depth. Our approach beats all competing methods across all metrics and achieves the state of the art on the unsupervised depth completion task.</p><p>to recent unsupervised methods <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b40">[40]</ref> that learn dense depth from both sparse depth and RGB images of the target (real) domain. We note that ScaffNet has never been trained on real data, yet, outperforms <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref> by as much as 11.29% and 8.28%, respectively, in the MAE metric. Our findings verify that it is indeed possible to produce a decent topology estimate from only sparse inputs. More importantly, this shows that despite being trained on synthetic data, ScaffNet can bypass the domain gap and generalize to real scenarios. We note that the topology produced by ScaffNet is only an initial approximation. By fusing image information with the approximation, our FusionNet outperforms the top unsupervised depth completion methods on every metric. Our approach also outperforms the state of the art, VGG11 <ref type="bibr" target="#b37">[37]</ref>, across every metric while having a 19.6% parameter reduction on the KITTI depth completion benchmark testing set <ref type="table" target="#tab_0">(Table III)</ref>. We attribute part of our success to ScaffNet, whose output serves as both an initialization as well as a prior for FusionNet. On its own, ScaffNet outperforms scaffolding [37] by as much as 28.22% on the MAE metric (see <ref type="table" target="#tab_0">Table II</ref>). We note key comparisons: in contrast to <ref type="bibr" target="#b37">[37]</ref>, we learn ? (multiplicative scale) and ? (additive residual) around the initial approximation and hence alleviates FusionNet from having to re-learn depth, allowing us to reduce parameters while achieving better performance. Also, unlike <ref type="bibr" target="#b40">[40]</ref>, our prior does not require an image, which is subject to domain gap between real and synthetic data.</p><p>To evaluate the contribution of SPP, we provide ablation study in <ref type="table" target="#tab_0">Table IV</ref>. As seen in row 1 and 2, by augmenting our topology estimator with SPP, we gain a 22.32% error reduction on the MAE metric. This shows that the increase in receptive field and weighted multi-scale densification of the sparse depth inputs are important elements in tackling the sparse depth completion problem. We illustrate the benefits of our SPP module in <ref type="figure" target="#fig_1">Fig. 5</ref>. The model with SPP consistently outperforms the one without and is able to retain more details about the scene with significantly lower errors.</p><p>To understand the architectural choice, we evaluated Fu-  Results of <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b40">[40]</ref> are taken from <ref type="bibr" target="#b37">[37]</ref>. Because there are many textureless regions in indoor scenes, locally, the image does not inform the scene structure. Hence, a prior informed by data is even more important. Our method outperforms all competing methods on the VOID depth completion benchmark to achieve the state of the art. sionNet using different output in rows 3 to 5 and 7 of Table IV. When learning ? (scale) and ? (residual) individually, FusionNet performs worse than directly mapping (learning from scratch) the initial approximationd 0 and the image to dense depth. However, when combined together, ? and ? achieves the state of the art. We note the behavior of direct mapping during early stages of training is learning to copy depth. This is because the network weights are initialized with Gaussian noises and thus the corresponding pixel in the input prediction has the highest weight on the output prediction <ref type="bibr" target="#b20">[20]</ref>. By learning ? and ? aroundd 0 , we relieve the network of this onerous task. In regards to why ? and ? alone cannot achieve the same performance, we believe it is related to their distribution. We observed that, when used separately, both ? and ? have long tail distributions (to refine regions with no sparse depth) and large local disparities (e.g. object boundaries); whereas when used together, the range of their values are much smaller. We hypothesize that the sharp local changes make it harder to learn just ? or ?. Finally, we assess the effect of our topology prior in rows 6 and 7. Row 6 (no l tp ) omits the topology prior from the objective function (Eqn. 3). We see an overall improvement by leveraging what we have learned from synthetic datasets. This also shows that the topology obtained from synthetic data can generalize to real datasets. The proposed method (FusionNet) surpasses all baselines and variants to justify each architectural and loss component choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VOID Depth Completion Benchmark</head><p>We provide quantitative <ref type="table" target="#tab_4">(Table V)</ref> and qualitative ( <ref type="figure" target="#fig_2">Fig. 6</ref>) evaluations of our approach on the indoor and outdoor VOID <ref type="bibr" target="#b37">[37]</ref> depth completion benchmark. Indoor scenes are composed of many textureless surfaces. Locally, they do not give any information about the scene structure; hence, learning a topology prior that is informed by data is even more important. The challenge here is that indoor scenes contain objects with more shape variations (from simple flat surfaces like walls to complex ones like chairs). Moreover, the density of the sparse points (tracked by VIO and SLAM systems) is ?0.5% in contrast to outdoor driving scenarios (lidar), where density is ?5% concentrated on the lower half of the image space. To demonstrate that ScaffNet can generalize even when the scene structures in the synthetic dataset is different from that of the real dataset, we trained our ScaffNet on SceneNet <ref type="bibr" target="#b22">[22]</ref> and evaluated on VOID -SceneNet consists of randomly arranged synthetic indoor household rooms while VOID contains real indoor and outdoor scenes of laboratories, classrooms and gardens. Despite having far fewer points, more complex geometry, and being trained on a synthetic dataset with different scene distribution, ScaffNet is still able to predict reasonable topology -in fact, it beats all of the competing methods that are trained on real data using both image and sparse depth. This is because ScaffNet does not learn the scenes themselves, but the shapes of natural objects populating them, which allows the network to exploit the abundance of synthetic data to learn patterns from sparse points to infer dense topology. Our FusionNet further incorporates the image information into the topology estimate to achieve the state of the art, beating VGG11 <ref type="bibr" target="#b37">[37]</ref> by as much as ?30% on MAE. We also outperform their hybrid model VGG11+SLAM (which uses accurate SLAM pose instead of learning it from scratch) by ?18.6% on MAE. To show the effect of different densities of sparse inputs, we provide an ablation study in the Sec. II of Supp.</p><p>Mat. -we show that while performance does degrade with fewer points, it degrades more gracefully than <ref type="bibr" target="#b37">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>To revisit the question, "is it possible to learn dense topology from sparse points?", we have demonstrated that it is, indeed, not only possible to learn the association of sparse points with dense natural shapes, but also using only synthetic data. While one may surmise that ScaffNet requires similar distributions of 3D scenes between synthetic and real datasets in order to generalize, we show the contrary; ScaffNet learns the shape of objects populating a scene rather than the scene itself as demonstrated in Sec. VI-B. This is especially important in the indoor settings where not only do the scene layouts vary a lot, but also consist of many textureless surfaces (for which one needs a prior on the shapes in the scene). We must note that the topology estimated by ScaffNet is only a "guess" and therefore must be reconciled with the image via FusionNet (e.g. regions with very few or no sparse points, where estimates from ScaffNet are less reliable). Hence, by leveraging the topology as a prior and learning the residual over it, we allow FusionNet the freedom to amend the scene as needed. Also, we did not consider the case where the shapes in synthetic data are not representative of those in the real data. Hence, this is only the first step. We believe our findings demonstrate the benefits of leveraging synthetic data for learning topology from sparse points and motivates further exploration to incorporate the virtually unlimited amount of synthetic data into multi-sensor fusion pipelines for the 3D reconstruction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Summary of content: In Sec. I, we begin by validating the claim made in Sec. III-A of the main text regarding the sparsity in the feature maps produced by early convolutional layers without Spatial Pyramid Pooling. To demonstrate the effect of Spatial Pyramid Pooling, we show, in <ref type="figure">Fig. 7</ref>, a visualization of this phenomenon and how augmenting the network with a Spatial Pyramid Pooling module can produce much denser feature maps. Our choice of kernel sizes for Spatial Pyramid Pooling is detailed in Sec. I-B. In Sec. I-C, we show (i) additional ablation studies on the performance benefits of Spatial Pyramid Pooling, (ii) discuss how Spatial Pyramid Pooling balances the trade-off between level of density in the extracted features and the level of detail that they capture in Sec. I-D, and (iii) how our variant of Spatial Pyramid Pooling is different from previous works in Sec. I-E. In Sec. II, we examine the impact of various levels of input sparse depth density for both ScaffNet and FusionNet <ref type="table" target="#tab_0">(Table IX and</ref>  <ref type="figure">Fig. 9, 8</ref>). In Sec. III, we examine ScaffNet's sensitivity to different distributions of sparse depth sampling strategies.</p><p>In Sec. IV, we shift our focus to examining the full system (ScaffNet with FusionNet) and show the different options for training them (either separately or jointly). We provide quantitative results, in <ref type="table" target="#tab_0">Table XI</ref>, to demonstrate effects of separate and joint training to justify our choice in training procedure and our use of synthetic data. To understand the effect of domain gap on performance, in Sec. V, we evaluate ScaffNet and FusionNet on a dataset where camera setup and scene distribution differs from that of the training set.</p><p>Finally, in Sec. VII, we further show quantitative and qualitative comparisons with other unsupervised methods on the KITTI depth completion benchmark. We also compare our approach to supervised methods and show that our method is closing the gap between supervised and unsupervised learning paradigms. In <ref type="figure" target="#fig_4">Fig. 13</ref>, we show screenshots of the unsupervised approaches with their respective ranks on the KITTI benchmark webpage. In Sec. VIII, we detail the network architectures of our topology estimator (ScaffNet) and depth-RGB image fusion network (FusionNet), both were specifically designed to be light-weight with the intention of being able to be deployed on standard embedded systems for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I ABLATION STUDY ON THE EFFECTS OF SPATIAL PYRAMID POOLING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Regarding its effect on feature maps</head><p>The challenge of sparse depth completion is precisely the sparsity. Because of the numerous "holes" (or zeroes) in the sparse input, the activations of the earlier convolutions layers tend to be zero as well. Therefore, much of the earlier layers are dedicated to "densifying" the feature maps. This is illustrated in the right column of <ref type="figure">Fig. 7</ref>. Our goal is to enable better learning by generating a denser representation. To this end, we proposed augmenting the topology estimator network (ScaffNet, Sec. III-A of main text) with our Spatial Pyramid Pooling module, where multiple max pools of different scales are performed on the sparse input for an increase in receptive field and input densification. In effect, the features generated from Spatial Pyramid Pooling become much denser than those from simple convolutions. This, in turn, allows the activations of the subsequent convolutional layers in the encoder to be dense as well. We illustrate this phenomenon in <ref type="figure">Fig. 7</ref> where we show that the feature maps of ScaffNet without the use of Spatial Pyramid Pooling is still sparse and resembles the input sparse depth; whereas, the feature maps of ScaffNet with Spatial Pyramid Pooling is much denser in comparison. By providing the encoder with dense outputs from Spatial Pyramid Pooling, we, not only, relieve the encoder from the onerous task of propagating the sparse signal spatially, but also provide larger receptive field and context to the subsequent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Regarding the choice of kernel sizes</head><p>For Spatial Pyramid Pooling, we choose the following kernel sizes for max pooling: 5 ? 5, 7 ? 7, 9 ? 9, and 11 ? 11 for KITTI, and 5 ? 5, 7 ? 7, 9 ? 9, 11 ? 11, and 13 ? 13 for VOID. These settings are chosen empirically and we validate our choice of kernel sizes used in ScaffNet in <ref type="table" target="#tab_0">Table VI</ref>, <ref type="figure">Fig. 7</ref>. A comparison of feature maps produced by ScaffNet with and without Spatial Pyramid Pooling (SPP) (best viewed 5? and in color). Hidden layer outputs (feature maps) after 5 ? 5 convolutional layers with and without SPP. The feature maps produced by ScaffNet without SPP are sparse and resembles the input sparse depth, which illustrates our claim in the main paper -neurons are not activated because of the missing sparse depth measurements. In contrast, when using SPP, ScaffNet produces a much denser representation. ScaffNet is trained, using various max pooling kernel sizes, on SceneNet and evaluated on VOID. When using just a 5 by 5 kernel (row 2), performance is similar to not using SPP at all (row 1, none). Including 7 by 7 max pooling increases performance across all metrics. We observe consistent performance gain as we increase kernel size up to 13 by 13 (row 6, the proposed method). Adding more max pooling layers with larger kernel size (e.g. 15 in row 8, 17 in row 9) does not increase performance. Adding small kernel size e.g. (3, in row 7) also does not increase performance.</p><p>where we show quantitative results on various kernel sizes used in Spatial Pyramid Pooling. With small pool sizes e.g. 5 ? 5 (row 2), there is only small performance gain over not using SPP (row 1, none). This is because the feature maps will still be sparse and therefore the network must dedicate its early layers to propagating the signal. As soon as we add a 7 ? 7 max pool (row 2), we observe a performance boost across all metrics; this is consistently the case as we add more max pool layers with larger kernel sizes. The best performing combination is the proposed setting, <ref type="bibr">(5, 7, 9, 11, 13, row 6)</ref>. Increasing the number max pooling layers with larger kernel sizes (15 ? 15, row 8 and 17 ? 17, row 9) does not increase performance; neither does adding max pooling layer with smaller kernel size (3 ? 3, row 7). This is likely because local information captured by a 3 ? 3 can likely also be captured by the combination of 5 ? 5 max pooling and the original input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Regarding its effect on performance</head><p>To understand the benefits of using Spatial Pyramid Pooling, we show an ablation study on its impact on ScaffNet (when augmented with this module) and also its effect on later stages of inference (FusionNet) in both outdoors (KITTI <ref type="table" target="#tab_0">, Table VII</ref>) and indoors (VOID <ref type="table" target="#tab_0">, Table VIII)</ref> scenarios. Overall, the performance benefits are apparent in <ref type="table" target="#tab_0">Table VII</ref> and VIII, where both ScaffNet augmented with Spatial Pyramid Pooling and its associated FusionNet (marked with w/ SPP) outperform their variants without the module (marked with w/o SPP) by large margins across all metrics. Because ScaffNet consistently performs worse without Spatial Pyramid Pooling, FusionNet is given a lower quality initial topology; therefore, FusionNet like-wise performs worse -justifying the use of Spatial Pyramid Pooling in ScaffNet. This ablation study also shows that errors are propagated downstream. However, while errors do get introduced to FusionNet when using a ScaffNet without Spatial Pyramid Pooling, this is not a point of failure as FusionNet (w/o SPP) is still able to amend the mistakes and improve the reconstruction. We also note that our ScaffNet without Spatial Pyramid Pooling, in fact, still outperforms <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref> in <ref type="table" target="#tab_0">Table VIII</ref>. We note it is possible to use ScaffNet to obtain dense topology from sparse depth to benefit existing supervised and unsupervised depth completion methods including, but not limited to <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Regarding the trade-off between detail and density</head><p>We note that for the purpose of densification, one may just use a single max pool layer with a large kernel size. However, such a max pool layer would decimate the details of the sparse input. One may also choose to leverage heuristics such as nearest neighbor and local smoothness <ref type="bibr" target="#b37">[37]</ref> to interpolate depth between sparse points. However, when the nearest neighboring points are far away (both in 3D world or 2D image space), the plane interpolated between the points may contain incorrect values as the points may violate the local connectivity assumption. Hence, we use multiple kernel sizes for our max pool layers to capture local fine details (with small kernel sizes) and global denser structures (with large kernel sizes). To determine the trade-off between details and density, we leverage synthetic data to train three 1 ? 1 convolutional layers to weight the output of the max pool layers. For network structure details, please see Sec. VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Differences from previous use cases</head><p>While variants of Spatial Pyramid Pooling have been employed in other problems such as classification and stereo Ablation study on the effect of Spatial Pyramid Pooling on VOID depth completion benchmark using ?1500 points (?0.5% density). Results of <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b37">[37]</ref> are taken from <ref type="bibr" target="#b37">[37]</ref>. Results of w/o SPP consistently performs worse than our model with SPP (marked with w/ SPP). We note that while errors do propagate from ScaffNet to FusionNet, FusionNet is able to amend them as see in the entry "Our FusionNet w/o SPP". We also note that our ScaffNet w/o SPP still outperforms <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref>.</p><p>matching, our use of Spatial Pyramid Pooling is unique. <ref type="bibr" target="#b13">[14]</ref> introduced Spatial Pyramid Pooling to ensure the same size feature maps are maintained when different size of inputs are fed through the network. Unlike us, <ref type="bibr" target="#b13">[14]</ref> does not reweight the features and directly feed max pooled results to fully connected layers. <ref type="bibr" target="#b1">[2]</ref> used Spatial Pyramid (Average) Pooling with large kernels to create "region-level" features for increasing receptive field. In contrast to <ref type="bibr" target="#b1">[2]</ref>, we use max pooling to avoid loss of local detail from averaging large regions. Also unlike our use case in the depth completion problem, in classification and stereo, the inputs are dense; whereas, ours are sparse. Hence, we leverage the assumption that surfaces exhibit local smoothness and connectivity and perform max pooling with large kernels over local regions to get coarse local representations and max pooling with small kernels to retain detail. To balance the trade-off between detail and density, we re-weight the pooled features with 1 ? 1 convolutions. This design not only allows us to obtain a denser representation, but also to obtain larger receptive field (with similar motivation as <ref type="bibr" target="#b1">[2]</ref>) -differentiating our variant of Spatial Pyramid Pooling from previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II ABLATION STUDIES ON SPARSE INPUTS WITH VARIOUS DENSITY LEVELS</head><p>To understand the effect of sparse inputs of various density levels, we also show an quantitative study in <ref type="table" target="#tab_0">Table IX</ref> and <ref type="figure">Fig. 9, 8</ref>). <ref type="figure">Fig. 9</ref> compares the MAE metric between ScaffNet, FusionNet and VGG11 <ref type="bibr" target="#b37">[37]</ref> across three density levels: ?0.5%, ?0.15%, and ?0.05%, which corresponds to ?1500, ?500, and ?150 points, respectively. Each model is trained on ?0.5% and tested on each density level. While ScaffNet beats VGG11 at the highest density (?0.5%), with fewer points, ScaffNet performance degrades at a much faster rate than FusionNet and VGG11 <ref type="bibr" target="#b37">[37]</ref>. This is because ScaffNet only takes sparse points as input (i.e without RGB <ref type="figure">Fig. 8</ref>. Qualitative comparison of ScaffNet predictions across 0.5%, 0.15%, and 0.05% densities on VOID. Orange bounding boxes show the magnified sparse depth. Because ScaffNet predictions are only informed by sparse points, performance degrades as density decreases. The shape of the printer (highlighted in yellow), loses its structure when density decreases from 0.5% to 0.15% and 0.05%. At 0.05%, ScaffNet starts to produce some artifacts ("holes", highlighted in red) because there are large missing regions. <ref type="figure">Fig. 9</ref>. Plot of MAE at different densities on the VOID depth completion benchmark. Results of <ref type="bibr" target="#b37">[37]</ref> are taken directly from their paper. The three density levels examined are ?0.5%, ?0.15%, and ?0.05%, which corresponds to ?1500, ?500, and ?150 points, respectively. We show the trends of the MAE metric for ScaffNet, FusionNet and VGG11 <ref type="bibr" target="#b37">[37]</ref>. While ScaffNet beats VGG11 at the highest density (?0.5%), with fewer points, ScaffNet performance degrades more quickly than FusionNet and VGG11 <ref type="bibr" target="#b37">[37]</ref>. This is because ScaffNet only takes sparse points as input (i.e without RGB image) and therefore cannot reliably infer the scene if there are very few or no sparse points. The improvement from ScaffNet to FusionNet is the effect of the errors amended by FusionNet. image) and therefore cannot reliably estimate the topology when there are very few or no sparse points. Even though ScaffNet has comparable performance as many previous methods that use both sparse depth and image as input, this is precisely why FusionNet is critical in the success of our method -by performing cross-modal validation using the image and predicted topology to amend the incorrect predictions. <ref type="table" target="#tab_0">Table IX</ref> shows that FusionNet is the best performing method across all metrics at every density level. As expected, Results of <ref type="bibr" target="#b37">[37]</ref> are taken from their papers. We examined two other density levels, ?0.15%, and ?0.05%, in addition to ?0.5% density shown in the official benchmark (see <ref type="table" target="#tab_0">Table VIII</ref>), which corresponds to ?500, and ?150 points, respectively. The performance of ScaffNet and FusionNet decreased (as expected) proportional to the density, but ScaffNet decreases at a faster rate with fewer points, especially at 0.05% density. This is because ScaffNet needs to infer topology from only ?150 points without the help of an image. However, with an input of ?0.15% density, our approach still produces reasonable results, with numbers similar to that of <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b37">[37]</ref> using ?0.5% input density (see <ref type="table" target="#tab_0">Table VIII</ref>).</p><p>performance of both ScaffNet and FusionNet degrade with lower density; however, we note that at ?0.15% density, both ScaffNet and FusionNet are still comparable with the performance of methods at ?0.5% density (see <ref type="table" target="#tab_0">Table VIII</ref>). This shows the effectiveness of our method even at low density levels, which is a common scenario for indoor scenes (e.g. features of SLAM/VIO systems where points tracked must be visually discriminative and hence generally comprise of a small set). While our method does degrade with density (as do all known depth completion methods), we degrade more gracefully than <ref type="bibr" target="#b37">[37]</ref>. We also note that the amount of ScaffNet performance degradation with respect to density is similar for all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX III SENSITIVITY STUDIES ON SCAFFNET FOR SPARSE INPUTS WITH SAMPLING STRATEGIES</head><p>There exists different sparse point sampling strategies (e.g. horizontal scanning from lidar in KITTI, corner detection in VOID, or even random uniform sampling). In this section, we study ScaffNet's sensitivity to different sparse depth distributions and whether it is necessary to match the distribution to achieve good performance.</p><p>ScaffNet is not too sensitive to the mismatch of sparse depth distribution between synthetic and real data. We demonstrate this in <ref type="table" target="#tab_9">Table X</ref> where we train ScaffNet on SceneNet with sparse depth points uniformly randomly sampled (row 1) and evaluated it on VOID where sparse points are from VIO using corner-base feature detector. Even though they are trained on different distributions, ScaffNet can still generalized to VOID. As expected, ScaffNet trained on SceneNet with points chosen based on a corner detector (row 3) can improve performance on MAE and RMSE and is comparable on iMAE and iRMSE. FusionNet trained using a frozen ScaffNet pretrained with uniform sampling (row 2) have little performance difference compared to FusionNet trained using a frozen ScaffNet pretrained with sparse points constructed using a corner detector (row 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX IV DIFFERENT PROCEDURES FOR TRAINING SCAFFNET AND FUSIONNET</head><p>There are several ways to train the full system (ScaffNet and FusionNet) either by training them separately or jointly. <ref type="figure">Fig. 10</ref>. Finetuning pretrained ScaffNet and FusionNet on real data. While we can get some performance gain if we were to jointly finetune ScaffNet (trained on synthetic data) and FusionNet (trained on real data with frozen ScaffNet) on real data, performance degrades after 2 epochs. This is because the image reconstruction term l ph is susceptible to transparent, specular surfaces. Hence, the gradients computed for these terms, backpropagated through FusionNet to ScaffNet, will destroy the topology information learned from synthetic data in ScaffNet. can be some slight improvements to performance. However, finetuning ScaffNet jointly with FusionNet can also cause harm to the performance due to the image reconstruction term l ph (Eqn. 5, main text), which is susceptible to transparent, specular surfaces. Hence, the gradients computed for these terms, backpropagated through FusionNet to ScaffNet, will destroy the topology information learned from synthetic data in ScaffNet. We show this phenonmenon in <ref type="figure">Fig. 10</ref>. We further consider the the case of training ScaffNet and FusionNet end-to-end on real data to quantify the effect of using synthetic data on the full model. Quantitative results are shown in row 1 of <ref type="table" target="#tab_0">Table XI</ref>. Training ScaffNet and FusionNet jointly from scratch (without synthetic data), performs worse than training ScaffNet on synthetic data first to learn sparse geometry to dense topology (rows 2, <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4)</ref>. This is because the model cannot leverage side information learned from the ground-truth annotations (that come for free) in synthetic data. The results in <ref type="table" target="#tab_0">Table XI</ref> validates the proposed approach of using synthetic data to learn topology and demonstrates the benefits for using synthetic data in multi-modal sensor fusion for 3D reconstruction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX V GENERALIZATION OF SCAFFNET AND FUSIONNET TO DIFFERENCE SCENES</head><p>In this section, we consider the generalization capabilities of ScaffNet and FusionNet for different scene distributions captured by different camera and depth sensor set ups. To examine such, we consider VOID <ref type="bibr" target="#b37">[37]</ref> and NYUv2 <ref type="bibr" target="#b31">[31]</ref> for the depth completion task. VOID is comprised of scenes found in an university campus, including, but not limited to classrooms, laboratories, copy rooms, staircases, gardens, and courtyards; whereas, NYUv2 consists of household and commercial areas such as bedrooms, living rooms, dental offices, and stores. VOID is captured using an Intel RealSense D435i and NYUv2 is captured with a Microsoft Kinect. In addition to the differences in the scene distributions, and camera and depth sensor set ups, NYUv2 images also have white borders, and VOID does not. Hence, there exists a domain gap (mainly photometric) between the two datasets. The goal of this section is to understand the effects of this domain gap on performance, and whether it is possible to adapt (not just to the scenes, but also to new equipment) and recover performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX VI COMPARISONS WITH MONOCULAR DEPTH PREDICTION</head><p>Here, We compare quantitatively <ref type="table" target="#tab_0">(Table XIII)</ref> and qualitatively ( <ref type="figure" target="#fig_0">Fig. 12)</ref> to the state-of-the-art monocular (single image) depth prediction models, PackNet <ref type="bibr" target="#b11">[12]</ref> and Monodepth2 <ref type="bibr" target="#b10">[11]</ref>, on the KITTI validation set. For this comparison, we use the PackNet and Monodepth2 models, pretrained on KITTI, provided by the authors.</p><p>We note that there are some differences in evaluation protocols between those typically used in monocular depth prediction literature and the ones that we employ. For instance, the metrics used here are in millimeters; whereas, those used in monocular depth prediction literature are in meters. Also, because there is a scale ambiguity in the videobased monocular depth models, it is common to perform scale matching between the predictions and ground truth. In <ref type="table" target="#tab_0">Table XIII</ref>, we do not perform scale matching because (i) Monodepth2 is trained using stereo pairs, where scale can be directly learned, and (ii) while PackNet is trained on video sequences, it also uses inertials where scale is observable. Thus, both models, like ours, should output predictions in metric scale and so we forgo the use of scale matching for fair comparison.</p><p>In <ref type="table" target="#tab_0">Table XIII</ref>, we evaluate Monodepth2 and PackNet on the KITTI depth completion validation set using standard protocol described in the main paper. While scale can be We compare the proposed depth completion methods Scaffnet and Fu-sionNet with state-of-the-art monocular (single image only) depth networks on the KITTI validation set. Since the scale is off for Mon-odepth2 <ref type="bibr" target="#b10">[11]</ref>, it performs poorly and MAE is ? 15K. PackNet <ref type="bibr" target="#b11">[12]</ref> performs relatively well, but still 6 times worse than Scaffnet (only using sparse depth e.g. lidar measurements) in MAE. The best performing method is FusionNet, which uses both image and sparse depth.</p><p>learned through stereo training, Monodepth2 performs poorly because the scale of its predictions is off. PackNet uses velocity to learn a prior on scale so predictions are closer to metric scale, but performance is still 6 times worse than ScaffNet in MAE. This is surprising because ScaffNet does not conditioned on the "dense" image at all, but only the sparse point cloud. The best performing model is FusionNet, which is conditioned on both image and sparse depth. <ref type="figure" target="#fig_0">Fig. 12</ref> shows a qualitative head-to-head comparison between PackNet and our method. As we can see, PackNet does recover the general shape of the scene, but structures are often over-smoothed (see bush in the center of the image, and vegetation and railing located in right side of the image). Our method produces depth maps with higher detail that capture the leaves and branches of the plants and the thin structure of the railings. The error maps (row 3) shows that our method performs better overall.  Quantitative results on the supervised KITTI depth completion benchmark. All results are taken from the online benchmark <ref type="bibr" target="#b32">[32]</ref>. Methods are ordered based on all metrics rather than just RMSE (ordering of benchmark). We note <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref> compete in both unsupervised and supervised benchmarks. We compare our unsupervised method (italized, row 3 in the table) against supervised methods on the KITTI depth completion benchmark. We also note that while most supervised methods still do better, our approach surpasses some supervised methods: <ref type="bibr" target="#b4">[5]</ref> across all metrics, <ref type="bibr" target="#b5">[6]</ref> on MAE, iMAE, and iRMSE metrics and <ref type="bibr" target="#b21">[21]</ref> on iMAE. This demonstrates the potential of our method in closing the gap between supervised and unsupervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX VII KITTI DEPTH COMPLETION BENCHMARK</head><p>We showed quantitative comparisons against state-of-theart unsupervised methods in <ref type="table" target="#tab_0">Table II</ref> and III of the main text. Here, we compare our approach against the both top performing supervised <ref type="table" target="#tab_0">(Table XIV)</ref> and unsupervised (Table XV) methods on the KITTI depth completion benchmark <ref type="bibr" target="#b32">[32]</ref>. Our method is the state of the art on the unsupervised depth completion task, outperforming all competing methods across all metrics. We note <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref> compete in both unsupervised and supervised benchmarks. A key comparison is the unsupervised approach of <ref type="bibr" target="#b40">[40]</ref>, who also used synthetic data (both image and depth), yet, we outperform them across all metrics. We attribute our successes over their method to Quantitative results on the unsupervised KITTI depth completion benchmark. Results are taken from the benchmark <ref type="bibr" target="#b32">[32]</ref>. Our approach beats all competing methods across all metrics and achieves the state of the art on the unsupervised depth completion task. We note <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b40">[40]</ref> compete in both unsupervised and supervised benchmarks and that the unsupervised approach of <ref type="bibr" target="#b40">[40]</ref> also uses synthetic data. However, as <ref type="bibr" target="#b40">[40]</ref> used both image and sparse depth from the synthetic domain, they were plagued by the domain gap between synthetic and real data. We outperform them on all metrics while using fewer parameters (also see <ref type="table" target="#tab_0">Table II</ref> and III in main text) and we attribute some of such successes to the way we leveraged synthetic data -bypassing the need to adapt to different domains and learning to predict topology from only sparse points.</p><p>the way we exploited synthetic data -learning to recover topology from sparse points. As <ref type="bibr" target="#b40">[40]</ref> used both image and sparse depth from the synthetic domain, they were plagued by the domain gap between synthetic and real data. Our approach, instead, leverages geometry from the synthetic domain, where shapes of objects persist regardless of real or synthetic domains. Hence, our model is able to bridge the domain gap without any sort of adaptation. Although we are the best performing method on the unsupervised setting, we note that the benchmark is still dominated by supervised methods. However, our approach shows promise as we surpass some supervised methods: <ref type="bibr" target="#b4">[5]</ref> across all metrics, <ref type="bibr" target="#b5">[6]</ref> on MAE, iMAE, and iRMSE metrics and <ref type="bibr" target="#b21">[21]</ref> on iMAE. We hope that our approach will lay the foundation for the push to close the gap between supervised and unsupervised learning frameworks for the depth completion task. Qualitative comparison with <ref type="bibr" target="#b21">[21]</ref> (best viewed 2? and in color). Our approach performs better on cars, walls, trees, poles, and far regions (highlighted in green on error maps). Specifically, we show lower errors (dark blue) on the cars and walls than <ref type="bibr" target="#b21">[21]</ref> (white and light blue). In regions of the error map corresponding to walls and poles, we similarly improve (white, light blue, dark blue) over <ref type="bibr" target="#b21">[21]</ref> (red, orange, white). <ref type="figure" target="#fig_1">Fig. 15</ref>. Qualitative comparison with <ref type="bibr" target="#b30">[30]</ref> (best viewed 2? and in color). Our approach performs better on cars, walls, trees, poles, and far regions (highlighted in green on error maps). In top 3 rows, <ref type="bibr" target="#b30">[30]</ref> shows errors (white) in the trees and cars. In rows 4 to 7, <ref type="bibr" target="#b30">[30]</ref> shows similar levels of errors in regions corresponding to the car, walls and traffic gate. Similar errors are present in the cars in the last 3 rows. Our approach consistently performs better (dark blue) for the corresponding regions in the error maps.</p><p>In <ref type="figure" target="#fig_4">Fig. 13</ref>, we compiled screenshots of each of the competing unsupervised depth completion method and their respective ranks on the KITTI benchmark at the time of submission. Individual screenshots of each were concatenated together to form the figure. The unsupervised version of <ref type="bibr" target="#b40">[40]</ref> is omitted because they did not release their results on the online benchmark. Their numbers were directly taken from their paper and are shown in <ref type="table" target="#tab_4">Table XV</ref>. Our method (ScaffFusion) is the state of the art on the unsupervised depth completion task.</p><p>In the main paper, we showed qualitative comparisons with the state of the art, VGG11 <ref type="bibr" target="#b37">[37]</ref>. Here, in <ref type="figure" target="#fig_1">Fig. 14 and 15</ref>, we show additional comparisons with other top unsupervised depth completion methods <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b30">[30]</ref> on the KITTI online benchmark <ref type="bibr" target="#b32">[32]</ref>. Compared to <ref type="bibr" target="#b21">[21]</ref>, our approach performs better on cars, walls, trees, poles, and far regions. Also, in the predicted depth maps, <ref type="bibr" target="#b21">[21]</ref> shows artifacts resembling scanlines. Compared to <ref type="bibr" target="#b30">[30]</ref>, we similarly improve on cars, walls, trees and poles. <ref type="bibr" target="#b30">[30]</ref> tend to predict irregular shapes incorrectly, as seen in the error map of the traffic gate in rows 4 to 7 in <ref type="figure" target="#fig_1">Fig. 15</ref>; whereas, we capture its geometry. Most of these regions are complex and thus demonstrates the effectiveness of our topology estimator, ScaffNet, (Sec. III-A in main text) and the selective use our topology prior (Eqn. 8, 9 in main text) in our objective function. Error maps in <ref type="figure" target="#fig_1">Fig. 14 and 15</ref> have been highlighted in green to show the comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX VIII NETWORK ARCHITECTURE</head><p>We present our network architectures for our topology estimator, and our depth-RGB image fusion network, ScaffNet and FusionNet, respectively (see <ref type="figure">Fig. II</ref> in main text). We design our network structures carefully to make them lightweight so that they can be employed on standard embedded systems for real-time applications.</p><p>Our ScaffNet is an SPP module followed by an encoder and decoder, which all together consists of only ?1.4M parameters. Given sparse depth measurements, our ScaffNet outputs an initial estimate of the topology,d 0 , which is refined by our FusionNet. Our FusionNet follows the late fusion paradigm with an image branch and a depth branch. The latent representation of both branches are concatenated and fed to decoder. The decoder produces multiplicative scales ?(x) and additive residuals ?(x) to construct our final depth predictiond(x) = ?(x)d 0 (x) + ?(x) for x ? ?. Our FusionNet is extremely light-weight and only consists of ?6.4M parameters. Together, our model has a total of ?7.8M parameters, much fewer than the competing methods ?9.7M <ref type="bibr" target="#b37">[37]</ref>, ?18.8M <ref type="bibr" target="#b40">[40]</ref>, and ?27.8M <ref type="bibr" target="#b21">[21]</ref> while outperforming all of them <ref type="table" target="#tab_4">(Table XV)</ref>) across all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ScaffNet</head><p>ScaffNet is an encoder-decoder network augmented with an Spatial Pyramid Pooling module. Our SPP module is comprised of several max pool layers followed by three 1?1 convolutional layers to weight the trade-off between small (fine details, but sparse) and large (coarse, but dense) kernel sizes. The output of our SPP module is fed into an encoder with five layers. The first layer uses a 5 ? 5 kernel with 32 filters. The remaining four layers use 3 ? 3 kernels with 64, 96, 128, and 196 filters, respectively. The latent representation is then fed into a decoder with skip connections. The decoder is separated into five modules, each corresponds to a resolution level. For each module, we perform deconvolution using 3 ? 3 kernels with 128, 96, 64, 64, and 32 filters, respectively. The result is concatenated with the output of the corresponding resolution (skip connection) from the encoder. The output of which is convolved with another 3?3 and passed to the next decoder module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FusionNet</head><p>Our FusionNet consists of two encoders (branches), one for processing image and the other for depth. Both encoders contain five convolutional layers with the first layer using a 5 ? 5 kernel. The remaining four layers for each branch use 3 ? 3 kernels. For the image encoder, the convolutional layers consist of 48, 96, 192, 384, and 384 filters. For the depth encoder, the layers consist of 16, 32, 64, 128, and 128 filters. The output latent representations of the encoders are concatenated together and fed into the decoder. The FusionNet decoder, unlike the ScaffNet decoder, contains four modules, where each of the modules is comprised of 3 ? 3 de-convolution, followed by concatenation with skip connections, and a 3 ? 3 convolution. The skip connections are the concatenation of the feature maps from the corresponding image and depth encoders. The de-convolution and convolution layers of each module use 256, 128, 64, 64 filters, respectively. The low resolution output is produced by a 3 ? 3 convolution with 2 filters, for ? and ?. The last layer is a nearest-neighbor upsampling layer to produce the full resolution ? and ?.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview. Sparse points (z) are first densified by SPP (see Fig. 3), and fed to ScaffNet (trained with synthetic data) to produce an approximate topologyd 0 . FusionNet, trained with an unsupervised loss (Eqn. 3), refine? d 0 with ? and ? by fusing thed 0 with the information from the image It to produce the final predictiond(x) = ?(x)d 0 (x) + ?(x) for x ? ?. Only ScaffNet (red) and FusionNet (green) are required for inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative ablation on the effect of SPP. We show the benefits of leveraging SPP to increase receptive field and densify the sparse input. ScaffNet with SPP consistently outperforms the model without SPP in most regions of the error map. Using SPP, ScaffNet captures more details about the scene. For example, in the top panel, ScaffNet with SPP recovers the person while the model without SPP misses a portion of the person. In the bottom panel, we see that SPP helps retain more details about complex objects (e.g. plants and rails). Regions are highlighted in yellow for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>VOID depth completion testing set. Although ScaffNet only uses sparse points (magnified) and is trained on synthetic data, it can still capture the topology of the scene. FusionNet approximated topology and refines the incorrect predictions (regions highlighted in green) with image information. Note: in the bottom two rows, the initial estimate is mostly correct; hence FusionNet does not modify those regions and instead leverages it as a prior (Eqn. 9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 12 .</head><label>12</label><figDesc>Qualitative comparison to state-of-the-art monocular depth network. The input images are shown in the first (top) row, predictions of PackNet<ref type="bibr" target="#b11">[12]</ref> (left) and our FusionNet (right) are shown in the second (middle) row and the corresponding error maps are given in the third (bottom) row. Our method is able to recover sharper depth maps i.e. the bush in the center of the image, and vegetation and railings on the right side of the image. We also perform better overall as seen in the error maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 13 .</head><label>13</label><figDesc>Compilation of Screenshots of the KITTI online benchmark at the time of submission. Our method (ScaffFusion) ranks higher than all competing unsupervised methods. The figure was produced by concatenating screenshots of published unsupervised depth completion method on the benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Qualitative comparison with [21] (best viewed 2? and in color). Our approach performs better on cars, walls, trees, poles, and far regions (highlighted in green on error maps). Specifically, we show lower errors (dark blue) on the cars and walls than [21] (white and light blue). In regions of the error map corresponding to walls and poles, we similarly improve (white, light blue, dark blue) over [21] (red, orange, white).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell>ERROR METRICS</cell></row><row><cell>Metric</cell><cell>Definition</cell></row><row><cell>MAE</cell><cell>1 |?|</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">ON THE KITTI VALIDATION SET</cell></row><row><cell>Method</cell><cell># Param</cell><cell>MAE</cell><cell>RMSE</cell><cell cols="2">iMAE iRMSE</cell></row><row><cell>Scaffolding [37]</cell><cell>0</cell><cell cols="2">443.57 1990.68</cell><cell>1.72</cell><cell>6.43</cell></row><row><cell>Our ScaffNet</cell><cell>?1.4M</cell><cell cols="2">318.41 1425.53</cell><cell>1.39</cell><cell>5.01</cell></row><row><cell>Ma [21]</cell><cell cols="3">?27.8M 358.92 1384.85</cell><cell>1.60</cell><cell>4.32</cell></row><row><cell>Yang [40]</cell><cell cols="3">?18.8M 347.17 1310.03</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>VGG8 [37]</cell><cell>?6.4M</cell><cell cols="2">308.81 1230.85</cell><cell>1.29</cell><cell>3.84</cell></row><row><cell>VGG11 [37]</cell><cell>?9.7M</cell><cell cols="2">305.06 1239.06</cell><cell>1.21</cell><cell>3.71</cell></row><row><cell>Our FusionNet</cell><cell>?7.8M</cell><cell cols="2">286.35 1182.81</cell><cell>1.18</cell><cell>3.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III RESULTS</head><label>III</label><figDesc></figDesc><table><row><cell cols="6">ON THE KITTI DEPTH COMPLETION BENCHMARK</cell></row><row><cell>Method</cell><cell># Param</cell><cell>MAE</cell><cell>RMSE</cell><cell cols="2">iMAE iRMSE</cell></row><row><cell>Schneider [29]</cell><cell>n/a</cell><cell cols="2">605.47 2312.57</cell><cell>2.05</cell><cell>7.38</cell></row><row><cell>Ma [21]</cell><cell cols="3">?27.8M 350.32 1299.85</cell><cell>1.57</cell><cell>4.07</cell></row><row><cell>Yang [40]</cell><cell cols="3">?18.8M 343.46 1263.19</cell><cell>1.32</cell><cell>3.58</cell></row><row><cell>Shivakumar [30]</cell><cell>n/a</cell><cell cols="2">429.93 1206.66</cell><cell>1.79</cell><cell>3.62</cell></row><row><cell>VGG8 [37]</cell><cell>?6.4M</cell><cell cols="2">304.57 1164.58</cell><cell>1.28</cell><cell>3.66</cell></row><row><cell>VGG11 [37]</cell><cell>?9.7M</cell><cell cols="2">299.41 1169.97</cell><cell>1.20</cell><cell>3.56</cell></row><row><cell>Our FusionNet</cell><cell>?7.8M</cell><cell cols="2">280.76 1121.93</cell><cell>1.15</cell><cell>3.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY ON THE KITTI VALIDATION SET SPP) for processing the sparse inputs. SPP improves performance across all metrics by large margins. Rows 3 to 5 and 7: we justify learning ? and ? for refining the initial estimated 0 . ? and ? alone are unable to capture the scene and performs worse than direct mapping. When combined together ? and ? surpasses direct mapping on all metrics. Rows 6 and 7: the topology prior (Eqn. 9) allows us to leverage what we have learned from synthetic data in regions that are compatible with the image, providing a consistent performance boost. Note that Our ScaffNet is using sparse-depth only.</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>ScaffNet w/o SPP</cell><cell>409.93</cell><cell>1776.42</cell><cell>1.72</cell><cell>6.40</cell></row><row><cell>ScaffNet w/ SPP</cell><cell>318.41</cell><cell>1425.53</cell><cell>1.39</cell><cell>5.01</cell></row><row><cell>FusionNet (? only)</cell><cell>301.08</cell><cell>1297.02</cell><cell>1.30</cell><cell>4.43</cell></row><row><cell>FusionNet (? only)</cell><cell>299.90</cell><cell>1304.77</cell><cell>1.32</cell><cell>4.34</cell></row><row><cell>FusionNet (direct map)</cell><cell>292.74</cell><cell>1213.68</cell><cell>1.20</cell><cell>3.59</cell></row><row><cell>FusionNet (no ltp)</cell><cell>297.39</cell><cell>1204.88</cell><cell>1.19</cell><cell>3.58</cell></row><row><cell>FusionNet</cell><cell>286.35</cell><cell>1182.81</cell><cell>1.18</cell><cell>3.55</cell></row><row><cell cols="5">Rows 1 and 2: we show a comparison between our SPP module (w/</cell></row><row><cell cols="3">SPP) and conventional convolutions (w/o</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V QUANTITATIVE</head><label>V</label><figDesc>RESULTS ON THE VOID BENCHMARK</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>Ma [21]</cell><cell>178.85</cell><cell>243.84</cell><cell>80.12</cell><cell>107.69</cell></row><row><cell>Yang [40]</cell><cell>151.86</cell><cell>222.36</cell><cell>74.59</cell><cell>112.36</cell></row><row><cell>VGG8 [37]</cell><cell>98.45</cell><cell>169.17</cell><cell>57.22</cell><cell>115.33</cell></row><row><cell>VGG11 [37]</cell><cell>85.05</cell><cell>169.79</cell><cell>48.92</cell><cell>104.02</cell></row><row><cell>Our ScaffNet</cell><cell>70.16</cell><cell>156.99</cell><cell>42.78</cell><cell>91.48</cell></row><row><cell>VGG11 + SLAM [37]</cell><cell>73.14</cell><cell>146.40</cell><cell>42.55</cell><cell>93.16</cell></row><row><cell>Our FusionNet</cell><cell>59.53</cell><cell>119.14</cell><cell>35.72</cell><cell>68.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF SCAFFNET POOLING KERNEL SIZES ON VOID</figDesc><table><row><cell>Pool Sizes</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>none</cell><cell>100.75</cell><cell>242.27</cell><cell>71.32</cell><cell>191.60</cell></row><row><cell>5</cell><cell>102.82</cell><cell>217.38</cell><cell>69.05</cell><cell>195.51</cell></row><row><cell>5, 7</cell><cell>86.44</cell><cell>173.06</cell><cell>56.68</cell><cell>174.86</cell></row><row><cell>5, 7, 9</cell><cell>81.48</cell><cell>169.24</cell><cell>51.76</cell><cell>140.33</cell></row><row><cell>5, 7, 9, 11</cell><cell>77.104</cell><cell>154.604</cell><cell>46.886</cell><cell>121.256</cell></row><row><cell>5, 7, 9, 11, 13</cell><cell>70.16</cell><cell>156.99</cell><cell>42.78</cell><cell>91.48</cell></row><row><cell>3, 5, 7, 9, 11, 13</cell><cell>71.49</cell><cell>154.91</cell><cell>44.01</cell><cell>91.65</cell></row><row><cell>5, 7, 9, 11, 13, 15</cell><cell>72.55</cell><cell>159.31</cell><cell>44.67</cell><cell>99.15</cell></row><row><cell>5, 7, 9, 11, 13, 15, 17</cell><cell>71.93</cell><cell>156.09</cell><cell>46.15</cell><cell>97.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII ABLATION</head><label>VII</label><figDesc>STUDY ON THE EFFECT OF SPATIAL PYRAMID POOLING</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>Scaffolding [37]</cell><cell>443.57</cell><cell>1990.68</cell><cell>1.72</cell><cell>6.43</cell></row><row><cell>Our ScaffNet w/o SPP</cell><cell>409.93</cell><cell>1776.42</cell><cell>1.72</cell><cell>6.40</cell></row><row><cell>Our ScaffNet w/ SPP</cell><cell>318.41</cell><cell>1425.53</cell><cell>1.39</cell><cell>5.01</cell></row><row><cell>Ma [21]</cell><cell>358.92</cell><cell>1384.85</cell><cell>1.60</cell><cell>4.32</cell></row><row><cell>Yang [40]</cell><cell>347.17</cell><cell>1310.03</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>VGG8 [37]</cell><cell>308.81</cell><cell>1230.85</cell><cell>1.29</cell><cell>3.84</cell></row><row><cell>VGG11 [37]</cell><cell>305.06</cell><cell>1239.06</cell><cell>1.21</cell><cell>3.71</cell></row><row><cell>Our FusionNet w/o SPP</cell><cell>306.54</cell><cell>1219.92</cell><cell>1.24</cell><cell>3.65</cell></row><row><cell>Our FusionNet w/ SPP</cell><cell>286.35</cell><cell>1182.81</cell><cell>1.18</cell><cell>3.55</cell></row><row><cell cols="5">Ablation study on the effect of Spatial Pyramid Pooling on KITTI validation</cell></row><row><cell cols="5">set. Results of [21], [40], [37] are directly taken from their papers. Our</cell></row><row><cell cols="5">ScaffNet w/o SPP omits the Spatial Pyramid Pooling (SPP) module. Our</cell></row><row><cell cols="5">FusionNet w/o SPP uses ScaffNet w/o SPP as the initial topology estimator.</cell></row></table><note>Results of our ScaffNet with SPP consistently impoves its variant without the module. Similarly, because ScaffNet with SPP provides FusionNet with more accurately estimated topology, FusionNet like-wise improves.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII ABLATION</head><label>VIII</label><figDesc>STUDY ON THE EFFECT OF SPATIAL PYRAMID POOLING</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>Ma [21]</cell><cell>198.76</cell><cell>260.67</cell><cell>88.07</cell><cell>114.96</cell></row><row><cell>Yang [40]</cell><cell>151.86</cell><cell>222.36</cell><cell>74.59</cell><cell>112.36</cell></row><row><cell>Our ScaffNet w/o SPP</cell><cell>100.75</cell><cell>242.27</cell><cell>71.32</cell><cell>191.60</cell></row><row><cell>VGG8 [37]</cell><cell>98.45</cell><cell>169.17</cell><cell>57.22</cell><cell>115.33</cell></row><row><cell>VGG11 [37]</cell><cell>85.05</cell><cell>169.79</cell><cell>48.92</cell><cell>104.02</cell></row><row><cell>Our FusionNet w/o SPP</cell><cell>77.62</cell><cell>140.36</cell><cell>51.58</cell><cell>91.84</cell></row><row><cell>Our ScaffNet w/ SPP</cell><cell>70.16</cell><cell>156.99</cell><cell>42.78</cell><cell>91.48</cell></row><row><cell>VGG11 + SLAM [37]</cell><cell>73.14</cell><cell>146.40</cell><cell>42.55</cell><cell>93.16</cell></row><row><cell>Our FusionNet w/ SPP</cell><cell>59.53</cell><cell>119.14</cell><cell>35.72</cell><cell>68.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX ABLATION</head><label>IX</label><figDesc>STUDY ON VARIOUS DENSITIES OF SPARSE INPUTS</figDesc><table><row><cell></cell><cell></cell><cell cols="2">?0.15% density</cell><cell></cell></row><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>Our ScaffNet</cell><cell>139.60</cell><cell>308.47</cell><cell>71.50</cell><cell>151.49</cell></row><row><cell>VGG11 [37]</cell><cell>124.11</cell><cell>217.43</cell><cell>66.95</cell><cell>121.23</cell></row><row><cell>Our FusionNet</cell><cell>108.44</cell><cell>195.82</cell><cell>57.52</cell><cell>103.33</cell></row><row><cell></cell><cell></cell><cell cols="2">?0.05% density</cell><cell></cell></row><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>Our ScaffNet</cell><cell>260.13</cell><cell>531.69</cell><cell>115.21</cell><cell>218.27</cell></row><row><cell>VGG11 [37]</cell><cell>179.66</cell><cell>281.09</cell><cell>95.27</cell><cell>151.66</cell></row><row><cell>Our FusionNet</cell><cell>150.65</cell><cell>255.08</cell><cell>80.79</cell><cell>133.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X COMPARISON</head><label>X</label><figDesc>OF SAMPLING STRATEGIES ON THE VOID BENCHMARK</figDesc><table><row><cell></cell><cell cols="2">ScaffNet trained with</cell><cell></cell><cell cols="2">Error metrics</cell></row><row><cell>Method</cell><cell>Uniform</cell><cell>Corner</cell><cell>MAE</cell><cell cols="2">RMSE iMAE iRMSE</cell></row><row><cell>ScaffNet</cell><cell></cell><cell></cell><cell cols="3">76.44 177.63 40.565 92.589</cell></row><row><cell>FusionNet</cell><cell></cell><cell></cell><cell cols="3">60.393 126.457 33.237 69.166</cell></row><row><cell>ScaffNet</cell><cell></cell><cell></cell><cell cols="3">70.16 156.99 42.78</cell><cell>91.48</cell></row><row><cell>FusionNet</cell><cell></cell><cell></cell><cell cols="2">59.53 119.14</cell><cell>35.72</cell><cell>68.36</cell></row><row><cell cols="6">Comparison of ScaffNet and FusionNet trained with different sampling</cell></row><row><cell cols="6">strategies and evaluated on the VOID dataset (points tracked by VIO with</cell></row><row><cell cols="6">corner based feature detector). Rows 1: ScaffNet is trained on SceneNet</cell></row><row><cell cols="6">where points are sampled uniformly randomly. Row 2: FusionNet is trained</cell></row><row><cell cols="6">on VOID using a ScaffNet from row 1 with weights frozen. Rows 3:</cell></row><row><cell cols="6">ScaffNet is trained on SceneNet where points are chosen by running Harris</cell></row><row><cell cols="6">corner detector [13] on the corresponding image. Row 4: FusionNet is</cell></row><row><cell cols="6">trained on VOID using the ScaffNet from row 3 with weights frozen.</cell></row><row><cell cols="6">Despite being trained a synthetic dataset with points sampled uniformly,</cell></row><row><cell cols="6">ScaffNet can generalize to real dataset with points chosen based on corner</cell></row><row><cell cols="6">detection. Both FusionNets likewise have similar performance even though</cell></row><row><cell cols="6">their respective ScaffNet is trained on different sparse point distributions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XI COMPARISON</head><label>XI</label><figDesc>Input image and sparse depth; orange bounding boxes show the magnified sparse depth. Column 2: Despite being trained only on synthetic data, ScaffNet is able to generalize to NYUv2. Column 3: FusionNet trained on VOID performs worse than ScaffNet because of the domain gap. Column 4: FusionNet trained on VOID recovers performance quickly by adapting to the camera and scenes of NYUv2 when finetuned for just 1 epoch. Column 5, 6: When finetuned longer on NYUv2, FusionNet trained on VOID performs comparably to FusionNet trained on NYUv2.</figDesc><table><row><cell></cell><cell cols="4">OF TRAINING METHODS ON KITTI</cell></row><row><cell>ScaffNet</cell><cell>FusionNet</cell><cell></cell><cell cols="2">Error metrics</cell></row><row><cell>Pretrain Freeze</cell><cell>Pretrain</cell><cell>MAE</cell><cell>RMSE</cell><cell cols="2">iMAE iRMSE</cell></row><row><cell></cell><cell></cell><cell cols="2">327.74 1283.02</cell><cell>1.34</cell><cell>3.79</cell></row><row><cell></cell><cell></cell><cell cols="2">305.90 1249.64</cell><cell>1.25</cell><cell>3.61</cell></row><row><cell></cell><cell></cell><cell cols="2">286.35 1182.81</cell><cell>1.18</cell><cell>3.55</cell></row><row><cell></cell><cell></cell><cell cols="2">282.91 1176.09</cell><cell>1.17</cell><cell>3.54</cell></row><row><cell cols="6">We compare different methods for training ScaffNet and FusionNet on</cell></row><row><cell cols="6">the KITTI validation set. For ScaffNet: pretrain option denotes pretrain-</cell></row><row><cell cols="6">ing on synthetic data (Virtual KITTI) before training FusionNet, freeze</cell></row><row><cell cols="6">denotes weights are frozen when training FusionNet. For FusionNet: pre-</cell></row><row><cell cols="6">train option denotes pretraining on real data (KITTI) using a pretrained</cell></row><row><cell cols="6">frozen ScaffNet. Row 1: when both ScaffNet and FusionNet are jointly</cell></row><row><cell cols="6">trained from scratch on real data, performance is the worst because it</cell></row><row><cell cols="6">cannot leverage side information e.g. topology from synthetic data. Row</cell></row><row><cell cols="6">2: pretraining ScaffNet on synthetic data and jointly training ScaffNet</cell></row><row><cell cols="6">and FusionNet on real data performs better than if ScaffNet was trained</cell></row><row><cell cols="6">from scratch. Row 3 (the proposed method): pretraining Scaffnet on</cell></row><row><cell cols="6">synthetic data and freezing its weights while training FusionNet per-</cell></row><row><cell cols="6">forms better than jointly training. Row 4: pretraining ScaffNet on sy-</cell></row><row><cell cols="6">thetic data, freezing its weights to train FusionNet, and finetuning both</cell></row><row><cell cols="6">jointly on real data performs slightly better than the proposed method.</cell></row><row><cell cols="6">Our training procedure is as follows: (i) train ScaffNet</cell></row><row><cell cols="6">on synthetic data, (ii) freeze ScaffNet weights, (iii) train</cell></row><row><cell cols="6">FusionNet with frozen ScaffNet on real data. This is denoted</cell></row><row><cell cols="6">in row 3 of Table XI. We do this consciously to enable</cell></row><row><cell cols="6">ScaffNet to be flexible so that it can not only be used as part</cell></row><row><cell cols="6">of the FusionNet inference pipeline, but also for other tasks</cell></row><row><cell cols="6">that would benefit from a topology prior. Moreover, freezing</cell></row><row><cell cols="6">ScaffNet allows for faster training with lesser hardware</cell></row><row><cell cols="6">requirements, whereas, joint training requires extra memory,</cell></row><row><cell cols="6">compute, and time. That being said, if we were to (i) pretrain</cell></row><row><cell cols="6">ScaffNet on synthetic data, (ii) freeze ScaffNet weights,</cell></row><row><cell cols="6">(iii), train FusionNet with frozen ScaffNet on real data and</cell></row><row><cell cols="6">(iv) unfreeze ScaffNet and finetune ScaffNet and FusionNet</cell></row><row><cell cols="6">jointly on real data, as denoted by row 4 of Table XI, there</cell></row></table><note>Fig. 11. Qualitative evaluation on the NYUv2 test set. Left to right, Column 1:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XII RESULTS</head><label>XII</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">ON THE NYUV2 TEST SET</cell></row><row><cell>Method</cell><cell>Dataset</cell><cell cols="2"># Epoch MAE RMSE iMAE iRMSE</cell></row><row><cell>ScaffNet</cell><cell>SceneNet</cell><cell>0</cell><cell>136.55 240.63 30.77 59.10</cell></row><row><cell>FusionNet</cell><cell>VOID</cell><cell>0</cell><cell>155.20 241.42 31.77 52.62</cell></row><row><cell>FusionNet</cell><cell>NYUv2</cell><cell>10</cell><cell>117.49 199.31 24.89 44.06</cell></row><row><cell cols="2">FusionNet VOID, NYUv2</cell><cell>1</cell><cell>124.46 205.27 27.37 47.56</cell></row><row><cell cols="2">FusionNet VOID, NYUv2</cell><cell>10</cell><cell>116.58 198.20 24.88 44.25</cell></row><row><cell cols="4">We evaluate Scaffnet and FusionNet on the NYUv2 test set. # Epoch denotes</cell></row><row><cell cols="4">the number of epochs the method is trained or finetuned on NYUv2. Row</cell></row><row><cell cols="4">1: ScaffNet is trained only on SceneNet and is able to generalize again</cell></row><row><cell cols="4">from synthetic to real dataset. Row 2: FusionNet is trained on VOID using</cell></row><row><cell cols="4">a frozen ScaffNet (from row 1). Row 3: FusionNet is trained on NYUv2</cell></row><row><cell cols="4">using a frozen ScaffNet trained on SceneNet. Row 4, 5: FusionNet is trained</cell></row><row><cell cols="4">on VOID using a frozen ScaffNet (from row 1) and finetuned on NYUv2</cell></row><row><cell cols="4">for 1 and 10 epochs, respectively. VOID and NYUv2 are comprised of</cell></row><row><cell cols="4">different distribution of scenes and are captured with different camera and</cell></row><row><cell cols="4">depth sensor. Hence, while FusionNet trained on VOID (row 2) performs</cell></row><row><cell cols="4">reasonably, it is worse than ScaffNet trained on SceneNet (row 1) due to the</cell></row><row><cell cols="4">photometric domain gap. However, FusionNet trained on VOID can quickly</cell></row><row><cell cols="4">adapt to the new cameras and scenes when finetuned on NYUv2 for just 1</cell></row><row><cell cols="4">epoch (row 4). When finetuned for 10 epochs on NYUv2 (row 5), FusionNet</cell></row><row><cell cols="4">trained on VOID performs comparably to FusionNet trained on NYUv2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XIII COMPARISON</head><label>XIII</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">WITH MONOCULAR DEPTH METHODS ON KITTI</cell></row><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>Mono2 [11]</cell><cell>15491.00</cell><cell>19151.41</cell><cell>8844.36</cell><cell>9890.20</cell></row><row><cell>PackNet [12]</cell><cell>1808.63</cell><cell>3883.92</cell><cell>6.93</cell><cell>10.20</cell></row><row><cell>Our ScaffNet</cell><cell>318.41</cell><cell>1425.53</cell><cell>1.39</cell><cell>5.01</cell></row><row><cell>Our FusionNet</cell><cell>286.35</cell><cell>1182.81</cell><cell>1.18</cell><cell>3.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIV</head><label>XIV</label><figDesc></figDesc><table><row><cell cols="5">SUPERVISED KITTI DEPTH COMPLETION BENCHMARK</cell></row><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>Chodosh [5]</cell><cell>439.48</cell><cell>1325.37</cell><cell>3.19</cell><cell>59.39</cell></row><row><cell>Dimitrievski [6]</cell><cell>310.49</cell><cell>1045.45</cell><cell>1.57</cell><cell>3.84</cell></row><row><cell>Our FusionNet</cell><cell>280.76</cell><cell>1121.93</cell><cell>1.15</cell><cell>3.30</cell></row><row><cell>Ma [21]</cell><cell>249.95</cell><cell>814.73</cell><cell>1.21</cell><cell>2.80</cell></row><row><cell>Qui [25]</cell><cell>226.50</cell><cell>758.38</cell><cell>1.15</cell><cell>2.56</cell></row><row><cell>Xu [39]</cell><cell>235.73</cell><cell>785.57</cell><cell>1.07</cell><cell>2.52</cell></row><row><cell>Chen [3]</cell><cell>221.19</cell><cell>752.88</cell><cell>1.14</cell><cell>2.34</cell></row><row><cell>Van Gansbeke [33]</cell><cell>215.02</cell><cell>772.87</cell><cell>0.93</cell><cell>2.19</cell></row><row><cell>Yang [40]</cell><cell>203.96</cell><cell>832.94</cell><cell>0.85</cell><cell>2.10</cell></row><row><cell>Cheng [4]</cell><cell>209.28</cell><cell>743.69</cell><cell>0.90</cell><cell>2.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XV</head><label>XV</label><figDesc></figDesc><table><row><cell cols="5">UNSUPERVISED KITTI DEPTH COMPLETION BENCHMARK</cell></row><row><cell>Method</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>Schneider [29]</cell><cell>605.47</cell><cell>2312.57</cell><cell>2.05</cell><cell>7.38</cell></row><row><cell>Ma [21]</cell><cell>350.32</cell><cell>1299.85</cell><cell>1.57</cell><cell>4.07</cell></row><row><cell>Ku [18]</cell><cell>302.60</cell><cell>1288.46</cell><cell>1.29</cell><cell>3.78</cell></row><row><cell>Shivakumar [30]</cell><cell>429.93</cell><cell>1206.66</cell><cell>1.79</cell><cell>3.62</cell></row><row><cell>Yang [40]</cell><cell>343.46</cell><cell>1263.19</cell><cell>1.32</cell><cell>3.58</cell></row><row><cell>VGG8 [37]</cell><cell>304.57</cell><cell>1164.58</cell><cell>1.28</cell><cell>3.66</cell></row><row><cell>VGG11 [37]</cell><cell>299.41</cell><cell>1169.97</cell><cell>1.20</cell><cell>3.56</cell></row><row><cell>Our FusionNet</cell><cell>280.76</cell><cell>1121.93</cell><cell>1.15</cell><cell>3.30</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Safa Cicek is with the Department of Electrical and Computer Engineering, University of California, Los Angeles. Email: safacicek@ucla.edu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More formally, ? and ? should be written as a function of position x ? ?, ? ? (It(x), z(x),d 0 (x)). For simplicity, we will refer to them as ?(x) and ?(x) without the parameters and inputs.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We note that the max pooling layers specified here (5 ? 5, 7 ? 7, 9 ? 9, and 11 ? 11) are designed for KITTI <ref type="bibr" target="#b32">[32]</ref>. We use an extra max pooling layer with kernel size 13 ? 13 for the VOID dataset <ref type="bibr" target="#b37">[37]</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial framework for depth filling via wasserstein metric, cosine transform and domain transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>De La Garanderie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">91</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05377</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Convolutional Compressed Sensing for LiDAR Depth Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning morphological operators for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dimitrievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geo-supervised visual depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey vision conference</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1988" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 15th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised sparseto-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scenenet rgb-d: 5m photorealistic images of synthetic indoor trajectories with ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05079</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust monocular visual-inertial depth completion for embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adadepth: Unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Krishna</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bayesian deep basis fitting for depth completion with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15254</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth completion via deep basis fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep depth estimation from visual-inertial slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sartipi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Roumeliotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10038" to="10045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Targeted adversarial perturbations for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An adaptive framework for learning unsupervised depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3120" to="3127" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised depth completion from visual inertial odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsuei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5644" to="5653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Codevio: Visual-inertial odometry with learned optimizable dense depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
