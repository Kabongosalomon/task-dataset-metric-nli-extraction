<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantifying Facial Age by Posterior of Age Comparisons</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Zhang</surname></persName>
							<email>zhangyunxuan@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
							<email>liuli@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
							<email>chengli@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quantifying Facial Age by Posterior of Age Comparisons</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ZHANG ET AL.: QUANTIFYING FACIAL AGE BY POSTERIOR OF AGE COMPARISONS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel approach for annotating large quantity of in-the-wild facial images with high-quality posterior age distribution as labels. Each posterior provides a probability distribution of estimated ages for a face. Our approach is motivated by observations that it is easier to distinguish who is the older of two people than to determine the person's actual age. Given a reference database with samples of known ages and a dataset to label, we can transfer reliable annotations from the former to the latter via human-in-the-loop comparisons. We show an effective way to transform such comparisons to posterior via fully-connected and SoftMax layers, so as to permit end-to-end training in a deep network. Thanks to the efficient and effective annotation approach, we collect a new large-scale facial age dataset, dubbed 'MegaAge', which consists of 41, 941 images 1 . With the dataset, we train a network that jointly performs ordinal hyperplane classification and posterior distribution learning. Our approach achieves state-of-theart results on popular benchmarks such as MORPH2, Adience, and the newly proposed MegaAge. Note: There are mistakes in our original paper. Please check the appendix for errata.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Is there a moment when you try to guess the age of your new friend or a stranger and you seriously doubt your estimation? You may try very hard to find clues on his/her face -determining the number of wrinkles surround the eyes or sagging skin above the upper eyelids. Even with multiple trials, your guess sometimes may still far from the true answer.</p><p>The aforementioned difficulty is frequently encountered during the preparation of facial age datasets, which are needed for training models for automatic facial age estimation. Given face samples whose age is unknown, it is notoriously hard and unreliable to label them with actual age. The best we can do is resort to providing approximate age range annotation,  <ref type="figure">Figure 1</ref>: (a) Comparing the proposed target annotation (also the prediction) with existing schemes. In contrast to current schemes that either provide an actual age label or age range, we introduce a new annotation that faithfully captures the distribution of estimated ages. The annotation can be obtained from age comparisons between query and reference images. (b) We propose a network that can jointly estimate ordinal classifications and age posterior distribution as outputs.</p><p>which is adopted by Adience dataset <ref type="bibr" target="#b2">[3]</ref>, one of the largest in-the-wild facial age databases. An alternative is to collect photos from subjects whose age is known. Under such restrictions, the sample size is usually small and the images are constrained by the environment from which we capture them. It is thus inevitably hard to generalise a model learned from such a dataset to novel scenes. Early databases, such as MORPH <ref type="bibr" target="#b18">[19]</ref> and FG-NET <ref type="bibr" target="#b17">[18]</ref>, fall into this category. While it is hard to guess the actual age of an individual, one may find it effortless to tell apart who is elder or younger when two persons stand next to each other. Age comparison is usually deemed easier since we can use the appearance of a person as a reference while examining the other. Nevertheless, it remains an open problem on how we can exploit such comparisons as ground-truths for training a robust and accurate age estimation model. In this paper, we introduce an effective way of exploiting age comparisons for labelling massive quantity of in-the-wild face images. We further propose a new paradigm for mapping a series of such comparisons to meaningful age distribution posterior that can be used to train an age estimation deep network in an end-to-end manner. We detail our contributions as follows. 1) New large-scale age posterior dataset -As the first contribution of this study, we propose a novel age annotation approach given face images collected in the wild. Specifically, we first conduct a user study and show that humans are poor in guessing the actual age of a face but they perform much better at comparing the age of two faces. The observation then leads to our new approach -given an image with unknown age, we compare the image with some selected images from a reference database with known age labels. We treat each comparison as an independent random event and determine the age distribution as the posterior given multiple observed events. The posterior is set as the target label of the image. As shown in <ref type="figure">Fig. 1(a)</ref>, the posterior faithfully reflects the true age with an estimated range to capture uncertainty, which otherwise cannot be achieved by existing annotation schemes. The proposed approach enables us to extend reliable age annotations from any existing datasets (e.g., FG-NET <ref type="bibr" target="#b17">[18]</ref>), which are captured in constrained environments, to a massive in-thewild dataset with human-in-the-loop. The resulting dataset, called MegaAge, consists of 41, 941 faces annotated with age posterior distributions. The images are randomly selected from MegaFace <ref type="bibr" target="#b12">[13]</ref> and YFCC100M <ref type="bibr" target="#b15">[16]</ref> dataset. 2) Joint cost sensitive and posterior losses -The second contribution of this work is a new deep convolutional network architecture that learns from age posterior distribution. The network also generates age posterior as its output. The network is inspired by existing ordinal hyperplane methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>, which learn multiple binary 'is this face older than age k?' classifiers for each possible age k. Ordinal hyperplane method has shown powerful capability of keeping ordinal information of age labels. Extending from the idea, our network consists of a ordinal hyperplane module that captures ordinal information. Different from ordinal hyperplane methods, our network further maps the responses of the module to generate age posterior distribution. We show that the mapping can be accomplished by using a fully-connected layer and a softmax layer. The proposed network is unique in that it can be jointly trained with two losses, namely, the cost sensitive loss that is typically applied for learning an ordinal hyperplanes ranker <ref type="bibr" target="#b0">[1]</ref>, and Kullback-Leibler (KL) divergence loss that supervises the learning of posterior distribution. The two losses are complementary in nature. The cost sensitive loss enforces the learning of ordinal relationships, while the KL divergence loss ensures the estimated age to fall within a distribution and captures uncertainties. In addition, with the KL divergence loss, our network can generate smoother ordinal classifications (as a side product) in comparison to conventional ordinal hyperplane methods, as shown in <ref type="figure">Fig. 1</ref></p><formula xml:id="formula_0">(b).</formula><p>Extensive experiments are conducted on MORPH2, Adience and MegaAge datasets. Thanks to the novel annotation approach, we collected a large quantity of training samples with good quality to train the proposed network. Our approach outperforms state-of-theart approaches by reducing over 12% and 10% exact error on the MORPH2 <ref type="bibr" target="#b16">[17]</ref> and Adience <ref type="bibr" target="#b13">[14]</ref> datasets, respectively 2 . We also set a new benchmark on the proposed MegaAge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Quantifying Age Posterior from Age Comparisons</head><p>In this section, we first present a user study and then discuss how we could employ the observations of the user study and present our approach for mapping age comparisons to age posterior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">How Good can Human Predict Facial Age?</head><p>Test I -Guessing Actual Age: In the first experiment, we asked 30 volunteers to guess the actual age given face images randomly drawn from the FG-NET <ref type="bibr" target="#b17">[18]</ref> dataset. All participants were not familiar with the dataset. A total of 1002 images were presented to each of the participants. <ref type="figure">Figure 2</ref>(a) shows the results. The participants were fairly poor at estimating the actual age through face appearance on this dataset. The average recall rate within an ?3 years old is only 43.2%. Test II -Comparing Age: In the second experiments, the same group of participants were asked to compare the age of two faces from FG-NET. <ref type="figure">Figure 2</ref>(b) shows the relationship between age difference (face A minus face B), and the probability that participants find A is older than B. It is observed that when the age difference is over 10 years, participants gained over 95% accuracy in predicting the relative age order. Even when the age difference is only 5 years, participants still performed well with an accuracy of 85%. Discussion: Albeit the performances of the two tests are not directly comparable due to their different natures, the study does reflect that humans are more comfortable in telling apart  <ref type="figure">Figure 2</ref>: Results of the user study: (a) Test I -volunteers were asked to guess the actual age given an image. (b) Test II -volunteers were asked to compare the age of two persons.</p><p>persons of different ages, especially when the age difference is large. Another important observation is that human's performance on age comparisons can be approximated by a logistic function, as exhibited by the curve depicted in <ref type="figure">Fig. 2</ref></p><formula xml:id="formula_1">(b).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">From Age Comparisons to Age Posterior</head><p>We learn from the user study presented in Sec. 2.1 that each pair of age comparison can be approximated by a logistic function. Specifically, we treat each comparison between a query face I and a reference face I ref of known age k as a random event denoted as</p><formula xml:id="formula_2">C k ? {0, 1}, where C k = 1 indicates I is older than I ref .</formula><p>The likelihood of event C k given that face I has an age a is denoted P(C k |a),</p><formula xml:id="formula_3">P(C k |a) = ? (? (a ? k))/Z if C k = 1 ? (? (k ? a))/Z if C k = 0 ,<label>(1)</label></formula><p>where ? (?) is a logistic function with ? (x) = 1/(1 + exp(?x)), Z is a partition function with Z = P(C k = 0|a) + P(C k = 1|a). The parameter ? is a value that can be obtained by fitting the curve generated from our user study ( <ref type="figure">Fig. 2(b)</ref>). We use ? = 0.36 throughout our experiments. <ref type="figure">Figure 3</ref> depicts a few examples that show the likelihood P(C k |a) derived from age comparisons across with different reference images. Let's denote the events of M comparisons as C k 1 ,C k 2 , . . . ,C k M , we can compute the age posterior probability as</p><formula xml:id="formula_4">P(a|{C k m }) ? P(a) ? M m=1 P(C k m |a),<label>(2)</label></formula><p>where P(a) is the prior of age, which we assume to be uniform in this study. An example of posterior distribution is shown in <ref type="figure">Fig. 3</ref>. It is worth pointing out that a narrower posterior distribution can be obtained if more comparisons are conducted. Discussion: One may expect different logistic curves for different age ranges, since the difficulty level may increase from comparing younger ages to older ages. In this paper, we simplified the assumption on the logistic function by using a single ? , but with satisfactory results. Future work can explore different logistic functions across ages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">MegaAge Dataset</head><p>In this section we detail the construction of the MegaAge dataset with posterior distribution serving as the target label of each image. We randomly sampled query images from the challenging MegaFace dataset <ref type="bibr" target="#b12">[13]</ref> 3 (which contains a million unconstrained photos that capture</p><formula xml:id="formula_5">? ? ? ? ? Query face, is older than 1 ( |{ . / }) ( . 2 | ) ( . 3 | ) ( . 4 | ) ( . 5 | )</formula><p>Reference face, 1 Reference face, 2 Reference face, <ref type="bibr" target="#b4">5</ref> Reference face, 6</p><p>is older than 2 is younger than 5 is younger than 6 ? <ref type="figure">Figure 3</ref>: An illustration to show the age posterior distribution P(a|{C k m }) as a product of a prior P(a) and all likelihoods P(C k m |a).</p><p>Outlier <ref type="figure">Figure 4</ref>: Examples of images in the proposed MegaAge dataset and the associated age posterior label. Each blue vertical line represents a comparison (between the query face and a reference face) and its direction of supporting the posterior. The last figure shows an outlier that arises due to disagreement of annotators.</p><p>more than 690K different individuals) and YFCC100M dataset <ref type="bibr" target="#b15">[16]</ref>. We adopted the widely used FG-NET as our reference database. The labelling process takes the same procedures as illustrated in <ref type="figure">Fig. 3</ref>. Specifically, given a query image sampled from MegaFace/YFCC100M, we compared it with six face images selected from the reference database. We introduced a few constraints in the selection process to ensure meaningful comparisons. Firstly, the reference images need to have the same gender as the query image so as to avoid gender bias. An accurate gender classifier with an accuracy rate of over 97% was used. Secondly, we roughly estimated the age of the query image using an existing model trained on MORPH and selected three images smaller than the estimated age and another three that are larger than the estimated age. Selection is random apart from the constraints. Age posterior was generated following the method presented in Sec. 2.2. We repeated this process to label all query images with posterior. Outliers with a posterior distribution with 90% confidence interval larger than 15 years old were discarded. This kind of outliers constituted only 3% of queries. More than 80% of queries have their posterior's confidence interval less than 8 years old. The final number of images we collected with posterior ground-truth is 41, 941. Note that actual age ground-truth can be easily derived from the posterior by taking the mode of the distribution. Some of the samples are shown in <ref type="figure">Fig. 4</ref>.  <ref type="figure">Figure 5</ref>: The proposed network can be trained end-to-end. It is supervised by two signals: cost sensitive loss and KL divergence loss. The formal is used to regularise the ordinal hyperplane module. The latter provides further constraints to ensure the estimated age to fall within the desired age distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Learning from Age Posterior</head><p>We now describe the deep convolutional network that learns from age posterior. <ref type="figure">Figure 5</ref> illustrates the proposed network. The network comprises a truncated VGG CNN network for extracting 128-dimensional features from a given image. Further details on this CNN is given in Sec. 5. Apart from the feature extraction layer, the network consists of an ordinal hyperplane module and an age posterior distribution module. The network is jointly trained with cost sensitive loss and KL divergence loss. We detail the modules as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ordinal Hyperplane Module</head><p>Ordinal hyperplane method <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> has been shown very powerful in capturing ordinal information of age labels. To gain advantage from this nice characteristic, we formulate an ordinal hyperplane module in our network. The module can be trained with cost sensitive loss, and the gradient can be back-propagated to further update the parameters of the feature extraction CNN. It is noted that our network can be trained without using the cost sensitive loss (using KL divergence loss alone), but we observed better performance when the network is trained with the loss as a regulariser. Design: The module comprises of a fully-connected (FC) layer, an output layer, and a sigmoid layer, as illustrated in <ref type="figure">Fig. 5</ref>. The output layer treats the age labels y i as a rank order, y i ? {1, . . . , K}, where K is the number of labels (typically set as 70 in our approach). The FC layer basically learns K classification function f k (x), to model the confidence of "face's age is larger than k", that is f k (I) =&lt; w k , ? (I) &gt;, where ? (I) is the feature of a face image, and w k is the linear weight of the classifier encapsulated in the FC. The sigmoid layer converts the ordinal classifications to responses as an input for the subsequent age posterior distribution module, which is described next in Sec. 3.2. Loss: We follow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> to use cost sensitive loss as an auxiliary loss to regularise our network. The cost sensitive loss is a relaxed regression loss, which can be written as</p><formula xml:id="formula_6">L hyper = ? k Cost k (a gt ) f k (I) ? 1[a gt &gt; k] 2 2 ,<label>(3)</label></formula><p>where a gt is the ground-truth age, 1[?] is an indicator function, which equals 1 for true argument, and 0 otherwise. Here Cost k (a gt ) is a truncated cost <ref type="bibr" target="#b0">[1]</ref> which equals 0 when |a gt ? k| &lt; L. Here we set L as 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Age Posterior Distribution Module</head><p>Note that the ordinal hyperplane module only captures the ordinal information of age labels but falls short of representing the uncertainty of an estimated range of ages. Here we show that it is possible to extend the ordinal hyperplane module to a probabilistic one. Design: Recall that we learn classifiers f k (I) in the ordinal hyperplane module. The response of a k-th classifier can be treated as the probability of a random event E k ? {0, 1}, where E k = 1 when the classifier thinks that a face is older than age k, that is f k (I) = P(E k = 1|I) = ? (w T k ? (I)), where w k is a learnable weight and ? (I) is the deep convolutional feature. Next, we assume all K classifiers are independent, and estimate the age posterior given all {E k } K k=1 ,</p><formula xml:id="formula_7">P(a|I) = P(a|{E k }) = 1 Z P(a) ? K k=1 P(E k |a) = 1 Z P(a) ? K k=1 P(E k = 1|a) f k (I) P(E k = 0|a) (1? f k (I)) .<label>(4)</label></formula><p>Notice here we approximate P(E k |a) as an exponential combination P(E k = 1|a) f k (I) ? P(E k = 0|a) <ref type="bibr">(</ref> (5) Similar to the assumption we made in Sec. 2.2, we assume that a likelihood can be modelled as a logistic function, e.g., P(E k = 1|a) = ? (? (k ? a)), which only depends on the age difference. Here we use the same ? coefficient as in Sec 2.2. Note that the posterior log P(a|I) is just a linear function of ordinal hyperplane classifier f k (I), which means we can conveniently compute P(a|I) by linking a fully connected layer after the ordinal hyperplane module, and followed by a SoftMax layer for normalisation. This design is parameter free and easy to implement. Loss: We can train the network end-to-end using the ground-truth age posterior distribution, denoted as P gt (a). We employ KL-divergence between two distribution as our loss, L KL = D KL (P gt (a)||P(a|I)) = ? ? a P gt (a) log P(a|I) ? Const.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(6)</head><p>Here P(a|I) is the soft output of our network. There are three possible ways to prepare for P gt (a): (1) for dataset that is labelled only with actual age, e.g., MORPH, we set P gt as a sharp Gaussian distribution with ? = 2. (2) For category based dataset, e.g., Adience, the confidence interval of each distribution of each sample is equivalent to the age range each specific age category covers. (3) for MegaAge dataset, we use the ground-truth age posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets: We evaluate our method on MORPH2 <ref type="bibr" target="#b18">[19]</ref>, Adience <ref type="bibr" target="#b2">[3]</ref> and MegaAge. 1) MORPH2 contains more than 55, 000 face images of 13, 000 individuals aged from 16 to 77 years. On average, each individual has more than 3 images and exact age was given. MORPH2 is the largest publicly available aging dataset. We follow the experimental setting in <ref type="bibr" target="#b16">[17]</ref>, where the the data is randomly divided into 80%/20% exclusive training/test partitions. 2) Adience provides 26, 580 images, which cover an age range from 0 to 70. The samples are divided into 8 age groups (0-2, 4-6, <ref type="bibr">8-13, 15-20, 25-32, 38-43, 48-53, 60-)</ref>. We follow the standard protocol <ref type="bibr" target="#b2">[3]</ref> to perform a 5-fold cross validation.</p><p>3) MegaAge, as described in Sec. 2.3, contains 41, 941 images encompassing ages from 0 to 70. We reserve 8, 530 images as test data. Each sample was labelled with age posterior as the ground-truth.</p><p>Evaluation Metric: The performance on MORPH2 is evaluated by mean average error (MAE). As for Adience, we report the exact mean accuracy on 8-class classification. We also provide the 1-off accuracy <ref type="bibr" target="#b2">[3]</ref>, i.e., a prediction is considered correct if it hits the exact age group or its neighbouring groups. For MegaAge, we employ cumulative accuracy as our metric, that is, CA(n) = K n /K ? 100, where K n is the number of test images whose absolute estimated error is smaller than n. To align with the results of Adience, of which the range of each age group is around 5, we report CA(3) and CA(5) in our experiments.</p><p>Network Architecture: In this study we use a truncated VGG-network, which only has one forth of filter number in each convolutional layer of the original VGG network <ref type="bibr" target="#b21">[22]</ref>. Thus the computational cost is reduced to 1/16 of the original. Our network runs at 10 frames per second (FPS) on a i7 desktop and 50 FPS on a GTX 970 GPU. We initialise all networks with a face verification model trained on MS-Celeb-1M dataset <ref type="bibr" target="#b8">[9]</ref>, of which the result on LFW <ref type="bibr" target="#b10">[11]</ref> is 99.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluating The Quality of MegaAge Annotations</head><p>To show that the proposed annotation approach (Sec. 2.2) is capable of generating highquality ground-truth to boost the performance of existing models, we conduct an experiment in which we gradually add more MegaAge data to the initial pool of training set. The initial pool is formed by the training partition of MORPH2 and Adience. Here we use a variant of the proposed network -we use the truncated VGG network and train it with cost sensitive loss only. Prediction is made via ordinal classification and the aggregation rule presented by <ref type="bibr" target="#b0">[1]</ref>. We feed this network with increasing MegaFace training data from 0% to 100%.</p><p>To report results on Adience, the method first estimates the exact age and then assigns the estimation to the nearest age group defined. It is evident from <ref type="table" target="#tab_0">Table 1</ref> that annotations of MegaAge is of high quality ones and they can improve the deep learning based method with a considerable margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with Existing Methods and Ablation Study</head><p>Results on Adience and MegaAge. In this experiment we compared our method with stateof-the-art methods [2, 3, 14] on both the Adience and MegaAge datasets. For the Cumulative Attribute method <ref type="bibr" target="#b1">[2]</ref>, we retrained the method using VGG face verification features trained on MS-Celeb-1M. For Deep CNN <ref type="bibr" target="#b13">[14]</ref>, we reported the results given in the original paper. In addition to the aforementioned comparisons, we also evaluated the importance of different losses used in the training of our network: 1) w/o L KL -we removed the KL divergence loss and used only the cost sensitive loss. In this setting, the age estimation is given by aggregating the ordinal classifications of the ordinal hyperplane module following <ref type="bibr" target="#b0">[1]</ref>. Note that one will need actual age labels for using the cost sensitive loss. To enable us for training our network with MegaAge, we used MAP estimate, the mode of the ground-truth distribution, as an approximation to the actual age ground-truth.</p><p>2) w/o L hyper -we discarded the cost sensitive loss and used only the KL-divergence loss. 3) Full model -our full method with both the L KL and L hyper . Results in <ref type="table" target="#tab_5">Table 5</ref> show that the performance of our network drops considerably without either of the cost sensitive loss, L Hyper , or the KL divergence loss, L KL , suggesting the complementary nature and importance of these two losses. As shown earlier in <ref type="figure">Fig. 1(b)</ref>, the KL divergence loss actually helps the ordinal hyperplane module to produce smoother classifications. The proposed full method outperforms state-of-the-art Deep CNN <ref type="bibr" target="#b13">[14]</ref> and Cumulative Attribute (re-trained with deep features) approaches <ref type="bibr" target="#b1">[2]</ref>. Qualitative results on Adience are depicted in <ref type="figure">Fig. 6</ref>. Results on MORPH2. Here we provide additional results on MORPH2, despite the performance on it were long saturated. We use the same setting following <ref type="bibr" target="#b19">[20]</ref>. The following baselines are tested: 1) DEX w/o IMDB-WIKI <ref type="bibr" target="#b19">[20]</ref>: A VGG-16 Net pretrained on Ima-geNet 2) DEX w/ IMDB-WIKI <ref type="bibr" target="#b19">[20]</ref>: The same model as 1) but the ImageNet pre-trained model is further pretrained on a large-scale age dataset, named as IMDB-WIKI, which contains 523,051 images crawled from IMDb and Wikipedia. 3) Ours w/o IMDB-WIKI: our full model with mixed loss, i.e., cost sensitive loss and KL divergence loss. The model is pre-trained on a face verification task. 4) Ours w/ IMDB-WIKI: a VGG-16 network which is the same as 2) but it employs the proposed mixed loss. The model was pre-trained on IMDB-WIKI. x <ref type="figure">Figure 6</ref>: a) We illustrate the predictions of our full model on the Adience dataset. The box plot shows the median, first/third quartiles, and min/max on actual age predictions, and how they fall into different age groups defined by Adience. (b) Some qualitative results are shown. Underscored images are failure cases (under the 1-off evaluation metric), which are caused by heavy occlusion and wrong faces detected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel annotation scheme for facial age database. Instead of labelling actual age or age range as practiced in existing studies, we label each face with age posterior distribution to better capture the uncertainty and ordinal information of age labels. The distribution can be conveniently and reliably obtained through a small number of comparisons between a query face with reference faces from another database. Experimental results suggest the good quality of the annotations. It is noteworthy that the proposed annotation scheme is not limited to facial age labelling, it is suitable for other subjective labelling targets, e.g., degree of continuous facial expression change. In this study, we also introduced a way to train a deep network directly using age posterior ground-truths via the KL divergence loss. The loss is found to be complementary with the commonly applied cost sensitive loss. Acknowledgement: This work is supported by SenseTime Group Limited and the General Research Fund sponsored by the Research Grants Council of the Hong Kong SAR (CUHK 14224316).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Reference</head><p>We also wish to acknowledge a prior work <ref type="bibr" target="#b3">[4]</ref>, which we were not aware the time of preparation and submission of this paper. The paper <ref type="bibr" target="#b3">[4]</ref> proposes posterior distribution learning with deep network for age estimation, head pose estimation, multi-label classification, and semantic segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1? f k (I)) instead of linear one f k (I)P(E k = 1|a) +(1 ? f k (I))P(E k = 0|a). If we take log on both sides of Eq. (4), the equation turns to log P(a|I) = ? log Z + log P(a) + ? k ( f k (I) log P(E k = 1|a) + (1 ? f k (I)) log P(E k = 0|a)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>We first train a deep model with the training partition of MORPH2 and Adience (indicated by '0%'). We then gradually add MegaAge training samples to the initial training pool to boost the model's performance.?  3.83 53.92 ? 4.78 54.52 ? 3.74 Adience (1-off) 88.18 ? 3.26 91.29 ? 2.64 91.29 ? 1.63</figDesc><table><row><cell>Percentage of MegaAge Added</cell><cell>0%</cell><cell>50%</cell><cell>100%</cell></row><row><cell cols="2">Adience (Exact) 53.43 MegaAge (CA(3)) 39.70</cell><cell>52.27</cell><cell>63.09</cell></row><row><cell>MegaAge (CA(5))</cell><cell>53.26</cell><cell>76.48</cell><cell>80.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>A comparison with existing methods and ablation study. The Cumulative Attribute<ref type="bibr" target="#b1">[2]</ref> method was re-trained with VGG face verification features.</figDesc><table><row><cell>Test Data</cell><cell>Adience</cell><cell></cell><cell cols="2">MegaAge</cell></row><row><cell>Metric</cell><cell>Exact</cell><cell>1-off</cell><cell cols="2">CA(3) CA(5)</cell></row><row><cell>Existing methods:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LBP+FPLBP+Dropout 0.8 [3] Deep CNN [14] Cumulative Attribute [2] Proposed method and variants:</cell><cell cols="3">--52.34 ? 3.72 89.34 ? 1.79 64.21 45.10 ? 2.60 79.50 ? 1.40 50.70 ? 5.10 84.70 ? 2.20</cell><cell>--81.44</cell></row><row><cell>w/o L hyper w/o L KL Full model</cell><cell cols="3">49.46 ? 5.51 82.99 ? 3.74 59.44 54.52 ? 3.74 91.29 ? 1.63 63.09 56.01 ? 4.41 91.42 ? 2.15 64.51</cell><cell>77.19 80.79 82.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>summarises our result on MORPH2. With the additional supervision using the age posterior distribution by KL-divergence loss, we reduced MAE on MORPH2 dataset by 0.16, when both DEX and our model are trained with IMDB-WIKI. The improvement is significant considering that MORPH2 is a rather mature benchmark with saturated performance.</figDesc><table><row><cell>G1</cell><cell>0-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G2</cell><cell>4-6</cell><cell></cell><cell>G1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>G3</cell><cell>8-13</cell><cell></cell><cell></cell><cell>0.7</cell><cell>2.0</cell><cell>2.4</cell><cell>23.7</cell></row><row><cell>G4</cell><cell>15-20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G5</cell><cell>25-32</cell><cell>G5</cell><cell></cell><cell></cell><cell>G3</cell><cell></cell></row><row><cell>G6</cell><cell>38-43</cell><cell>33.1</cell><cell>11.7</cell><cell>66.8</cell><cell></cell><cell>3.6</cell><cell>11.3</cell></row><row><cell>G7</cell><cell>48-53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G8</cell><cell>60-100</cell><cell></cell><cell>G6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Predicted Age (a)</cell><cell></cell><cell>(b)</cell><cell>6.1</cell><cell>31.5</cell><cell>41.9</cell><cell>48.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparative results based on mean absolute error (MAE). Results are migrated from<ref type="bibr" target="#b19">[20]</ref>. The proposed method achieves the state-of-the-art performance on MORPH2 (* indicates different data split).</figDesc><table><row><cell>Method</cell><cell>MORPH2[19]</cell><cell>Method</cell><cell>MORPH2[19]</cell></row><row><cell>Human workers[10]</cell><cell>6.30</cell><cell>DIF[10]</cell><cell>3.80</cell></row><row><cell>AGES[5]</cell><cell>8.83</cell><cell>MTWGP[26]</cell><cell>6.28</cell></row><row><cell>CA-SVR[2]</cell><cell>5.88</cell><cell>SVR[8]</cell><cell>5.77</cell></row><row><cell>OHRank[1]</cell><cell>5.69</cell><cell>DLA[23]</cell><cell>4.77</cell></row><row><cell>Huerta et al. [12] Guo et al. [7]</cell><cell>4.25  *  3.92  *</cell><cell>Guo et al. [6] Yi et al. [25]</cell><cell>4.18  *  3.63  *</cell></row><row><cell>Rothe et al. [21]</cell><cell>3.45</cell><cell>Niu et al. [17]</cell><cell>3.27</cell></row><row><cell>DEX w/o IMDB-WIKI [20]</cell><cell>3.25</cell><cell>DEX w/ IMDB-WIKI [20]</cell><cell>2.68</cell></row><row><cell>Ours w/o IMDB-WIKI</cell><cell>2.87</cell><cell>Ours w/ IMDB-WIKI</cell><cell>2.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>We first train a deep model with the training partition of MORPH2 and Adience (indicated by '0%'). We then gradually add MegaAge (or MegaAge-Asian) training samples to the initial training pool to boost the model's performance.Exact) 53.43 ? 3.83 53.92 ? 4.78 54.52 ? 3.74 Adience (1-off) 88.18 ? 3.26 91.29 ? 2.64 91.29 ? 1.63</figDesc><table><row><cell>Percentage of MegaAge Added</cell><cell>0%</cell><cell></cell><cell>50%</cell><cell>100%</cell></row><row><cell>Adience (MegaAge (CA(3))</cell><cell>28.82</cell><cell></cell><cell>35.37</cell><cell>38.69</cell></row><row><cell>MegaAge (CA(5))</cell><cell>42.25</cell><cell></cell><cell>54.03</cell><cell>57.90</cell></row><row><cell>MegaAge (CA(7))</cell><cell>54.61</cell><cell></cell><cell>69.66</cell><cell>73.15</cell></row><row><cell cols="2">Percentage of MegaAge-Asian Added</cell><cell>0%</cell><cell>50%</cell><cell>100%</cell></row><row><cell>MegaAge-Asian (CA(3))</cell><cell></cell><cell cols="3">39.65 61.57 62.08</cell></row><row><cell>MegaAge-Asian (CA(5))</cell><cell></cell><cell cols="3">53.23 78.38 80.43</cell></row><row><cell>MegaAge-Asian (CA(7))</cell><cell></cell><cell cols="3">63.78 89.07 90.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>A comparison with existing methods and ablation study. The Cumulative Attribute<ref type="bibr" target="#b1">[2]</ref> method was re-trained with VGG face verification features.</figDesc><table><row><cell>Test Data</cell><cell cols="2">Adience</cell><cell cols="3">MegaAge-Asian</cell><cell></cell><cell>MegaAge</cell></row><row><cell>Metric</cell><cell>Exact</cell><cell>1-off</cell><cell cols="6">CA(3) CA(5) CA(7) CA(3) CA(5) CA(7)</cell></row><row><cell>Existing methods:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LBP+FPLBP+Dropout 0.8 [3]</cell><cell cols="2">45.10 79.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep CNN [14]</cell><cell cols="2">50.70 84.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Cumulative Attribute [2]</cell><cell cols="2">52.34 89.34</cell><cell>63.19</cell><cell>80.43</cell><cell>90.57</cell><cell>35.17</cell><cell>52.60</cell><cell>66.80</cell></row><row><cell>Proposed method and variants:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o L hyper</cell><cell cols="2">49.46 82.99</cell><cell>60.94</cell><cell>77.57</cell><cell>88.24</cell><cell>35.62</cell><cell>52.52</cell><cell>66.30</cell></row><row><cell>w/o L KL</cell><cell cols="2">54.52 91.29</cell><cell>62.08</cell><cell>80.43</cell><cell>90.42</cell><cell>38.69</cell><cell>57.90</cell><cell>73.15</cell></row><row><cell>Full model</cell><cell cols="2">56.01 91.42</cell><cell>64.23</cell><cell>82.15</cell><cell>90.80</cell><cell>41.17</cell><cell>58.37</cell><cell>72.31</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">On MORPH, we achieved mean absolute error of 2.87, compared to 3.27 reported in<ref type="bibr" target="#b16">[17]</ref>. On Adience, we achieved 56.01% exact accuracy compared to 50.70% reported in<ref type="bibr" target="#b13">[14]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://megaface.cs.washington.edu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">ZHANG ET AL.: QUANTIFYING FACIAL AGE BY POSTERIOR OF AGE COMPARISONS</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the accepted paper, we mistakenly reported results based on our private dataset that only consists of face images of Asian. The results in <ref type="table">Table 1 and Table 2</ref> thus do not reflect the actual results based on the proposed MegaAge dataset. We have re-run all the experiments and now provide the revised results based on MegaAge. We apologise for any inconvenience caused. The MegaAge dataset can be downloaded from our project page http://mmlab. ie.cuhk.edu.hk/projects/MegaAge/ and http://github.com/zyx2012/ Age_estimation_BMVC2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We describe the statistics and differences between the 'MegaAge' dataset and the private 'MegaAge-Asian' dataset.</p><p>1) MegaAge -as described in Sec. 2.3 of the accepted paper, the dataset contains 41, 941 images encompassing ages from 0 to 70. We reserve 8, 530 images as test data. Each sample was labelled with age posterior as the ground-truth.</p><p>2) MegaAge-Asian -this private dataset contains 40, 000 images encompassing ages from 0 to 70. We reserve 3, 945 images as test data. Each sample was labelled with age posterior as the ground-truth. The source of this dataset is much more controlled, and it consists only of Asian faces. Thus the results yielded are better than those obtained by using MegaAge.</p><p>Revised <ref type="table">Table 1</ref> Evaluation Metric: As describe in the main paper, we employ cumulative accuracy as our metric, that is, CA(n) = K n /K ? 100, where K n is the number of test images whose absolute estimated error is smaller than n. To align with the results of Adience, of which the range of each age group is around 5, we report CA(3), CA <ref type="bibr" target="#b4">(5)</ref> and CA <ref type="bibr" target="#b6">(7)</ref> in our experiments. Results: In the original submission of our paper, we mistakenly presented the results on MegaAge-Asian as MegaAge, we now report them separately in <ref type="table">Table 4</ref>. Here we use a variant of the proposed network -we use the truncated VGG network and train it with cost sensitive loss only. Prediction is made via ordinal classification and the aggregation rule presented by <ref type="bibr" target="#b0">[1]</ref>. We feed this network with increasing MegaAge training data from 0% to 100%. The conclusion made in our original paper remains.</p><p>Revised <ref type="table">Table 2</ref> In this experiment we compared our method with state-of-the-art methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref> on both the Adience and MegaAge datasets. For the Cumulative Attribute method <ref type="bibr" target="#b1">[2]</ref>, we retrained the method using VGG face verification features trained on MS-Celeb-1M. For Deep CNN <ref type="bibr" target="#b13">[14]</ref>, we reported the results given in the original paper. In addition to the aforementioned comparisons, we also evaluated the importance of different losses used in the training of our network. The details can be found in Sec. 4.2 in the original paper. We provide the revised results in <ref type="table">Table 5</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ordinal hyperplanes ranker with cost sensitivities for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ping</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic age estimation based on facial aging patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2234" to="2240" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simultaneous dimensionality reduction and human age estimation via kernel partial least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowang</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A framework for joint estimation of age, gender and ethnicity on a large database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowang</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="761" to="770" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-based human age estimation by manifold learning and locally adjusted robust regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1178" to="1188" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Demographic estimation from face images: Human vs. machine performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1148" to="1161" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facial age estimation through the fusion of texture and local appearance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Huerta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Prati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="667" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The MegaFace benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning ordinal discriminative features for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03409</idno>
		<title level="m">Large-scale deep learning on the yfcc100m dataset</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ordinal regression with multiple output CNN for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An overview of research activities in facial age estimation using the FG-NET aging database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Panis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lanitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MORPH: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamirat</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Some like it hot-visual guidance for preference prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5553" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeply-learned feature for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="534" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic age estimation from face images via deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Fang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Yao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Age estimation by multi-scale convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task warped gaussian process for personalized age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2622" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
