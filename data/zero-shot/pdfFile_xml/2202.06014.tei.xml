<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghao</forename><surname>Zang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In video surveillance, pedestrian retrieval (also called person re-identification) is a critical task. This task aims to retrieve the pedestrian of interest from non-overlapping cameras. Recently, transformer-based models have achieved significant progress for this task. However, these models still suffer from ignoring fine-grained, part-informed information. This paper proposes a multi-direction and multi-scale Pyramid in Transformer (PiT) to solve this problem. In transformerbased architecture, each pedestrian image is split into many patches. Then, these patches are fed to transformer layers to obtain the feature representation of this image. To explore the fine-grained information, this paper proposes to apply vertical division and horizontal division on these patches to generate different-direction human parts. These parts provide more finegrained information. To fuse multi-scale feature representation, this paper presents a pyramid structure containing global-level information and many pieces of local-level information from different scales. The feature pyramids of all the pedestrian images from the same video are fused to form the final multi-direction and multi-scale feature representation. Experimental results on two challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed PiT achieves state-of-the-art performance. Extensive ablation studies demonstrate the superiority of the proposed pyramid structure. The code is available at https://git.openi.org.cn/zangxh/PiT.git.</p><p>Index Terms-video-based pedestrian retrieval, vision transformer, multi-direction and multi-scale pyramid.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P EDESTRIAN retrieval is a critical task in intelligent surveillance <ref type="bibr" target="#b0">[1]</ref> [2] <ref type="bibr" target="#b2">[3]</ref>. Given a pedestrian image as the query, pedestrian retrieval aims to find the right images in a large gallery. The query image and the matched gallery images must be from different cameras. Pedestrian retrieval has important practical applications in both society and industry, such as finding the criminal suspects and tracking pedestrian movement. Compared to the image-based pedestrian retrieval, the video-based one can provide much more gait and view information of pedestrians and alleviate the negative effects of occlusion situations. Therefore, video-based pedestrian retrieval is getting more and more attention from researchers <ref type="bibr" target="#b3">[4]</ref>.</p><p>For CNN-based pedestrian retrieval, dividing the feature map into multiple horizontal stripes and individually training each one are general operations. After the training is complete, all the stripe features are assembled to generate the convolution descriptor for each image, which provides the model a rich feature representation <ref type="bibr" target="#b4">[5]</ref>. Based on the horizontal division strategy, a pyramid of stripes is proposed to exploit the partial information of each pedestrian <ref type="bibr" target="#b5">[6]</ref>. Although these methods above improve the model performance, the direction of division strategy is limited.</p><formula xml:id="formula_0">D ? h D ? p D ?</formula><p>Recently, the transformer structure has achieved incredible progress in computer vision. The transformer structure is a popular model in natural language processing (NLP) and can handle the sequence data effectively. In computer vision, the input image is split into many patches. These patches are regarded as tokens similar to the words in the NLP task. A sequence of feature embedding of these patches is fed to the transformer layers. With the help of the multi-head self-attention module, the transformer can obtain global-level relationships among all the patches without losing information. Meanwhile, the CNN convolution kernel can only perceive limited scope, and the down-sampling operation inevitably loses much information. Moreover, the transformer structure has achieved competitive performance compared with CNN <ref type="bibr" target="#b6">[7]</ref>. Although the transformer has a global perception, the fine-grained information may be neglected, which results in a limited performance. This paper proposes a multi-direction and multi-scale Pyra-0000-0000/00$00.00 ? 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. mid in Transformer (PiT) for video pedestrian retrieval, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The pyramid contains four layers, which adopt "no division", vertical, horizontal, and patch-based division strategies, respectively. The first layer includes a globallevel feature of the pedestrian image. The second, third, and fourth layers contain D v , D h , and D p part-level features. In this way, the proposed PiT applies multi-direction division strategies in the pedestrian image and extracts a multi-scale feature representation for each pedestrian image. Concretely, each pedestrian image is split into many patches. A class token and the feature embeddings of all patches are flattened and fed to multiple transformer layers. Then the processed patch tokens are rearranged into a twodimension structure according to their original positions. Different division strategies are applied to this two-dimension structure, which generates different-direction parts. The class token and patch tokens within the same part are flattened to form a new token sequence. After the process of the last transformer layer, the class token learns the multi-direction partinformed information. A feature pyramid for each pedestrian image is obtained by combining all the features with different scales from different layers. The corresponding features of all the images within the same video are fused to generate the final multi-direction and multi-scale feature pyramid.</p><p>The traditional CNN-based methods usually apply horizontal division to feature map <ref type="bibr" target="#b4">[5]</ref>, which is reasonable because each horizontal stripe usually contains the head, torso, or legs. However, the vertical division can divide the human body into the right limb, head and torso, and the left limb, which introduces part-informed clues with more dimensions. Applying vertical and horizontal division simultaneously, which forms the patch-based division, can also provide more finegrained information. Combining these multi-direction and multi-scale features can effectively improve the model performance. Experiments on two challenging video-based benchmarks, MARS <ref type="bibr" target="#b7">[8]</ref> and iLIDS-VID <ref type="bibr" target="#b9">[9]</ref>, show the proposed PiT achieves state-of-the-art performance. Extensive ablation studies also demonstrate the superiority of the proposed pyramid structure.</p><p>The main contributions of this paper can be summarized as follows:</p><p>? Multi-direction: the proposed vertical and horizontal division strategies in transformer introduce fine-grained, part-informed information from different directions. ? Multi-scale: the global and local-level features with different scales form a feature pyramid. This multi-scale combination makes the feature representation rich and discriminative. ? Performance: the proposed PiT achieves state-of-theart performance on two challenging video-based benchmarks, and extensive ablation studies demonstrate the superiority of the proposed multi-direction and multiscale pyramid structure. The rest of this paper is organized as follows: the related works are reviewed and analyzed in Section II, and then the proposed method is introduced in Section III. Experimental results and analysis are presented in Section IV, and Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video-based Pedestrian Retrieval</head><p>An early method for image-based pedestrian retrieval fused various features, such as RGB, HSV, HoG, LOMO, etc., to achieve the multi-feature fusion and overcome the challenge from pedestrian appearance changes <ref type="bibr" target="#b10">[10]</ref>. Whereas videobased pedestrian retrieval has multiple images for each pedestrian. These consecutive pedestrian images can provide abundant temporal and spatial information, which can alleviate the negative effects of appearance change, occlusion, pose variation, etc <ref type="bibr" target="#b11">[11]</ref>. Therefore, existing methods focus on exploiting both spatial and temporal clues from pedestrian video. GRL <ref type="bibr" target="#b12">[12]</ref> employ video-level features to guide the generation of correlation map and disentangle the frame-level features into high-correlation and low-correlation features. BiCnet-TKS <ref type="bibr" target="#b13">[13]</ref> introduced a bilateral complementary network to mine the divergent body parts of each pedestrian and proposed a temporal kernel selection module to explore temporal relations adaptively. CTL <ref type="bibr" target="#b14">[14]</ref> employed a key-point estimator to extract multi-scale semantic features to form a topology graph. Then a 3D graph convolution is used to capture hierarchical spatial and temporal dependencies. Besides, AGW + [15] employed a frame-level average pooling for video feature representation, which is simple but effective.</p><p>These methods above mainly utilized CNN to extract spatial and temporal clues. However, the CNN convolution kernel cannot capture long-range relationships, and the CNN downsampling operation results in inevitable information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vision Transformer</head><p>Recently, transformer architecture has become a de-facto standard for natural language processing. ViT <ref type="bibr" target="#b6">[7]</ref> introduced this architecture to computer vision and achieved better performance than many state-of-the-art methods on the image classification task. Following this improvement, many works were proposed to improve the performance of the transformerbased framework. For the video-based classification task, Swin transformer <ref type="bibr" target="#b16">[16]</ref> proposed a hierarchical structure and utilized shifted windows to solve the non-overlapping patch division problem. A 3D shifted window is also proposed to preprocess video data.</p><p>These methods above demonstrated the transformer structure could perform well for the video classification task. However, the video-based pedestrian retrieval is very different from the video classification task. The video-based pedestrian retrieval task depends highly on appearance information rather than motion information. Therefore, the transformer structure with the ability to perceive more fine-grained information is needed for the video-based pedestrian retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Division Strategies for Pedestrian Retrieval</head><p>For the pedestrian retrieval task, dividing the feature map into multiple stripes is a typical operation. PCB <ref type="bibr" target="#b4">[5]</ref> proposed to horizontally divide the feature map into multiple stripes and achieved significant performance improvement compared with the original model. SPP <ref type="bibr" target="#b17">[17]</ref> divided the feature map into equal patches and assembled the multi-scale feature patches into a pyramid structure. HPP <ref type="bibr" target="#b5">[6]</ref> employed the horizontal division to form a multi-scale pyramid and improved the model performance for this task.</p><p>Although the models above utilized different division strategies, performance comparison among different division strategies in a unified framework has not been conducted, thus the differences between these strategies remain to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTI-DIRECTION AND MULTI-SCALE PYRAMID IN TRANSFORMER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transformer-based Framework</head><p>The Vision Transformer (ViT) <ref type="bibr" target="#b6">[7]</ref> is employed to construct the framework, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Given some pedestrian videos {V 1 , V 2 , ? ? ? } and pedestrian IDs {y 1 , y 2 , ? ? ? }, each video V contains K pedestrian images I, as V = {I 1 , I 2 , ? ? ? , I K }. A convolution layer is used to embed the pedestrian image into multiple feature embeddings. Concretely, after applying convolution operation on the pedestrian image, a feature map f ? R h?w?c is obtained. Then the feature map is flattened to generate N features, where N = h ? w and the size of each feature is 1 ? c. In this way, each feature can be treated as the feature embedding of each image patch, and the size of each image patch is the same as convolution kernel size k. The convolution stride s determines the interval of the adjacent image patches.</p><p>The feature embedding of each image patch from the image I is also called patch token p. A class token ? cls with the size of 1 ? c is also introduced to represent the feature embedding of the whole image. After flattening class and patch tokens into a sequence {? cls ; p 1 ; . . . ; p N }, the patch tokens lose the location information in original pedestrian image. Therefore, a position embedding ? pos ? R (N +1)?c is employed to retain this location information. A camera embedding? view is also introduced to keep the camera information. Since the class token and all the patch tokens belong to the same camera, the size of? view is set to 1 ? c. Then? view is copied N + 1 times to form a new embedding ? view ? R (N +1)?c . After these operations, the token sequence z 0 from the pedestrian image I is calculated as follows,</p><formula xml:id="formula_1">z 0 = [? cls ; p 1 ; . . . ; p N ] + ? 1 ? pos + ? 2 ? view = [? 0 cls ; p 0 1 ; ? ? ? ; p 0 N ],<label>(1)</label></formula><p>where ? 1 and ? 2 are trade-off parameters. Then m transformer layers are employed to learn the relationship between the class token and all patch tokens. Each transformer layer is composed of a Multi-head Self-Attention (MSA) block and a Multi-Layer Perception (MLP) block. A LayerNorm (LN) layer is applied before MSA and MLP blocks, and a shortcut connection is also employed as follows,</p><formula xml:id="formula_2">z = z d?1 + MSA(LN(z d?1 )), z d = z + MLP(LN(z )).<label>(2)</label></formula><p>After m transformer layers, the vector sequence z m is obtained.</p><formula xml:id="formula_3">z m = [? m cls ; p m 1 ; p m 2 ; ? ? ? ; p m N ] := [? m cls ; P m ].<label>(3)</label></formula><p>In Eq. 3, N patch tokens {p m i } N i=1 are denoted as P m ,</p><formula xml:id="formula_4">P m = [p m 1 ; p m 2 ; ? ? ? ; p m N ].<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-direction and Multi-scale Pyramid</head><p>To explore the fine-grained, part-informed information, the patch tokens P m are rearranged into a new form P according to their original positions in pedestrian image I,</p><formula xml:id="formula_5">[p m 1 ; p m 2 ; ? ? ? ; p m N ] P m ?R N ?c rearrange ? ???? ? ? ? ? ? ? p m 1 p m 2 ? ? ? p m w p m w+1 p m w+2 ? ? ? p m 2w . . . . . . . . . . . . ? ? ? ? ? ? ? ? ? p m N ? ? ? ? ? P?R h?w?c . (5)</formula><p>The rearranged patch tokens P have the same size as the feature map f ? R h?w?c . We learn from the division strategies in Convolution Neural Network (CNN) and apply similar strategies to patch tokens P.</p><p>1) Multi-direction Division Strategies: The rearranged tokens P are copied four times. Multi-direction division strategies are applied to these four copies.</p><p>For the first copy, "no division" is applied to the tokes P. Then the class token ? m cls along with P are flattened to a new token sequence z global ? R (N +1)?c , which equals z m .</p><formula xml:id="formula_6">z global = [? m cls ; p m 1 ; p m 2 ; ? ? ? ; p m N ] ? z m .<label>(6)</label></formula><p>The transformer layer L g receives this sequence and outputs a new sequence z global ? R (N +1)?c . We follow the general operation in [7] <ref type="bibr" target="#b16">[16]</ref>, which discard all the patch tokens and only keep the class token for the following operations. Therefore, only the class token ? global cls ? R 1?c is kept as the feature representation of the whole pedestrian image I.</p><p>For the second copy, "vertical division" is applied to the patch tokens P along the vertical direction to generate D v parts. Each part has N/D v patch tokens. The class token is copied D v times, and each one is assigned to one part. Then the class token and all the patch tokens in the corresponding part are flattened along the vertical direction to form a new vector sequence z vertical ? R (N/Dv+1)?c . There are D v sequences in total. For example, the first sequence z vertical,1 is shown as follows,</p><formula xml:id="formula_7">z vertical,1 = [? m cls ; p m 1 ; p m w+1 ; p m 2w+1 ; ? ? ? ].<label>(7)</label></formula><p>Each sequence z vertical,i is followed by the parameter-sharing transformer layer L v . In this way, the relationship between the class token and a specific vertical part is explored. After the process of the transformer layer L v , D v class tokens {? vertical cls,i } Dv i=1 are kept and denoted as ? vertical cls . Moreover, this paper first proposes the "vertical division" strategy, which extracts fine-grained feature representation and also introduces significant performance improvement.</p><p>For the third copy, "horizontal division" is applied by dividing the patch tokens P into D h parts. Horizontal division strategy is applied along the horizontal direction, and each part has N/D h patch tokens. After the division operation, the class token is copied D h times. Each one and its corresponding patch tokens are flattened along the horizontal direction to form the token sequence z horizontal ? R (N/D h +1)?c . The first sequence z horizontal,1 is shown below as an example, The "horizontal division" is proposed for CNN in previous work <ref type="bibr" target="#b5">[6]</ref>. However, we propose a manner to apply this strategy to the new structure, i.e., vision transformer, which is proven to be effective.</p><formula xml:id="formula_8">z horizontal,1 = [? m cls ; p m 1 ; p m 2 ; p m 3 ; ? ? ? ; p m N/D h ].<label>(8)</label></formula><p>For the fourth copy, the vertical and horizontal division strategies are applied simultaneously to the rearranged patch tokens P to form "patch-based division". Each part has N/D p patch tokens and has a more fine-grained receptive field. After the division operation, the class token and all patch tokens in the corresponding part are flattened along the horizontal direction to form D p token sequences z patch ? R (N/Dp+1)?c , where D p = D v ? D h . The first sequence z patch,1 is shown below as an example,</p><formula xml:id="formula_9">z patch,1 = [? m cls ; p m 1 ; p m 2 ; ? ? ? ; p m N/Dv ; p m w+1 ; p m w+2 ; ? ? ? ].<label>(9)</label></formula><p>Each token sequence z patch,i is followed by the transformer layers L p , which generate D p class tokens {? patch cls,i } Dp i=1 . These class tokens are denoted as ? patch cls . A similar patch-based division strategy was proposed in CNN structure <ref type="bibr" target="#b17">[17]</ref>. However, this paper proposes multi-direction division strategies, which are very different from the previous model.</p><p>After the transformer layer, each class token has the perception within its corresponding global/vertical/horizontal/patchbased part, making it obtain more fine-grained local information.</p><p>2) Multi-scale Pyramid Structure: The rearranged P is divided using different scales, which generates multi-scale feature representations. These features are concatenated to form a pyramid structure z ? ? R (1+Dv+D h +Dp)?c for each pedestrian image I as follows,</p><formula xml:id="formula_10">z ? = [? global cls ; ? vertical cls ; ? horizontal cls ; ? patch cls ],<label>(10)</label></formula><p>where ? global cls , ? vertical cls , ? horizontal cls , and ? patch cls are in the space of R 1?c , R Dv?c , R D h ?c , and R Dp?c , respectively.</p><p>There are K images in each pedestrian video V , and each image is represented by z ? . The feature pyramidz ? of the video V is generated as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Each feature of the video V is the corresponding average feature within it. For example, the class token? global cls is calculated as follows,</p><formula xml:id="formula_11">? global cls = K k=1 ? global cls,k .<label>(11)</label></formula><p>The feature pyramidz ? ? R (1+Dv+D h +Dp)?c of video V is expressed as follow,</p><formula xml:id="formula_12">z ? = [? global cls ;? vertical cls ;? horizontal cls ;? patch cls ].<label>(12)</label></formula><p>The CNN-based methods usually design various fusion strategies to combine pedestrian image features within the same video. The feature map from CNN contains rich spatial and temporal information, and a sophisticated fusion strategy can effectively combine these features. However, in a transformer-based framework, the class token is not generated from the input image directly. In other words, these class tokens do not contain spatial and temporal information explicitly. Therefore, each feature of the video V is obtained in an average manner as Eq. 11.</p><p>The multi-scale pyramid structurez ? ? R (1+Dv+D h +Dp)?c contains part-informed information of different scales, which is more rich and discriminative than the original global-level feature representation. The ablation study in the latter section demonstrates the superiority of this multi-scale pyramid structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Training and Testing</head><p>This section describes the details of model training and testing. To simplify the expression, the class token ? ? cls ? R 1?c is used to represent each element inz ? . Each class token ? ? cls is followed by a BatchNorm layer and a classifier layer to generate the final probability p ? , as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. There are 1+D v +D h +D p classifier layers to train each token ? ? cls individually. The classification loss L cls and triplet loss L tri are used to supervise this model by averaging the losses of these independent branches.</p><formula xml:id="formula_13">L cls = ? 1 T N c T i=1 Nc j=1 y j log p ? i,j ,<label>(13)</label></formula><formula xml:id="formula_14">L tri = 1 BT T i=1 a?bi ln{1+ exp[max d(? ? i,a , ? ? i,p ) ? min d(? ? i,a , ? ? i,n )]},<label>(14)</label></formula><p>where T =1+D v +D h +D p , N c is the class number for a specific benchmark, b i represents the i th mini-batch, B is the number of pedestrian images in this mini-batch, a, p, n are anchor, positive, negative samples, respectively. Function d(?)</p><p>calculates the Euclidean distance between two features. The overall loss function L is calculated as follows,</p><formula xml:id="formula_15">L = L cls + L tri .<label>(15)</label></formula><p>In the testing process, the multi-direction and multi-scale feature pyramidz ? ? R (1+Dv+D h +Dp)?c is used to represent the feature representation of pedestrian video V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Experimental Setting 1) Benchmarks: The proposed Pyramid in Transformer (PiT) is evaluated in two challenging benchmarks: MARS <ref type="bibr" target="#b7">[8]</ref> and iLIDS-VID <ref type="bibr" target="#b9">[9]</ref>. There are also two other popular benchmarks: DukeMTMC-VideoReID and PRID2011. Existing methods <ref type="bibr" target="#b12">[12]</ref> [13] <ref type="bibr" target="#b34">[34]</ref> have achieved more than 0.95 in terms of Rank-1 metric on these two benchmarks. However, there is still much room for improvement on the challenging MARS and iLIDS-VID benchmarks.</p><p>? MARS is the largest video-based pedestrian retrieval benchmark and captured by six cameras on a university campus. It contains 20,478 videos from 1,261 identities. These videos are generated by employing the DPM detector and GMMCP tracker, which results in many videos with poor qualities. ? iLIDS-VID is captured by two cameras in an airport hall.</p><p>It contains 600 videos from 300 identities. This benchmark is very challenging due to pervasive background clutter, mutual occlusions, and lighting variations. 2) Evaluation Protocol and Metrics: For MARS benchmark, the standard training and testing split provided by <ref type="bibr" target="#b7">[8]</ref> is used for training the proposed PiT. The Cumulative Matching Characteristic (CMC) curve and mean Average Precision (mAP) are employed for evaluation. For iLIDS-VID benchmark, the whole set of videos is randomly divided into two halves. Then the trials are repeated ten times, and the CMC curve is used to evaluate the average results. For convenience, Rank-1, Rank-5, Rank-10, and Rank-20 are employed to represent the CMC curve.</p><p>3) Implementation Details: The proposed PiT is implemented using Pytorch. The transformer ViT-B16 <ref type="bibr" target="#b6">[7]</ref>, pretrained on ImageNet, is employed as the backbone. We follow the operation in <ref type="bibr" target="#b35">[35]</ref> to preprocess all the videos. Specifically, each video is divided equally into K snippets, where K equals 8. The first pedestrian image in each snippet is selected as the keyframe, and K images are used to represent this video. Each pedestrian image is resized to 256 ? 128. The batch size and parameter m are set to 16 and 11. The kernel size k and stride s of the convolution layer are set to 16 and 12. The dimension h ? w ? c of feature embedding outputted by convolution layer is 21?10?768. The trade-off parameters ? 1 and ? 2 are set to 1.0 and 1.5. The division parameters D v , D h , D p are 2, 3, and 6. The standard Stochastic Gradient Descent (SGD) with momentum and an initial learning rate of 0.01 is used to train these models 120 epochs for each benchmark. Cosine annealing is employed to schedule the learning rate. The convolution layer and transformer layers are frozen in the first five epochs to train the classifier layers. After these five epochs, the whole network is trained.  <ref type="table" target="#tab_0">Table I</ref> shows the comparison between the proposed PiT and twenty other state-of-the-art methods in terms of mAP score and CMC accuracy. These state-of-the-art methods are all within three years and employed ResNet50 as their backbone to explore the spatial and temporal information among pedestrian images. They used attribute information <ref type="bibr" target="#b18">[18]</ref> [24], attention mechanism [20] <ref type="bibr" target="#b27">[27]</ref>, graph convolution <ref type="bibr" target="#b28">[28]</ref> [29] <ref type="bibr" target="#b14">[14]</ref>, 3D convolution <ref type="bibr" target="#b32">[32]</ref>  <ref type="bibr" target="#b34">[34]</ref>, relation-guided models <ref type="bibr" target="#b22">[22]</ref> [23] <ref type="bibr" target="#b12">[12]</ref>, Generative Adversarial Networks (GAN) <ref type="bibr" target="#b30">[30]</ref>, and new network architectures <ref type="bibr" target="#b15">[15]</ref>   <ref type="bibr" target="#b13">[13]</ref>, respectively, to generate the feature representation of each pedestrian video. Meanwhile, the proposed PiT employs a transformer-based framework and utilizes the simple average fusion to obtain the multi-direction and multi-scale feature pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-art Methods</head><p>1) Performances on MARS: Compared with other stateof-the-art methods, the proposed PiT achieves the best mAP score and competitive CMC accuracy. The best competitor, CTL <ref type="bibr" target="#b14">[14]</ref>, utilized a key-points estimator to extract human body local features as graph nodes and achieved topology learning for video-based pedestrian retrieval. In comparison, the proposed PiT does not explore the relationship among different pedestrian images within the same video and reaches a better mAP value. This demonstrates the proposed feature pyramid containing more fine-grained local information has a better generalization performance.</p><p>2) Performances on iLIDS-VID: Compared with other methods, the proposed PiT achieves state-of-the-art performance. The best competitor, GRL <ref type="bibr" target="#b12">[12]</ref>, used global correlation estimation to disentangle features into high-correlation and low-correlation features. Then GRL proposed temporal reciprocating learning to enhance the high-correlation semantic clues and accumulate the low-correlation sub-critical clues for the final feature representation. Meanwhile, the proposed PiT has a concise network structure and achieves 1.67% performance improvement on the metric of Rank-1 than GRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>Different division strategies in a unified framework are compared in this section. In this paper, each image is split into 210 patches. They are then further divided into 2, 3, 5, 6, 7 parts. In other words, the optional values of parameters D v and D h are 2, 3, 5, 6, 7. To explicitly show the division details, 105?2, 70?3, 42?5, 35?6, and 30?7 are used to represent the division parameters using vertical division. 2?105, 3?70, 5?42, 6?35, and 7?30 are denoted as division parameters using horizontal division. Applying 105?2 vertical division and 3?70 horizontal division simultaneously forms a patchbased division and generates 6 parts. 105?2 and 7?30 generate 14 parts, and 42?5 and 3?70 generate 15 parts (i.e., the  <ref type="figure" target="#fig_0">6, 14, 15</ref>). These patchbased division strategies are denoted as 6p, 14p, 15p. "No division" is denoted as 1?210. 1) Effectiveness of Different-direction Division Strategies: This section compares different-direction division strategies in the transformer-based framework. The proposed pyramid with only one layer is used to compare the performances, as illustrated in <ref type="table" target="#tab_0">Table II</ref>. Compared with the baseline method, this table shows that using different-direction division strategies improves the performance. Two conclusions can be made. First, although horizontal division is a commonly used strategy, the vertical and patch-based division strategies can also improve performance effectively. Second, the best division strategy and the number of parts are different for different benchmarks. These conclusions demonstrate the need to adopt a strategy based on the practical scene.   <ref type="table" target="#tab_0">Table II.</ref> To allow for the performance analysis visually, the mAP scores of different division strategies on MARS benchmark are illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. In this figure, increasing the number of parts can improve the performance gradually. However, more number of parts also introduces more computation complexity. For different division strategies, dividing the patch tokens into six parts achieves a better trade-off between the performance and computation complexity. Therefore, the parameter of proposed PiT is 1?210 105?2 3?70 6p, and its fourth layer splits the patch tokens into six parts.</p><p>2) Effectiveness of Multi-scale Pyramid Structure: This section shows the performances of pyramid structures with different layers, as illustrated in <ref type="table" target="#tab_0">Table IV</ref>. In this table, the proposed pyramid with one layer only employs "no division". The proposed pyramid with two layers additionally uses the vertical division strategy. Then the horizontal division and patch-based division strategies are added one by one. Each layer contains part-informed information with different scales. As the number of layers increases, the performance improves gradually. These improvements demonstrate that fusing multiscale feature representations can improve performance effectively.</p><p>3) Effectiveness of Multi-direction Pyramid Structure: This section compares performances between multi-direction and single-direction pyramid structures, as illustrated in <ref type="table" target="#tab_0">Table III</ref>. All the pyramids in this table have four layers, and the differences between them are dependent upon which division strategy is employed. The type "Vertical Division" only employs vertical division strategies. The types "Horizontal Division" and "Patch-based Division" have the same meanings. For vertical and horizontal division strategies, the part numbers 2, 5, 7 are chosen to form the bottom three layers. For patchbased division, 6p, 14p, 15p are employed to form the bottom three layers.</p><p>Compared with other single-direction pyramids, the proposed PiT achieves the best performance. These comparisons demonstrate fusing multi-direction division strategies provides more improvement. On the other side, the type "Horizontal Division" performs better than the "Vertical Division". This shows the horizontal division strategy is more suitable for the pedestrian retrieval task. In conclusion, the proposed PiT fusing multi-direction and multi-scale feature representations is the best combination.  represents "no division" (1?210), vertical division (105?2), horizontal division (3?70), and patch-based division (6p) are employed.</p><p>As illustrated in <ref type="table" target="#tab_5">Table V</ref>, the proposed PiT with parameter 1?210 105?2 3?70 6p achieves the best performance. The other two pyramids introduce more fine-grained parts, yet their performances get poor. On the other side, the pyramid with the parameter 1?210 105?2 7?30 14p splits the patch tokens into more horizontal parts than using the parameter 1?210 42?5 3?70 15p and achieves better performance. Although their fourth layers have a close number of parts, more horizontal parts introduce more performance improvement.</p><p>5) Qualitative Analysis:</p><p>The retrieval examples are illustrated in <ref type="figure" target="#fig_7">Fig. 6</ref>. One pedestrian image is selected to represent the video for convenience, and the top eight retrieval results for each query are illustrated in this figure. We select the query pedestrian video from MARS benchmark according to the Average Precision (AP) value. <ref type="figure" target="#fig_7">Fig. 6(a)</ref> For the successful case in <ref type="figure" target="#fig_7">Fig. 6(a)(b)</ref>, the proposed PiT retrieves many correct videos in the gallery. In contrast, the baseline method retrieves many incorrect results, including a man in a blue shirt in <ref type="figure" target="#fig_7">Fig. 6</ref>(a) and a road sign in the second and third places in <ref type="figure" target="#fig_7">Fig. 6(b)</ref>. For the failed case in <ref type="figure" target="#fig_7">Fig. 6(d)</ref>, the baseline method puts the correct videos in the top two places. However, the proposed PiT also retrieves pedestrians with similar appearances. In <ref type="figure" target="#fig_7">Fig. 6</ref>, we employ the AP value to determine whether the proposed method is successful or not. For practical application, people usually look for the person of interest from the top-k results, not just the top-1 result. Therefore, the proposed PiT can be utilized effectively for <ref type="figure" target="#fig_7">Fig. 6</ref>(e)(f).</p><p>To explore the difference between the proposed PiT and the baseline method, the attention maps of the query pedestrian image are illustrated in <ref type="figure" target="#fig_6">Fig. 5</ref>. With the same input image, the proposed PiT and the baseline method have different attention maps. The baseline method cannot extract the fine-grained local features. Therefore, it cannot reduce the unfavorable impacts from the man in a blue shirt in <ref type="figure" target="#fig_6">Fig. 5</ref>(a) and the road sign in <ref type="figure" target="#fig_6">Fig. 5(b)</ref>. Therefore, more fine-grained feature representation can help the model recognize the pedestrian of interest.</p><p>6) Computation Complexity and Running Time:  For the MARS benchmark, the training and testing processes of the proposed PiT take 5.00 hours, including training 8,298 videos from 625 pedestrian IDs and testing another 11,310 videos for evaluation. For the iLIDS-VID benchmark, the experiments include ten trials to ensure statistical stability, and the total running time takes 10.70 hours. Each trial contains training 300 videos from 150 IDs and testing another 300 videos. In <ref type="table" target="#tab_0">Table VI</ref>, the proposed PiT has acceptable MACs, trainable parameters, and running time, which have the same order of magnitude as the baseline method. The superiority of the proposed method can be seen from the performance improvement in terms of the Rank-1 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes a multi-direction and multi-scale Pyramid in Transformer (PiT) for video-based pedestrian retrieval. The proposed PiT contains four layers, and each layer applies different division strategies on the patch tokens to generate different-direction parts. The class token and the patch tokens in each generated part are fed to the corresponding transformer layer. In this way, the class token perceives the fine-grained, part-informed features. Then multi-direction and multi-scale features are combined to form a feature pyramid for each pedestrian image. The feature pyramids of pedestrian images </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Multi-direction and multi-scale pyramid in transformer for video-based pedestrian retrieval. Different layers employ different-direction division strategies. After the process of transformer layer, part-informed features are extracted. Each layer contains features with different scales. The four layers have 1, D v , D h , and D p features, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The proposed feature Pyramid in Transformer (PiT) for each pedestrian image. The feature pyramid with four layers contains multi-direction and multi-scale feature representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>All the sequences {z horizontal,i } D h i=1 are followed by the parameter-sharing transformer layers L h , and D h class tokens The generation and training process of the feature pyramid for each video. The corresponding features of each pedestrian image are averaged to generate the final feature, and each feature is trained individually. {? horizontal cls,i } D h i=1 are obtained and denoted as ? horizontal cls .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>The mAP score changes of different-direction division strategies on MARS benchmark. The values in this figure follow the data in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(b)(c)(d) show the successful cases where the proposed PiT has a better AP than the baseline method, and Fig. 6(e)(f) show the failed cases where the baseline method performs better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Attention map examples. Three images in each group are the input image, the attention map of this image, and the product result between input image and its attention map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Retrieval examples. Each video is represented by one pedestrian image within it. The query video is selected from MARS benchmark, and the top eight retrieval results for each query are illustrated in this figure. (a)(b)(c)(d) show the successful cases, and (e)(f) show the failed cases. The correct results are in green boxes. belonging to the same video are fused to generate the final feature pyramid. Experimental results on two challenging benchmarks, MARS and iLIDS-VID, show the proposed PiT achieves state-of-the-art results. The comprehensive ablation studies demonstrate the superiority of the proposed multidirection and multi-scale pyramid structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Performance comparisons between proposed PiT and state-of-the-art methods on MARS and iLIDS-VID.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">MARS</cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID</cell></row><row><cell>Methods</cell><cell>Venue</cell><cell cols="4">Rank-1 Rank-5 Rank-10 mAP</cell><cell cols="4">Rank-1 Rank-5 Rank-10 Rank-20</cell></row><row><cell>ADFD [18]</cell><cell>CVPR2019</cell><cell>87.00</cell><cell>95.40</cell><cell></cell><cell cols="2">78.20 86.30</cell><cell>97.40</cell><cell></cell><cell>99.70</cell></row><row><cell>GLTR [19]</cell><cell>ICCV2019</cell><cell>87.02</cell><cell>95.76</cell><cell></cell><cell cols="2">78.47 86.00</cell><cell>98.00</cell><cell></cell></row><row><cell>COSAM [20]</cell><cell>ICCV2019</cell><cell>84.90</cell><cell>95.50</cell><cell></cell><cell cols="2">79.90 79.60</cell><cell>95.30</cell><cell></cell></row><row><cell>AGW + [15]</cell><cell>TPAMI2020</cell><cell>87.60</cell><cell></cell><cell></cell><cell cols="2">83.00 83.20</cell><cell>98.30</cell><cell></cell></row><row><cell>RTF [21]</cell><cell>AAAI2020</cell><cell>87.10</cell><cell></cell><cell></cell><cell cols="2">85.20 87.70</cell><cell></cell><cell></cell></row><row><cell>FGRA [22]</cell><cell>AAAI2020</cell><cell>87.30</cell><cell>96.00</cell><cell></cell><cell cols="2">81.20 88.00</cell><cell>96.70</cell><cell>98.00</cell><cell>99.30</cell></row><row><cell>RGSATR [23]</cell><cell>AAAI2020</cell><cell>89.40</cell><cell>96.90</cell><cell></cell><cell cols="2">84.00 86.00</cell><cell>98.00</cell><cell></cell><cell>99.40</cell></row><row><cell>AMEM [24]</cell><cell>AAAI2020</cell><cell>86.70</cell><cell>94.00</cell><cell></cell><cell cols="2">79.30 87.20</cell><cell>97.70</cell><cell></cell><cell>99.50</cell></row><row><cell>CSTNet [25]</cell><cell>IJCAI2020</cell><cell>90.20</cell><cell>96.80</cell><cell></cell><cell cols="2">83.90 87.80</cell><cell>98.50</cell><cell></cell><cell>99.60</cell></row><row><cell>ASTA-Net [26]</cell><cell cols="2">ACM MM2020 90.40</cell><cell>97.00</cell><cell></cell><cell cols="2">84.10 88.10</cell><cell>98.60</cell><cell></cell></row><row><cell>MG-RAFA [27]</cell><cell>CVPR2020</cell><cell>88.80</cell><cell>97.00</cell><cell></cell><cell cols="2">85.90 88.60</cell><cell>98.00</cell><cell></cell><cell>99.70</cell></row><row><cell>MGH [28]</cell><cell>CVPR2020</cell><cell>90.00</cell><cell>96.70</cell><cell></cell><cell cols="2">85.80 85.60</cell><cell>97.10</cell><cell></cell><cell>99.50</cell></row><row><cell>STGCN [29]</cell><cell>CVPR2020</cell><cell>89.95</cell><cell>96.41</cell><cell></cell><cell>83.70</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VRSTC [30]</cell><cell>CVPR2020</cell><cell>88.50</cell><cell>96.50</cell><cell>97.40</cell><cell cols="2">82.30 83.40</cell><cell>95.50</cell><cell>97.70</cell><cell>99.50</cell></row><row><cell>TCLNet-tri* [31]</cell><cell>ECCV2020</cell><cell>89.80</cell><cell></cell><cell></cell><cell cols="2">85.10 86.60</cell><cell></cell><cell></cell></row><row><cell>AP3D [32]</cell><cell>ECCV2020</cell><cell>90.70</cell><cell></cell><cell></cell><cell cols="2">85.60 88.70</cell><cell></cell><cell></cell></row><row><cell>AFA [33]</cell><cell>ECCV2020</cell><cell>90.20</cell><cell>96.60</cell><cell></cell><cell cols="2">82.90 88.50</cell><cell>96.80</cell><cell></cell><cell>99.70</cell></row><row><cell>SSN3D [34]</cell><cell>AAAI2021</cell><cell>90.10</cell><cell>96.60</cell><cell>98.00</cell><cell cols="2">86.20 88.90</cell><cell>97.30</cell><cell></cell><cell>98.80</cell></row><row><cell>GRL [12]</cell><cell>CVPR2021</cell><cell>91.00</cell><cell>96.70</cell><cell></cell><cell cols="2">84.80 90.40</cell><cell>98.30</cell><cell></cell><cell>99.80</cell></row><row><cell>BiCnet-TKS [13]</cell><cell>CVPR2021</cell><cell>90.20</cell><cell></cell><cell></cell><cell>86.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTL [14]</cell><cell>CVPR2021</cell><cell>91.40</cell><cell>96.80</cell><cell></cell><cell cols="2">86.70 89.70</cell><cell>97.00</cell><cell></cell><cell>100.00</cell></row><row><cell cols="2">Proposed PiT</cell><cell>90.22</cell><cell>97.23</cell><cell>98.04</cell><cell cols="2">86.80 92.07</cell><cell>98.93</cell><cell>99.80</cell><cell>100.00</cell></row></table><note>1 The best results are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Performance comparisons between differentdirection division strategies.</figDesc><table><row><cell>One layer</cell><cell>Parameter</cell><cell cols="2">MARS</cell><cell>iLIDS-VID</cell></row><row><cell></cell><cell></cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell cols="2">No Division (Baseline) 1?210</cell><cell>87.33</cell><cell>84.00</cell><cell>89.87</cell></row><row><cell></cell><cell>105?2</cell><cell>88.26</cell><cell>85.07</cell><cell>89.73</cell></row><row><cell></cell><cell>70?3</cell><cell>88.64</cell><cell>85.72</cell><cell>90.60</cell></row><row><cell>Vertical Division</cell><cell>42?5</cell><cell>88.80</cell><cell>85.56</cell><cell>90.93</cell></row><row><cell></cell><cell>35?6</cell><cell>89.08</cell><cell>85.96</cell><cell>90.40</cell></row><row><cell></cell><cell>30?7</cell><cell>89.78</cell><cell>85.99</cell><cell>89.93</cell></row><row><cell></cell><cell>2?105</cell><cell>88.26</cell><cell>85.33</cell><cell>90.07</cell></row><row><cell></cell><cell>3?70</cell><cell>89.35</cell><cell>85.86</cell><cell>90.20</cell></row><row><cell>Horizontal Division</cell><cell>5?42</cell><cell>89.46</cell><cell>86.24</cell><cell>91.40</cell></row><row><cell></cell><cell>6?35</cell><cell>89.18</cell><cell>85.94</cell><cell>90.73</cell></row><row><cell></cell><cell>7?30</cell><cell>89.35</cell><cell>86.00</cell><cell>90.67</cell></row><row><cell></cell><cell>6p</cell><cell>89.13</cell><cell>86.01</cell><cell>91.00</cell></row><row><cell>Patch-based Division</cell><cell>14p</cell><cell>89.24</cell><cell>86.11</cell><cell>91.67</cell></row><row><cell></cell><cell>15p</cell><cell>89.73</cell><cell>86.17</cell><cell>90.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Performance comparisons between different combinations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">MARS</cell><cell>iLIDS-VID</cell></row><row><cell></cell><cell>Type</cell><cell>Parameter</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell></cell><cell>Vertical Division</cell><cell>1?210 105?2 42?5 30?7</cell><cell>89.34</cell><cell>86.30</cell><cell>91.13</cell></row><row><cell>Four Layers</cell><cell cols="2">Horizontal Division Patch-based Division 1?210 6p 14p 15p 1?210 2?105 5?42 7?30</cell><cell>89.56 89.51</cell><cell>86.32 85.96</cell><cell>91.40 90.47</cell></row><row><cell></cell><cell>Proposed PiT</cell><cell>1?210 105?2 3?70 6p</cell><cell>90.22</cell><cell>86.80</cell><cell>92.07</cell></row></table><note>optional values of parameter D p are</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Performance comparisons between the proposed pyramid with different numbers of layers.</figDesc><table><row><cell>Number of Layers</cell><cell>Parameter</cell><cell cols="2">MARS Rank-1 mAP</cell><cell>iLIDS-VID Rank-1</cell></row><row><cell>1</cell><cell>1?210</cell><cell>87.33</cell><cell>84.00</cell><cell>89.87</cell></row><row><cell>2</cell><cell>1?210 105?2</cell><cell>88.59</cell><cell>85.65</cell><cell>90.20</cell></row><row><cell>3</cell><cell>1?210 105?2 3?70</cell><cell>88.75</cell><cell>85.72</cell><cell>91.13</cell></row><row><cell>4</cell><cell>1?210 105?2 3?70 6p</cell><cell>90.22</cell><cell>86.80</cell><cell>92.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Performance comparisons between proposed PiT with different parameters. The proposed PiT contains four layers, and each layer adopts different division strategies. The performances of the proposed PiT with different parameters are presented inTable V. For example, the parameter 1?210 105?2 3?70 6p</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MARS</cell><cell>iLIDS-VID</cell></row><row><cell></cell><cell>Parameter</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>Four Layers</cell><cell>1?210 105?2 3?70 6p 1?210 105?2 7?30 14p 1?210 42?5 3?70 15p</cell><cell>90.22 89.24 89.24</cell><cell>86.80 86.13 85.82</cell><cell>92.07 91.20 90.40</cell></row><row><cell cols="5">4) Performances of Proposed Pyramid with Different Pa-</cell></row><row><cell>rameters:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Computation complexity and running time of the proposed PiT with different parameters.</figDesc><table><row><cell>Number of Layers</cell><cell>Parameter</cell><cell>MACs</cell><cell>Trainable Parameters</cell><cell cols="4">Using One 24G NVIDIA TITAN RTX GPU MARS iLIDS-VID (10 trials)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Running Time Rank-1 Running Time Rank-1</cell></row><row><cell>1</cell><cell>1?210</cell><cell>18.05G</cell><cell>85.76M</cell><cell>4.25h</cell><cell>87.33</cell><cell>8.00h</cell><cell>89.87</cell></row><row><cell>2</cell><cell>1?210 105?2</cell><cell>19.56G</cell><cell>93.09M</cell><cell>4.45h</cell><cell>88.59</cell><cell>8.75h</cell><cell>90.20</cell></row><row><cell>3</cell><cell>1?210 105?2 3?70</cell><cell>21.06G</cell><cell>100.53M</cell><cell>4.70h</cell><cell>88.75</cell><cell>9.50h</cell><cell>91.13</cell></row><row><cell>4</cell><cell>1?210 105?2 3?70 6p</cell><cell>22.59G</cell><cell>108.32M</cell><cell>5.00h</cell><cell>90.22</cell><cell>10.70h</cell><cell>92.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table VIadopts Multiply-ACcumulate operations (MACs) and trainable parameters to show the computation complexity. We use one 24G NVIDIA TITAN RTX GPU to conduct the experiments.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving night-time pedestrian retrieval with distribution alignment and contextual distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="615" to="624" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple view oriented matching algorithm for people reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>L?zaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1841" to="1851" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to disentangle scenes for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">104330</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Twostream multirate recurrent neural network for video-based pedestrian reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3179" to="3186" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8295" to="8302" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<title level="m">Conference Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person reidentification via multi-feature fusion with adaptive graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1592" to="1601" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting robust unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Watching you: Globalguided reciprocal learning for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="334" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bicnet-tks: Learning efficient spatial-temporal representation for video person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial-temporal correlation and topology learning for person re-identification in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4370" to="4379" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Video swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1904" to="1920" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attribute-driven feature disentangling and temporal aggregation for video person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4913" to="4922" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global-local temporal representations for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3958" to="3967" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Co-segmentation inspired attention networks for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking temporal fusion for video-based person reidentification on semantic and time aspect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Frame-guided regionaligned representation for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="591" to="601" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relation-guided spatial attention and temporal refinement for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">441</biblScope>
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Appearance and motion enhancement for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11" to="394" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Co-saliency spatio-temporal interaction network for person re-identification in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific Rim International Conference on Artificial Intelligence IJCAI-PRICAI-20</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asta-net: Adaptive spatio-temporal attention network for person re-identification in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1706" to="1715" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-granularity reference-aided attentive feature aggregation for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="407" to="417" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning multi-granular hypergraphs for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2899" to="2908" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatialtemporal graph convolutional network for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3289" to="3299" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vrstc: Occlusion-free video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal complementary learning for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="388" to="405" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Appearancepreserving 3d convolution for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="228" to="243" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal coherence or temporal motion: Which is more critical for video-based person re-identification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ssn3d: Self-separated network to align parts for 3d convolution in video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1691" to="1699" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust re-identification by multiple views knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="93" to="110" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
