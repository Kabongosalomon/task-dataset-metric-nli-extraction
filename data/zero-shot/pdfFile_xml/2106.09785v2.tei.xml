<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFFICIENT SELF-SUPERVISED VISION TRANSFORMERS FOR REPRESENTATION LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research at Redmond</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research at Redmond</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research at Redmond</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud + AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud + AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
							<email>xidai@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud + AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<email>luyuan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud + AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research at Redmond</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EFFICIENT SELF-SUPERVISED VISION TRANSFORMERS FOR REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task of region matching which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3% top-1 accuracy on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and pre-trained models are released at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Self-supervised learning (SSL) with <ref type="bibr">Transformers (Vaswani et al., 2017)</ref> has become a de facto standard of model choice in natural language processing (NLP). The dominant approaches such as <ref type="bibr">GPT (Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b16">(Devlin et al., 2019)</ref> are pre-training on a large text corpus and then fine-tuning to various smaller task-specific datasets, showing superior performance. Larger Transformers pre-trained with larger-scale language datasets often lead to a stronger generalization ability, demonstrated by improved performance in downsteam tasks (with no sign of performance saturation yet), as exemplified in <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>.</p><p>In computer vision (CV), however, self-supervised visual representation learning is still dominated by convolutional neural networks (CNNs). Sharing a similar goal/spirit with NLP, SSL in CV aims to learn general-purpose image features from raw pixels without relying on manual supervisions, and the learned networks are expected to serve as the backbone of various downstream tasks such as classification, detection and segmentation. Recently, impressive performance have been achieved by CNN-based SSL, outperforming state-of-the-art (SoTA) fully-supervised pre-training methods <ref type="bibr" target="#b27">(He et al., 2020;</ref><ref type="bibr" target="#b4">Caron et al., 2020)</ref> on tasks with a limited number of labels. The key to success is view-level learning: maximizing agreement of learned representations between differently augmented views of the same example. Recent works, including SimCLR-v2 <ref type="bibr" target="#b9">(Chen et al., 2020d)</ref>, BYOL <ref type="bibr" target="#b25">(Grill et al., 2020)</ref> and SwAV <ref type="bibr" target="#b4">(Caron et al., 2020)</ref>, have scaled up the CNN-based models to hundreds of millions of parameters. However, SSL has not enjoyed the same scaling success in CV as that in NLP.</p><p>Several attempts have been made to close the gap by combining SSL with Transformer and selfattention architectures. Early works include Selfie <ref type="bibr">(Trinh et al., 2019)</ref>, which generalizes the concept of masked language modeling of BERT for images. The idea has been recently revisited in Vision Transformer (ViT) <ref type="bibr" target="#b20">(Dosovitskiy et al., 2021)</ref> via pre-training on a much larger scale dataset, e.g., JFT-300M. ImageGPT (iGPT) <ref type="bibr" target="#b7">(Chen et al., 2020b)</ref> generalizes the concept of auto-regressive language modeling of GPT for images, showing encouraging ImageNet recognition accuracy with a large model size. Contrastive learning with ViT has also been studied very recently in DINO  and MoCo-v3 <ref type="bibr" target="#b11">(Chen et al., 2021)</ref> Right: performance over varied parameter counts for models with moderate (throughout/#parameters) ratio. EsViT pre-trained with and without the region-matching task are shown before and after the arrows, respectively. Please refer Section 4.1 for details.</p><p>ImageNet-1K is achieved, by exhaustively consuming computation resource on full self-attention operators with long sequences of split image patches.</p><p>Aiming to improve the efficiency of Transformer-based SSL, this paper presents Efficient selfsuperivsed Vision Transformers (EsViT), by using a multi-stage architecture and a region-based pre-training task for self-supervised representation learning. Our main findings and contributions can be summarized as follows:</p><p>(1) An intriguing property of self-supervised monolithic Transformers is firstly reported in our paper: automatic discovery of semantic correspondence between local regions.</p><p>(2) We present the first comprehensive empirical study to show the pros and cons of multi-stage vision Transformer architectures for SSL. Though greatly reducing compute complexity, we find that the multi-stage architecture causes the loss of the property in (1).</p><p>(3) A region matching pre-train task is proposed to alleviate the issue in (2), and further improve the learned representations and attentions.</p><p>(4) We validate the new EsViT, which combines the two techniques, on a range of tasks. It significantly reduces the cost in building SoTA SSL vision systems, as summarized in <ref type="figure" target="#fig_0">Figure 1</ref>, and shows better scaling performance on accuracy vs. throughput and model size. Under the linear evaluation protocol, EsViT achieves 81.3% top-1 accuracy, showing the best performance compared with all systems, and is 3.5? parameter-efficient and has at least 10? higher throughput than previous SoTA (81.0%, MoCo-v3 with ViT-BN-L/7 <ref type="bibr" target="#b11">(Chen et al., 2021)</ref>). Compared with its supervised counterpart Swin Transformers , EsViT shows superior performance on 17 out 18 datasets, when transferring the learned representations to downstream linear classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>Transformer-based SSL methods emerge very recently to lead the state-of-the-art performance on the ImageNet linear probe task <ref type="bibr" target="#b11">(Chen et al., 2021;</ref>. It inherits the successes from (1) monolithic Transformer architectures that dominate in NLP <ref type="bibr" target="#b16">(Devlin et al., 2019;</ref><ref type="bibr">Radford et al., 2018)</ref>, and (2) instance-level contrastive learning objectives that demonstrate arguably the best SSL performance in computer vision <ref type="bibr" target="#b8">(Chen et al., 2020c)</ref>. Though simple and effective, the existing Transformer-based SSL methods require a large amount of compute resources (e.g., &gt;1.7 TPU years of training) to reach SoTA performance. We believe that the SSL system efficiency is highly related to two ingredients: the network architecture and the pre-train task. To strike for a better tradeoff between accuracy and efficiency, we present EsViT, showing better synergy of networks (a multi-stage Transformer architecture) and pre-train tasks (a non-contrastive region-matching task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NETWORK ARCHITECTURES: FROM MONOLITHIC TO MULTI-STAGE VIT BACKBONE</head><p>Multi-stage ViT. This paper presents the first empirical study of multi-stage Transformer architectures <ref type="bibr">(Vaswani et al., 2021;</ref><ref type="bibr" target="#b24">Wang et al., 2021;</ref> for SSL. Each stage consists of a patch merging/embedding module, and a Transformer with sparse self-attention module. (i) The patch merging module plays a slightly different roles in different stages. In the first stage, it splits an input RGB image into non-overlapping patches. Each patch is treated as a "token", constructed as a concatenation of the raw pixel RGB values, which is further projected into a C-dimension feature. In the later stage, the patch merging module concatenates the features of each group of 2 ? 2 neighboring patches, and applies a linear layer on the 4C-dimensional concatenated features. This reduces the number of tokens by a multiple of 2 ? 2 = 4, and the output dimension is set to 2C. (ii) A Transformer with sparse self-attention module are then employed to enable interactions among the merged features. The two modules above are repeated for multiple times, typically 4 times, resulting in a multi-stage ViT. As a result, a hierarchical representation is generated: the number of tokens is reduced and the feature dimension (and the number of heads in self-attentions) of each token is increased, as the network gets deeper. An overview comparison of the monolithic and multi-stage Transformer architectures for SSL is illustrated in <ref type="figure" target="#fig_6">Figure 7</ref> in Appendix.</p><p>An intriguing property of self-supervised monolithic ViT. Though straightforward in implementation, changing from monolithic to multi-stage architecture without careful treatments may lose some desirable properties of self-supervised Transformers In out study, we first empirically note an intriguing property of self-supervised monolithic ViT : the pre-trained model exhibits a very strong ability to automatically discovers correspondences, even without a region-level matching objective specified in training.</p><p>We quantitatively evaluate the correspondence learning to illustrate this property, as discussed in the following process. (i) Simulated benchmark. Based on 50K images in the ImageNet validation dataset, we create a simple evaluation benchmark with mild augmentations: For a center-crop image, we apply HorizontalFlip, then ColorJitter and RandomGrayscale to create a new augmented view. In this way, ground-truth correspondences are created. (ii) Evaluation process. Given two views of the same image, we use the pre-trained backbone to extract the top-layer features, and find the feature vector in one view that best matches the other in terms of highest cosine similarity. The accuracy is measured as the averaged percentage of correctly identifying the region-to-region correspondences. Please see details in Section C.7 in Appendix. (iii) Results. We quantitatively show that a self-supervised monolithic ViT yields 95% accuracy. However, simply replacing the network with a multi-stage Transformer yields only 66% accuracy. This significant degradation (absolute 29% accuracy drop) reveals the loss of the correspondence learning property. We first raise this critical problem, and believe that it has a large impact on the pre-trained model's performance in various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PRE-TRAINING TASKS: DELVING INTO VIEWS WITH REGIONS</head><p>We employ a non-contrastive learning framework to build our SSL method. Specifically, Selfdistillation with no labels (DINO)  is considered. It leverages the knowledge distillation learning paradigm where a student network g ?s is trained to match the output of a given teacher network g ?t , parameterized by ? s and ? t respectively. The neural network g is composed of a backbone f (e.g., Transformers or ConvNets), and of a projection head h: g = h ? f . The features used in downstream tasks are the output of backbone f . In SSL, different augmented viewsx of an image x are fed into backbone network to obtain feature maps z = f (x). Two MLP heads followed by softmax per network further convert the feature vectors z ? z into probability vectors p = h(z); one head for view-level and the other head for region-level, respectively.</p><p>More precisely, from a given image, we generate a set V of different views 1 following . The resulting feature map at the top layer for each view is z = [z 1 , . . . , z T ], where T is the sequence length, and z i is a region-level representation for the local patch at position i. Average pooling is applied to obtain the view-level representationz = avg-pool(z).</p><p>View-level task Given the augmented view set for student V and teacher V * , a set of pairs P = {(s, t)|x s ? V,x t ? V * and s = t } is constructed to perform cross-view prediction tasks. We consider the pre-training task at the view level proposed by :</p><formula xml:id="formula_0">L V = 1 |P| (s,t)?P M V (s, t), with M V (s, t) = ?p s log p t ,<label>(1)</label></formula><p>where p s = h(z s ) and p t = h(z t ) are the probability output of an MLP head h over the view-level representationsz s andz t , learned by student and teacher, respectively. In DINO, ViT/DeiT are considered, hence the view-level representation is the feature of the [CLS] token.</p><p>Region-level task In , the L V encourages "local-to-global" correspondences only at a coarse level: the large crop and the small crop are matched in the view level, leaving region-to-region correspondence unspecified. In monolithic Transformers, the drop paths and skip connections from low-level features to high-level features help the the latter to remain discriminative, thus maintain good region-matching performance. However, such a property gets diluted due to the merging operators in multi-stage Transformers. As shown in our experiments later, training a multi-stage network with L V only indeed results in sub-optimal representations.</p><p>Further, it could be a waste of computation not to leverage region-level features z that are computed in the process of extracting view-level feature. Inspired by the success of masked language modeling task in BERT, we argue that it is important to have region-level pre-training task for computer vision, so that the model can (1) amortize the computation and fully leverage the extracted region-level features, and (2) take into account the co-occurrences/structures between local features. Unfortunately, directly performing masked patch prediction (MPP) for the multi-stage Transformer architecture is infeasible, as the one-to-one correspondences between the input visual tokens and output features get diluted due to the merging operation. Even for monolithic architectures, MPP has not been proved effective in computer vision, as empirically shown in <ref type="bibr" target="#b20">(Dosovitskiy et al., 2021)</ref>.</p><p>To address this problem, we propose a non-contrastive, region-matching method that directly works at the level of local features by taking into account their correspondences:</p><formula xml:id="formula_1">L R = 1 |P| (s,t)?P M R (s, t), with M R (s, t) = ? 1 T T i=1 p j * log p i , j * = arg max j z T i z j z i z j ,<label>(2)</label></formula><p>where p i = h (z i ) and p j = h (z j ) are the probability outputs of a new MLP head h over the local features of student z i ? z s and teacher z j ? z t , respectively. j * is the index of the feature in z t that best matches the i-th feature in z s , in the sense of highest cosine similarity. Note that z i and z j * are contextualized features of two best matched regions from different augmentated views, minimizing L R encourages different contexts (i.e., surrounding regions) to learn invariant features, and thus captures the region-dependency.</p><p>Global Token Local Tokens (Top-layer feature maps)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP Head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP Head</head><p>View-level Prediction The overall pre-training objective of EsViT is L = L R + L V , we learn to match the feature distributions at both the view and region levels by minimizing the cross-entropy loss w.r.t. the parameters of the student network g ?s . A visual illustration is in <ref type="figure" target="#fig_1">Figure 2</ref>, and the full algorithm is in Appendix. We updates teacher/student network alternatively: (i) Given a fixed teacher network, the student network is updated by minimizing the full cross-entropy loss: ? s ? arg min ?s L(s, t; ? s ). (ii) The teacher model is updated as an exponential moving average (EMA) of the student weights ? t ? ?? t + (1 ? ?)? s , with ? following a cosine schedule from 0.996 to 1 during training. By default, the full objective L is used from the beginning. One can also load a checkpoint trained by L V only, and add L R for continual pre-training, which is shown effective in boosting performance in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region-level Prediction</head><p>Computational overhead Note that applying L R on the traditional monolithic Transformer architecture can be prohibitively computationally expensive, as it requires O(T 2 ) to compute L R . For a typical image of resolution 224?224, the feature map length of ViT/DeiT (with patch size 16) at the top layer is T = 196, while the multi-stage architecture yields T = 49, which requires 3 times less compute in computing L R . To empirically illustrate this, we show in Appendix Section C.2 that L R adds acceptable extra memory and computational cost (around 1.2 and 1.05 ?, respectively) for multi-stage Transformers, while it will quickly go out-of-memory for monolithic Transformers when the batch size is increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORKS</head><p>Relation to mask prediction tasks We can consider the proposed L R as a proxy to mimick masked language modeling in BERT, where the "ground-truth" local token is a soft label provided by the teacher network, while the student network makes predictions to match that target, based on the context of regions in a different augmented view. Importantly, our L R considers softmax with cross-entropy in the objective, rather than MSE as in MPP. A very sharp teacher distribution is used by choosing small temperatures. This encourages the model to focus on the salient dimensions, rather than waste modeling capability on training short-range dependencies and high-frequency details <ref type="bibr">(Ramesh et al., 2021)</ref>.</p><p>Relation to DenseCL The proposed L R mostly related to DenseCL <ref type="bibr" target="#b43">(Wang et al., 2020b)</ref> in that the region correspondences in both methods are determined as the two most similar grid features. One critical difference is that DenseCL is a contrastive region-matching task, while our L R is a non-contrastive region-matching task, where no negative samples/queue is needed. This technical difference has a significant impact on the downstream task performance. We find that L R is particularly effective in serving our goal to improve image classification performance and build efficient &amp; affordable SoTA SSL system; In contrast, DenseCL degrades the classification performance.</p><p>Relation to other region-level tasks The ideas of leveraging local region-level pre-training tasks for visual representation learning have been explored for ConvNets <ref type="bibr" target="#b42">(Misra &amp; Maaten, 2020;</ref><ref type="bibr" target="#b50">Xiong et al., 2020;</ref><ref type="bibr" target="#b43">Wang et al., 2020b;</ref><ref type="bibr" target="#b46">Xie et al., 2021a;</ref><ref type="bibr" target="#b49">Xie et al., 2021c)</ref>. We summarize the differences in three aspects: (i) Motivation. Our region-matching task L R aims to recover the lost property of automatic correspondence learning in self-supervised monolithic Transformers, while most existing region-level tasks aim to improve dense visual prediction tasks. (ii) Technical difference. Our L R is a non-contrastive region-matching task, while others are contrastive learning. (iii) Empirical performance. Most region-level tasks improve dense visual prediction tasks but sacrifice their image classification performance, while L R consistently improves classification performance. Among them, EsViT training method achieves the best ImageNet linear probe performance with minimum computational overhead. For detailed comparisons, please refer to <ref type="table">Table 7</ref> in Appendix.</p><p>Self-supervised vision Transformers. The research on Transformer-based self-supervised representation learning just scratches the tip of the iceberg, and only a few attempts are made on this topic. ImageGPT <ref type="bibr" target="#b7">(Chen et al., 2020b)</ref> and MoCo-v3 <ref type="bibr" target="#b11">(Chen et al., 2021)</ref> dedicate huge compute resource with large models to exploring the frontier. DINO  achieves comparable performance of large self-supervised ConvNets using small/medium-size Transformers. The proposed EsViT further pursues efficient and affordable solutions to self-supervised vision Transformers. For more general related works on Transformers for vision tasks and self-supervised ConvNets, please refer to Section B in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We describe the experimental settings in Appendix Section C.3, and evaluate the proposed EsViT to answer three questions: Q1: How does EsViT perform on standard ImageNet benchmark compared to SoTA methods? Q2: How effective EsViT is when transferring to downstream tasks? Q3: What are the design choices and empirical contributions of L R ? Q4: When does the intriguing property of self-supervised Transformers exist, including learned correspondence and attentions?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">COMPARISONS WITH PRIOR ART ON IMAGENET</head><p>We report top-1 linear probe and k-NN accuracy on the ImageNet validation set. <ref type="table">Table 1</ref> presents comparisons with SoTA SSL systems across various architectures. Please refer to <ref type="figure" target="#fig_0">Figure 1</ref> for comparisons over scaling parameter counts and throughput. Our findings are summarized below.</p><p>Comparisons with self-supervised Transformers. The DINO-and MoCo-based ViT has higher accuracy and smaller models than iGPT, under the same linear probing protocol and training data.  <ref type="bibr" target="#b21">(Frankle et al., 2020)</ref>, and "/P " denotes a patch size of P ?P . "?" indicates through-puts estimated by comparing different papers, detailed in Appendix. ? The mask patch prediction in <ref type="bibr" target="#b20">(Dosovitskiy et al., 2021)</ref> is pre-trained on JFT-300M and end-to-end fine-tuned in ImageNet, which we append as a reference.</p><p>At the similar level of model size and compute complexity, the proposed EsViT improve SoTA methods DINO/MoCo-v3 by a large margin: EsViT (Swin-B) outperforms DINO (ViT-B/16) by 2.2% linear probe accuracy and 2.8% k-NN accuracy in absolute values. EsViT (Swin-B) even performs slightly better than DINO (ViT-B/8) (0.3% higher linear probe accuracy and 1.5% higher k-NN accuracy), with 4? higher throughput. MoBY <ref type="bibr" target="#b48">(Xie et al., 2021b</ref>) is a con-current work that investigates multi-stage ViT in SSL. With the same architecture Swin-T, our EsViT pre-training tasks significantly outperform MoBY, showing 3% higher accuracy. In EsViT, longer sequences in self-attention is implemented by increasing the window size. We experiment this by considering a window size of W = 14. Overall, the proposed EsViT (Swin-B/W =14) shows the best performance (top-1 accuracy 81.3%, top-5 accuracy 95.5%, k-NN accuracy 79.3%), compared with all systems, and is 3.5? parameter-efficient and has at least 10? higher throughput than previous SoTA MoCo-v3.</p><p>Comparisons with big ConvNets. We compare with the SoTA big ResNets reported by SimCLR-v2 <ref type="bibr" target="#b9">(Chen et al., 2020d)</ref>, BYOL <ref type="bibr" target="#b25">(Grill et al., 2020)</ref> and SwAV <ref type="bibr" target="#b4">(Caron et al., 2020)</ref>. Among them, the best accuracy 79.8% under the linear probing protocol is reported by SimCLR-v2 with SK-ResNet, where Selective Kernel (SK) <ref type="bibr" target="#b37">(Li et al., 2019c</ref>) is a form of attention to enhance CNNs. It is clear in <ref type="figure" target="#fig_0">Figure 1</ref> (b) that all ConvNets-based SSL methods show an envelope in the regime of scaling up model sizes after passing 500M. EsViT achieves better accuracy than their highest envelope, with 16? less model parameters and 8? higher throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TRANSFER LEARNING</head><p>We also conduct transfer learning in downstream tasks to evaluate the quality of learned representations. Two sets of tasks are considered:</p><p>? Classification on a suite of 18 small datasets. As exemplified in <ref type="bibr">(Radford et al., 2021)</ref>, it is a common and clean approach to evaluate a learned representation by fitting a linear classifier on the representation and measuring its performance across multiple datasets. We study 18 datasets used in <ref type="bibr">(Radford et al., 2021)</ref>. Automatic hyper-parameter tuning is considered to ensure fairness of comparison. Besides averaged scores, we report # wins as the number of datasets on which the model outperforms its supervised counterpart. Detailed dataset description and settings are in Appendix.    Comparison with supervised counterparts. We compare with the supervised-learning Swin, whose checkpoints are downloaded from the official codebase 2 . <ref type="figure">Figure 3</ref> shows the classification results of Swin-S, EsViT consistently outperforms its supervised variant, often by a large margin. Similar conclusions are drawn for other model sizes. On COCO detection and segmentation task, however, EsViT shows comparable results with the variant with L V only (shown in parentheses) and the supervised counterpart (Swin-T trained with 3? schedule), as shown in <ref type="table" target="#tab_2">Table 2</ref>. We hypothsize this is related to the non-constrastive nature of EsViT, as explained later.</p><p>Effects of larger, less-curated pre-train datasets. The performance of Transformer-based SSL research has thus far been limited to highly curated pre-train data such as ImageNet-1K. To push the frontier in leveraging large amounts of unlabeled data, we explore the effects of pre-training from larger, less-curated image datasets: WebVision-v1 <ref type="bibr" target="#b36">(Li et al., 2017)</ref>, OpenImages-v4 <ref type="bibr" target="#b30">(Kuznetsova et al., 2020)</ref> and ImageNet-22K <ref type="bibr" target="#b15">(Deng et al., 2009)</ref>, described in Appendix. The pre-train epochs on different datasets are adjusted so that all models see a similar number of augmented views. We summarize the results in <ref type="table" target="#tab_3">Table 3</ref> and would like to emphasize the following findings. First, L R improves L V (shown in parentheses) on all datasets. Second, all EsViT pre-trained checkpoints outperform supervised checkpoint in downstream classification tasks, but performance varies a lot, with ImageNet-22K checkpoint showing the best transfer ability. Third, ImageNet-1K pre-trained model shows the best ImageNet-1K linear probe performance. We hypothesize that it is not only the size of pre-train dataset matters, but also the distribution of image classes matters: more diverse and well-balanced distribution results in a stronger generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DISCUSSION ON THE NON-CONTRASTIVE REGION-MATCHING TASK</head><p>Compatibility with various network architectures. We investigate ResNet-50 and different efficient sparse Transformers in <ref type="table" target="#tab_5">Table 4</ref>. DeiT is shown as a baseline reference. Batch size = 1024 in this experiment. To ensure fair comparison, we modify all into a 4-stage architecture with the number of Transformer layers in each stage as 2-2-6-2. We see that L R improves all network architectures, including ResNet-50, Swin , ViL , CvT  and PvT <ref type="bibr" target="#b24">(Wang et al., 2021)</ref>. Though directly adding L R to monolithic ViT is computationally infeasible, we uniformly sampled top-layer grid features of DeiT and then add L R , but did not observe performance improvement. This is partly because the monolithic ViT itself already has a good corresponding ability, an extra region-matching task does not provide new learning signals. As    <ref type="table" target="#tab_2">Table 12</ref> with the ResNet-50 backbone, EsViT learning method shows the highest accuracy, compared with existing SSL methods.</p><p>Model scaling with L R . We compare the pre-training objective with and without L R in <ref type="table">Table 1</ref>. Across different model scales and window sizes, the proposed region level L R can consistently improve the performance. The gains can be clearly seen by k-NN accuracy (around 1-2%), where no additional tuning is needed as in linear probe. <ref type="figure">Figure 4</ref> demonstrates that L R helps model convergence, and can be used as a drop-in to improve models trained with the view level task.</p><p>Contrastive vs Non-contrastive region-matching tasks. The proposed L R adds a non-contrastive region-matching task to the non-contrastive view-level task L V ; On the contrary, DenseCL adds a contrastive region-matching task to the contrastive view-level task MoCo-v2. In <ref type="table" target="#tab_6">Table 5</ref>, we compare four methods in the same setting with ResNet-50. DenseCL improves dense visual prediction performance, but hurts classification performance. L R improves both tasks, especially the classification performance. One limitation is that the non-contrastive methods show lower performance in dense prediction tasks, this is consistent with the observations for BYOL in <ref type="bibr" target="#b43">(Wang et al., 2020b)</ref>. The simple L R shows the best ImageNet accuracy compared with all sophisticated region-level tasks in this 200-epoch setting in <ref type="table">Appendix Table 7</ref>, and the best overall accuracy in <ref type="table" target="#tab_2">Table 12</ref>. It indicates that L R well serves our goal in building efficient SoTA SSL systems.</p><p>Design choices of L R . We ablate a couple of choices in constructing L R in Eq. (2). (i) Softmax vs MSE. One alternative way to measure the distance between two projected vectors is MSE, as employed in the popular non-contrastive SSL algorithm BYOL <ref type="bibr" target="#b25">(Grill et al., 2020)</ref>. When adding region-matching tasks to BYOL and pre-training 50 epochs, Softmax and MSE yield k-NN accuracy of 37.2% and 34.9%, while the baseline BYOL yields 33.1%. We also replace the region-matching metric in EsViT as MSE, yielding k-NN accuracy 72.6%, which lower than the view-level task only (74.2%). These results show that Softmax is essential in L R . (ii) Optimal Transport (OT) vs Simple Argmax. To avoid heavy computational overhead, a simple feature-level argmax solution is considered in Eq.</p><p>(2) to pair two local regions. To study the impact of high region-matching quality, we consider OT. Empirically, we observe OT yields slightly higher k-NN accuracy at the early stage, but the gain is diminished in the end. Considering the extra computational cost of solving OT with an inner loop in sinkhorn algorithm <ref type="bibr" target="#b13">(Cuturi, 2013)</ref>, we opt for simple argmax in our experiments.  <ref type="figure">Figure 6</ref>: Visualization of the the learned attention map for different heads in the last layer. The query is the blue dot in the center of the images. We visualize masks (as red) obtained by thresholding the self-attention maps to keep 60% of the probability mass. Note that all 6 heads are visualized for DINO with DeiT-S, and 6 out of 24 heads in EsViT are chosen to visualize (ranked by entropy values). Please see enlarged pictures with all heads in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">QUALITATIVE STUDIES</head><p>Visualization of correspondences. Given two views of the same image, we use the pre-trained backbone to extract the top-layer features z 1 and z 2 . For each feature vector in z 1 , we find the feature vector in z 2 that best matches it in terms of highest cosine similarity, as defined in Equation (2). In <ref type="figure" target="#fig_3">Figure 5</ref>, we show the top-10 correspondences between two views for three methods. In <ref type="figure" target="#fig_3">Figure 5</ref> (b), EsViT with L V tends to identify pairs in the background as the most matched ones (and in a wrong way in this example). This could be a valid solution to L V , as the invariance in the level of aggregated global features does not necessarily induce invariances in the local region level. This is significantly alleviated with L R (shown in <ref type="figure" target="#fig_3">Figure 5 (c)</ref>), a task that implicitly requires local matching. Surprisingly, DINO is able to learn good correspondences even without the region-level matching task. To the best of our knowledge, this is a previously unreported intriguing property of self-supervised Transformers with monolithic architectures: good semantic correspondences are automatically learned. We hypothesize that features at lower layers (image patch itself in the extreme case) can directly pass to higher layers, and the former regularizes the latter to remain discriminative. Nevertheless, the proposed L R can dramatically reduce the issue, and is good remedy to rescue the loss of semantic correspondence for the multi-stage architecture. In Appendix, we quantitatively measures the correspondence learning ability of these SSL methods on ImageNet validation dataset, the observations are consistent: L R improves the matching accuracy from 66% to 91%.</p><p>Visualization of attention maps. We look at the self-attention in the different heads of the last layer in <ref type="figure">Figure 6</ref>. A local region on the edge of the main object is employed as query, and the attended regions are highlighted in red for those the query's top 60% mass are assigned. In Appendix, we visualize more examples with different query positions. DINO tends to automatically learn class-specific attention maps leading to foreground object segmentation, regardless of its query located in foreground or background. This is probably because main objects remain as the major invariance factor in different augmented views. This property is lost when a multi-stage architecture is employed, as shown in EsViT with L V . These patterns are consistent for different heads. After introducing L R for EsViT, we note that the attention maps become more diverse in different heads, i.e., entropy values of attentions get more skewed, and attended regions are more different. This is perhaps because L R requires each region to consider many matching tasks to regions in different augmented views, each head automatically learns to distribute the tasks and complete a few of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we first discover the automatic correspondence learning property of self-supervised monolithic Transformers. Inspired by this, we present efficient self-supervised vision Transformers (EsViT) to with two major insights: a multi-stage Transformer architecture with sparse self-attentions, and a non-contrastive region-matching pre-training task. The synergy of both helps EsViT reach the SoTA performance of SSL vision systems with significantly less compute and smaller model size.</p><p>Our study also reveals that exploration of effective solutions to learn from larger and less curated pre-training data in the wild is a key but less studied factor in paving the way toward the scaling success of SSL vision systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>Though self-supervised learning (SSL) has great potentials to learn powerful representation without human annotation, the existing techniques to build SoTA SSL vision systems tend to be Red AI (Schwartz et al., 2020): it could be environmentally unfriendly and the computational cost is extensively high. The required training resource is typically not accessible for a lab environment (thus raising barriers to participation in AI research). For example, the prior art MoCo-v3 has greatly pushes the performance limit of SSL system <ref type="bibr" target="#b11">(Chen et al., 2021)</ref>. The authors kindly reported that "it (MoCo-v3, ViT-H) takes 9.8 hours per 100 epochs using 512 TPUs. This is a gigantic scale of training: for the 300-epoch ViT-H, this amounts to ?625 TPU days, or ?1.7 TPU years of training." The SoTA model MoCo-v3 with ViT-BN-L/7 should have a higher cost than this. Even for a smaller model ViT-B, "it takes 24 hours in 128 GPUs (vs. 2.1 hours in 256 TPUs)". Hence, improving the efficiency of building SoTA SSL systems is of high value for the community and society to achieve Green AI (Schwartz et al., 2020).</p><p>To this end, we propose EsViT to provide more affordable and efficient solutions for the community to experiment and explore the directions of SoTA SSL in computer vision. Our EsViT model shows the best ImageNet linear probe performance compared with all existing SSL vision systems, and is 3.5? parameter-efficient and has 10? higher throughput than previous SoTA. This efficiency gain can significantly decrease its carbon footprint and increase its inclusivity, encouraging more researchers to participate the study of the SSL topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>Our paper provides comprehensive empirical studies on the EsViT algorithm. We provide PyTorchstyle pseudo-code in Appendix. We also include an example code with instruction as supplementary material to ensure the reproducibility. For empirical results on both various network architecture and large-scale datasets, we provide detailed hyper-parameter specifications. We release the pre-trained checkpoints and codebase for the research community for reproducible research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 ALGORITHMS</head><p>We summarize the training algorithm procedure of EsViT with L V +L R in Algorithm 1. To clearly outline the main idea of the algorithm, we show the algorithm for two augmented views. For the full algorithm to deal with multi-crop, please refer to our codebase. In Algorithm 1, for a mini-batch of size n, the teacher/student network consists of three output variables: (1) p ? R n?K is the probability vector for the view-level representation, output by an MLP head. (2) z ? R n?T ?P is the feature map, containing T region-level features of dimension P . (3) pz ? R n?T ?K are probability vectors of z, output by a different MLP head.</p><p>Algorithm 1: EsViT with L V +L R , pseudocode with 2-crop.  <ref type="figure" target="#fig_0">.matmul(zs , zt.permute(0, 2, 1)</ref>  <ref type="figure" target="#fig_0">.gather(pt, 1, sim idx.expand(-1, -1, pt.size(2)</ref>)) 32 return -(pt idxed * log(ps)).sum(dim=-1).mean()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 NETWORK ARCHITECTURE CONFIGURATIONS AND IMPLEMENTATION DETAILS</head><p>Inspired by great successes of the multi-stage ConvNet architecture such as VGG (Simonyan &amp; Zisserman, 2014)/ResNets <ref type="bibr" target="#b26">(He et al., 2016)</ref> for computer vision, the multi-stage Transformer-based networks have been explored very recently in the supervised learning setting <ref type="bibr">(Vaswani et al., 2021;</ref><ref type="bibr" target="#b24">Wang et al., 2021;</ref>. In multi-stage vision Transformers, since a larger number of patches is often produced at the early stages, an efficient Transformer with sparse self-attentions is considered to reduce the computational complexity. The basic idea is to split the feature maps into non-overlapping local windows (with size W ?W ), and self-attention is performed within each local window. This however has one drawback that features in different local windows cannot interact. Various methods have been proposed to best approximate full-attention, with different trade-off between accuracy and efficiency.</p><p>We briefly describe three schemes as follows, and benchmark them in the experiments. (i) Swin Transformer : A shifted window partitioning approach is proposed, which alternates between two partitioning configurations in consecutive Transformer blocks, so that each local feature is grouped into different windows in self-attentions. The window size is set to W = 7 by default. The query dimension of each head in self-attentions is d = 32, and the hidden layer width of each MLP is 4? of its input's width, for all experiments. The architecture configurations of model variants employed in the experiments are summarized in <ref type="table" target="#tab_10">Table 6</ref>. Some notable implementation detailed are described as follows:</p><p>? The three configurations Swin-T, Swin-S and Swin-B indicate Tiny, Small, and Base models, respectively, which are almost identical to the original implementation , except that we add special treatments to deal with input augmented views of different resolutions, when the resolution (feature map size more specifically) is not divisible by the window size (i.e., resolution 96 and window size=7 or 14).</p><p>? Swin-T and Swin-S with window size W = 14 are customized by us to allow full self-attention in stage 3 (where the majority of model capacity is allocated to) and stage 4, to study the impact of longer sequences in EsViT.</p><p>? In the original ViL  and CvT  papers, different positional embedding strategies and multi-stage network configurations were employed. We modify them by only utilizing relative position bias and their proposed sparse self-attention mechanisms, and create a similar 4-stage architecture with Swin-T for fair comparison.</p><p>Relative Position Bias. To facilitate SSL, we consider relative position bias  to characterize the spatial information between features for the three efficient Transformers aforementioned, and do not use absolute position embeddings. This is because augmented views of varied resolutions can be cropped from anywhere in an image in SSL, maintaining the relative positions is easy in implementation, and is largely sufficient for invariance learning among these views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RELATED WORK</head><p>Self-supervised ConvNets. ConvNets-based SSL has been extensively studied in the literature. Based on the pre-training tasks, they can be broadly categorized into three classes: Handcrafted pretext tasks <ref type="bibr" target="#b17">(Doersch et al., 2015;</ref><ref type="bibr">Noroozi &amp; Favaro, 2016;</ref><ref type="bibr">Pathak et al., 2016;</ref><ref type="bibr" target="#b22">Gidaris et al., 2018;</ref><ref type="bibr" target="#b31">Larsson et al., 2016;</ref><ref type="bibr" target="#b58">Zhang et al., 2017;</ref><ref type="bibr">Pu et al., 2016;</ref><ref type="bibr" target="#b18">Donahue &amp; Simonyan, 2019)</ref>, contrastive learning <ref type="bibr" target="#b19">(Dosovitskiy et al., 2015;</ref><ref type="bibr" target="#b62">Zhuang et al., 2019;</ref><ref type="bibr">Oord et al., 2018;</ref><ref type="bibr" target="#b28">Hjelm et al., 2018;</ref><ref type="bibr" target="#b0">Bachman et al., 2019;</ref><ref type="bibr" target="#b27">He et al., 2020;</ref><ref type="bibr" target="#b8">Chen et al., 2020c;</ref><ref type="bibr" target="#b25">Grill et al., 2020)</ref> and prototype learning <ref type="bibr" target="#b3">(Caron et al., 2018;</ref><ref type="bibr" target="#b47">Xie et al., 2016;</ref><ref type="bibr" target="#b53">Yang et al., 2016;</ref><ref type="bibr" target="#b29">Ji et al., 2019;</ref><ref type="bibr" target="#b55">Zhan et al., 2020)</ref>. It is also known that data augmentations play a crucial role in SSL pipeline <ref type="bibr" target="#b10">(Chen et al., 2020e;</ref><ref type="bibr" target="#b4">Caron et al., 2020;</ref><ref type="bibr">Tian et al., 2020;</ref><ref type="bibr" target="#b32">Li et al., 2020a)</ref>. The impact of pre-training dataset size/quality is explored for ConvNets in SSL <ref type="bibr" target="#b24">(Goyal et al., 2021;</ref><ref type="bibr" target="#b54">Yonglong et al., 2021)</ref>. To date, the search of best pre-taining tasks/datasets and augmentations are based on CNNs. Among them,    SimCLR-v2 <ref type="bibr" target="#b9">(Chen et al., 2020d)</ref>, BYOL <ref type="bibr" target="#b25">(Grill et al., 2020)</ref> and SwAV <ref type="bibr" target="#b4">(Caron et al., 2020)</ref> achieve the highest ImageNet linear probe performance with large ConvNet architectures. The performance tends to saturate with an increasingly growing model size, raising a question if ConvNets reach a limit in SSL.</p><p>Transformers for vision. Vision Transformers (ViT) <ref type="bibr" target="#b20">(Dosovitskiy et al., 2021)</ref> shows the great potentials of generalizing Transformers for computer vision, by achieving compelling accuracy in supervised learning, especially with large-scale data and high capacity models. <ref type="bibr">DeiT (Touvron et al., 2020)</ref> further provides an effective ViT training strategy to ease the adaption of Transformers for practitioners. Transformers have also been applied to other vision tasks, ranging from low-level tasks such as image generation <ref type="bibr">(Parmar et al., 2018;</ref><ref type="bibr" target="#b7">Chen et al., 2020b)</ref> and enhancement <ref type="bibr" target="#b6">(Chen et al., 2020a;</ref><ref type="bibr" target="#b52">Yang et al., 2020)</ref>, to high-level tasks such as object detection <ref type="bibr" target="#b2">(Carion et al., 2020;</ref><ref type="bibr" target="#b61">Zhu et al., 2020;</ref><ref type="bibr" target="#b59">Zheng et al., 2020;</ref> and segmentation <ref type="bibr" target="#b6">(Wang et al., 2020a;</ref><ref type="bibr">c)</ref>, and to vision-language tasks <ref type="bibr" target="#b41">(Lu et al., 2019;</ref><ref type="bibr">Tan &amp; Bansal, 2019;</ref><ref type="bibr" target="#b12">Chen et al., 2019;</ref><ref type="bibr">Su et al., 2019</ref>; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>63.6%</head><p>DetCo Contrastive Multi-level features with three contrastive tasks between global images and local patches are considered: global-global, global-local, local-local.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving dense visual prediction task performance</head><p>Improving object detection performance. DetCo achieves the best performance trade-off on both classification and detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>68.6%</head><p>PixPro Contrastive Features from the two views are encouraged to be consistent between a regular patch representation and a smoothed patch representation within the same image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving dense visual prediction task performance</head><p>Mostly focusing on the improved performance on detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>66.3%</head><p>InstLoc Contrastive image instances are pasted at various locations and scales onto background images. The pretext task is to predict the instance category given the composited images and the foreground bounding boxes Improving dense visual prediction task performance</p><p>Mostly focusing on the improved performance on detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61.7%</head><p>EsViT (ours) Noncontrastive A pairwise cross-entropy loss at the patch level between two positive views.</p><p>No need/interaction with other images in the batch (eg, no negative samples)</p><p>Recovering the automatic correspondence learning property of self-supervised monolithic transformers, and thus improving learning efficiency.</p><p>Consistently improving image classification tasks. It creates new SoTA 81.3% on ImageNet linear probe accuracy, showing 3.5x parameter-efficient and has 10x higher throughput than previous SoTA MoCo-v3. Reporting 75.7% ImageNet linear probe performance for ResNet-50. <ref type="table">Table 7</ref>: Discussion of related works on various region-level tasks. The last columns reports the ImageNet linear probe performance for ResNet-50 trained with 2 augmented views for 200 epochs. <ref type="bibr" target="#b35">Li et al., 2019b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b60">Zhou et al., 2020;</ref><ref type="bibr" target="#b38">Li et al., 2020c)</ref>. Marrying Transformers with multi-stage architectures <ref type="bibr">(Vaswani et al., 2021;</ref><ref type="bibr" target="#b24">Wang et al., 2021;</ref> show higher classification accuracy in supervised learning, and enables applicability of Transformers for a broader range of vision tasks. Given these properties, we believe multi-stage ViT is a must-study baseline for SSL in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>69.9%</head><p>Discussion with other region-level tasks. In <ref type="table">Table 7</ref>, we compare L R against the existing regionlevel tasks, including PIRL <ref type="bibr" target="#b42">(Misra &amp; Maaten, 2020)</ref>, DenseCL <ref type="bibr" target="#b43">(Wang et al., 2020b)</ref>, DetCo <ref type="bibr" target="#b46">(Xie et al., 2021a)</ref>, InstLoc , PixPro <ref type="bibr" target="#b49">(Xie et al., 2021c)</ref>. Most of these region-level tasks improve object detection tasks, but hurt the ImageNet classification accuracy. DetCo achieves the best trade-off: improving the performance of both tasks, but with a sophisticated multi-level, global-local interaction algorithm. With the same number of pre-training epochs and augmented views, EsViT achieve the best ImageNet linear probe accuracy among all region-level tasks, with as minimum computational overhead as possible. This well serves our goal of building efficient SSL SoTA image classification system. <ref type="table" target="#tab_2">Table  2</ref> in DINO    <ref type="table">Table 8</ref>: Throughput estimate and standardization. All numbers in orange are estimated/converted, while numbers in blue are collected from the papers, and numbers in green are runs on our machines. All papers report the throughput of ViT-B or DeiT-B, which are essentially the same model. We use this fact to align the throughput reported in different papers. ? This number is estimated via the statement in <ref type="bibr" target="#b11">(Chen et al., 2021)</ref> that "reducing the patch size to 7 ? 7 keeps the model size unchanged, but increases FLOPs to ? 6?". All numbers are standardized into throughput reported by .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTS C.1 THROUGHPUT ESTIMATE AND CONVERSION</head><p>Since different papers report throughput on different hardwares, it is not ready to compare the numbers directly. Noting that all papers report the throughput for ViT-B/DeiT-B, we use this number to align and convert the throughput. In <ref type="table">Table 8</ref>, we describe our process and results of standardizing the throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 THE COMPUTATION AND MEMORY OVERHEAD OF THE PROPOSED L R</head><p>We emphasize that adding L R to the multi-stage transformer architectures yields acceptable extra computational cost, while adding L R directly to the monolithic transformer architectures has a huge computational overhead. To demonstrate this, we report the cost comparisons in <ref type="table" target="#tab_14">Table 9</ref>. For each setting, we report [Memory Usage (MB) / Running time per iteration (second/iteration)]. In <ref type="table" target="#tab_14">Table Table 9</ref> (a), when the batch size is gradually increased (e.g., , batch-size=12), the memory cost increases nearly 4 times for monolithic architectures, while increases 1.6 times for multi-stage architectures. Similar trends are shown for training cost per iteration increase ratio (1.47 vs 1.15). This indicates L R can more naturally fit multi-stage architectures.</p><p>Similarly, we compare computational cost comparisons [Memory Usage (MB) / Running time per iteration (second/iteration)] in <ref type="table" target="#tab_14">Table 9</ref> (b), for other network architecture configurations . From the increased cost ratio, we see that L R adds acceptable cost in terms of both memory and training time, compared with the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 EXPERIMENTAL SETTINGS OF PRE-TRAINING AND EVALUATION ON IMAGENET</head><p>We study unsupervised pre-training performed in ImageNet-1K dataset <ref type="bibr" target="#b15">(Deng et al., 2009</ref>) without labels. The default training details are described as follows, mostly following . We train with the Adamw optimizer <ref type="bibr" target="#b40">(Loshchilov &amp; Hutter, 2018)</ref>, a batch size of 512, and total epochs 300. Linear warmup of the learning rate is used during the first 10 epochs, with its base value   determined with the linear scaling rule <ref type="bibr" target="#b23">(Goyal et al., 2017)</ref>: lr = 0.0005 * batchsize/256. After this warmup, the learning rate is decayed with a cosine schedule. We build our systems based on Swin Transformers  in our experiments. Swin-B has a model size and computation complexity similar to ViT-B/DeiT-B (patch size 16). We also considered Swin-T and Swin-S, which have the complexity that are similar to those of ResNet-50 (DeiT-S) and ResNet-101, respectively. The default window size is W = 7.</p><p>One major common protocol to evaluate SSL is linear probe on ImageNet-1K, where features are extracted from a frozen backbone, and a supervised linear classifier is trained. For all Transformer models, we use the concatenation of view-level featuresz in the last 4 layers (the results are similar to the use of last 3 or 5 layers in our initial experiments).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 LINEAR PROBE ON A SUITE OF SMALL DATASETS</head><p>Datasets. <ref type="table" target="#tab_15">Table 10</ref> shows details and source of all datasets used for linear probe, including the number of classes, the size of training set and testing set, metrics used in evaluation, as well as a public source of the dataset. Note that original UCF101 dataset is a video dataset. Here the middle frame of each video is extracted to form a classification dataset. There are 3 train/val splits in Tensorflow, we use the first one.</p><p>Automatic hyper-parameter tuning. We rigorously follow <ref type="bibr">(Radford et al., 2021)</ref> to conduct training and evaluation for linear probe on the downstream datasets. We train a logistic regression classifier using scikit-learn's L-BFGS implementation, with maximum 1, 000 iterations, and report the corresponding metric for each dataset. We determine the L 2 regularization strength ? using a hyperparameter sweep on the validation sets over the range between 10 ?6 and 10 6 , with 96 logarithmically spaced steps. To save compute required for the sweeps, we perform a parametric binary search that starts with ? = [10 ?6 , 10 ?4 , 10 ?2 , 1, 10 2 , 10 4 , 10 6 ] and iteratively halves the interval around the peak until it reaches a resolution of 8 steps per decade. The hyperparameter sweeps are performed on a validation split of each dataset. For the datasets that contain a validation split in addition to a test split, we use the provided validation set to perform the hyperparameter search, and for the datasets that do not provide a validation split or have not published labels for the test data, we split the training dataset to perform the hyperparameter search. For the final result, we combine the validation split back with the training split and report the performance on the unused split.</p><p>Detailed results. Only the last layer feature is considered for all models for simplicity, though adding features from more layers may potentially improve the results. <ref type="table" target="#tab_17">Table 11</ref> shows the results for architectures at a similar scale of ResNet-50 or Swin-T. The first two columns are numbers from <ref type="bibr">(Radford et al., 2021)</ref>. CLIP with ResNet-50 is pre-trained on 400 million image-text pairs. Supervised ResNet-50 and Swin-T are pre-trained on ImageNet-1K, on which EsViT with Swin-T is pre-trained as well (Batch Size=512). EsViT outperforms its supervised counterpart, and is on par with the performance of CLIP in a similar image encoder architecture scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 PRE-TRAINING DATASETS</head><p>We describe the statistics and training schedule on larger and less curated datasets in <ref type="table" target="#tab_3">Table 13</ref>. The pre-training epochs are chosen so that the model is trained with a similar number of augmented views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Description Size (#Images) Epochs Warmup</head><p>ImageNet-1K <ref type="bibr" target="#b15">(Deng et al., 2009)</ref> Images evenly distributed in 1K object concepts 1.2 million 300 10 WebVision-v1 <ref type="bibr" target="#b36">(Li et al., 2017)</ref> Web images with 1K concept queries from ImageNet-1K 2.4 million 150 5 OpenImages-v4 <ref type="bibr" target="#b30">(Kuznetsova et al., 2020)</ref> Diverse/complex scenes with several objects for detection 7.5 million 50 2 ImageNet-22K <ref type="bibr" target="#b15">(Deng et al., 2009)</ref> Images distributed in 22K object concepts in a hierarchy 14.2 million 30 1 <ref type="table" target="#tab_3">Table 13</ref>: Pre-train dataset statistics and training schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 RESULTS ON CORRESPONDENCE LEARNING</head><p>We first quantitatively evaluate the correspondence learning results with 50K images in the ImageNet validation dataset. We create a simple evaluation dataset with mild augmentations. For a center-crop image, we apply HorizontalFlip, then ColorJitter and RandomGrayscale to create a new augmented view. In this way, ground-truth correspondences are created. Please see the 1st row of <ref type="figure">Figure 9</ref> for one such example. The top-10 correspondences are used for evaluation. Two metrics are considered: (1) Accuracy measures the percentage of correctly matched region pairs, (2) distance error indicates the averaged 2 distance between the predicted matched region and ground-truth region (the value is 0 for perfect matching). The results are reported in <ref type="figure">Figure 8</ref>. DINO with monolithic Transformers shows surprisingly good performance on correspondence learning. The use of multi-stage Transformer architecture reduces this ability, shows a lack of good region correspondence. With L R , the region matching ability is significantly recovered.</p><p>In <ref type="figure">Figure 9</ref>, we visualize the correspondences for more images. Overall, DINO with monolithic Transformers is able to discover most salient correspondences of semantic meaning in the mild augmentation conditions, even without an implicit region matching loss in training. We believe this previously underestimated property is whole-noting, and has potentials to enable more applications. However, this desired property gets dilated when changing from monolithic to multi-stage Transformer architecture (from column 1 to column 2), then the proposed region level task can alleviate this issue (from column 2 to column 3).</p><p>To more specifically analyze the correspondences, we note the following results. The first row shows a simple case, where only images of left-to-right flipped views are presented. The ground-truth (a) Accuracy (b) Distance error <ref type="figure">Figure 8</ref>: Quantitative evaluation on correspondence learning on ImageNet validation set. L R can significantly improve correspondence learning quality for multi-stage architectures. As a reference, DINO (L V with monolithic Transformer architecture) achieves 0.95 accuracy and 2.49 distance error, which we believe is a strong evidence to identify the intriguing property of automatic correspondence learning.</p><p>correspondences should be horizontal lines that link the two flipped regions. It reveals that the view-level pre-train task alone is insufficient to learn good correspondences for the multi-stage Transformer architecture, while region matching task can alleviate this issue significantly. Similar observations are shown in row 3 and row 4.</p><p>We further study more cases that requires real-world correspondences in row 2, row 5 and row 6. These views are not generated with data augmentation (as in model pre-training), but are often presented in more practical scenarios: one-to-many mappings, cartoon-to-toy, seasonal changing of the scene, respectively. The proposed region matching task can work particularly well in those cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 MORE VISUALIZATION RESULTS OF ATTENTION MAPS</head><p>We visualize attention maps at the top layer in <ref type="figure" target="#fig_0">Figure 10</ref>, 11, 12. With a monolithic Transformer architecture, DINO can automatically identify the main foreground objects. Unfortunately, changing from monolithic to the multi-stage Transformer architecture (From left column to middle column), this property gets lost. There are more heads in the multi-stage architecture than monolithic architecture (24 heads vs 6 heads in this case) in the last year. A fair number of heads in EsViT shows redundant patterns, this issue can be reduced when the region-level matching task is added (From middle column to right column).</p><p>We observed that DINO with monolithic Transformer architecture only learns to attend the foreground objects, even when the query is a background region (see <ref type="figure" target="#fig_0">Figure 12</ref>). This is perhaps because DINO models are trained to learn view-level invariance, the main objects in the pre-train dataset ImageNet tend to be the principle factor that remains invariant across different augmented views. Hence, all backgrounds are ignored, regardless of the query positions. This is improved in EsViT with the region-level pre-train task, as the model is trained to match individual regions.</p><p>DINO shows high entropy values in all of 6 heads (perhaps a required condition to cover all regions of the main object). In EsViT, L R plays an interesting role in modulating the entropy distributions among heads: it increases those with larger entropy values, while decreasing those with lower entropy values. In another word, it makes the attention patterns in different heads more diverse. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>, where new SoTA result by linear probe evaluation on Linear probing accuracy (Efficiency vs accuracy comparison under the linear classification protocol on ImageNet. Left: Throughput of all SoTA SSL vision systems, circle sizes indicates model parameter counts;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>&lt; l a tFigure 2 :</head><label>2</label><figDesc>e x i t s h a 1 _ b a s e 6 4 = " 0 f q N N X 3 h j o d X G n 3 X o g T / 1 + e P t 9 I = " &gt; A A A B / 3 i c b V D L S s N A F J 3 U V 6 2 v q O D G T b A I r k o i o i 6 L b l y 4 q G I f 0 I Q w m U 7 b o Z N J m L k R S 8 z C X 3 H j Q h G 3 / o Y 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J Y s 4 U 2 P a 3 U V p Y X F p e K a 9 W 1 t Y 3 N r f M 7 Z 2 W i h J J a J N E P J K d A C v K m a B N Y M B p J 5 Y U h w G n 7 W B 0 m f v t e y o V i 8 Q d j G P q h X g g W J 8 R D F r y z T 0 3 x D A k m K f X m Z + 6 Q B 8 g v c 0 y 3 6 z a N X s C a 5 4 4 B a m i A g 3 f / H J 7E U l C K o B w r F T X s W P w U i y B E U 6 z i p s o G m M y w g P a 1 V T g k C o v n e T P r E O t 9 K x + J P U T Y E 3 U 3 x s p D p U a h 4 G e z N O q W S 8 X / / O 6 C f T P v Z S J O A E q y P R Q P + E W R F Z e h t V j k h L g Y 0 0 w k U x n t c g Q S 0 x A V 1 b R J T i z X 5 4 n r e O a c 1 p z b k 6 q 9 Y u i j j L a R w f o C D n o D N X R F W q g J i L o E T 2 j V / R m P B k v x r v x M R 0 t G c X O L v o D 4 / MH E a q W y Q = = &lt; / l a t e x i t &gt; L R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y i n n P X K R C u C q M A F Z i N e Z 7 W u y 5 2 w = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 C R b B V U l E 1 G X R j Q s X F e w D m h A m 0 0 k 7 d D I J M z d i i V n 4 K 2 5 c K O L W 3 3 D n 3 z h p s 9 D W A w O H c + 7 l n j l B w p k C 2 / 4 2 F h a X l l d W K 2 v V 9 Y 3 N r W 1 z Z 7 e t 4 l Q S 2 i I x j 2 U 3 w I p y J m g L G H D a T S T F U c B p J x h d F X 7 n n k r F Y n E H 4 4 R 6 E R 4 I F j K C Q U u + u e 9 G G I Y E 8 + w m 9 z M X 6 A N k 7 T z 3 z Z p d t y e w 5 o l T k h o q 0 f T N L 7 c f k z S i A g j H S v U c O w E v w x I Y 4 T S v u q m i C S Y j P K A 9 T Q W O q P K y S f 7 c O t J K 3 w p j q Z 8 A a 6 L + 3 s h w p N Q 4 C v R k k V b N e o X 4 n 9 d L I b z w M i a S F K g g 0 0 N h y i 2 I r a I M q 8 8 k J c D H m m A i m c 5 q k S G W m I C u r K p L c G a / P E / a J 3 X n r O 7 c n t Y a l 2 U d F X S A D t E x c t A 5 a q B r 1 E Q t R N A j e k a v 6 M 1 4 M l 6 M d + N j O r p g l D t 7 6 A + M z x 8 X w p b N &lt; / l a t e x i t &gt; Pre-training objectives, including viewlevel (left) and region-level (right) prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>c h C a m e ly o n O x fo r d P e t s S t a n fo r d C a r s S T L 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The learned correspondences. Yellow lines are the top-10 correspondences between two views, where the numbers indicates the rankings of similarity scores, yellow dots with the same number are paired. (a) DINO: DeiT-S (b) EsViT: L V (c) EsViT: L V +L R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>##</head><label></label><figDesc>gs, gt: student and teacher networks # Cv, Cr: view and region center (K) # tmp s, tmp t: student and teacher temperatures # a, b: network and center momentum rates. # n: batch size, K: MLP-head-projected probability vector length, T: last layer feature map length, P: last layer feature vector length 1 gt.params = gs.params # The main training loop 2 for x in loader:3 x1, x2 = augment(x), augment(x) # two random views 4 # student output, p:n?K, pz:n?T?K, z:n?T?P 5 p s1, pz s1, z s1 = gs(x1) 6 p s2, pz s2, z s2 = gs(x2) # teacher output, p:n?K, pz:n?T?K, z:n?T?P 7 p t1, pz t1, z t1 = gt(x1) Hr(pz s1, pz t2, z s1, z t2)/2 + Hr(pz s2, pz t1, z s2, z t1)/2 12 loss = loss v/2 + loss r/2 13 loss.backward() # back-propagate 14 # update student, teacher and centers 15 update(gs) # AdamW for student 16 gt.params = a * gt.params + (1 ? a) * gs.params # EMA for teacher 17 Cv = b * Cv + (1 ? b) * cat([p t1,p t2].mean(0)) # EMA for view center 18 Cr = b * Cr + (1 ? b) * cat([pz t1,pz t2].mean(0)) # EMA for region center 19 ((pt -Cr) / tmp t, dim=-1) # n?T?K 29 sim matrix = torch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(ii) Vision Longformer (ViL) (Zhang et al., 2021): Features in each local window are further allowed to attend all features in the 8-neighboring windows. (iii) Convolution vision Transformer (CvT) (Wu et al., 2021): Features in neighboring windows are considered in the convolutional projection in self-attentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Architecture comparison. (a) The monolithic transformer. For all layers, the transformer blocks share the same network configurations and input token sequence sizes are the same. (b) The multi-stage Transformer organizes an input image into a long sequence of smaller patches, sparse self-attentions (S.A.) are utilized at early stages to maintain model expressiveness while reducing computational complexity; The neighboring tokens at an intermediate layer are gradually merged, constituting a short sequence to ease the compute burden of self-attention at late stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :Figure 10 :Figure 12 :</head><label>91012</label><figDesc>The learned correspondences. Yellow lines are the top-10 correspondences between two views, where the numbers indicates the rankings of similarity scores, yellow dots with the same number are paired. The blue dot and red triangle indicates the most similar local regions that correspond to the global feature of the view itself and the other view, respectively. Please zoom in for detailed correspondence mappings. (b) EsViT: L V (c) EsViT: L V +L R The learned attention maps for all heads at the top layer, ranked by the entropy of softmax probability. Query is the blue dot in the top-left of the image. Top: Entropy of each heads. Middle: top 60% probability mass. Bottom: full attention maps. L R shows more attention patterns than L V only. (a) DINO: DeiT-S (b) EsViT: L V (c) EsViT: L V +L R The learned attention maps for all heads at the top layer, ranked by the entropy of softmax probability. Query is the blue dot in the top-left of the image. Top: Entropy of each heads. Middle: top 60% probability mass. Bottom: full attention maps. DINO mainly attends the main object even when the query is a background region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>COCO Detection &amp; Segmentation.</figDesc><table><row><cell>Pre-train Data</cell><cell cols="2">ImageNet-1K</cell><cell cols="2">18 Datasets</cell></row><row><cell></cell><cell>Linear</cell><cell>k-NN</cell><cell cols="2">Scores # Wins</cell></row><row><cell>Supervised</cell><cell>-</cell><cell>-</cell><cell>77.29</cell><cell>-</cell></row><row><cell>ImageNet-1K</cell><cell cols="3">78.0 (77.1) 75.7 (73.7) 80.66</cell><cell>16</cell></row><row><cell>WebVision-v1</cell><cell cols="3">75.9 (75.4) 71.2 (69.4) 80.00</cell><cell>14</cell></row><row><cell cols="4">OpenImages-v4 70.6 (69.6) 62.0 (60.3) 77.97</cell><cell>10</cell></row><row><cell>ImageNet-22K</cell><cell cols="3">75.0 (73.5) 67.9 (66.1) 81.03</cell><cell>17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Impact of the pre-train datasets.? Detection and segmentation on COCO. Different from previous monolithic self-supervised ViT, the multi-stage architecture in EsViT can be readily used for dense visual tasks that require hierarchical feature representations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Method #Param. Im./s Pre-train tasks Linear k-NN</figDesc><table><row><cell>DeiT R-50 Swin ViL CvT PvT</cell><cell>21 24 28 28 29 24</cell><cell>1007 1237 808 386 848 851</cell><cell>L V L V L V L V +L R L V L V +L R L V L V +L R L V L V +L R L V L V +L R</cell><cell>75.9 75.3  ? 67.5  ? 73.2 75.0 69.3 75.7 71.2 77.1 73.7 77.6 75.4 77.3 73.9 77.5 74.5 77.6 74.8 78.5 76.7 75.4 72.0 76.3 72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Different architectures with and without L R . DeiT and ResNet-50 are shown as references.</figDesc><table><row><cell></cell><cell cols="2">LV (Tiny) LV + LR (Tiny)</cell><cell cols="2">LV (Small) LV + LR (Small)</cell><cell>LV (Base) LV + LR (Base)</cell></row><row><cell></cell><cell>78</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>76</cell><cell></cell><cell></cell><cell></cell></row><row><cell>k-NN (%)</cell><cell>74</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell>100</cell><cell># Epoch</cell><cell>200</cell></row><row><cell cols="6">Figure 4: Learning curves of different pre-</cell></row><row><cell cols="6">training tasks. For Base model, L R is added from the 200th epoch.</cell></row></table><note>? Numbers reported in (Caron et al., 2021).9 (+0.7) 61.7 (+1.8) 38.0 (+0.2) 33.2 (+0.1)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Comparison between contrastive and non-contrastive region-matching tasks.compared in Appendix</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Model configurations considered in our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>in MLP-Mixer (Tol-</cell><cell>Table 1 in Swin (Liu</cell><cell>Our runs</cell></row><row><cell></cell><cell></cell><cell>stikhin et al., 2021)</cell><cell>et al., 2021)</cell></row><row><cell>DeiT-S / P = 16</cell><cell>1007</cell><cell></cell><cell>940.4</cell></row><row><cell>DeiT-B / P = 16</cell><cell>312</cell><cell></cell><cell>292.3</cell></row><row><cell>DeiT-S / P = 8</cell><cell>180</cell><cell></cell><cell></cell></row><row><cell>DeiT-B / P = 8</cell><cell>63</cell><cell></cell><cell></cell></row><row><cell>ViT-B / P = 16</cell><cell>312</cell><cell>861</cell><cell></cell></row><row><cell>ViT-S / P = 16</cell><cell>102</cell><cell>280</cell><cell></cell></row><row><cell>ViT-H / P = 14</cell><cell>32</cell><cell>87</cell><cell></cell></row><row><cell>ViT-L / P = 7</cell><cell>17</cell><cell>47  ?</cell><cell></cell></row><row><cell>Swin-T / W = 7</cell><cell>808</cell><cell></cell><cell>755.2</cell><cell>726.13</cell></row><row><cell>Swin-S / W = 7</cell><cell>467</cell><cell></cell><cell>436.9</cell></row><row><cell>Swin-B / W = 7</cell><cell>297</cell><cell></cell><cell>278.1</cell></row><row><cell cols="2">Swin-T / W = 14 660</cell><cell></cell><cell></cell><cell>593.24</cell></row><row><cell cols="2">Swin-S / W = 14 383</cell><cell></cell><cell></cell><cell>344.20</cell></row><row><cell cols="2">Swin-B / W = 14 254</cell><cell></cell><cell></cell><cell>228.36</cell></row><row><cell>ViL-T / W = 7</cell><cell>386</cell><cell></cell><cell></cell><cell>346.72</cell></row><row><cell>CvT-T / W = 7</cell><cell>848</cell><cell></cell><cell></cell><cell>761.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="6">: Computational cost comparisons in the format of [Memory Usage (MB) / Running time per</cell></row><row><cell>iteration (second/iteration)].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">Classes Train size Test size Evaluation metric</cell><cell>Source link</cell></row><row><cell>Food-101</cell><cell>102</cell><cell>75,750</cell><cell>25,250</cell><cell>Accuracy</cell><cell>Tensorflow</cell></row><row><cell>CIFAR-10</cell><cell>10</cell><cell>50,000</cell><cell>10,000</cell><cell>Accuracy</cell><cell>TensorFlow</cell></row><row><cell>CIFAR-100</cell><cell>100</cell><cell>50,000</cell><cell>10,000</cell><cell>Accuracy</cell><cell>TensorFlow</cell></row><row><cell>SUN397</cell><cell>397</cell><cell>19,850</cell><cell>19,850</cell><cell>Accuracy</cell><cell>Tensorflow</cell></row><row><cell>Stanford Cars</cell><cell>196</cell><cell>8,144</cell><cell>8,041</cell><cell>Accuracy</cell><cell>Stanfold Cars</cell></row><row><cell>FGVC Aircraft (variants)</cell><cell>100</cell><cell>6,667</cell><cell>3,333</cell><cell>Mean-per-class</cell><cell>FGVC website</cell></row><row><cell>VOC2007 classification</cell><cell>20</cell><cell>5,011</cell><cell>4,952</cell><cell>11-point mAP</cell><cell>voc2007</cell></row><row><cell>Describable Textures</cell><cell>47</cell><cell>3,760</cell><cell>1,880</cell><cell>Accuracy</cell><cell>TensorFlow</cell></row><row><cell>Oxford-IIIT Pets</cell><cell>37</cell><cell>3,680</cell><cell>3,669</cell><cell>Mean-per-class</cell><cell>Oxford-IIIT Pet</cell></row><row><cell>Caltech-101</cell><cell>102</cell><cell>3,060</cell><cell>6084</cell><cell>Mean-per-class</cell><cell>TensorFlow</cell></row><row><cell>Oxford Flowers 102</cell><cell>102</cell><cell>2,040</cell><cell>6,149</cell><cell>Mean-per-class</cell><cell>TensorFlow</cell></row><row><cell>MNIST</cell><cell>10</cell><cell>60,000</cell><cell>10,000</cell><cell>Accuracy</cell><cell>TensorFlow</cell></row><row><cell>Facial Emotion Recog. 2013  *</cell><cell>8</cell><cell>32,298</cell><cell>3,589</cell><cell>Accuracy</cell><cell>Kaggle fer2013</cell></row><row><cell>STL10</cell><cell>10</cell><cell>5,000</cell><cell>8,000</cell><cell>Accuracy</cell><cell>TensorFlow</cell></row><row><cell>GTSRB  *</cell><cell>43</cell><cell>26,728</cell><cell>12,630</cell><cell>Accuracy</cell><cell>GTSRB website</cell></row><row><cell>PatchCamelyon</cell><cell>2</cell><cell>294,912</cell><cell>32,768</cell><cell>Accuracy</cell><cell>TensorFlow</cell></row><row><cell>UCF101  *</cell><cell>101</cell><cell>9,537</cell><cell>3783</cell><cell>Accuracy</cell><cell>TensorFlow</cell></row><row><cell>Hateful Memes</cell><cell>2</cell><cell>8,500</cell><cell>500</cell><cell>ROC-AUC</cell><cell>FaceBook</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>A suite of 18 datasets used in linear probe. * indicates dataset whose train/test size we obtained is slightly different fromTable 9in(Radford et al., 2021).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>The linear probe results on 18 datasets at the scale of ResNet-50/Swin-T. ? indicates the results reproduced by us, which verifies that our implementation pipeline is consistent with(Radford  et al., 2021).</figDesc><table><row><cell>Method</cell><cell>View-level</cell><cell>Region-level</cell><cell>Top-1 Accuracy (%)</cell></row><row><cell cols="3">Performance comparison of ResNet-50 with 200 epochs and 2 augmented views</cell><cell></cell></row><row><cell>MoCo-v2</cell><cell>Contrastive</cell><cell>-</cell><cell>67.5</cell></row><row><cell>DenseCL</cell><cell>Contrastive</cell><cell>Contrastive</cell><cell>63.6</cell></row><row><cell>DetCo</cell><cell>Contrastive</cell><cell>Contrastive</cell><cell>68.6</cell></row><row><cell>DINO</cell><cell>Non-Contrastive</cell><cell>-</cell><cell>69.2</cell></row><row><cell>EsViT</cell><cell>Non-Contrastive</cell><cell>Non-Contrastive</cell><cell>69.9</cell></row><row><cell cols="4">SoTA performance comparison of ResNet-50 with numbers and settings reported in each paper</cell></row><row><cell>MoCo-v2 (800 epochs)</cell><cell>Contrastive</cell><cell>-</cell><cell>72.2</cell></row><row><cell>SwAV (800 epochs, w/ multi-crop)</cell><cell>Contrastive</cell><cell>-</cell><cell>75.3</cell></row><row><cell>Barlow Twins (1000 epochs)</cell><cell>-</cell><cell>-</cell><cell>73.2</cell></row><row><cell>VICReg (1000 epochs)</cell><cell>-</cell><cell>-</cell><cell>73.2</cell></row><row><cell>SimSiam (800 epochs, 2 views)</cell><cell>Non-Contrastive</cell><cell>-</cell><cell>71.3</cell></row><row><cell cols="2">BYOL (1000 epochs, w/ multi-crop) Non-Contrastive</cell><cell>-</cell><cell>74.3</cell></row><row><cell>DINO (300 epochs, w/ multi-crop)</cell><cell>Non-Contrastive</cell><cell>-</cell><cell>75.0</cell></row><row><cell>EsViT (300 epochs, w/ multi-crop)</cell><cell>Non-Contrastive</cell><cell>Non-Contrastive</cell><cell>75.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Linear probe performance of a ResNet-50 network with different SSL methods.C.4 COMPARISON WITH A RESNET-50 BACKBONETo compare our EsViT learning method with other SSL algorithms, we conduct experiments with a ResNet-50 backbone, and show the results inTable 12.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This set often contains views of two different resolutions V = [Vg, V l ], where Vg = {xg i |i = 1, 2} is a global-view set of higher resolution, and V l = {x l i |i = 1, . . . , 8} is a local-view set of lower resolution. All views V are passed through the student while only the global views Vg are passed through the teacher.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/microsoft/Swin-Transformer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2022 (a) DINO: DeiT-S</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">UP-DETR: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Discriminative unsupervised feature learning with exemplar convolutional neural networks. T-PAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari S Morcos. Training</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batchnorm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batchnorm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00152</idno>
		<title level="m">On the expressive power of random features in CNNs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-supervised pre-training with hard examples improves visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13493</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pretraining for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Fixing weight decay regularization in Adam</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">VilBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09157</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Detco: Unsupervised contrastive learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Self-supervised learning with swin transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><forename type="middle">Urtasun</forename><surname>Loco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01342</idno>
		<title level="m">Local contrastive representation learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Instance localization for self-supervised detection pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Divide and contrast: Self-supervised learning from uncurated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yonglong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08054</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Yew-Soon Ong, and Chen Change Loy. Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15358</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09315</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Unified vision-language pre-training for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">(a) DINO: DeiT-S (b) EsViT: L V (c) EsViT: L V +L R Figure 11: The learned attention maps for all heads at the top layer, ranked by the entropy of softmax probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Local aggregation for unsupervised learning of visual embeddings. Query is the blue dot in the center of the image. Top: Entropy of each heads. Middle: top 60% probability mass. Bottom: full attention maps. L R shows more attention patterns than L V only</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
