<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LOCL: Learning Object-Attribute Composition using Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Kumar</surname></persName>
							<email>satishkumar@ucsb.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekta</forename><surname>Prashnani</surname></persName>
							<email>eprashnani@ucsb.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ASM Iftekhar</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ECE Department</orgName>
								<orgName type="institution">University of California Santa</orgName>
								<address>
									<settlement>Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LOCL: Learning Object-Attribute Composition using Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>KUMAR ET. AL.: LOCL 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes LOCL: Learning Object-Attribute (O-A) Composition using Localization -that generalizes composition zero shot learning to objects in cluttered/more realistic settings. The problem of unseen O-A associations has been well studied in the field, however, the performance of existing methods is limited in challenging scenes. In this context, our key contribution is a modular approach to localizing objects and attributes of interest in a weakly supervised context that generalizes robustly to unseen configurations. Localization coupled with a composition classifier significantly outperforms state-of-the-art (SOTA) methods, with an improvement of about 12% on currently available challenging datasets. Further, the modularity enables the use of localized feature extractor to be used with existing O-A compositional learning methods to improve their overall performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: The object of interest shown in images A.1, A.2, and A.3 presents simple scenarios where all SOTA (SymNet <ref type="bibr" target="#b16">[17]</ref>, CGE <ref type="bibr" target="#b21">[22]</ref>, CompCos <ref type="bibr" target="#b17">[18]</ref>) methods make correct O-A associations. However, for the same object (apple) in a more cluttered scene in image B.1, these methods fail. Even in cases where there is a dominant object of interest, such as a bird in (B.2), where there is significant background clutter, most of the SOTA methods have incorrect O-A associations.</p><p>Inspired by these limitations, we propose Learning Object-Attribute Composition using Localization (LOCL). Our model (LOCL) leverages spatially-localized learning, which is not present in the existing CZSL networks. It is reasonable to ask Why not first localize the objects and then associate the attributes? In principle, this can be done, however, the SOTA methods for object detection and localization use extensive datasets for their training. Hence it will not be possible to meaningfully test the CZSL with pre-trained detectors. The images shown in <ref type="figure">Fig. 1</ref> are from existing datasets for CZSL methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr">37]</ref>. We note that all the experiments reported in this paper use the datasets that are created for evaluating CZSL approaches.</p><p>Existing SOTA object-attribute detection approaches do not take into account the possibility of scene attributes confounding with correct O-A composition prediction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. These methods are designed to work with wholistic image features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr">36]</ref>. Some recent work address this issue by partitioning the image into regions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">35]</ref> or equal-size grid cells <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">38]</ref>, but they are not very effective in capturing distinctive object features.</p><p>Our approach towards better generalization of CZSL to more challenging images ( <ref type="figure">Fig.  1.B</ref>) with background clutter is to leverage localized feature extraction in O-A composition. Specifically, we adopt a two step approach. First, a Localized Feature Extractor (LFE) associates an object with its attribute by reducing the interference arising from additional attribute-related visual cues occurring elsewhere in the image. The CZSL benchmark datasets do not contain any localization information. As noted before, off-the-shelf object detectors can be inadvertently exposed <ref type="bibr" target="#b26">[27]</ref> to unseen OA compositions. Therefore, we developed a weakly supervised method for localized feature extraction. Second, the composition classifier uses the localized distinctive visual features to predict an O-A pair.</p><p>The proposed LOCL outperforms competing CZSL methods on all existing datasets -including the more challenging CGQA -providing a strong evidence in favor of its applicability to more realistic scenes. Further, the performance of all existing methods improve when our localized feature extractor is included as a pre-processing module -although LOCL still outperforms these methods. <ref type="figure">Figure 2</ref>: LOCL architecture. The Localized Feature Extractor (Section: 3.1) generates proposals that are likely to contain objects. These proposals are refined with the object and attribute semantics using Composition Classifier (Section: 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Existing work in object attribute (O-A) CZSL task typically assume that the images of the object of interest present in uncluttered context. This assumption is also true for the initial CZSL datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">37]</ref>. As a result, most of the CZSL methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr">36]</ref> perform quite well on uncluttered scenes. As noted before, some of the methods reduce the interference from confounding elements by partitioning the image. <ref type="bibr">[38]</ref> is designed for datasets with a dominant object with a clear background. <ref type="bibr" target="#b6">[7]</ref> partitions the image to equal-sized grid cells and relies on aligning the attribute semantic and visual features. Further, the dataset used in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">38]</ref> consist of one object type,e.g., face or bird images.</p><p>The O-A problem is considered as a matching problem in a latent space <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. For this matching task, <ref type="bibr" target="#b24">[25]</ref> proposes a modular network with a dynamic gating whereas <ref type="bibr" target="#b20">[21]</ref> defines objects and attributes using a support vector machine (SVM). On the other hand, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> consider attributes as functional operation over objects. More recently <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr">36]</ref> focus on the relationships among attributes and objects. <ref type="bibr" target="#b0">[1]</ref> disentangle attributes and objects with metric based learning. <ref type="bibr" target="#b21">[22]</ref> learns attribute-object dependence in a semi-supervised graph structure where unseen combinations are considered connected during training. This is extended in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">36]</ref> where all possible combination of objects and attributes are considered during inference.</p><p>In general, the performance of current methods drop significantly on images with background clutter. Taking inspiration from the generic pipeline of weakly supervised object detection (WSOD) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>, LOCL consists of a region proposal network with pseudo-label generation module that leverages supervision from linguistic embeddings in a novel contrastive framework. This feature extraction can be utilized to improve the existing network's performance in images with background clutter as shown in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The primary issue to be addressed is the scene complexity, that require that the methods are able to make the correct associations during the training phase and predict the unseen configuration during testing. The proposed method is intuitive and straightforward in creating a weakly supervised framework that is modular and generalizes well. The LOCL extracts localized features of object regions in the image, which allows it to learn useful OA relationships and also suppress spurious O-A relations from the background clutter.</p><p>First, we pre-train a Localized Feature Extractor LFE(.) (Sec. 3.1) network to extract features from multiple regions of the image. Second, the pre-trained LFE(.) along with a Composition Classifier CC(.) (Sec. 3.2) network learns to detect the O-A composition. The Proposed Network: The proposed LOCL is as (?,?) = CC ( LFE (I), T a , T o ), where LFE(.) and CC(.) are trainable networks. LOCL is trained in two stages. In the first stage, given an image I, pre-training of LFE(.) is done to generate multiple localized features. The details of the LFE(.) module are discussed in Sec. 3.1. The output of the trained network LFE(.) is a list of n features of object regions identified in the image.</p><p>In the second stage, out of these n features, r features (r &lt; n) are input to the composition classifier ( Sec. 3.2) to make the final prediction of attribute and object present in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-training Localized Feature Extractor(LFE(.))</head><p>Our Localized Feature Extractor network LFE(.) is a combination of an image encoder (ResNet-50 <ref type="bibr" target="#b4">[5]</ref>), text encoder, Region Proposal Network (RPN), and a pseudo label generator, as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. The RPN is inspired from F-RCNN <ref type="bibr" target="#b26">[27]</ref>. It is trained from scratch using a contrastive learning framework. It generates proposals features for regions in the image that has high likelihood of object presence. The pseudo label generator creates labels to supervise the visual space that have high semantic similarity with ground truth O-A pair.</p><p>Given the input image I, the image encoder generates feature map F ? R H ?W ?C where H ,W and C are the height, width and channel dimensions. Then n valid anchors (based on input image size, in our case for an image of 256 ? 256, n = 576,) are generated on the input image <ref type="bibr" target="#b26">[27]</ref>. Anchors are a set of rectangular boxes with different aspect ratio and scale generated at each pixel of the input image <ref type="bibr" target="#b26">[27]</ref>. Corresponding to each anchor, a list of features is pooled from F. The pooled anchor features are [f anc 1 , f anc 2 , ..., f anc n ] ? R C shown as output of "Anchor Generator" in <ref type="figure" target="#fig_0">Fig. 3</ref> The text encoder generates semantic pair embedding from the input text label (a : Blue, o : Bird) pair. With the help of these semantic pair embedding, we generate pseudo ground truth labels to train LFE(.) network with weak supervision <ref type="bibr" target="#b28">[29]</ref>. In the following, we refer to "pseudo ground truth labels" as "pseudo labels" for simplicity.</p><p>Pseudo Label Generator: The ground truth O-A semantic pair embeddings generated from text encoder are projected through fully connected layers (FFN) into a common subspace as visual embeddings. The output of FFN is denoted by f text ao ? R C , where "ao" index is the ground truth (in our case one per image). Here the length C of semantic embedding equal to channel dimension C of visual feature vector F. Now to generate pseudo labels, a cosine similarity score is computed between each f anc k and f text ao .</p><formula xml:id="formula_0">? k = f text ao ? f anc k ||f text ao || ||f anc k || ? f anc k , where (? = [? 1 , ? 2 , .., ? k , .., ? n ])<label>(1)</label></formula><formula xml:id="formula_1">y = 1 argsort(? )[0 : l] 0 f or all other indexes<label>(2)</label></formula><p>where y = [y 1 , y 2 , .., y k , .., y n ], l top anchors are selected out of n based on cosine similarity score ? . They are assigned with label 1 in y and rest are assigned 0 as shown above with Eq. 8. Here each y k represents the presence/absence of object of interest regions in the image.</p><p>Intuition here is that f anc k 's which contains the object will lie closer to f text ao in feature space. Contrastive Pre-training: Recall that the current benchmark CZSL datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">37]</ref> do not have ground truth bounding boxes for objects of interest. For this reason, we use both the anchor features and proposal features to localize the objects of interest. This is different from regular object detection networks <ref type="bibr" target="#b26">[27]</ref>. The pseudo label y informs the network which anchor features are likely to represent object(s) of interest. Using contrastive learning, we train the regional proposal network branch to localize the object with weak supervision. The goal of contrastive learning is to maximize the similarity between similar feature vectors and minimize the similarity between the dissimilar feature vectors. Here, the objective is to maximize the cosine similarity between &lt; f anc k , f p k &gt; features where there is a possibility of object being present and minimize in all other cases. The contrastive objective function is:</p><formula xml:id="formula_2">L CON = n ? k=1 (1 ? y k ) * d 2 k + y k * max(0, 1 ? d 2 k ),<label>(3)</label></formula><p>where * is element wise multiplication, y k tells us which features have the possibility of having an object (Eq. 8), d k is the cosine distance between &lt; f anc k , f p k &gt;. Along with contrastive loss, we optimize binary cross entropy over the objectness score predicted by region proposal network. The overall loss function is:</p><formula xml:id="formula_3">d k = f p k ? f anc k ||f p k || ||f anc k || ? k = [1, 2, ..n]<label>(4)</label></formula><formula xml:id="formula_4">L total = ? * L CON + ? * L BCE (o, ? ),<label>(5)</label></formula><p>where, ? and ? are empirically-determined scaling parameters, o is the objectness score from RPN and ? is the cosine distance from Eq. 1. Once the LFE(.) network is trained, the output of trained model are the proposal feature vectors</p><formula xml:id="formula_5">[f p 1 ,f p 2 , ...,f p n ] along with objectness scor? o = [? 1 ,? 2 , ..,? n ].</formula><p>This learnt parameter objectness score ensures selection of features with object information, thereby minimizing the interference from potential confusing elements as shown in <ref type="figure">Fig. 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Composition Classifier CC(.)</head><p>The ability to learn individual representation of O-A in visual domain is crucial for transferring knowledge from seen to unseen O-A associations. Existing SOTA works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> use homogeneous features from whole image as without localizing the object, they ignore the discriminative visual features of object and its attributes. Our Composition Classifier network CC(.) leverages the distinctive features extracted by LFE(.) to predict the object and its corresponding attribute as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. It is challenging to associate right attribute with the object by using homogeneous features, as there can be interference from prominent confounding elements like examples shown in Fig1B. Similarly in <ref type="figure" target="#fig_3">Fig. 5</ref> (row-2, column-1 image), the attribute "green color" is so prominent, and using homogeneous features can lead to wrong O-A prediction.</p><p>The input to CC(.) is a set of top r (r &lt; n) proposal feature vectors from pre-trained</p><formula xml:id="formula_6">LFE(.) [f p 1 , f p 2 , ..., f p r ] ? R r?C sorted in descending order based on objectness score o = [o 1 , o 2 , .., o r ].</formula><p>The proposal feature vectors are fused into a single visual feature f p all ? R 1?C using weighted average with learnable parameters <ref type="bibr" target="#b15">[16]</ref> as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Input to the learnable parameters has r ?C dimension. r is the number of proposals and C is the number of channels. We swap the dimensions at the input to fuse different proposals together. The output of it is a single feature vector of length C. The fusion operation is a learnable weighted average operation, that learns to create joint representation from features of object regions. Next, f p all is projected to two different representation via two feed forwards networks FC AH and FC OH layers as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> </p><formula xml:id="formula_7">F text A = [f text a1 , f text a2 .., f text ai ] and all objects F text O = [f text o1 , f text o2 .., f text o j ]</formula><p>(generated from T a and T o ) are projected via feed forward network FC AS and FC OS . Here T a and T o are the list of all attributes and objects in the dataset respectively, and i = len(T a ), j = len(T o ). len(.) is length. Inspired by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, the visual feature projections are refined as shown below. The particular choice of refinement by one-to-one multiplication is an empirical choice. We explore different refinement techniques in <ref type="table" target="#tab_4">Table 3</ref> of supplementary materials. This all is done by Re f inement block as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><formula xml:id="formula_8">F re f ined O = FC OH (f all p ) ? FC OS (F txt O ) ; F re f ined A = FC AH (f all p ) ? FC AS (F txt A )<label>(6)</label></formula><p>where, "?" represents element-wise multiplication and the feed forward network are two fully connected layers with ReLu activation. This refinement aggregates semantic information with the visual features. These refined features are passed through another feed forward network ( <ref type="figure" target="#fig_2">Fig. 4</ref>) and softmax layers to make the final decision on the object? and attribut? a present in the image I. To train the composition classifier function CC(.), we optimize binary cross entropy function over the O-A prediction. <ref type="table" target="#tab_0">Table 1</ref> summarizes the datasets used. In the MIT-states dataset the images are of natural objects collected using an older search engine with limited human annotation causing a significant label noise <ref type="bibr" target="#b0">[1]</ref>. For UT-Zappos [37] the simplicity of the images (one object with white background) makes it unsuitable to work in natural surroundings. We performed experiments on very recently released Compositional GQA (CGQA) dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>. This dataset is proposed to evaluate existing CZSL models in a more realistic challenging scenarios with background clutter. More details about the datasets can be found in supplementary materials section 4. Both the localized feature extractor LFE(.) and Composition Classifier CC(.) are trained on all the datasets. To pre-train LFE(.), an efficient contrastive pre-training framework is used with a margin distance of 1. Then this pre-trained network is used with CC(.) to do an end-to-end training of LOCL. We have provided the complete implementation details in the supplementary materials. However, during inference time, LFE(.) do not have any text embedding branch, hence it generates proposals for every potential object as shown in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Following current methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>, we evaluate our network's performance in Generalized Compositional Zero Shot Learning Protocol (GCZSL). Under this protocol, we draw seen class accuracy vs unseen class accuracy curve at different operating points of the network. These operating points are selected from a single calibration scaler that is added to our network's predictions of the unseen classes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. We report area under "seen class accuracy vs unseen class accuracy curve" (AUC) as our performance metrics. Additionally, we report our network's performance on Top-1 accuracy in seen and unseen classes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>LOCL outperforms current methods in the test set of all benchmark datasets in almost all categories as shown in <ref type="table" target="#tab_2">Table 2</ref>. We evaluate LOCL's performance in terms of AUC under Generalized Compositional Zero Shot Learning (GCZSL) protocol. We also report Top-1 accuracy in seen and unseen classes and accuracy in detecting objects and attributes. In MIT-States <ref type="bibr" target="#b10">[11]</ref> LOCL outperforms SOTA method by 8% on unseen class accuracy and 1.7% AUC. In UT-Zappos [37] LOCL's unseen class accuracy is 5.2% better than the SOTA method. Moreover, it almost doubles the unseen class accuracy while achieving 1.1% improvement in terms of AUC for the more challenging CGQA <ref type="bibr" target="#b21">[22]</ref> dataset. Current CZSL methods use homogeneous features from the backbone instead of using distinctive visual features of objects and attributes. While such techniques may work on simpler datasets like UT-Zappos [37], as evidenced by the high performance, the more realistic datasets such as CGQA pose challenges. <ref type="table" target="#tab_2">Table 2</ref> shows, LOCL achieves the best results on the challenging CGQA dataset. However, bias towards seen O-A compositions is a common issue <ref type="bibr" target="#b24">[25]</ref> in current CZSL methods. Recent approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">36]</ref> have utilized a graph structure with message passing/blocking <ref type="bibr">[36]</ref> or prior possible O-A knowledge <ref type="bibr" target="#b21">[22]</ref> to reduce this bias. However they tend to be biased towards seen O-A pairs at inference as pointed out by the authors of <ref type="bibr">[36]</ref>. In contrast, LOCL learns distinct object and attribute representations in the two separate branches of the CC(.) and achieves high unseen class accuracy and AUC. In UT-Zappos, high AUC of [36] stems from high seen class accuracy with inferior unseen class performance. <ref type="bibr">[36]</ref> do not evaluate their model CGQA dataset <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Backbone and Localized Feature Extractor: Our Localized Feature Extractor LFE(.) is modular and can easily be adapted to other methods. In <ref type="table" target="#tab_4">Table 3</ref>, we show different SOTA methods' improved performances with our feature extraction. For the sake of fair comparison with SOTA methods: SymNet <ref type="bibr" target="#b16">[17]</ref>, ComCos <ref type="bibr" target="#b17">[18]</ref> and CGE <ref type="bibr" target="#b21">[22]</ref>, we replace their backbones with our ResNet-50 pre-trained on a larger dataset <ref type="bibr" target="#b25">[26]</ref>. As expected, both our backbone and LFE(.) improve the existing networks' performances. This shows that the performance im-  provement is because of localized features generated from LFE(.). All existing methods use ResNet-18 as the backbone following the seminal work of <ref type="bibr" target="#b20">[21]</ref>. We recommend that CZSL networks should utilize stronger backbones for challenging datasets like CGQA <ref type="bibr" target="#b21">[22]</ref>. However, our improved performance is not just coming from a stronger backbone. With LFE(.), the performance boost is more significant (specially in terms of unseen class accuracy) than the performance boost with our backbone for the CGQA dataset. In particular, LFE(.) increases the unseen class accuracy of CGE <ref type="bibr" target="#b21">[22]</ref> in CGQA by 86%, and other methods also get great improvement with LFE(.). The performance improvement in the MIT-States dataset is less due to the noisy annotations <ref type="bibr" target="#b0">[1]</ref> of this dataset. In summary, LFE(.) improves three different architectures thus proving the effectiveness of localized feature extraction. We additionally ablate LOCL's performance for different number of proposals, number of pseudo labels, refinement techniques, margin distance for contrastive loss, and scaling parameters ? and ? . All these experiments along with LOCL's performance on detecting individual objects and attributes are reported in the appendix section.</p><p>Qualitative Results: We show results for unseen novel compositions with top-1 prediction in <ref type="figure" target="#fig_3">Fig. 5</ref>. They represent the scenes with clutter or confounding elements. For example, column-1 shows where the confounding element color attribute &lt; green &gt; causes wrong O-A association for most of the SOTA methods while LOCL makes the right association. Similar in the first row. The last column shows where our network predictions do not match with the ground truth labels. In the top image our network focuses more on prominent object clock, while the image is labeled for &lt; red, bus &gt;. For row-2 image, it contains attribute categories like size, color, texture but the label only has attribute size. We should, however, note that the ground truth labels in these datasets contain only one O-A pair. This puts an artificial limitation on the evaluation metric even when the predictions are perceivably correct but does not match labels. We also show LFE(.) proposals quality in eliminating background in <ref type="figure">Fig. 6</ref> Multi O-A prediction: We extended LOCL to unconstrained setting than existing evaluation methods allows, i.e. detecting multiple O-A pairs. LOCL has flexibility of establishing the right O-A association for multiple objects in the scene. We are showing the top-3 O-A pairs prediction in <ref type="figure" target="#fig_3">Fig. 5</ref> last row. As can been seen the prediction are perceivably correct but unavailability of ground-truth annotation in existing datasets puts a limit on quantitative  <ref type="figure">Figure 6</ref>: Proposals selected based on objectness score. We can see that the proposals are generated on the object of interest. Though LOCL is not designed for multi O-A, but in case of multiple objects, the proposals are distributed over multiple objects. evaluation. Also, there can be multiple correct attribute related to one object as shown in the two central images in <ref type="figure" target="#fig_3">Fig. 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a novel two-step approach, LOCL, for recognizing O-A pairs. Our approach includes a robust local feature extractor followed by a composition classifier. LOCL is evaluated on benchmark datasets. Additionally, our experiments show that the local feature extractor improves the performance of current SOTA CZSL models by a significant margin of 12%. The code and trained model will be made available on Github at the time of publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>The authors would like to thank Dr. Suya You, Army Research Laboratory (ARL), for the many discussions that contributed to the methods development. We also thank Mr. Pushkar Shukla from Toyota Technological Institute, Chicago and Mr. Rahul Vishwakarma from Hitachi Labs America for their suggestions and critical reviews of the paper. This research was in part supported by NSF SI2-SSI award #1664172 and by US Army Research Laboratory (ARL) under agreement # W911NF2020157. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of US Army Research Laboratory (ARL) or the U.S. Government </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Implementation Details</head><p>Both the localized feature extractor LFE(.) and Composition Classifier CC(.) are trained on all the datasets. To train LFE(.), an efficient contrastive pre-training framework is used with a margin distance of 1. As backbone image encoder, we use ResNet-50 <ref type="bibr" target="#b4">[5]</ref> pre-trained on <ref type="bibr" target="#b25">[26]</ref>. For text encoding we utilize text encoder similar to <ref type="bibr" target="#b25">[26]</ref>. feature have potential object. The scaling parameters for contrastive loss ? and classification loss ? are set to 0.6, 0.4 respectively. The network is trained for 100 epochs, convergence is observed around 45 epochs, based on that, early stopping is done at 50 epochs. The learning rate starts with 1e ?5 with decay of 0.1 after every 10 epochs. The batch size is set at 24. The optimizer used is Adam optimizer. The region proposal branch of the network learns to select features from regions where the objects are present. During training, we restrict the learning rate of linear projection layer of f txt ao to a low value to stabilize the region proposal branch. Compositional Classifier CC(.): The ability to learn individual representation of O-A in visual domain is crucial for transferring knowledge from seen to unseen O-A associations. Existing SOTA works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> use homogeneous features from whole image as without localizing the object, they ignore the discriminative visual features of object and its attributes. Our Composition Classifier network CC(.) leverages the distinctive features extracted by LFE(.) to predict the object and its corresponding attribute. It is challenging to associate right attribute with the object by using homogeneous features, as there can be interference from prominent confounding elements. CC(.) takes as input, the top 10 pooled features [f p 1 , f p 2 , ..., f p 10 ] from pre-trained LFE(.) sorted in descending order based on objectness score? = [? 1 ,? 2 , ..,? 10 ]. Each block in CC(.) consists of two fully connected layer with ReLU activation. The initial learning rate for CC(.) network is set to 1e ?3 with a decay of 0.1 after every 7 epochs. We observed that fine tuning LFE(.) with a lower learning rate of 1e ?6 while training CC(.) performed better than freezing it. The batch size used is 32. All the experiments are done on a single nvidia V100 Tesla.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results &amp; Analysis</head><p>We report Top-1 accuracy in seen and unseen classes and accuracy in detecting objects and attributes. LOCL achieve best accuracy in individual detection of objects and attributes as shown in <ref type="table" target="#tab_8">Table 4</ref>. This is interesting as our simple CC(.) do not a have dedicated object detector similar to SymNet <ref type="bibr" target="#b16">[17]</ref>. In MIT-States <ref type="bibr" target="#b10">[11]</ref> LOCL outperforms SOTA method by 12% on object detection accuracy and 19% attribute detection. In UT-Zappos [37] LOCL's performance is slightly better than SOTA methods since each image has one dominant object with clear white background. Moreover the performance improvement is significant when it comes to more challenging and realistic dataset CGQA <ref type="bibr" target="#b21">[22]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Datasets</head><p>The splits used on all the datasets are as follows. MIT-States <ref type="bibr" target="#b10">[11]</ref> has a total of 53,000 images with 245 objects and 115 attributes. The splits for MIT-States dataset have 1262 object-attribute pairs (34,000 images) for the training set, 600 object-attribute pairs (10,000 images) as the validation set and 800 pairs (12,000 images) as test set. All the images in MIT-states dataset are of natural objects collected using an older search engine with limited human annotation causing a significant label noise <ref type="bibr" target="#b0">[1]</ref>. UT-Zappos [37] has 29,000 images of shoes catalogue. The splits used are of 83 object-attribute pairs (23,000 images) for the training set, 30 object-attribute pairs (3,000 images) for the validation set and 36 pairs (3,000 images) for test set. The images in UT-Zappos [37] dataset are not really entirely a compositional dataset as the attributes like Faux Leather vs Leather are material differences but not specifically any visual difference <ref type="bibr" target="#b17">[18]</ref>. Also, the simplicity of the images (one object with white background) makes it unsuitable to work in natural surroundings where the object of interest has interference confounding elements in the scene. These splits are selected following previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. The third dataset used is Compositional-GQA (CGQA) dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. It has 453 attributes and 870 objects. The splits for CGQA have 5592 objectattribute pairs (26,000 images) for training set, 2292 pairs (7,000 images) for validation set and 1811 pairs (5,000 images) for testing set. These splits are as proposed by <ref type="bibr" target="#b17">[18]</ref>. The CGQA dataset have images curated from visual genome dataset <ref type="bibr" target="#b13">[14]</ref> which comprises of images from natural and realistic settings. Most of the images in CGQA have an object of interest with confounding elements in the background, that makes it an extremely challenging dataset to evaluate CGQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Ablation Study</head><p>In this section, we discuss about the additional design choices for training of the Localized Feature Extractor LFE(.) and composition classifier CC(.). <ref type="table">Table 5</ref> shows the selection criterion of number of proposal selected from pre-trained LFE(.) the goes as input to CC(.). With r &lt; 10, the proposals features miss regions of the object, which leads to poor performance. While when r &gt; 10, more background features are picked that suppress the prominent object and lead to drop in prediction quality. 27.6 28.4 6.5 <ref type="table">Table 5</ref>: Performance of LOCL as we select different number of top r proposals from pretrained LFE. Best performance is observed with r=10. With r &gt; 10, more background features are picked that suppress the prominent objects. <ref type="table" target="#tab_10">Table 6</ref> shows refinement operations done on visual features f all p as shown in Eq.6 in the main paper. The multiplication operations generates more selective information and suppresses the redundant information as compared to concatenation and addition operation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>. As discussed in section 3.1 of the main paper, we use OA pair name &lt; Blue, Bird &gt; as input during the pre-training of LEF(.). We also test with using only the object names &lt; Bird &gt; as the input. We observe 3% drop in accuracy as compared to OA pair names in the unseen category as shown in <ref type="table">Table 7</ref>. This is expected as the text embeddings generated by the text encoder are more meaningful and have closer representation with the visual features when we provide a complete description of the object in the image i.e. OA pair name.  <ref type="table">Table 7</ref>: Performance of the network in MIT-States <ref type="bibr" target="#b10">[11]</ref> with different names used as input to the text encoder while pre-training LFE(.). Bold numbers are the best performance settings. The network performs well with Ob j ? Attr names as input compared to just Ob j names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Number of Proposals:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of proposals MIT-States</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Object\Attribute Refinement:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.4">Number of Pseudo Labels:</head><p>For creating pseudo labels y during pre-training, as mentioned in section 3.1 equation 4, we assign value 1 to top 20 indexes and rest are assigned 0. The equation is:</p><formula xml:id="formula_9">? = [? 1 , ? 2 , .., ? k , .., ? n ]<label>(7)</label></formula><formula xml:id="formula_10">y = 1 argsort(? )[0 : l] 0 f or all other indexes<label>(8)</label></formula><p>where y = [y 1 , y 2 , .., y k , .., y n ], 20 anchors are selected based on cosine similarity score ? (equation 2 in main paper). They are assigned with label 1 in y and rest are assigned 0 as shown above with Eq. 8. Here each y k represents the presence/absence of object of interest regions in the input image. We experiment with different values for number of potential objects. As shown in <ref type="table">Table 8</ref>, the overall performance of the model drops if we pick a number greater than or less than 20. This is because for smaller value, the LFE(.) is penalized for detecting even the right regions of interests and for larger value than 20, we are learning information from confounding elements from the background where the object may/may not be present.  <ref type="table">Table 8</ref>: Performance of the network in MIT-States <ref type="bibr" target="#b10">[11]</ref> with different number of region of interest while pre-training LFE(.). Bold numbers are the best performance settings. Here # is "Number of".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.5">Margin distance for contrastive loss:</head><p>For pre-training LEF(.) with contrastive loss, we use a margin distance of 1 as shown in equation 5 in the main paper. We experimented with different distances for the margin for MIT-states <ref type="bibr" target="#b10">[11]</ref> dataset. We achieved best performance at a margin of 1. The experimental evaluation with different margin distance is shown in <ref type="table">Table 9</ref>. Our observations of is that with bigger margin, the network start clustering features from those regions also, which have object of interest along with a significant section of background regions. This leads to drop in attribute detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Margin</head><p>MIT  <ref type="table">Table 9</ref>: Performance of pre-training the LFE(.) using different margin distance for contrastive learning. We achieve best performance when margin is 1. For higher margin, LFE(.) cluster features of object of interest which have significant region of background/confounding regions also. Leaning to poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.6">Scaling parameters of loss function:</head><p>While pre-training, we combine contrastive loss and binary cross entropy loss using scaling parameters ? and ? . The equation is:</p><formula xml:id="formula_11">L total = ? * L CON + ? * L BCE (o, ? ),<label>(9)</label></formula><p>where, L CON is the contrastive loss and L BCE is the binary cross entropy loss. We test for different values ? and ? as shown in <ref type="table" target="#tab_0">Table 10</ref>. It appears giving a bit more weight to the contrastive loss helps LFE(.) to extract better localized features.  <ref type="table" target="#tab_0">Table 10</ref>: Performance of the network with different scaling parameters of the loss function during pre-training. Bold numbers are the best performance settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Qualitative Results</head><p>We add more qualitative results for unseen novel composition with top-1 prediction in <ref type="figure">Figure 7</ref>. The examples are presented from datasets : CGQA <ref type="bibr" target="#b21">[22]</ref>, MIT-States <ref type="bibr" target="#b10">[11]</ref>, and UT-Zapos <ref type="bibr">[37]</ref>. The order of the datasets is in decreasing order of the clutter in the images. As can be seen that in the CGQA dataset, the images contains object of interest with lot of confounding elements creating background clutter. MIT-States <ref type="bibr" target="#b10">[11]</ref> is also of natural images. However, most of the images have a dominant object. On the other hand, in UT-Zappos [37] all the images contain a single object with clear white background. This shows the complexity and the challenges of CGQA dataset compared to the existing ones. <ref type="figure">Figure 7</ref>: Qualitative results of LOCL. Left three columns show correct predictions from our network. Rightmost column shows missed predictions, here, ground truth labels are marked with green box and our predictions are marked in red box. The datasets contain only one OA pair and our predictions though visually correct, do not match with the ground-truth OA in these cases.</p><p>The first three columns represent the examples where our model is making the right predictions. The last column in each dataset shows examples where our model makes the visually correct prediction. However, it does not match with the ground truth label of object and attribute. Our model is selecting object of interest, and it is creating the right attribute-object associations. For example in case of fourth row on the rightmost column, our prediction of the object is right but the image contains multiple attributes, while the ground truth contains only one OA pair. This put an artificial limitation on the evaluation metric even when the predictions are perceiveably correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BMP-Net [36]</head><p>achieves state-of-the-art (SOTA) performance in seen classes of MIT-States <ref type="bibr" target="#b20">[21]</ref> and UT-Zappos [37]. However, their sub-optimal performance in unseen classes indicates a bias towards seen classes. To further investigate this bias, we evaluate BMPNet on the challenging CGQA dataset <ref type="bibr" target="#b21">[22]</ref>. We utilize the official repository provided by the authors for this evaluation and report performance in the same matrices used for other datasets. In <ref type="table" target="#tab_0">Table 11</ref>, we can observe LOCL outperforms BMPNet in all category. Especially in unseen classes, LOCL achieves more than double accuracy than BMPNet. This poor performance indicates a seen class bias of BMPNet. This bias is mainly due to creating the graph network with a large number of seen and plausible OA pairs. More discussion on this phenomenon is available in section 4.4 in the main paper. Moreover, LOCL is very efficient and utilizes only ? 5GB memory for training in the large-scale dataset CGQA. Current graph-based SOTA networks CGE <ref type="bibr" target="#b21">[22]</ref> (? 10GB), BMPNet [36] (? 40GB) utilize much higher GPU memory for the same batch size in CGQA dataset. Therefore, LOCL is suitable for training on large scale challenging CZSL datasets.  <ref type="table" target="#tab_0">Table 11</ref>: Performance comparison on CGQA <ref type="bibr" target="#b21">[22]</ref> dataset. LOCL significantly outperform BMP-Net [36] in a challenging (significant background clutter) dataset. The performance of LOCL shows the effectiveness of LEF in unseen OA associations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Summary of pre-training the localized feature extractor.The image encoder and region proposal are jointly trained to generate features of object of interest. During training time, we use text embeddings to generate pseudo labels to train the image encoder and region proposal using contrastive learning. At the test time, the learned image encoder and region proposal network are used to generate features from object regions. key insight is to leverage the features from regions containing the object of interest to learn accurate O-A associations. Fig. 2 summarizes the overall LOCL architecture. Problem Setting: Let {I, T o , T a , (a, o)} be the training dataset with N samples, where I is the input image. T o , T a are the list of all object and attribute labels, respectively, and (a, o) is the tuple of attribute-object pairs in the image. The O-A pairs labels used during training are categorised as seen pairs. The goal of CZSL trained model is to take in an input image I and predict (?,?). The O-A pairs labels used during inference are novel and unseen. Here the seen and unseen object-attribute pairs are mutually exclusive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Region proposal Network (RPN): The RPN branch shown in Fig. 3 is inspired from FasterRCNN [27]. The RPN generated proposals are used to pool a list of features from the feature map F. The pooled features are [f p 1 , f p 2 , ..., f p n ] ? R C . Now the anchor features [f anc 1 , f anc 2 , ..., f anc n ], proposal features [f p 1 , f p 2 , ..., f p n ] and pseudo label y = [y 1 , y 2 , .., y n ] are used to train the function LFE(.) using contrastive learning as explained in next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Composition Classifier CC(.) architecture. The proposal features [f p 1 ,f p 2 , ...,f p r ] are the outputs from LFE(.) which are combined into a single representation f p all . The attribute and object semantics are the semantic encoding of all attributes and objects under consideration. Two branches predict attribute and object from semantically refined f p all .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of LOCL. Row-1 &amp; 2 (col-1,2,3)show correct predictions. Col-4 shows missed predictions, ground truth labels are marked with green box and our predictions in red box. The datasets has one O-A pair per image. Though our predictions are visually correct, do not match the ground-truth. This puts an artificial limit on the evaluation metric. Row-3 shows multiple O-A detections</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The Anchor Generator generates 576 valid anchor boxes. Corresponding to each of these anchor boxes, features [f anc 1 , f anc 2 , ..., f anc 576 ] are pooled from F. To generate ? according to Eq. 7, each f anc j is matched with semantic word embedding vector, and top 20 scores are labeled as 1 and rest as 0 as shown in Eq. 8 to create the pseudo label y. This is an empirically selected value, it covers almost all the object regions in the image. The Region Proposal Network generates proposal boxes and an objectness score corresponding to each proposal box. The number of proposal boxes are equal to the number of anchor boxes. Corresponding to these proposals boxes, features [f p 1 , f p 2 , ..., f p 576 ] are pooled from feature map F. Contrastive loss is used for pre-training. The cosine distance d k is computed between each anchor feature f anc k and f p k , the total number of features are 576 for anchors and 576 proposals. The pseudo label y is of length 576. y k is equal to 1 where the f anc k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>. Similarly, the semantic embeddings of all attributes Comparison of different CZSL datasets<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref> 37]    </figDesc><table><row><cell></cell><cell></cell><cell># Images</cell><cell></cell><cell cols="3"># Objects #Attributes # OA Pairs</cell></row><row><cell></cell><cell cols="3">Train Val Test</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MIT-States [21]</cell><cell>30k</cell><cell cols="2">10k 13k</cell><cell>245</cell><cell>115</cell><cell>1962</cell></row><row><cell>UT-Zappos [37]</cell><cell>23k</cell><cell>3k</cell><cell>3k</cell><cell>12</cell><cell>16</cell><cell>116</cell></row><row><cell>CGQA [22]</cell><cell>26k</cell><cell>7k</cell><cell>5k</cell><cell>870</cell><cell>453</cell><cell>9378</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance comparisons on MIT-States [11], UT-Zappos [37], CGQA [22] Datasets. '-' means unreported performance in a particular category. In all three datasets, LOCL significantly outperform current methods. Specially, for the more challenging (sig- nificant background clutter) CGQA dataset, the effectiveness of LFE is clearly demonstrated by its performance on the unseen O-A associations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance of SOTA methods with our backbone (BB) and LFE. LFE significantly boosts all SOTA network performances specially in the CGQA dataset. Row 2 if each methods shows performance of all SOTA models with a common and better backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>last row. &lt; W hite &gt;, &lt; Standing &gt; both are correct attribute for object &lt; Horse &gt;, similarly &lt; Dark &gt;, &lt; Large &gt; are correct attributes for object &lt; Elephant &gt;. Multiple OA pairs detection is an interesting direction for future research.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>[ 35 ]</head><label>35</label><figDesc>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pages 2048-2057. PMLR, 2015. [36] Ziwei Xu, Guangzhi Wang, Yongkang Wong, and Mohan S Kankanhalli. Relationaware compositional zero-shot learning for attribute-object pair recognition. IEEE Transactions on Multimedia, 2021.</figDesc><table><row><cell>[37] Aron Yu and Kristen Grauman. Semantic jitter: Dense supervision for visual compar-</cell></row><row><cell>isons via synthetic images. In Proceedings of the IEEE International Conference on</cell></row><row><cell>Computer Vision, pages 5570-5579, 2017.</cell></row><row><cell>[38] Xiangyun Zhao, Yi Yang, Feng Zhou, Xiao Tan, Yuchen Yuan, Yingze Bao, and Ying</cell></row><row><cell>Wu. Recognizing part attributes with insufficient data. In Proceedings of the IEEE/CVF</cell></row><row><cell>International Conference on Computer Vision, pages 350-360, 2019.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons on detecting individual objects and attributes. LOCL outperforms all compared methods with a significant margin.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Performance of compositional classifier with different refinement operations. 7.4.3 Pre-training LEF(.) with object embeddings:</figDesc><table><row><cell></cell><cell>MIT-States</cell><cell>[11]</cell></row><row><cell></cell><cell cols="2">Seen Unseen AUC</cell></row><row><cell>Addition</cell><cell>28.5 29.6</cell><cell>6.6</cell></row><row><cell cols="2">Multiplication 35.3 36.0</cell><cell>7.7</cell></row><row><cell cols="2">Concatenation 32.7 33.1</cell><cell>7.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A causal view of compositional zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1010cedf85f6a7e24b087e63235dc12e-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1462" to="1473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="914" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Gonthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa?d</forename><surname>Ladjal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01178</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compositional zero-shot learning via fine-grained dense feature composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-grained generalized zero-shot learning via dense attribute-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4483" to="4493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gtnet: Guided transformer network for detecting human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Asm Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>Mcever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00596</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What to look at and where: Semantic and spatial refined transformer for detecting human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Asm Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="5353" to="5363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering states and transformations in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1383" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep remote sensing methods for methane detection in overhead hyperspectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alana</forename><surname>Ayasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1776" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stressnet: detecting stress in thermal videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bullock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Santander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giesbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Grafton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="999" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Symmetry and group in attributeobject compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11316" to="11325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning graph embeddings for open world compositional zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Ferjad</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01017</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open world compositional zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Ferjad</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5222" to="5230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Context-driven detection of invertebrate species in deep-sea video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>R Austin Mcever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00718</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From red wine to red tomato: Composition with context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1792" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning graph embeddings for compositional zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Muhammad Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="953" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attributes as operators: factorizing unseen attribute-object compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="169" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing unseen attribute-object pair with generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiong</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8811" to="8818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aurelio Ranzato. Task-driven modular networks for zero-shot compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3593" to="3602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Independent prototype propagation for zero-shot compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Ruis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertjan</forename><surname>Burghours</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Bucur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00305</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13617" to="13626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial finegrained composition learning for unseen attribute-object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Zero-shot compositional concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Kordjamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce Y</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05176</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
