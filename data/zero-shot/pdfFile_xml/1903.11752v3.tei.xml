<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-time generic object detection on mobile platforms is a crucial but challenging computer vision task. Prior lightweight CNN-based detectors are inclined to use onestage pipeline. In this paper, we investigate the effectiveness of two-stage detectors in real-time generic detection and propose a lightweight two-stage detector named Thun-derNet. In the backbone part, we analyze the drawbacks in previous lightweight backbones and present a lightweight backbone designed for object detection. In the detection part, we exploit an extremely efficient RPN and detection head design. To generate more discriminative feature representation, we design two efficient architecture blocks, Context Enhancement Module and Spatial Attention Module. At last, we investigate the balance between the input resolution, the backbone, and the detection head. Benefit from the highly efficient backbone and detection part design, ThunderNet surpasses previous lightweight one-stage detectors with only 40% of the computational cost on PAS-CAL VOC and COCO benchmarks. Without bells and whistles, ThunderNet runs at 24.1 fps on an ARM-based device with 19.2 AP on COCO. To the best of our knowledge, this is the first real-time detector reported on ARM platforms. Our code and models are available at https: //github.com/qinzheng93/ThunderNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-time generic object detection on mobile devices is a crucial but challenging task in computer vision. Compared with server-class GPUs, mobile devices are computationconstrained and raise more strict restrictions on the computational cost of detectors. However, modern CNN-based detectors are resource-hungry and require massive computation to achieve ideal detection accuracy, which hinders them from real-time inference in mobile scenarios.</p><p>From the perspective of network structure, CNN-based detectors can be divided into the backbone part which ex-* Equal contribution. ? This work was done when Zheng Qin was an intern at Megvii Inc. ? Corresponding author. tracts features for the image and the detection part which detects object instances in the image. In the backbone part, state-of-the-art detectors are inclined to exploit huge classification networks (e.g., ResNet-101 <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>) and large input images (e.g., 800?1200 pixels), which requires massive computational cost. Recent progress in lightweight image classification networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref> has facilitated real-time object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref> on GPU. However, there are several differences between image classification and object detection, e.g., object detection needs large receptive field and low-level features to improve the localization ability, which is less crucial for image classification. The gap between the two tasks restricts the performance of these backbones on object detection and obstructs further compression without harming detection accuracy.</p><p>In the detection part, CNN-based detectors can be categorized into two-stage detectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14]</ref> and onestage detectors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17]</ref>. For two-stage detectors, the detection part usually consists of Region Proposal Network (RPN) <ref type="bibr" target="#b26">[27]</ref> and the detection head (including RoI warping and R-CNN subnet). RPN first generates RoIs, and then the RoIs are further refined through the detection head. Stateof-the-art two-stage detectors tend to utilize a heavy detec- Backbone Part Detection Part <ref type="figure">Figure 2</ref>. The overall architecture of ThunderNet. ThunderNet uses the input resolution of 320?320 pixels. SNet backbone is based on ShuffleNetV2 and specifically designed for object detection. In the detection part, RPN is compressed, and R-CNN subnet uses a 1024-d fc layer for better efficiency. Context Enhancement Module leverages semantic and context information from multiple scales. Spatial Attention Module introduces the information from RPN to refine the feature distribution.</p><p>tion part (e.g., over 10 GFLOPs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>) for better accuracy, but it is too expensive for mobile devices. Light-Head R-CNN <ref type="bibr" target="#b13">[14]</ref> adopts a lightweight detection head and achieves real-time detection on GPU. However, when coupled with a small backbone, Light-Head R-CNN still spends more computation on the detection part than the backbone, which leads to a mismatch between a weak backbone and a strong detection part. This imbalance not only induces great redundancy but makes the network prone to overfitting. On the other hand, one-stage detectors directly predict bounding boxes and class probabilities. The detection part of this category is composed of the additional layers to generate predictions, which usually involves little computation. For this reason, one-stage detectors are widely regarded as the key to real-time detection. However, as one-stage detectors do not conduct RoI-wise feature extraction and recognition, their results are coarser than two-stage detectors. The problem is aggravated for lightweight detectors. Prior lightweight one-stage detectors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13]</ref> do not obtain an ideal accuracy/speed trade-off: there is a huge accuracy gap between them and the large detectors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>, while they fail to achieve real-time detection on mobile devices. It inspires us to rethink: can two-stage detectors surpass one-stage detectors in real-time detection?</p><p>In this paper, we propose a lightweight two-stage generic object detector named ThunderNet. The design of Thun-derNet aims at the computationally expensive structures in state-of-the-art two-stage detectors. In the backbone part, we investigate the drawbacks in previous lightweight backbones, and present a lightweight backbone named SNet designed for object detection. In the detection part, we follow the detection head design in Light-Head R-CNN, and further compress RPN and R-CNN subnet. To eliminate the performance degradation induced by small backbones and small feature maps, we design two efficient architecture blocks, Context Enhancement Module (CEM) and Spatial Attention Module (SAM). CEM combines the feature maps from multiple scales to leverage local and global context information, while SAM uses the information learned in RPN to refine the feature distribution in RoI warping. At last, we investigate the balance between the input resolution, the backbone, and the detection head. <ref type="figure">Fig. 2</ref> illustrates the overall architecture of ThunderNet.</p><p>ThunderNet surpasses prior lightweight one-stage detectors with significantly less computational cost on PASCAL VOC <ref type="bibr" target="#b4">[5]</ref> and COCO <ref type="bibr" target="#b17">[18]</ref> benchmarks ( <ref type="figure" target="#fig_0">Fig. 1</ref>). ThunderNet outperforms Tiny-DSOD <ref type="bibr" target="#b12">[13]</ref> with only 42% of the computational cost and obtains gains of 6.5 mAP on VOC and 4.8 AP on COCO under similar complexity. Without bells and whistles, ThunderNet runs in real time on ARM (24.1 fps) and x86 (47.3 fps) with MobileNet-SSD level accuracy. To the best of our knowledge, this is the first real-time detector and the fastest single-thread speed reported on ARM platforms. These results have demonstrated the effectiveness of two-stage detectors in real-time object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNN-based object detectors. CNN-based object detectors are commonly classified into two-stage detectors and one-stage detectors. In two-stage detectors, R-CNN <ref type="bibr" target="#b7">[8]</ref> is among the earliest CNN-based detection systems. Since then, progressive improvements <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref> are proposed for better accuracy and efficiency. Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> proposes Region Proposal Network (RPN) to generate regions proposals instead of pre-handled proposals. R-FCN [4] designs a fully convolutional architecture which shares computation on the entire image. On the other hand, one-stage detectors such as SSD <ref type="bibr" target="#b18">[19]</ref> and YOLO <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> achieve real-time inference on GPU with very competitive accuracy. Reti-naNet <ref type="bibr" target="#b16">[17]</ref> proposes focal loss to address the foregroundbackground class imbalance and achieves significant accuracy improvements. In this work, we present a two-stage detector which focuses on efficiency. Real-time generic object detection. Real-time object detection is another important problem for CNN-based detectors. Commonly, one-stage detectors are regarded as the key to real-time detection. For instance, YOLO <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> and SSD <ref type="bibr" target="#b18">[19]</ref> run in real time on GPU. When coupled with small backbone networks, lightweight one-stage detectors, such as MobileNet-SSD <ref type="bibr" target="#b10">[11]</ref>, MobileNetV2-SSDLite <ref type="bibr" target="#b27">[28]</ref>, Pelee <ref type="bibr" target="#b30">[31]</ref> and Tiny-DSOD <ref type="bibr" target="#b12">[13]</ref>, achieve inference on mobile devices at low frame rates. For two-stage detectors, Light-Head R-CNN <ref type="bibr" target="#b13">[14]</ref> utilizes a light detection head and runs at over 100 fps on GPU. This raises a question: are two-stage detectors better than one-stage detectors in realtime detection? In this paper, we present the effectiveness of two-stage detectors in real-time detection. Compared with prior lightweight one-stage detectors, ThunderNet achieves a better balance between accuracy and efficiency. Backbone networks for detection. Modern CNN-based detectors typically adopt image classification networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12]</ref> as the backbones. FPN <ref type="bibr" target="#b15">[16]</ref> exploits the inherent multi-scale, pyramidal hierarchy of CNNs to construct feature pyramids. Lightweight detectors also benefit from the recent progress in small networks, such as MobileNet <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> and ShuffleNet <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>. However, image classification and object detection require different properties of networks. Therefore, simply transferring classification networks to object detection is not optimal. For this reason, DetNet <ref type="bibr" target="#b14">[15]</ref> designs a backbone specifically for object detection. Recent lightweight detectors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref> also design specialized backbones. However, this area is still not well studied. In this work, we investigate the drawbacks of prior lightweight backbones and present a lightweight backbone for real-time detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ThunderNet</head><p>In this section, we present the details of ThunderNet. Our design mainly focuses on efficiency, but our model still achieves superior accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone Part</head><p>Input Resolution. The input resolution of two-stage detectors is usually very large, e.g., FPN <ref type="bibr" target="#b15">[16]</ref> uses input images of 800? pixels. It brings several advantages but involves enormous computational cost as well. To improve the inference speed, ThunderNet utilizes the input resolution of 320?320 pixels. Moreover, in practice, we observe that the input resolution should match the capability of the backbone. A small backbone with large inputs and a large backbone with small inputs are both not optimal. Details are discussed in Sec. 4.4.1.</p><p>Backbone Networks. Backbone networks provide basic feature representation of the input image and have great influence on both accuracy and efficiency. CNN-based detectors usually use classification networks transferred from Im-ageNet classification as the backbone. However, as image classification and object detection require different properties from the backbone, simply transferring classification networks to object detection is not optimal.</p><p>Receptive field: The receptive field size plays an important role in CNN models. CNNs can only capture information inside the receptive field. Thus, a large receptive field can leverage more context information and encode longrange relationship between pixels more effectively. This is crucial for the localization subtask, especially for the localization of large objects. Previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> have also demonstrated the effectiveness of the large receptive field in semantic segmentation and object detection.</p><p>Early-stage and late-stage features: In the backbone, early-stage feature maps are larger with low-level features which describe spatial details, while late-stage feature maps are smaller with high-level features which are more discriminative. Generally, localization is sensitive to low-level features while high-level features are crucial for classification. In practice, we observe that localization is more difficult than classification for larger backbones, which indicates that early-stage features are more important. And the weak representation power restricts the accuracy in both subtasks for extremely tiny backbones, suggesting that both early-stage and late-stage features are crucial at this level.</p><p>The designs of prior lightweight backbones violate the aforementioned factors: ShuffleNetV1/V2 <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref> have restricted receptive field (121 pixels vs. 320 pixels of input), ShuffleNetV2 <ref type="bibr" target="#b19">[20]</ref> and MobileNetV2 <ref type="bibr" target="#b27">[28]</ref> lack early-stage features, and Xception <ref type="bibr" target="#b2">[3]</ref> suffer from the insufficient high-level features under small computational budgets.</p><p>Based on these insights, we start from ShuffleNetV2, and build a lightweight backbone named SNet for real-time detection. We present three SNet backbones: SNet49 for faster inference, SNet535 for better accuracy, and SNet146 for a better speed/accuracy trade-off. First, we replace all 3?3 depthwise convolutions in ShuffleNetV2 with 5?5 depthwise convolutions. In practice, 5?5 depthwise convolutions provide similar runtime speed to 3?3 counterparts while effectively enlarging the receptive field (from 121 to 193 pixels). In SNet146 and SNet535, we remove Conv5 and add more channels in early stages. This design generates more low-level features without additional computational cost. In SNet49, we compress Conv5 to 512 channels instead of removing it and increase the channels in the early stages for a better balance between low-level and high-level features. If we remove Conv5, the backbone cannot encode adequate information. But if the 1024-d Conv5 layer is preserved, the backbone suffers from limited low-level features. <ref type="table" target="#tab_0">Table 1</ref> shows the overall architecture of the backbones. Besides, the last output feature maps of Stage3 and Stage4 (Conv5 for SNet49) are denoted as C 4 and C 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Detection Part</head><p>Compressing RPN and Detection Head. Two-stage detectors usually adopt large RPN and a heavy detection head. Although Light-Head R-CNN <ref type="bibr" target="#b13">[14]</ref> uses a lightweight detection head, it is still too heavy when coupled with small backbones and induces imbalance between the backbone and the detection part. This imbalance not only leads to redundant computation but increases the risk of overfitting.</p><p>To address this issue, we compress RPN by replacing the original 256-channel 3?3 convolution with a 5?5 depthwise convolution and a 256-channel 1?1 convolution. We increase the kernel size to enlarge the receptive field and encode more context information. Five scales {32 2 , 64 2 , 128 2 , 256 2 , 512 2 } and five aspect ratios {1:2, 3:4, 1:1, 4:3, 2:1} are used to generate anchor boxes. Other hyperparameters remain the same as in <ref type="bibr" target="#b13">[14]</ref>.</p><p>In the detection head, Light-Head R-CNN generates a thin feature map with ? ? p ? p channels before RoI warping, where p = 7 is the pooling size and ? = 10. As the backbones and the input images are smaller in ThunderNet, we further narrow the feature map by halving ? to 5 to eliminate redundant computation. For RoI warping, we opt for PSRoI align as it squeezes the number of channels to ?.</p><p>As the RoI feature from PSRoI align is merely 245-d, we apply a 1024-d fully-connected (fc) layer in R-CNN subnet. As demonstrated in Sec. <ref type="bibr" target="#b3">4</ref> C4_lat, 20x20</p><p>C5_lat, 20x20</p><p>Cglb_lat, 20x20 F CEM <ref type="figure">Figure 3</ref>. Structure of Context Enhancement Module (CEM). CEM combines feature maps from three scales and encodes more context information. It enlarges the receptive field and generates more discriminative features.</p><p>Context Enhancement Module. Light-Head R-CNN applies Global Convolutional Network (GCN) <ref type="bibr" target="#b22">[23]</ref> to generate the thin feature map. It significantly increases the receptive field but involves enormous computational cost. Coupled with SNet146, GCN requires 2? the FLOPs needed by the backbone (596M vs. 298M). For this reason, we decide to abandon this design in ThunderNet.</p><p>However, the network suffers from the small receptive field and fails to encode sufficient context information without GCN. A common technique to address this issue is Feature Pyramid Network (FPN) <ref type="bibr" target="#b15">[16]</ref>. However, prior FPN structures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> involve many extra convolutions and multiple detection branches, which increases the computational cost and induces enormous runtime latency.</p><p>For this reason, we design an efficient Context Enhancement Module (CEM) to enlarge the receptive field. The key idea of CEM is to aggregate multi-scale local context information and global context information to generate more discriminative features. In CEM, the feature maps from three scales are merged: C 4 , C 5 and C glb . C glb is the global context feature vector by applying a global average pooling on C 5 . We then apply a 1 ? 1 convolution on each feature map to squeeze the number of channels to ? ? p ? p = 245. Afterwards, C 5 is upsampled by 2? and C glb is broadcast so that the spatial dimensions of the three feature maps are equal. At last, the three generated feature maps are aggregated. By leveraging both local and global context, CEM effectively enlarges the receptive field and refines the representation ability of the thin feature map. Compared with prior FPN structures, CEM involves only two 1?1 convolutions and a fc layer, which is more computation-friendly. <ref type="figure">Fig. 3</ref> illustrates the structure of this module.</p><p>Spatial Attention Module. During RoI warping, we expect the features in the background regions to be small and the foreground counterparts to be high. However, compared with large models, as ThunderNet utilizes lightweight backbones and small input images, it is more difficult for the network itself to learn a proper feature distribution.</p><p>For this reason, we design a computation-friendly Spatial Attention Module (SAM) to explicitly re-weight the fea- ture map before RoI warping over the spatial dimensions.</p><p>The key idea of SAM is to use the knowledge from RPN to refine the feature distribution of the feature map. RPN is trained to recognize foreground regions under the supervision of ground truths. Therefore, the intermediate features in RPN can be used to distinguish foreground features from background features. SAM accepts two inputs: the intermediate feature map from RPN F RPN and the thin feature map from CEM F CEM . The output of SAM F SAM is defined as:</p><formula xml:id="formula_0">F SAM = F CEM ? sigmoid(?(F RPN )).<label>(1)</label></formula><p>Here ?(?) is a dimension transformation to match the number of channels in both feature maps. The sigmoid function is used to constrain the values within [0, 1]. At last, F CEM is re-weighted by the generated feature map for better feature distribution. For computational efficiency, we simply apply a 1?1 convolution as ?(?), so the computational cost of CEM is negligible. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the structure of SAM. SAM has two functions. The first one is to refine the feature distribution by strengthening foreground features and suppressing background features. The second one is to stabilize the training of RPN as SAM enables extra gradient flow from R-CNN subnet to RPN:</p><formula xml:id="formula_1">?L ?F RPN i = ?L RPN ?F RPN i + ?j ?L R-CNN ?F SAM j ? ?F SAM j ?F RPN i .<label>(2)</label></formula><p>As a result, RPN receives additional supervision from R-CNN subnet, which helps the training of RPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the effectiveness of Thun-derNet on PASCAL VOC <ref type="bibr" target="#b4">[5]</ref> and COCO <ref type="bibr" target="#b17">[18]</ref> benchmarks. Then we conduct ablation studies to evaluate our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Our detectors are trained end-to-end on 4 GPUs using synchronized SGD with a weight decay of 0.0001 and a momentum of 0.9. The batch size is set to 16 images per GPU. Each image has 2000/200 RoIs for training/testing. For efficiency, the input resolution of 320?320 pixels is used instead of 600? or 800? pixels in common large two-stage detectors. Multi-scale training with {240, 320, 480} pixels is adopted. As the input resolution is small, we use heavy data augmentation <ref type="bibr" target="#b18">[19]</ref>. The networks are trained for 62.5K iterations on VOC dataset and 375K iterations on COCO dataset. The learning rate starts from 0.01 and decays by a factor of 0.1 at 50% and 75% of the total iterations. Online hard example mining <ref type="bibr" target="#b28">[29]</ref> is adopted and Soft-NMS <ref type="bibr" target="#b0">[1]</ref> is used for post-processing. Cross-GPU Batch Normalization (CGBN) <ref type="bibr" target="#b21">[22]</ref> is used to learn batch normalization statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on PASCAL VOC</head><p>PASCAL VOC dataset consists of natural images drawn from 20 classes. The networks are trained on the union set of VOC 2007 trainval and VOC 2012 trainval, and we report single-model results on VOC 2007 test. The results are exhibited in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>ThunderNet surpasses prior state-of-the-art lightweight one-stage detectors. ThunderNet with SNet49 outperforms MobileNet-SSD with merely 21% of the FLOPs, while the SNet146-based model surpasses Tiny-DSOD by 2.9 mAP with about 43% of the FLOPs. Moreover, ThunderNet with SNet146 performs better than Tiny-DSOD by 6.5 mAP under similar computational cost.</p><p>Furthermore, ThunderNet achieves superior results to state-of-the-art large object detectors such as YOLOv2 <ref type="bibr" target="#b24">[25]</ref>, SSD300* <ref type="bibr" target="#b18">[19]</ref>, SSD321 <ref type="bibr" target="#b18">[19]</ref> and R-FCN <ref type="bibr" target="#b3">[4]</ref>, and is on a par with DSSD321 <ref type="bibr" target="#b5">[6]</ref>, but reduces the computational cost by orders of magnitude. We note that the backbone of Thun-derNet is significantly weaker and smaller than the large detectors. It demonstrates that ThunderNet achieves a much better trade-off between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on MS COCO</head><p>MS COCO dataset consists of natural images from 80 object categories. Following common practice <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref>, we use trainval35k for training, minival for validation, and report single-model results on test-dev.</p><p>As shown in <ref type="table" target="#tab_3">Table 3</ref>, ThunderNet with SNet49 achieves MobileNet-SSD level accuracy with 22% of the FLOPs. ThunderNet with SNet146 surpasses MobileNet-SSD <ref type="bibr" target="#b10">[11]</ref>, MobileNet-SSDLite <ref type="bibr" target="#b27">[28]</ref>, and Pelee <ref type="bibr" target="#b30">[31]</ref> with less than 40% of the computational cost. It is noteworthy that our approach achieves considerably better AP 75 , which suggests our model performs better in localization. This is consistent with our initial motivation to design two-stage real-time detectors. Compared with Tiny-DSOD <ref type="bibr" target="#b12">[13]</ref>, ThunderNet achieves better AP but worse AP 50 with 42% of the FLOPs. We conjecture that deep supervision and feature pyramid in Tiny-DSOD contribute to better classification accuracy. However, ThunderNet is still better in localization.</p><p>ThunderNet with SNet535 achieves significantly better detection accuracy under comparable computational cost. As shown in <ref type="table" target="#tab_3">Table 3</ref>   counterparts by at least 4.8 AP, 5.8 AP 50 and 6.7 AP 75 . The gap in AP 75 is larger than the gap in AP 50 , which means our model provides more accurate bounding boxes than other detectors. This further demonstrates that two-stage detectors are prior to one-stage detectors in real-time detection task. <ref type="figure" target="#fig_3">Fig. 5</ref> visualizes several examples on COCO test-dev. We also compare ThunderNet with large one-stage detectors. ThunderNet with SNet146 surpasses YOLOv2 <ref type="bibr" target="#b24">[25]</ref> with 37? fewer FLOPs. And ThunderNet with SNet535 significantly outperforms YOLOv2 and SSD300 <ref type="bibr" target="#b18">[19]</ref>, and rivals SSD321 <ref type="bibr" target="#b5">[6]</ref> and DSSD321 <ref type="bibr" target="#b5">[6]</ref>. It suggests that Thun-derNet is not only efficient but highly accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Input Resolution</head><p>We first explore the relationship between the input resolution and the backbone. <ref type="table">Table 4</ref> reveals that large backbones with small images and small backbones with large images are both not optimal. There is a trade-off between the two factors. On the one hand, small images lead to lowresolution feature maps and induce severe loss of detail features. It is hard to be remedied by simply increasing the capacity of the backbones. On the other hand, small backbones are too weak to encode sufficient information from large images. The backbone and the input images should match for a better balance between the representation ability and the resolution of the feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Backbone Networks</head><p>We then evaluate the design of the backbones. SNet146 and SNet49 are used as the baselines. SNet146 achieves 32.5% top-1 error on ImageNet classification and 23.6 AP on COCO test-dev ( putational cost unchanged. This model performs worse on both image classification (by 0.2%) and object detection (by 0.9 AP) (Table 5(b)). Compared with 3?3 depthwise convolutions, 5?5 depthwise convolutions considerably increase the receptive fields, which helps in both tasks. We then add another 3?3 depthwise convolution before the first 1?1 convolution in all building blocks as in Shuf-fleNetV2* <ref type="bibr" target="#b19">[20]</ref>. The number of channels is kept unchanged as the baseline. This model is comparable on image classification, but slightly worse on object detection (by 0.3 AP) (Table 5(c)). As this model and SNet146 have the same receptive fields theoretically, we conjecture that 5?5 depthwise convolutions can provide larger valid receptive fields, which is especially crucial in object detection. Early-stage and Late-stage Features. To investigate the trade-off between early-stage and late-stage features, we first add a 1024-channel Conv5 in SNet146. The channels in the early stages are reduced accordingly. This model slightly improves the top-1 error, but reduces AP by 0.4 (Table 5(d)). A wide Conv5 generates more discriminative features, which improves the classification accuracy. However, object detection focuses on both classification and localization. Increasing the channels in early stages encodes more detail information, which is beneficial for localization.</p><p>For SNet49, we first remove Conv5 in SNet49 and increase the channels from Stage2 to Stage4. Table 5(f) shows that both the classification and the detection performance suffer from severe degradation. Removing Conv5 cuts the output channels of the backbone by half, which hinders the model from learning adequate information.</p><p>We then extend Conv5 to 1024 channels as in the original ShuffleNetV2. The early-stage channels are compressed to maintain the same overall computational cost. This model surpasses SNet49 on image classification by 0.8%, but performs worse on object detection (Table 5(g)). By leveraging a wide Conv5, this model benefits from more high-level features in image classification. However, it suffers from the lack of low-level features in object detection. It further demonstrates the differences between image classification and object detection. Comparison with Lightweight Backbones. We further compare SNet with other lightweight backbones in Thun-derNet framework ( <ref type="table" target="#tab_6">Table 6</ref>). SNet146 surpasses Xception <ref type="bibr" target="#b2">[3]</ref>, MobileNetV2 <ref type="bibr" target="#b27">[28]</ref>, and ShuffleNetV1/V2/V2* <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>   on object detection under similar FLOPs. These results further demonstrate the effectiveness of our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Detection Part</head><p>We also investigate the effectiveness of the design of the detection part in ThunderNet. <ref type="table" target="#tab_7">Table 7</ref> describes the comparison of the model variants in the experiments.</p><p>Baseline. We choose a compressed Light-Head R-CNN <ref type="bibr" target="#b13">[14]</ref> with SNet146 as the baseline. C 5 is upsampled by 2? to obtain the same downsampling rate. C 4 and C 5 are then squeezed to 245 channels and sent to RPN and RoI warping respectively. We use a 256-channel 3?3 convolution in RPN and a 2048-d fc layer in R-CNN subnet. This model requires 703 MFLOPs and achieves 21.9 AP (Table 7(a)). Besides, we would mention that multi-scale training, CGBN <ref type="bibr" target="#b21">[22]</ref>, and Soft-NMS <ref type="bibr" target="#b0">[1]</ref> gradually improve the baseline by 1.4 AP (from 20.5 to 21.9 AP).</p><p>RPN and R-CNN subnet. We first replace the 3?3 convolution in RPN with a 5?5 depthwise convolution and a 1?1 convolution. The number of output channels remains unchanged. This design reduces the computational cost by 28% without harming the accuracy (  maps introduces semantic and context information of different levels, which improves the representation ability.</p><p>Spatial Attention Module. Adopting Spatial Attention Module (SAM) without CEM ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Balance between Backbone and Detection Head</head><p>We further explore the relationship between the backbone and the detection head. Two models are used in the experiments: a large-backbone-small-head (LBSH) model and a small-backbone-large-head (SBLH) model. The LBSH model is ThunderNet with SNet146. The SBLH model uses SNet49 and a heavier head: ? is 10, and a 2048-d fc layer is used in R-CNN subnet. As shown in <ref type="table" target="#tab_9">Table 8</ref>, the LBSH model outperforms the SBLH one by 3.4 AP even with less FLOPs. It suggests that the large-backbone-small-head design is better than the small-backbone-large-head design for lightweight two-stage detectors. We conjecture that the  <ref type="table">Table 9</ref>. Inference speed in fps on Snapdragon 845 (ARM), Xeon E5-2682v4 (CPU) and GeForce 1080Ti (GPU).</p><p>capability of the backbone and the detection head should match. In the small-backbone-large-head design, the features from the backbone are relatively weak, which makes the powerful detection head redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Inference Speed</head><p>At last, we evaluate the inference speed of ThunderNet on Snapdragon 845 (ARM), Xeon E5-2682v4 (CPU) and GeForce 1080Ti (GPU). On ARM and CPU, the inference is executed with a single thread. The batch normalization layers are merged with the preceding convolutions for faster inference speed. The results are shown in <ref type="table">Table 9</ref>. Thun-derNet with SNet49 achieves real-time detection on both ARM and CPU at 24.1 and 47.3 fps, respectively. To the best of our knowledge, this is the first real-time detector and the fastest single-thread speed on ARM platforms ever reported. ThunderNet with SNet146 runs at 13.8 fps on ARM and runs in real-time on CPU at 32.3 fps. All three models run at over 200 fps on GPU. These results suggest that ThunderNet is highly efficient in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We investigate the effectiveness of two-stage detectors in real-time generic object detection and propose a lightweight two-stage detector named ThunderNet. In the backbone part, we analyze the drawbacks in prior lightweight backbones and present a lightweight backbone designed for object detection. In the detection part, we adopt an extremely efficient design in the detection head and RPN. Context Enhancement Module and Spatial Attention Module are designed to improve the feature representation. At last, we investigate the balance between the input resolution, the backbone, and the detection head. ThunderNet achieves superior detection accuracy to prior one-stage detectors with significantly less computational cost. To the best of our knowledge, ThunderNet achieves the first real-time detector and the fastest single-thread speed reported on ARM platforms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of ThunderNet and previous lightweight detectors on COCO test-dev. ThunderNet achieves improvements in both accuracy and efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>arXiv:1903.11752v3 [cs.CV] 28 Mar 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Structure of Spatial Attention Module (SAM). SAM leverages the information learned in RPN to refine the feature distribution of the feature map from Context Enhancement Module. The feature map is then used for RoI warping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Examples visualization on COCO test-dev.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of the feature map before RoI warping. Spatial Attention Module (SAM) enhances the features in the foreground regions and weakens those in the background regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Architecture of the SNet backbone networks. SNet uses ShuffleNetV2 basic blocks but replaces all 3?3 depthwise convolutions with 5?5 depthwise convolutions.</figDesc><table><row><cell>Stage</cell><cell>Output Size</cell><cell>SNet49</cell><cell>Layer SNet146</cell><cell>SNet535</cell></row><row><cell>Input</cell><cell>224?224</cell><cell></cell><cell>image</cell><cell></cell></row><row><cell>Conv1</cell><cell>112?112</cell><cell>3?3, 24, s2</cell><cell>3?3, 24, s2</cell><cell>3?3, 48, s2</cell></row><row><cell>Pool</cell><cell>56?56</cell><cell></cell><cell>3?3 maxpool, s2</cell><cell></cell></row><row><cell>Stage2</cell><cell>28?28 28?28</cell><cell>[60, s2] [60, s1]?3</cell><cell>[132, s2] [132, s1]?3</cell><cell>[248, s2] [248, s1]?3</cell></row><row><cell>Stage3</cell><cell>14?14 14?14</cell><cell>[120, s2] [120, s1]?7</cell><cell>[264, s2] [264, s1]?7</cell><cell>[496, s2] [496, s1] ?7</cell></row><row><cell>Stage4</cell><cell>7?7 7?7</cell><cell>[240, s2] [240, s1]?3</cell><cell>[528, s2] [528, s1]?3</cell><cell>[992, s2] [992, s1]?3</cell></row><row><cell>Conv5</cell><cell>7?7</cell><cell>1?1, 512</cell><cell>-</cell><cell>-</cell></row><row><cell>Pool</cell><cell>1?1</cell><cell></cell><cell>global avg pool</cell><cell></cell></row><row><cell>FC</cell><cell></cell><cell></cell><cell>1000-d fc</cell><cell></cell></row><row><cell>FLOPs</cell><cell></cell><cell>49M</cell><cell>146M</cell><cell>535M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.4.3, this design further reduces the computational cost of R-CNN subnet without sacrificing accuracy. Besides, due to the small feature maps, we reduce the number of RoIs for testing as discussed in Sec. 4.1.</figDesc><table><row><cell></cell><cell>1x1 Conv, 245</cell></row><row><cell>C4, 20x20</cell><cell></cell></row><row><cell>stage 4</cell><cell></cell></row><row><cell></cell><cell>1x1 Conv, 245</cell></row><row><cell></cell><cell>2x Upsample</cell></row><row><cell>C5, 10x10</cell><cell></cell></row><row><cell>global avg pool</cell><cell>1x1 Conv, 245</cell></row><row><cell>Cglb, 1x1</cell><cell>Broadcast</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>,Table 2 .</head><label>2</label><figDesc>ThunderNet surpasses other one-stage Evaluation results on VOC 2007 test. ThunderNet surpasses competing models with significantly less computational cost.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Input</cell><cell></cell><cell>MFLOPs</cell><cell>mAP</cell><cell></cell></row><row><cell>YOLOv2 [25]</cell><cell>Darknet-19</cell><cell cols="2">416 ? 416</cell><cell>17400</cell><cell>76.8</cell><cell></cell></row><row><cell>SSD300* [19]</cell><cell>VGG-16</cell><cell cols="2">300 ? 300</cell><cell>31750</cell><cell>77.5</cell><cell></cell></row><row><cell>SSD321 [6]</cell><cell>ResNet-101</cell><cell cols="2">321 ? 321</cell><cell>15400</cell><cell>77.1</cell><cell></cell></row><row><cell>DSSD321 [6]</cell><cell>ResNet-101 + FPN</cell><cell cols="2">321 ? 321</cell><cell>21200</cell><cell>78.6</cell><cell></cell></row><row><cell>R-FCN [4]</cell><cell>ResNet-50</cell><cell cols="2">600 ? 1000</cell><cell>58900</cell><cell>77.4</cell><cell></cell></row><row><cell>Tiny-YOLO [25]</cell><cell>Tiny Darknet</cell><cell cols="2">416 ? 416</cell><cell>3490</cell><cell>57.1</cell><cell></cell></row><row><cell>D-YOLO [21]</cell><cell>Tiny Darknet</cell><cell cols="2">416 ? 416</cell><cell>2090</cell><cell>67.6</cell><cell></cell></row><row><cell>MobileNet-SSD [31]</cell><cell>MobileNet</cell><cell cols="2">300 ? 300</cell><cell>1150</cell><cell>68.0</cell><cell></cell></row><row><cell>Pelee [31]</cell><cell>PeleeNet</cell><cell cols="2">304 ? 304</cell><cell>1210</cell><cell>70.9</cell><cell></cell></row><row><cell>Tiny-DSOD [13]</cell><cell>DDB-Net + D-FPN</cell><cell cols="2">300 ? 300</cell><cell>1060</cell><cell>72.1</cell><cell></cell></row><row><cell>ThunderNet (ours)</cell><cell>SNet49</cell><cell cols="2">320 ? 320</cell><cell>250</cell><cell>70.1</cell><cell></cell></row><row><cell>ThunderNet (ours)</cell><cell>SNet146</cell><cell cols="2">320 ? 320</cell><cell>461</cell><cell>75.1</cell><cell></cell></row><row><cell>ThunderNet (ours)</cell><cell>SNet535</cell><cell cols="2">320 ? 320</cell><cell>1287</cell><cell>78.6</cell><cell></cell></row><row><cell>Model</cell><cell>Backbone</cell><cell>Input</cell><cell>MFLOPs</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell>YOLOv2 [25]</cell><cell>Darknet-19</cell><cell>416 ? 416</cell><cell>17500</cell><cell>21.6</cell><cell>44.0</cell><cell>19.2</cell></row><row><cell>SSD300* [19]</cell><cell>VGG-16</cell><cell>300 ? 300</cell><cell>35200</cell><cell>25.1</cell><cell>43.1</cell><cell>25.8</cell></row><row><cell>SSD321 [6]</cell><cell>ResNet-101</cell><cell>321 ? 321</cell><cell>16700</cell><cell>28.0</cell><cell>45.4</cell><cell>29.3</cell></row><row><cell>DSSD321 [6]</cell><cell>ResNet-101 + FPN</cell><cell>321 ? 321</cell><cell>22300</cell><cell>28.0</cell><cell>46.1</cell><cell>29.2</cell></row><row><cell>Light-Head R-CNN [20]</cell><cell>ShuffleNetV2*</cell><cell>800 ? 1200</cell><cell>5650</cell><cell>23.7</cell><cell>-</cell><cell>-</cell></row><row><cell>MobileNet-SSD [11]</cell><cell>MobileNet</cell><cell>300 ? 300</cell><cell>1200</cell><cell>19.3</cell><cell>-</cell><cell>-</cell></row><row><cell>MobileNet-SSDLite [28]</cell><cell>MobileNet</cell><cell>320 ? 320</cell><cell>1300</cell><cell>22.2</cell><cell>-</cell><cell>-</cell></row><row><cell>MobileNetV2-SSDLite [28]</cell><cell>MobileNetV2</cell><cell>320 ? 320</cell><cell>800</cell><cell>22.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Pelee [31]</cell><cell>PeleeNet</cell><cell>304 ? 304</cell><cell>1290</cell><cell>22.4</cell><cell>38.3</cell><cell>22.9</cell></row><row><cell>Tiny-DSOD [13]</cell><cell>DDB-Net + D-FPN</cell><cell>300 ? 300</cell><cell>1120</cell><cell>23.2</cell><cell>40.4</cell><cell>22.8</cell></row><row><cell>ThunderNet (ours)</cell><cell>SNet49</cell><cell>320 ? 320</cell><cell>262</cell><cell>19.2</cell><cell>33.7</cell><cell>19.7</cell></row><row><cell>ThunderNet (ours)</cell><cell>SNet146</cell><cell>320 ? 320</cell><cell>473</cell><cell>23.7</cell><cell>40.3</cell><cell>24.6</cell></row><row><cell>ThunderNet (ours)</cell><cell>SNet535</cell><cell>320 ? 320</cell><cell>1300</cell><cell>28.1</cell><cell>46.2</cell><cell>29.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Evaluation results on COCO test-dev. ThunderNet with SNet49 achieves MobileNet-SSD level accuracy with 22% of the FLOPs. ThunderNet with SNet146 achieves superior accuracy to prior lightweight one-stage detectors with merely 40% of the FLOPs. ThunderNet with SNet535 rivals large detectors with significantly less computational cost.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 Table 4 .</head><label>54</label><figDesc>Evaluation of different input resolutions on COCO testdev. Large backbones with small images and small backbones with large images are both not optimal.</figDesc><table><row><cell>(a)), while SNet49 achieves</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Evaluation of different backbones on ImageNet classification and COCO test-dev. DWConv: depthwise convolution.</figDesc><table><row><cell>Backbone</cell><cell></cell><cell>MFLOPs</cell><cell>Top-1 Err.</cell><cell>AP</cell></row><row><cell>(a) SNet146</cell><cell></cell><cell>146</cell><cell>32.5</cell><cell>23.7</cell></row><row><cell>(b) SNet146 + 3?3 DWConv</cell><cell></cell><cell>145</cell><cell>32.7</cell><cell>22.7</cell></row><row><cell cols="2">(c) SNet146 + double 3?3 DWConv</cell><cell>143</cell><cell>32.4</cell><cell>23.3</cell></row><row><cell>(d) SNet146 + 1024-d Conv5</cell><cell></cell><cell>147</cell><cell>32.3</cell><cell>23.2</cell></row><row><cell>(e) SNet49</cell><cell></cell><cell>49</cell><cell>39.7</cell><cell>19.2</cell></row><row><cell>(f) SNet49 + No Conv5</cell><cell></cell><cell>49</cell><cell>40.8</cell><cell>18.2</cell></row><row><cell>(g) SNet49 + 1024-d Conv5</cell><cell></cell><cell>49</cell><cell>38.9</cell><cell>18.8</cell></row><row><cell>Backbone</cell><cell>MFLOPs</cell><cell cols="2">Top-1 Err.</cell><cell>AP</cell></row><row><cell>ShuffleNetV1 [33]</cell><cell>137</cell><cell></cell><cell>34.8</cell><cell>20.8</cell></row><row><cell>ShuffleNetV2 [20]</cell><cell>147</cell><cell></cell><cell>31.4</cell><cell>22.7</cell></row><row><cell>ShuffleNetV2* [20]</cell><cell>145</cell><cell></cell><cell>32.2</cell><cell>23.2</cell></row><row><cell>Xception [3]</cell><cell>145</cell><cell></cell><cell>34.1</cell><cell>23.0</cell></row><row><cell>MobileNetV2 [28]</cell><cell>145</cell><cell></cell><cell>32.9</cell><cell>22.7</cell></row><row><cell>SNet146</cell><cell>146</cell><cell></cell><cell>32.5</cell><cell>23.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Evaluation of lightweight backbones on COCO test-dev.</figDesc><table><row><cell>SNet146 achieves better detection results though the classification</cell></row><row><cell>accuracy is lower.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>(b)). We then halve the number of outputs of the fc layer in R-CNN subnet to 1024, which achieves a further 13% compression on the FLOPs with a marginal decrease of 0.2 AP. (Table 7(c)). These results demonstrate that heavy RPN and R-CNN subnet introduce great redundancy for lightweight detectors. More details will be discussed in Sec. 4.4.4.Context Enhancement Module. We then insert Context Enhancement Module (CEM) after the backbone. The output feature map of CEM is used for both RPN and RoI warping. CEM achieves thorough improvements of 1.7 AP, 2.5 AP 50 and 1.8 AP 75 with negligible increase on FLOPs(Table 7(d)). The combination of the multi-scale feature</figDesc><table><row><cell cols="5">BL SRPN SRCN CEM SAM AP AP50 AP75 MFLOPs</cell></row><row><cell>(a)</cell><cell></cell><cell>21.9 37.6</cell><cell>22.5</cell><cell>714</cell></row><row><cell>(b)</cell><cell></cell><cell>21.8 37.5</cell><cell>22.4</cell><cell>516</cell></row><row><cell>(c)</cell><cell></cell><cell>21.6 37.4</cell><cell>22.2</cell><cell>448</cell></row><row><cell>(d)</cell><cell></cell><cell>23.3 39.9</cell><cell>24.0</cell><cell>449</cell></row><row><cell>(e)</cell><cell></cell><cell>23.0 39.0</cell><cell>24.0</cell><cell>473</cell></row><row><cell>(f)</cell><cell></cell><cell>23.7 40.3</cell><cell>24.6</cell><cell>473</cell></row><row><cell cols="5">Table 7. Ablation studies on the detection part on COCO test-</cell></row><row><cell cols="5">dev. We use a compressed Light-Head R-CNN with SNet146 as</cell></row><row><cell cols="5">the baseline (BL), and gradually add small RPN (SRPN), small R-</cell></row><row><cell cols="5">CNN (SRCN), Context Enhancement Module (CEM) and Spatial</cell></row><row><cell cols="3">Attention Module (SAM) for ablation studies.</cell><cell></cell></row><row><cell>Input</cell><cell>w/o SAM</cell><cell>w/ SAM</cell><cell cols="2">Ground Truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>(e)) improves AP by 1.4 with merely 5% extra computational cost compared withTable 7(c).Fig. 6visualizes the feature maps before RoI warping inTable 7(c) andTable 7(e). It is clear that SAM effectively refines the feature distribution with foreground feature enhanced and background features weakened.At last, we adopt both CEM and SAM to compose the complete ThunderNet (Table 7(f)). This setting improves AP by 1.8, AP 50 by 2.7, and AP 75 by 2.1 over the baseline while reducing the computational cost by 34%. These results have demonstrated the effectiveness of our design.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>MFLOPs and AP of different detection head designs on COCO test-dev. The large-backbone-small-head model outperforms the small-backbone-large-head model with less FLOPs.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>RPN</cell><cell>Head</cell><cell>Total</cell><cell>AP</cell></row><row><cell>large-backbone-small-head</cell><cell>338</cell><cell>43</cell><cell>92</cell><cell>473</cell><cell>23.6</cell></row><row><cell>small-backbone-large-head</cell><cell>154</cell><cell>70</cell><cell>286</cell><cell>510</cell><cell>20.2</cell></row><row><cell>Model</cell><cell>ARM</cell><cell></cell><cell>CPU</cell><cell>GPU</cell><cell></cell></row><row><cell>Thunder w/ SNet49</cell><cell>24.1</cell><cell></cell><cell>47.3</cell><cell>267</cell><cell></cell></row><row><cell>Thunder w/ SNet146</cell><cell>13.8</cell><cell></cell><cell>32.3</cell><cell>248</cell><cell></cell></row><row><cell>Thunder w/ SNet535</cell><cell>5.8</cell><cell></cell><cell>15.3</cell><cell>214</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is sponsored in part by the National Key R&amp;D Program of China (2018YFB2101100, 2017YFA0700800).  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chollet</forename><surname>Fran? Ois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tiny-dsod: Lightweight object detection for resource-restricted usages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11013</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Light-head r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
	</analytic>
	<monogr>
		<title level="m">defense of two-stage object detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Object detection at 200 frames per second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cemalettin</forename><surname>Ozturk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06361</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pelee: a real-time object detection system on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1963" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Examples visualization of ThunderNet with SNet535</title>
		<imprint/>
	</monogr>
	<note>Figure 9</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
