<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Concealed Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Concealed Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Concealed Object Detection</term>
					<term>Camouflaged Object Detection</term>
					<term>COD</term>
					<term>Dataset</term>
					<term>Benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the first systematic study on concealed object detection (COD), which aims to identify objects that are visually embedded in their background. The high intrinsic similarities between the concealed objects and their background make COD far more challenging than traditional object detection/segmentation. To better understand this task, we collect a large-scale dataset, called COD10K, which consists of 10,000 images covering concealed objects in diverse real-world scenarios from 78 object categories. Further, we provide rich annotations including object categories, object boundaries, challenging attributes, object-level labels, and instance-level annotations. Our COD10K is the largest COD dataset to date, with the richest annotations, which enables comprehensive concealed object understanding and can even be used to help progress several other vision tasks, such as detection, segmentation, classification etc. Motivated by how animals hunt in the wild, we also design a simple but strong baseline for COD, termed the Search Identification Network (SINet). Without any bells and whistles, SINet outperforms twelve cutting-edge baselines on all datasets tested, making them robust, general architectures that could serve as catalysts for future research in COD. Finally, we provide some interesting findings, and highlight several potential applications and future directions. To spark research in this new field, our code, dataset, and online demo are available at our project page: http://mmcheng.net/cod.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C AN you find the concealed object(s) in each image of <ref type="figure" target="#fig_0">Fig. 1</ref> within 10 seconds? Biologists refer to this as background matching camouflage (BMC) <ref type="bibr" target="#b1">[2]</ref>, where one or more objects attempt to adapt their coloring to match "seamlessly" with the surroundings in order to avoid detection <ref type="bibr" target="#b2">[3]</ref>. Sensory ecologists <ref type="bibr" target="#b3">[4]</ref> have found that this BMC strategy works by deceiving the visual perceptual system of the observer. Naturally, addressing concealed object detection (COD 1 ) requires a significant amount of visual perception <ref type="bibr" target="#b5">[6]</ref> knowledge. Understanding COD has not only scientific value in itself, but it also important for applications in many fundamental fields, such as computer vision (e.g., for search-and-rescue work, or rare species discovery), medicine (e.g., polyp segmentation <ref type="bibr" target="#b6">[7]</ref>, lung infection segmentation <ref type="bibr" target="#b7">[8]</ref>), agriculture (e.g., locust detection to prevent invasion), and art (e.g., recreational art <ref type="bibr" target="#b8">[9]</ref>).</p><p>In <ref type="figure" target="#fig_1">Fig. 2</ref>, we present examples of generic, salient, and concealed object detection. The high intrinsic similarities between the targets and non-targets make COD far more challenging than traditional object segmentation/detection <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Although it has gained increased attention recently, studies on COD still remain scarce, mainly due to the lack of a sufficiently large dataset and a standard benchmark like Pascal-VOC <ref type="bibr" target="#b12">[13]</ref>, ImageNet <ref type="bibr" target="#b13">[14]</ref>, MS-COCO <ref type="bibr" target="#b14">[15]</ref>, ADE20K <ref type="bibr" target="#b15">[16]</ref>, and DAVIS <ref type="bibr" target="#b16">[17]</ref>.</p><p>? Deng-Ping Fan is with the CS, Nankai University, Tianjin, China, and also with the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.</p><p>(E-mail: dengpfan@gmail.com) ? Ge-Peng Ji and Ling Shao are with the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE. (E-mail: gepeng.ji@inceptioniai.org; ling.shao@inceptioniai.org) ? Ming-Ming Cheng is with the CS, Nankai University, Tianjin, China. (Email: cmm@nankai.edu.cn) ? A preliminary version of this work has appeared in CVPR 2020 <ref type="bibr" target="#b0">[1]</ref>.</p><p>? The major part of this work was done in Nankai University.</p><p>? Ming-Ming Cheng is the corresponding author.</p><p>1. We define COD as segmenting objects or stuff (amorphous regions <ref type="bibr" target="#b4">[5]</ref>) that have a similar pattern, e.g., texture, color, direction, etc., to their natural or man-made environment. In the rest of the paper, for convenience, the concealed object segmentation is considered identical to COD and used interchangeably.  Given an input image (a), we present the ground-truth for (b) panoptic segmentation <ref type="bibr" target="#b4">[5]</ref> (which detects generic objects <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> including stuff and things), (c) instance level salient object detection <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and (d) the proposed concealed object detection task, where the goal is to detect objects that have a similar pattern to the natural environment. In this example, the boundaries of the two butterflies are blended with the bananas, making them difficult to identify.</p><p>In this paper, we present the first complete study for the concealed object detection task using deep learning, bringing a novel view to object detection from a concealed perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>Our main contributions are as follows:</p><p>1) COD10K Dataset. With the goal mentioned above, we carefully assemble COD10K, a large-scale concealed object detection dataset. Our dataset contains 10,000 images covering 78 object categories, such as terrestrial, amphibians, flying, aquatic, etc. All the concealed images are hierarchically annotated with category, bounding-box, object-level, and arXiv:2102.10274v2 [cs.CV] 10 Jun 2021</p><p>instance-level labels <ref type="figure">(Fig. 3)</ref>, benefiting many related tasks, such as object proposal, localization, semantic edge detection, transfer learning <ref type="bibr" target="#b20">[21]</ref>, domain adaption <ref type="bibr" target="#b21">[22]</ref>, etc. Each concealed image is assigned challenging attributes (e.g., shape complexity-SC, indefinable boundaries-IB, occlusions-OC) found in the real-world and matting-level <ref type="bibr" target="#b22">[23]</ref> labeling (which takes ?60 minutes per image). These high-quality labels could help provide deeper insight into the performance of models. 2) COD Framework. We propose a simple but efficient framework, named SINet (Search Identification Net). Remarkably, the overall training time of SINet takes 4 hours and it achieves the new state-of-the-art (SOTA) on all existing COD datasets, suggesting that it could offer a potential solution to concealed object detection. Our network also yield several interesting findings (e.g., search and identification strategy is suitable for COD), making various potential applications more feasible. 3) COD Benchmark. Based on the collected COD10K and previous datasets <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, we offer a rigorous evaluation of 12 SOTA baselines, making ours the largest COD study. We report baselines in two scenarios, i.e., super-class and sub-class. We also track the community's progress via an online benchmark (http://dpfan.net/camouflage/). 4) Downstream Applications. To further support research in the field, we develop an online demo (http://mc.nankai.edu. cn/cod) to enable other researchers to test their scenes easily.</p><p>In addition, we also demonstrate several potential applications such as medicine, manufacturing, agriculture, art, etc. 5) Future Directions. Based on the proposed COD10K, we also discuss ten promising directions for future research. We find that concealed object detection is still far from being solved, leaving large room for improvement.</p><p>This paper is based on and extends our conference version <ref type="bibr" target="#b0">[1]</ref> in terms of several aspects. First, we provide a more detailed analysis of our COD10K, including the taxonomy, statistics, annotations, and resolutions. Secondly, we improve the performance our SINet model by introducing neighbor connection decoder (NCD) and group-reversal attention (GRA). Thirdly, we conduct extensive experiments to validate the effectiveness of our model, and provide several ablation studies for the different modules within our framework. Fourth, we provide an exhaustive superclass and sub-class benchmarking and a more insightful discussion regarding the novel COD task. Last but not least, based on our benchmark results, we draw several important conclusions and highlight several promising future directions, such as concealed object ranking, concealed object proposal, concealed instance segmentation. CUT 3 . These applications have already drawn great attention (12K github stars) and have important real-world impacts. Although the term "salient" is essentially the opposite of "concealed" (standout vs. immersion), salient objects can nevertheless provide important information for COD, e.g., images containing salient objects can be used as the negative samples. Giving a complete review on SOD is beyond the scope of this work. We refer readers to recent survey and benchmark papers <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> for more details. Our online benchmark is publicly available at: http://dpfan.net/socbenchmark/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concealed Object Detection (COD).</head><p>Research into COD, which has had a tremendous impact on advancing our knowledge of visual perception, has a long and rich history in biology and art. Two remarkable studies on concealed animals from Abbott Thayer <ref type="bibr" target="#b36">[37]</ref> and Hugh Cott <ref type="bibr" target="#b37">[38]</ref> are still hugely influential. The reader can refer to the survey by Stevens et al. <ref type="bibr" target="#b3">[4]</ref> for more details on this history. There are also some concurrent works <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> that are accepted after this submission.</p><p>COD Datasets. CHAMELEON <ref type="bibr" target="#b23">[24]</ref> is an unpublished dataset that has only 76 images with manually annotated object-level ground-truths (GTs). The images were collected from the Internet via the Google search engine using "concealed animal" as a keyword. Another contemporary dataset is CAMO <ref type="bibr" target="#b24">[25]</ref>, which has 2.5K images (2K for training, 0.5K for testing) covering eight categories. It has two sub-datasets, CAMO and MS-COCO, each of which contains 1.25K images. Unlike existing datasets, the goal of our COD10K is to provide a more challenging, higher quality, and more densely annotated dataset. COD10K is the largest concealed object detection dataset so far, containing 10K images (6K for training, 4K for testing). See <ref type="table" target="#tab_0">Table 1</ref> for details.</p><p>Types of Camouflage. Concealed images can be roughly split into two types: those containing natural camouflage and those with artificial camouflage. Natural camouflage is used by animals (e.g., insects, sea horses, and cephalopods) as a survival skill to avoid recognition by predators. In contrast, artificial camouflage is usually used in art design/gaming to hide information, occurs in products during the manufacturing process (so-called surface defects <ref type="bibr" target="#b41">[42]</ref>, defect detection <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>), or appears in our daily life (e.g., transparent objects <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>).</p><p>COD Formulation. Unlike class-aware tasks such as semantic segmentation, concealed object detection is a class-agnostic task. Thus, the formulation of COD is simple and easy to define. Given an image, the task requires a concealed object detection algorithm to assign each pixel i a label Label i ? {0,1}, where Label i denotes the binary value of pixel i. A label of 0 is given to pixels that do not belong to the concealed objects, while a label of 1 indicates that a pixel is fully assigned to the concealed objects. We focus on object-level concealed object detection, leaving concealed instance detection to our future work. <ref type="bibr" target="#b2">3</ref>. https://github.com/AlbertSuarez/object-cut</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COD10K DATASET</head><p>The emergence of new tasks and datasets <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> has led to rapid progress in various areas of computer vision. For instance, ImageNet <ref type="bibr" target="#b49">[50]</ref> revolutionized the use of deep models for visual recognition. With this in mind, our goals for studying and developing a dataset for COD are: (1) to provide a new challenging object detection task from the concealed perspective, <ref type="bibr" target="#b1">(2)</ref> to promote research in several new topics, and (3) to spark novel ideas. Examples from COD10K are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We will provide the details on COD10K in terms of three key aspects including image collection, professional annotation, and dataset features and statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Collection</head><p>As discussed in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[51]</ref>, the quality of annotation and size of a dataset are determining factors for its lifespan as a benchmark. To this end, COD10K contains 10,000 images (5,066 concealed, 3,000 background, 1,934 non-concealed), divided into 10 super-classes (i.e., flying, aquatic, terrestrial, amphibians, other, sky, vegetation, indoor, ocean, and sand), and 78 sub-classes (69 concealed, 9 non-concealed) which were collected from multiple photography websites.</p><p>Most concealed images are from Flickr and have been applied for academic use with the following keywords: concealed animal, unnoticeable animal, concealed fish, concealed butterfly, hidden wolf spider, walking stick, dead-leaf mantis, bird, sea horse, cat, pygmy seahorses, etc. (see <ref type="figure">Fig. 4</ref>) The remaining concealed images (around 200 images) come from other websites, including Visual Hunt, Pixabay, Unsplash, Free-images, etc., which release public-domain stock photos, free from copyright and loyalties. To avoid selection bias <ref type="bibr" target="#b10">[11]</ref>, we also collected 3,000 salient images from Flickr. To further enrich the negative samples, 1,934 nonconcealed images, including forest, snow, grassland, sky, seawater and other categories of background scenes, were selected from the Internet. For more details on the image selection scheme, we refer to Zhou et al. <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Professional Annotation</head><p>Recently released datasets <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> have shown that establishing a taxonomic system is crucial when creating a largescale dataset. Motivated by <ref type="bibr" target="#b54">[55]</ref>   ? Categories. As illustrated in <ref type="figure" target="#fig_3">Fig. 6</ref>, we first create five superclass categories. Then, we summarize the 69 most frequently appearing sub-class categories according to our collected data. Finally, we label the sub-class and super-class of each image. If the candidate image does not belong to any established category, we classify it as 'other'.</p><p>? Bounding boxes. To extend COD10K for the concealed object proposal task, we also carefully annotate the bounding boxes for each image.</p><p>? Attributes. In line with the literature <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref>, we label each concealed image with highly challenging attributes faced in natural scenes, e.g., occlusions, indefinable boundaries. Attribute descriptions and the co-attribute distribution is shown in <ref type="figure" target="#fig_5">Fig. 7</ref>.</p><p>? Objects/Instances. We stress that existing COD datasets focus exclusively on object-level annotations <ref type="table" target="#tab_0">(Table 1)</ref>. However, being able to parse an object into its instances is important for computer vision researchers to be able to edit and understand a scene. To this end, we further annotate objects at an instance-level, like COCO <ref type="bibr" target="#b14">[15]</ref>, resulting in 5,069 objects and 5,930 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Features and Statistics</head><p>We now discuss the proposed dataset and provide some statistics.   ? Resolution distribution. As noted in <ref type="bibr" target="#b55">[56]</ref>, high-resolution data provides more object boundary details for model training and yields better performance when testing. <ref type="figure" target="#fig_6">Fig. 8</ref> presents the resolution distribution of COD10K, which includes a large number of Full HD 1080p resolution images.</p><p>? Object size. Following <ref type="bibr" target="#b10">[11]</ref>, we plot the normalized (i.e., related to image areas) object size in <ref type="figure">Fig. 9</ref> (top-left), i.e., the size distribution from 0.01%? 80.74% (avg.: 8.94%), showing a broader range compared to CAMO-COCO, and CHAMELEON.</p><p>? Global/Local contrast. To evaluate whether an object is easy to detect, we describe it using the global/local contrast strategy <ref type="bibr" target="#b56">[57]</ref>. <ref type="figure">Fig. 9</ref> (top-right) shows that objects in COD10K are more challenging than those in other datasets.</p><p>? Center bias. This commonly occurs when taking a photo, as humans are naturally inclined to focus on the center of a scene. We adopt the strategy described in <ref type="bibr" target="#b10">[11]</ref> to analyze this bias. <ref type="figure">Fig. 9</ref> (bottom-left/right) shows that our COD10K dataset suffers from less center bias than others.</p><p>? Quality control. To ensure high-quality annotation, we invited three viewers to participate in the labeling process for 10-fold cross-validation. <ref type="figure" target="#fig_0">Fig. 10</ref> shows examples that were passed/rejected. This matting-level annotation costs ? 60 minutes per image on average.</p><p>? Super/Sub-class distribution. COD10K includes five concealed super-classes (i.e., terrestrial, atmobios, aquatic, amphib-   ? Dataset splits. To provide a large amount of training data for deep learning algorithms, our COD10K is split into 6,000 images for training and 4,000 for testing, randomly selected from each sub-class.</p><p>? Diverse concealed objects. In addition to the general concealed patterns, such as those in <ref type="figure" target="#fig_0">Fig. 1</ref>, our dataset also includes various other types of concealed objects, such as concealed body paintings and conceale in daily life (see <ref type="figure" target="#fig_0">Fig. 12</ref>). <ref type="figure" target="#fig_0">Fig. 13</ref> illustrates the overall concealed object detection framework of the proposed SINet (Search Identification Network). Next, <ref type="figure" target="#fig_0">Fig. 11</ref>. Word cloud distribution. The size of a specific word is proportional to the ratio of that keyword. we explain our motivation and introduce the network overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COD FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation.</head><p>Biological studies <ref type="bibr" target="#b57">[58]</ref> have shown that, when hunting, a predator will first judge whether a potential prey exists, i.e., it will search for a prey. Then, the target animal can be identified; and, finally, it can be caught.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction.</head><p>Several methods <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> have shown that satisfactory performance is dependent on the re-optimization strategy (i.e., coarse-to-fine), which is regarded as the composition of multiple sub-steps. This also suggests that decoupling the complicated targets can break the performance bottleneck. Our  <ref type="table" target="#tab_4">2  3  2  4  2  5   3  3  3  4  3  5   2  3  2  4  2  5   3  3  3  4</ref> 3 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse Map</head><p>Sigmoid Deep supervision</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reverse attention Decoder flow</head><p>Feature flow</p><formula xml:id="formula_0">Addition Reverse Reverse Reverse C Texture Enhanced Module Conv1x1 Conv1x3 Conv3x1 Conv3x3 (dilate=3) Conv1x1 Conv1x5 Conv5x1 Conv3x3 (dilate=5) Conv1x1 Conv1x7 Conv7x1 Conv3x3 (dilate=7) Conv1x1 Conv1x1 ? Conv1x1 C Concatenation Down4</formula><p>Up2 Up2 SINet model consists of the first two stages of hunting, i.e., search and identification. Specifically, the former phase (Section 4.2) is responsible for searching for a concealed object, while the latter one (Section 4.3) is then used to precisely detect the concealed object in a cascaded manner. Next, we elaborate on the details of the three main modules, including a) the texture enhanced module (TEM), which is used to capture fine-grained textures with the enlarged context cues; b) the neighbor connection decoder (NCD), which is able to provide the location information; and c) the cascaded group-reversal attention (GRA) blocks, which work collaboratively to refine the coarse prediction from the deeper layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Search Phase</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction.</head><p>For an input image I ? R W?H?3 , a set of features f k , k ? {1, 2, 3, 4, 5} is extracted from Res2Net-50 <ref type="bibr" target="#b60">[61]</ref> (removing the top three layers, i.e., 'average pool', '1000d fc', and 'softmax'). Thus, the resolution of each feature f k is H/2 k ?W /2 k , k ? {1, 2, 3, 4, 5}, covering diversified feature pyramids from high-resolution, weakly semantic to low-resolution, strongly semantic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Texture Enhanced Module (TEM).</head><p>Neuroscience experiments have verified that, in the human visual system, a set of various sized population receptive fields helps to highlight the area close to the retinal fovea, which is sensitive to small spatial shifts <ref type="bibr" target="#b61">[62]</ref>. This motivates us to use the TEM <ref type="bibr" target="#b62">[63]</ref> to incorporate more discriminative feature representations during the searching stage (usually in a small/local space). As shown in <ref type="figure" target="#fig_0">Fig. 13</ref>, each TEM component includes four parallel residual branches {b i , i = 1, 2, 3, 4} with different dilation rates d ? {1, 3, 5, 7} and a shortcut branch (gray arrow), respectively. In each branch b i , the first convolutional layer utilizes a 1 ? 1 convolution operation (Conv1?1) to reduce the channel size to 32. This is followed by two other layers: a (2i ? 1) ? (2i ? 1) convolutional layer and a 3 ? 3 convolutional layer with a specific dilation rate (2i ? 1) when i &gt; 1. Then, the first four branches {b i , i = 1, 2, 3, 4} are concatenated and the channel size is reduced to C via a 3?3 convolution operation. Note that we set C = 32 in the default implementation of our network for time-cost trade-off. Finally, the identity shortcut branch is added in, then the whole module is fed to a ReLU function to obtain the output feature f k . Besides, several works (e.g., Inception-V3 <ref type="bibr" target="#b63">[64]</ref>) have suggested that the standard convolution operation of size (2i ? 1) ? (2i ? 1) can be factorized as a sequence of two steps with (2i ? 1) ? 1 and 1 ? (2i ? 1) kernels, speeding-up the inference efficiency without decreasing the representation capabilities. All of these ideas are predicated on the fact that a 2D kernel with a rank of one is equal to a series of 1D convolutions <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>. In brief, compared to the standard receptive fields block structure <ref type="bibr" target="#b61">[62]</ref>, TEM add one more branch with a larger dilation rate to enlarge the receptive field and further replace the standard convolution with two asymmetric convolutional layers. For more details please refer to <ref type="figure" target="#fig_0">Fig. 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighbor Connection Decoder (NCD).</head><p>As observed by Wu et al. <ref type="bibr" target="#b62">[63]</ref>, low-level features consume more computational resources due to their larger spatial resolutions, but contribute less to performance. Motivated by this observation, we decide to aggregate only the top-three highest-level features (i.e., { f k ? R W /2 k ?H/2 k ?C , k = 3, 4, 5}) to obtain a more efficient learning capability, rather than taking all the feature pyramids into consideration. To be specific, after obtaining the candidate features from the three previous TEMs, in the search phase, we need to locate the concealed object.</p><p>However, there are still two key issues when aggregating multiple feature pyramids; namely, how to maintain semantic consistency within a layer and how to bridge the context across layers. Here, we propose to address these with the neighbor connection decoder (NCD). More specifically, we modify the partial decoder component (PDC) <ref type="bibr" target="#b62">[63]</ref> with a neighbor connection function and get three refined features f nc k = F NC ( f k ; W u NC ), k ? {3, 4, 5} and u ? {1, 2, 3}, which are formulated as:</p><formula xml:id="formula_1">? ? ? f nc 5 = f 5 f nc 4 = f 4 ? g[? 2 ? ( f 5 ); W 1 NC ] f nc 3 = f 3 ? g[? 2 ? ( f nc 4 ); W 2 NC ] ? g[? 2 ? ( f 4 ); W 3 NC ]<label>(1)</label></formula><p>where g[?; W u NC ] denotes a 3?3 convolutional layer followed by a batch normalization operation. To ensure shape matching between candidate features, we utilize an upsampling (e.g., 2 times) operation ? 2 ? (?) before element-wise multiplication ?. Then, we feed f nc k , k ? {3, 4, 5} into the neighbor connection decoder (NCD) and generate the coarse location map C 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Identification Phase</head><p>Reverse Guidance.</p><p>As discussed in Section 4.2, our global location map C 6 is derived from the three highest layers, which can only capture a relatively rough location of the concealed object, ignoring structural and textural details (see <ref type="figure" target="#fig_0">Fig. 13</ref>). To address this issue, we introduce a principled strategy to mine discriminative concealed regions by erasing objects <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 14 (b)</ref>, we obtain the output reverse guidance r k 1 via sigmoid and reverse operation. More precisely, we obtain the output reverse attention guidance r k 1 by a reverse operation, which can be formulated as:</p><formula xml:id="formula_2">r k 1 = ? (? 4 ? (C k+1 )), E , k = 5, ? (? 2 ? (C k+1 )), E , k ? {3, 4},<label>(2)</label></formula><p>where ? 4 ? and ? 2 ? denote a ?4 down-sampling and ?2 up-sampling operation, respectively. ? (x) = 1/(1 + e ?x ) is the sigmoid function, which is applied to convert the mask into the interval [0, 1].</p><p>is a reverse operation subtracting the input from matrix E, in which all the elements are 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group Guidance Operation (GGO).</head><p>As shown in <ref type="bibr" target="#b6">[7]</ref>, reverse attention is used for mining complementary regions and details by erasing the existing estimated target regions from sideoutput features. Inspired by <ref type="bibr" target="#b68">[69]</ref>, we present a novel group-wise operation to utilize the reverse guidance prior more effectively. As can be seen in <ref type="figure" target="#fig_0">Fig. 14 (a)</ref>, the group guidance operation contains two main steps. First, we split the candidate features {p k i ? R H/2 k ?W /2 k ?C , k = 3, 4, 5} into multiple (i.e., m i = C/g i ) groups along the channel-wise dimension, where i = 1, 2, 3 and g i denotes the group size of processed features. Then, the guidance prior r k 1 is periodically interpolated among the split features</p><formula xml:id="formula_3">p k i, j ? R H/2 k ?W /2 k ?g i , where i ? {1, 2, 3}, j ? {1, . . . , m i }, k ? {3, 4, 5}. Thus, this operation (i.e., q k i+1 = F GGO [p k i , r k i ; m i ])</formula><p>can be decoupled as two steps:</p><p>Step I:</p><formula xml:id="formula_4">{p k i,1 , . . . , p k i, j , . . . , p k i,m i } ? F S (p k i ) Step II: q k i+1 ? F C ({p k i,1 , r k 1 }, . . . , {p k i, j , r k 1 }, . . . , {p k i,m i , r k 1 }),<label>(3)</label></formula><p>Sigmoid Reverse Conv where F S and F C indicate the channel-wise split and concatenation function for the candidates. Note that F GGO :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group Guidance Operation</head><formula xml:id="formula_5">!"# # ! $ ! Guidance Channel Split ! ! ! + m ! # ! Conv ("# ! $"# ! (a) GRA (b) Reverse $"# ! Channel Concatenation !,# % $ !,</formula><formula xml:id="formula_6">p k i ? R H/2 k ?W /2 k ?C ? q k i+1 ? R H/2 k ?W /2 k ?(C+m i ) , where k ? {3, 4, 5}</formula><p>. In contrast, <ref type="bibr" target="#b6">[7]</ref> puts more emphasis on ensuring that the candidate features are directly multiplied by the priors, which may incur two issues: a) feature confusion due to the limited discriminative ability of the network, and b) the simple multiplication introduces both true and false guidance priors and is thus prone to accumulating inaccuracies. Compared to <ref type="bibr" target="#b6">[7]</ref>, our GGO can explicitly isolate the guidance prior and candidate feature before the subsequent refinement process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group-Reversal Attention (GRA).</head><p>Finally, we introduce the residual learning process, termed the GRA block, with the assistance of both the reverse guidance and group guidance operation. According to previous studies <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, multi-stage refinement can improve performance. We thus combine multiple GRA blocks (e.g., G k i , i ? {1, 2, 3}, k ? {3, 4, 5}) to progressively refine the coarse prediction via different feature pyramids. Overall, each GRA block has three residual learning processes:</p><p>i) We combine candidate features p k i and r k 1 via the group guidance operation and then use the residual stage to produce the refined features p k i+1 . This is formulated as:</p><formula xml:id="formula_7">p k i+1 = p k i + g[F GGO [p k i , r k 1 ; m i ]; W v GRA ],<label>(4)</label></formula><p>where W v denotes the convolutional layer with a 3?3 kernel followed by batch normalization layer for reducing the channel number from C + m i to C. Note that we only reverse the guidance prior in the first GRA block (i.e., when i = 1) in the default implementation. Refer to Section 5.3 for detailed discussion. ii) Then, we get a single channel residual guidance: Quantitative results on three different datasets. The best scores are highlighted in bold. Note that the ANet-SRM model (only trained on CAMO) does not have a publicly available code, thus other results are not available. ? indicates the higher the score the better. E ? denotes mean E-measure <ref type="bibr" target="#b73">[74]</ref>.</p><formula xml:id="formula_8">r k i+1 = r k 1 + g[p k i+1 ; W w GRA ],<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAMELEON [24]</head><p>CAMO-Test <ref type="bibr" target="#b24">[25]</ref> COD10K-Test (OUR) Baseline Models which is parameterized by learnable weights W w GRA . iii) Finally, we only output the refined guidance, which serves as the residual prediction. It is formulated as:</p><formula xml:id="formula_9">S ? ? E ? ? F w ? ? M ? S ? ? E ? ? F w ? ? M ? S ? ? E ? ? F w ? ? M ? FPN [</formula><formula xml:id="formula_10">C k = r k i+1 + ? (C k+1 ),<label>(6)</label></formula><p>where ? (?) is ? 2 ? when k = {3, 4} and ? 4 ? when k = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Learning Strategy</head><p>Our loss function is defined as L = L W IoU + L W BCE , where L W IoU and L W BCE represent the weighted intersection-over-union (IoU) loss and binary cross entropy (BCE) loss for the global restriction and local (pixel-level) restriction. Different from the standard IoU loss, which has been widely adopted in segmentation tasks, the weighted IoU loss increases the weights of hard pixels to highlight their importance. In addition, compared with the standard BCE loss, L W BCE pays more attention to hard pixels rather than assigning all pixels equal weights. The definitions of these losses are the same as in <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b69">[70]</ref> and their effectiveness has been validated in the field of salient object detection. Here, we adopt deep supervision for the three side-outputs (i.e., C 3 , C 4 , and C 5 ) and the global map C 6 . Each map is up-sampled (e.g., C up 3 ) to the same size as the ground-truth map G. Thus, the total loss for the proposed SINet can be formulated as:</p><formula xml:id="formula_11">L total = L(C up 6 , G) + ? i=5 i=3 L(C up i , G).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Hyperparameter Settings</head><p>SINet is implemented in PyTorch and trained with the Adam optimizer <ref type="bibr" target="#b70">[71]</ref>. During the training stage, the batch size is set to 36, and the learning rate starts at 1e-4, dividing by 10 every 50 epochs. The whole training time is only about 4 hours for 100 epochs. The running time is measured on an Intel i9-9820X CPU @3.30GHz ? 20 platform and a single NVIDIA TITAN RTX GPU. During inference, each image is resized to 352?352 and then fed into the proposed pipeline to obtain the final prediction without any postprocessing techniques. The inference speed is ?45 fps on a single GPU without I/O time. Both PyTorch <ref type="bibr" target="#b71">[72]</ref> and Jittor <ref type="bibr" target="#b72">[73]</ref> verisons of the source code will be made publicly avaliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COD BENCHMARK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Evaluation Metrics</head><p>Mean absolute error (MAE) is widely used in SOD tasks. Following Perazzi et al. <ref type="bibr" target="#b82">[83]</ref>, we also adopt the MAE (M) metric to assess the pixel-level accuracy between a predicted map and ground-truth. However, while useful for assessing the presence and amount of error, the MAE metric is not able to determine where the error occurs. Recently, Fan et al. proposed a human visual perception based E-measure (E ? ) <ref type="bibr" target="#b73">[74]</ref>, which simultaneously evaluates the pixel-level matching and image-level statistics. This metric is naturally suited for assessing the overall and localized accuracy of the concealed object detection results. Note that we report mean E ? in the experiments. Since concealed objects often contain complex shapes, COD also requires a metric that can judge structural similarity. We therefore utilize the S-measure (S ? ) <ref type="bibr" target="#b83">[84]</ref> as our structural similarity evaluation metric. Finally, recent studies <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b83">[84]</ref> have suggested that the weighted Fmeasure (F w ? ) <ref type="bibr" target="#b84">[85]</ref> can provide more reliable evaluation results than the traditional F ? . Thus, we further consider this as an alternative metric for COD. Our one-key evaluation code is also available at the project page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baseline Models</head><p>We select 12 deep learning baselines <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b81">[82]</ref> according to the following criteria: a) classical architectures, b) recently published, and c) achieve SOTA performance in a specific field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Training/Testing Protocols</head><p>For fair comparison with our previous version <ref type="bibr" target="#b0">[1]</ref>, we adopt the same training settings <ref type="bibr" target="#b0">[1]</ref> for the baselines. <ref type="bibr" target="#b3">4</ref> We evaluate the models on the whole CHAMELEON <ref type="bibr" target="#b23">[24]</ref> dataset and the test sets of CAMO and COD10K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Data Analysis</head><p>This section provides the quantitative evaluation results on CHAMELEON, CAMO, and COD10K datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on CHAMELEON.</head><p>From <ref type="table" target="#tab_4">Table 2</ref>, compared with the 12 SOTA object detection baselines and ANet-SRM, our SINet achieves the new SOTA performances across all metrics. Note that our model does not apply any auxiliary edge/boundary features (e.g., EGNet <ref type="bibr" target="#b11">[12]</ref>, PFANet <ref type="bibr" target="#b80">[81]</ref>), pre-processing techniques <ref type="bibr" target="#b85">[86]</ref>, or post-processing strategies such as <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>. <ref type="bibr" target="#b3">4</ref>. To verify the generalizability of SINet, we only use the combined training set of CAMO <ref type="bibr" target="#b24">[25]</ref> and COD10K <ref type="bibr" target="#b0">[1]</ref> without EXTRA (i.e., additional) data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3</head><p>Quantitative results on four super-classes of the COD10K dataset in terms of four widely used evaluation metrics. All methods are trained using the same dataset as in <ref type="bibr" target="#b0">[1]</ref>. ? indicates the higher the score the better, and ?: the lower the better.</p><p>Amphibian (124 images) Aquatic (474 images) Flying (714 images) Terrestrial (699 images) Baseline Models <ref type="bibr" target="#b74">[75]</ref> 0 </p><formula xml:id="formula_12">S ? ? E ? ? F w ? ? M ? S ? ? E ? ? F w ? ? M ? S ? ? E ? ? F w ? ? M ? S ? ? E ? ? F w ? ? M ? FPN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on CAMO.</head><p>We also test our model on the CAMO <ref type="bibr" target="#b24">[25]</ref> dataset, which includes various concealed objects. Based on the overall performances reported in <ref type="table" target="#tab_4">Table 2</ref>, we find that the CAMO dataset is more challenging than CHAMELEON. Again, SINet obtains the best performance, further demonstrating its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on COD10K.</head><p>With the test set (2,026 images) of our COD10K dataset, we again observe that the proposed SINet is consistently better than other competitors. This is because its specially designed search and identification modules can automatically learn rich diversified features from coarse to fine, which are crucial for overcoming challenging ambiguities in object boundaries. The results are shown in <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-subclass Performance.</head><p>In addition to the overall quantitative comparisons on our COD10K dataset, we also report the quantitative per-subclass results in the <ref type="table" target="#tab_6">Table 4</ref> to investigate the pros and cons of the models for future researchers. In <ref type="figure" target="#fig_0">Fig. 15</ref>, we additionally show the minimum, mean, and maximum Smeasure performance of each sub-class over all baselines. The easiest sub-class is "Grouse", while the most difficult is the "LeafySeaDragon", from the aquatic and terrestrial categories, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results.</head><p>We present more detection results of our conference version model (SINet cvpr) for various challenging concealed objects, such as spider, moth, sea horse, and toad, in the supplementary materials. As shown in <ref type="figure" target="#fig_0">Fig. 16</ref>, SINet further improves the visual results compared to SINet cvpr in terms of different lighting (1 st row), appearance changes (2 nd row), and indefinable boundaries (3 rd to 5 th ). PFANet <ref type="bibr" target="#b80">[81]</ref> is able to locate the concealed objects, but the outputs are always inaccurate. By further using reverse attention module, PraNet <ref type="bibr" target="#b6">[7]</ref> achieves a relatively more accurate location than PFANet in the first case. Nevertheless, it still misses the fine details of objects, especially for the fish in the 2 nd and 3 rd rows. For all these challenging cases, SINet is able to infer the real concealed object with fine details, demonstrating the robustness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GOS vs. SOD Baselines.</head><p>One noteworthy finding is that, among the top-3 models, the GOS model (i.e., FPN <ref type="bibr" target="#b74">[75]</ref>) performs worse than the SOD competitors, CPD <ref type="bibr" target="#b62">[63]</ref>, EGNet <ref type="bibr" target="#b11">[12]</ref>, suggesting that the SOD framework may be better suited for extension to COD tasks. Compared with both the GOS <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b81">[82]</ref> and the SOD <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b80">[81]</ref> models, SINet significantly decrease the training time (e.g., SINet: 4 hours vs. EGNet: 48 hours) and achieve the SOTA performance on all datasets, showing that they are promising solutions for the COD problem. Due to the limited space, fully comparing them with existing SOTA SOD models is beyond the scope of this paper. Note that our main goal is to provide more general observations for future work. More recent SOD models can be found in our project page.  <ref type="bibr" target="#b0">[1]</ref>, (e) PraNet <ref type="bibr" target="#b6">[7]</ref>, and (f) PFANet <ref type="bibr" target="#b80">[81]</ref>.  <ref type="figure" target="#fig_6">? [84]</ref>) scores for cross-dataset generalization. SINet cvpr is trained on one dataset (rows) and tested on all datasets (columns). "Self": training and testing on the same dataset (diagonal). "Mean others": average score on all except self.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trained on:</head><p>Tested on: CAMO <ref type="bibr" target="#b24">[25]</ref> COD10K (OUR) Self Mean others Drop? CAMO <ref type="bibr" target="#b24">[25]</ref> 0.803 0.702 0.803 0.702 12.6% COD10K (OUR) 0.742 0.700 0.700 0.742 -6.0% Mean others 0.742 0.702</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization.</head><p>The generalizability and difficulty of datasets play a crucial role in both training and assessing different algorithms <ref type="bibr" target="#b34">[35]</ref>. Hence, we study these aspects for existing COD datasets, using the cross-dataset analysis method <ref type="bibr" target="#b89">[90]</ref>, i.e., training a model on one dataset, and testing it on others. We select two datasets, namely CAMO <ref type="bibr" target="#b24">[25]</ref>, and our COD10K. Following <ref type="bibr" target="#b34">[35]</ref>, for each dataset, we randomly select 800 images as the training set and 200 images as the testing set. For fair comparison, we train SINet cvpr on each dataset until the loss is stable. <ref type="table" target="#tab_7">Table 5</ref> provides the S-measure results for the cross-dataset generalization. Each row lists a model that is trained on one dataset and tested on all others, indicating the generalizability of the dataset used for training. Each column shows the performance of one model tested on a specific dataset and trained on all others, indicating the difficulty of the testing dataset. Please note that the training/testing settings are different from those used in <ref type="table" target="#tab_4">Table 2</ref>, and thus the performances are not comparable. As expected, we find that our COD10K has better generalization ability than the CAMO (e.g., the last column 'Drop?: -6.0%'). This is because our dataset contains a variety of challenging concealed objects (Section 3). We can thus see that our COD10K dataset contains more challenging scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>We now provide a detailed analysis of the proposed SINet on CHAMELEON, CAMO, and COD10K. We verify the effectiveness by decoupling various sub-components, including the NCD, TEM, and GRA, as summarized in <ref type="table" target="#tab_8">Table 6</ref>. Note that we maintain the same hyperparameters mentioned in Section 4.4 during the re-training process for each ablation variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of NCD.</head><p>We explore the influence of the decoder in the search phase of our SINet. To verify its necessity, we retrain our network without the NCD (No.#1) and find that, compared with #OUR (last row in <ref type="table" target="#tab_8">Table 6</ref>), the NCD is attributed to boosting the performance on CAMO, increasing the mean E ? score from 0.869 to 0.882. Further, we replace the NCD with the partial decoder <ref type="bibr" target="#b62">[63]</ref> (i.e., PD of No.#2) to test the performance of this scheme. Comparing No.#2 with #OUR, our design can enhance the performance slightly, increasing it by 1.7% in terms of F w ? on the CHAMELEON. As shown in <ref type="figure" target="#fig_0">Fig. 17</ref>, we present a novel feature aggregation strategy before the modified UNet-like decoder (removing the bottom-two high-resolution layers), termed the NCD, with neighbor connections between adjacent layers. This design is motivated by the fact that the high-level features are superior to semantic strength and location accuracy, but introduce noise and blurred edges for the target object.</p><p>Instead of broadcasting features from densely connected layers with a short connection <ref type="bibr" target="#b31">[32]</ref> or a partial decoder with a skip connection <ref type="bibr" target="#b62">[63]</ref>, our NCD exploits the semantic context through a neighbor connection, providing a simple but effective way to reduce inconsistency between different features. Aggregating all features by a short connection <ref type="bibr" target="#b31">[32]</ref> increases the parameters. This is one of the major differences between DSS <ref type="figure" target="#fig_0">(Fig. 17 a)</ref> and NCD. Compared to CPD <ref type="bibr" target="#b62">[63]</ref>  <ref type="figure" target="#fig_0">(Fig. 17 b)</ref>, which ignores   feature transparency between f 5 and f 4 , NCD is more efficient at broadcasting the features step by step.</p><formula xml:id="formula_13">? E ? ? F w ? ? M ? S ? ? E ? ? F w ? ? M ? S ? ? E ? ? F w ? ? M ? #1 {1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of TEM.</head><p>We provide two different variants: (a) without TEM (No.#3), and (b) with symmetric convolutional layers <ref type="bibr" target="#b63">[64]</ref> (No.#4). Comparing with No.#3, we find that our TEM with asymmetric convolutional layers (No.#OUR) is necessary for increasing the performance on the CAMO dataset. Besides, replacing the standard symmetric convolutional layer (No.#4) with an asymmetric convolutional layer (No.#OUR) has little impact on the learning capability of the network, while further increasing the mean E ? from 0.866 to 0.882 on the CAMO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of GRA.</head><p>Reverse Guidance. As shown in the 'Reverse' column of <ref type="table" target="#tab_8">Table 6</ref>, {*,*,*} indicates whether the guidance is reversed (see <ref type="figure" target="#fig_0">Fig. 14 (b)</ref>) before each GRA block G k i . For instance, {1,0,0} means that we only reverse the guidance in the first block (i.e., r k 1 ) and the remaining two blocks (i.e., r k 2 and r k 3 ) do not have a reverse operation. We investigate the contribution of the reverse guidance in the GRA, including three alternatives: (a) without any reverse, i.e., {0,0,0} of No.#5, (b) reversing the first two guidances r k i , i ? {1, 2}, i.e., {1,1,0} of No.#6, and (c) reversing all the guidances r k i , i ? {1, 2, 3}, i.e., {1,1,1} of No.#7. Compared to the default implementation of SINet (i.e., {1,0,0} of No.#OUR), we find that only reversing the first guidance may help the network to mine diversified representations from two perspectives (i.e., attention and reverse attention regions), while introducing reverse guidance several times in the intermediate process may cause confusion during the learning procedure, especially for setting #6 on the CHAMELEON and COD10K datasets.</p><p>Group Size of GGO. As shown in the 'Group Size' column of <ref type="table" target="#tab_8">Table 6</ref>, { * ; * ; * } indicates the number of feature slices (i.e., group size g i ) from the GGO of the first block G k 1 to last block G k 3 . For example, {32; 8; 1} indicates that we split the candidate feature p k i , i ? {1, 2, 3} into 32, 8, and 1 group sizes at each GRA block G k i , i ? {1, 2, 3}, respectively. Here, we discuss two ways of selecting the group size, i.e., the uniform strategy (i.e., {1; 1; 1} of #8, {8; 8; 8} of #9, {32; 32; 32} of #10) and progressive strategy (i.e., {1; 8; 32} of #11 and {32; 8; 1} of #OUR). We observe that our design based on the progressive strategy can effectively maintain the generalizability of the network, providing more satisfactory performance compared with other variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DOWNSTREAM APPLICATIONS</head><p>Concealed object detection systems have various downstream applications in fields such as medicine, art, and agriculture. Here, we envision some potential uses due to the common feature of these applications where the target objects share similar appearance with the background. Under such circumstances, COD models are very suitable to act as a core component of these applications to mine camouflaged objects. Note that these applications are only toy examples to spark interesting ideas for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Application I: Medicine</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Polyp Segmentation</head><p>As we all know, early diagnosis through medical imaging plays a key role in the treatment of diseases. However, the early disease area/lesions usually have a high degree of homogeneity with the surrounding tissues. As a result, it is difficult for doctors to identity the lesion area in the early stage from a medical image. One typical example is the early colonoscopy to segment polyps, which has contributed to roughly 30% decline in the incidence of colorectal cancer <ref type="bibr" target="#b6">[7]</ref>. Similar to concealed object detection, polyp segmentation (see <ref type="figure" target="#fig_0">Fig. 18</ref>) also faces several challenges, such as variation in appearance and blurred boundaries. The recent state-of-the-art polyp segmentation model, PraNet <ref type="bibr" target="#b6">[7]</ref>, has shown promising performance in both polyp segmentation (Top-1) and concealed object segmentation (Top-2). From this point of view, embedding our SINet into this application could potentially achieve more robust results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Lung Infection Segmentation</head><p>Another concealed object detection example is the lung infection segmentation task in the medical field. Recently, COVID-19 has been of particular concern, and resulted in a global pandemic. An  AI system equipped with a COVID-19 lung infection segmentation model would be helpful in the early screening of COVID-19. More details on this application can be found in the recent segmentation model <ref type="bibr" target="#b7">[8]</ref> and survey paper <ref type="bibr" target="#b91">[92]</ref>. We believe retrain our SINet model using COVID-19 lung infection segmentation datasets will be another interesting potential application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Application II: Manufacturing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Surface Defect Detection</head><p>In industrial manufacturing, products (e.g., wood, textile, and magnetic tile) of poor quality will inevitably lead to adverse effects on the economy. As can be seen from <ref type="figure" target="#fig_1">Fig. 20</ref>, the surface defects are challenging, with different factors including low contrast, ambiguous boundaries and so on. Since traditional surface defect detection systems mainly rely on humans, major issues are highly subjective and time-consuming to identify. Thus, designing an automatic recognition system based on AI is essential to increase productivity. We are actively constructing such a data set to advance related research. Some related papers can be found at: https://github.com/Charmve/Surface-Defect-Detection/ tree/master/Papers. (a) (b) (c) <ref type="figure" target="#fig_1">Fig. 22</ref>. Fruit maturity detection. Compared with the traditional manual inspection (c) of fruits, such as Persea Americana (a) and Myrica Rubra (b) for maturity, an AI-based maturity monitoring system will greatly improve production efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Application III: Agriculture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Pest Detection</head><p>Since early 2020, plagues of desert locusts have invaded the world, from Africa to South Asia. Large numbers of locusts gnaw on fields and completely destroy agricultural products, causing serious financial losses and famine due to food shortages. As shown in <ref type="figure" target="#fig_0">Fig. 21</ref>, introducing AI-based techniques to provide scientific monitoring is feasible for achieving sustainable regulation/containment by governments. Collecting relevant insect data for COD models requires rich biological knowledge, which is also a difficulty faced in this application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Fruit Maturity Detection</head><p>In the early stages of ripening, many fruits appear similar to green leaves, making it difficult for farmers to monitor production. We present two types of fruits, i.e., Persea Americana and Myrica Rubra, in <ref type="figure" target="#fig_1">Fig. 22</ref>. These fruits share similar characteristics to concealed objects, so it is possible to utilize a COD algorithm to identify them and improve the monitoring efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Application IV: Art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Recreational Art</head><p>Background warping to concealed salient objects is a fascinating technique in the SIGGRAPH community. <ref type="figure" target="#fig_1">Fig. 23</ref> presents some examples generated by Chu et al. in <ref type="bibr" target="#b8">[9]</ref>. We argue that this technique will provide more training data for existing data-hungry deep learning models, and thus it is of value to explore the underlying mechanism behind the feature search and conjunction search theory described by Treisman and Wolfe <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref>.</p><p>(a) (b) (c) <ref type="figure" target="#fig_1">Fig. 24</ref>. Converting concealed objects to salient objects. Source images from <ref type="bibr" target="#b24">[25]</ref>. One interesting application is to identify (b) a specific concealed object (a) and then convert it to a salient object (c).</p><p>(a) (b) (c) <ref type="figure" target="#fig_1">Fig. 25</ref>. Transparent stuff/objects detection. In our daily lives, we humans see, touch, or interact with various transparent stuff such as windows (a), glass doors (b), and glass walls (c). Second rows are corresponding ground-truths. It is essential to teach AI robots to identify transparent stuff/objects to avoid unseen obstacles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">From Concealed to Salient Objects</head><p>Concealed object detection and salient object detection are two opposite tasks, making it convenient for us to design a multitask learning framework that can simultaneously increase the robustness of the network. As shown in <ref type="figure" target="#fig_1">Fig. 24</ref>, there exist two reverse objects (a) and (c). An interesting application is to provide a scroll bar to allow users to customize the degree of salient objects from the concealed objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Application V: Daily Life</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Transparent Stuff/Objects Detection</head><p>Transparent objects, such as glass products, are commonplace in our daily life. These objects/things, including doors and walls, inherit the appearance of their background, making them unnoticeable, as illustrated in <ref type="figure" target="#fig_1">Fig. 25</ref>. As a sub-task of concealed object detection, transparent object detection <ref type="bibr" target="#b46">[47]</ref> and transparent object tracking <ref type="bibr" target="#b94">[95]</ref> have shown promise. <ref type="figure" target="#fig_1">Fig. 26</ref> shows an example of search results from Google. From the results <ref type="figure" target="#fig_1">(Fig. 26 a)</ref>, we notice that the search engine cannot detect the concealed butterfly, and thus only provides images with similar backgrounds. Interestingly, when the search engine is equipped with a concealed detection system (here, we just simply change the keyword), it can identify the concealed object and then feedback several butterfly images ( <ref type="figure" target="#fig_1">Fig. 26 b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Search Engines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">POTENTIAL RESEARCH DIRECTIONS</head><p>Despite the recent 10 years of progress in the field of concealed object detection, the leading algorithms in the deep learning era remain limited compared to those for generic object detection <ref type="bibr" target="#b95">[96]</ref> and cannot yet effectively solve real-world challenges as shown in our COD10K benchmark (Top-1: F w ? &lt; 0.7). We highlight some long-standing challenges, as follows:</p><p>? Concealed object detection under limited conditions: few/zero-shot learning, weakly supervised learning, unsupervised learning, self-supervised learning, limited training data, unseen object class, etc. ? Concealed object detection combined with other modalities:</p><p>Text, Audio, Video, RGB-D, RGB-T, 3D, etc. ? New directions based on the rich annotations provided in the COD10K, such as concealed instance segmentation, concealed edge detection, concealed object proposal, concealed object ranking, among others. Based on the above-mentioned challenges, there are a number of foreseeable directions for future research:</p><p>(1) Weakly/Semi-Supervised Detection: Existing deep-based methods extract the features in a fully supervised manner from images annotated with object-level labels. However, the pixel-level annotations are usually manually marked by LabelMe or Adobe Photoshop tools with intensive professional interaction. Thus, it is essential to utilize weakly/semi (partially) annotated data for training in order to avoid heavy annotation costs.</p><p>(2) Self-Supervised Detection: Recent efforts to learn representations (e.g., image, audio, and video) using self-supervised learning <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref> have achieved world-renowned achievements, attracting much attention. Thus, it is natural to setup a selfsupervised learning benchmark for the concealed object detection task.</p><p>(3) Concealed Object Detection in Other Modalities: Existing concealed data is only based on static images or dynamic videos <ref type="bibr" target="#b98">[99]</ref>. However, concealed object detection in other modalities can be closely related in domains such as pest monitoring in the dark night, robotics, and artist design. Similar to in RGB-D SOD <ref type="bibr" target="#b52">[53]</ref>, RGB-T SOD <ref type="bibr" target="#b99">[100]</ref>, CoSOD <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b101">[102]</ref>, and VSOD <ref type="bibr" target="#b102">[103]</ref>, these modalities can be audio, thermal, group image, or depth data, raising new challenges under specific scenes.</p><p>(4) Concealed Object Classification: Generic object classification is a fundamental task in computer vision. Thus concealed object classification will also likely gain attention in the future. By utilizing the class and sub-class labels provided in COD10K, one could build a large scale and fine-grain classification task.</p><p>(5) Concealed Object Proposal and Tracking: In this paper, the concealed object detection is actually a segmentation task. It is different from traditional object detection, which generates a proposal or bounding boxes as the prediction. As such, concealed object proposal and tracking is a new and interesting direction <ref type="bibr" target="#b103">[104]</ref> for future work.</p><p>(6) Concealed Object Ranking: Currently, most concealed object detection algorithms are built upon binary ground-truths to generate the masks of concealed objects, with only limited works analyzing the rank of concealed objects <ref type="bibr" target="#b38">[39]</ref>. However, understanding the level of concealment could help to better explore the mechanism behind the models, providing deeper insights into them. We refer readers to <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b104">[105]</ref> for some inspiring ideas.</p><p>(7) Concealed Instance Segmentation: As described in <ref type="bibr" target="#b19">[20]</ref>, instance segmentation is more crucial than object-level segmentation for practical applications. For example, we can push the research on camouflaged object segmentation into camouflaged instance segmentation.</p><p>(8) Universal Network for Multiple Tasks: As studied by Zamir et al. in Taskonomy <ref type="bibr" target="#b20">[21]</ref>, different visual tasks have strong relationships. Thus, their supervision can be reused in one universal system without piling up complexity. It is natural to consider devising a universal network to simultaneously localize, segment and rank concealed objects.</p><p>(9) Neural Architecture Search: Both traditional algorithms and deep learning-based models for concealed object detection require human experts with strong prior knowledge or skilled expertise. Sometimes, the hand-crafted features and architectures designed by algorithm engineers may not optimal. Therefore, neural architecture search techniques, such as the popular automated machine learning <ref type="bibr" target="#b105">[106]</ref>, offer a potential direction.</p><p>(10) Transferring Salient Objects to Concealed Objects: Due to space limitations, we only evaluated typical salient object detection models in our benchmark section. There are several valuable problems that deserve further studying, however, such as transferring salient objects to concealed objects to increase the training data, and introducing a generative adversarial mechanism between the SOD and COD tasks to increase the feature extraction ability of the network.</p><p>The ten new research directions listed for concealed object remain far from being solved. However, there are many famous works that can be referred to, providing us a solid basis for studying the object detection task from a concealed perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We have presented the first comprehensive study on object detection from a concealed vision perspective. Specifically, we have provided the new challenging and densely annotated COD10K dataset, conducted a large-scale benchmark, developed a simple but efficient end-to-end search and identification framework (i.e., SINet), and highlighted several potential applications. Compared with existing cutting-edge baselines, our SINet is competitive and generates more visually favorable results. The above contributions offer the community an opportunity to design new models for the COD task. In the future, we plan to extend our COD10K dataset to provide inputs of various forms, such as multi-view images (e.g., RGB-D SOD <ref type="bibr" target="#b106">[107]</ref>, <ref type="bibr" target="#b107">[108]</ref>), textual descriptions, video (e.g., VSOD <ref type="bibr" target="#b102">[103]</ref>), among others. We also plan to automatically search the optimal receptive fields <ref type="bibr" target="#b108">[109]</ref> and employ improved feature representations <ref type="bibr" target="#b109">[110]</ref> for better model performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of background matching camouflage (BMC).There are seven and six birds for the left and right image, respectively. Answers in color are shown inFig. 27.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 .</head><label>2</label><figDesc>(a) Image (b) Generic object (c) Salient object (d) Concealed object Fig. Task relationship.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>0Fig. 5 .</head><label>5</label><figDesc>Object and instance distributions of each concealed category in the COD10K. COD10K consists of 5,066 concealed images from 69 categories. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Taxonomic system. We illustrate the histogram distribution for the 69 concealed categories in our COD10K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>MO</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Attribute distribution. Top-left: Co-attributes distribution over COD10K. The number in each grid indicates the total number of images. Top-right: Multi-dependencies among these attributes. A larger arc length indicates a higher probability of one attribute correlating to another. Bottom: attribute descriptions. Examples could be found in the first row of Fig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Image resolution (unit for the axis: pixel) distribution of COD datasets. From left to right: CHAMELEON<ref type="bibr" target="#b23">[24]</ref>, CAMO-COCO<ref type="bibr" target="#b24">[25]</ref> and COD10K datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Comparison between the proposed COD10K and existing datasets. COD10K has smaller objects (top-left), contains more difficult conceale (top-right), and suffers from less center bias (bottom-left/right).PassReject High-quality annotation. The annotation quality is close to the existing matting-level<ref type="bibr" target="#b22">[23]</ref> annotation. ian, other) and 69 sub-classes (e.g., bat-fish, lion, bat, frog, etc). Examples of the word cloud and object/instance number for various categories are shown inFig. 5 &amp; Fig. 11, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Diverse types of concealed objects in our COD10K. For instance, concealed human in art (1 st column), and concealed animals (2 nd column) in our daily life.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Pipeline of our SINet framework. It consists of three main components: the texture enhanced module (TEM), neighbor connection decoder (NCD), and group-reversal attention (GRA). The TEM is introduced to mimic the textural structure of receptive fields in the human visual system. The NCD is responsible for locating the candidates with the assistance of the TEM. The GRA blocks reproduce the identification stages of animal predation. Note that f k = p k 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Component details. Details on the group-reversal attention (b) block G k i in the identification phase, where i denotes the number of GRAs in the k-th feature pyramids. Note that m i = C/g i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>.745 0.776 0.497 0.065 0.684 0.732 0.432 0.103 0.726 0.766 0.440 0.061 0.601 0.656 0.353 0.109 MaskRCNN [76] 0.665 0.785 0.487 0.081 0.560 0.721 0.344 0.123 0.644 0.767 0.449 0.063 0.611 0.630 0.380 0.075 PSPNet [77] 0.736 0.774 0.463 0.072 0.659 0.712 0.396 0.111 0.700 0.743 0.394 0.067 0.669 0.718 0.332 0.071 UNet++ [78] 0.677 0.745 0.434 0.079 0.599 0.673 0.347 0.121 0.659 0.727 0.397 0.068 0.608 0.749 0.288 0.070 PiCANet [79] 0.686 0.702 0.405 0.079 0.616 0.631 0.335 0.115 0.663 0.676 0.347 0.069 0.658 0.708 0.273 0.074 MSRCNN [80] 0.722 0.786 0.555 0.055 0.614 0.686 0.398 0.107 0.675 0.744 0.466 0.058 0.594 0.661 0.361 0.081 PFANet [81] 0.693 0.677 0.358 0.110 0.629 0.626 0.319 0.155 0.658 0.648 0.299 0.102 0.611 0.603 0.237 0.111 CPD [63] 0.794 0.839 0.587 0.051 0.739 0.792 0.529 0.082 0.777 0.827 0.544 0.046 0.714 0.771 0.445 0.058 HTC [82] 0.606 0.598 0.331 0.088 0.507 0.495 0.183 0.129 0.582 0.559 0.274 0.070 0.530 0.485 0.170 0.078 EGNet [12] 0.785 0.854 0.606 0.047 0.725 0.793 0.528 0.080 0.766 0.826 0.543 0.044 0.700 0.775 0.445 0.053 PraNet [7] 0.842 0.905 0.717 0.035 0.781 0.883 0.696 0.065 0.819 0.888 0.669 0.033 0.756 0.835 0.565 0.046 SINet cvpr [1] 0.827 0.866 0.654 0.042 0.758 0.803 0.570 0.073 0.798 0.828 0.580 0.040 0.743 0.778 0.491 0.050 SINet (OUR) 0.858 0.916 0.756 0.030 0.811 0.883 0.696 0.051 0.839 0.908 0.713 0.027 0.787 0.866 0.623 0.039 Per-subclass performance. Sub-classes are sorted by difficulty, determined by the mean S ? [84] across 12 baselines. We also provide the minimum (bottom line) and maximum (top line) S ? for each sub-class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Comparison of various types of inter-layer feature aggregation strategies with a short connection. (a) DSS [32] introduce the densely connected short connection in a top-down manner. (b) CPD [63] constructs a partial decoder by discarding larger resolution features of shallower layers for memory and speed enhancement. (c) Our neighbor connection decoder only propagates between neighboring layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18 .Fig. 19 .</head><label>1819</label><figDesc>Polyp segmentation. (a) &amp; (c) are input polyp images. (b) &amp; (d) are corresponding ground-truths. Lung infection segmentation. The first row presents COVID-19 lung infection CT scans, while the second row shows their groundtruths labeled by doctors. From (a) to (d), COVID-19 patients from mild to severe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 20 .</head><label>20</label><figDesc>Surface defect detection. The defect types are textile (a), stone (b), magnetic tile (c), and wood (d), respectively. The second row presented their corresponding ground truths. Source images are derived from<ref type="bibr" target="#b90">[91]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 21 .</head><label>21</label><figDesc>Pest detection. For pest detection applications, the system can generate a bounding box (b) for each locally screened image (a) or provide statistics (pest counting) for locust plague density monitoring in the whole environment (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 23 .</head><label>23</label><figDesc>Recreational art. Some animals are embedded into the background by algorithms. Source images from Chu et al. [9] and all rights reserved by 2010 John Van Straalen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 26 .</head><label>26</label><figDesc>Search engines. Internet search engine application equipped without (a)/with (b) a concealed detection system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Summary of COD datasets, showing that COD10K offers much richer annotations and benefits many tasks. Att</head><label>1</label><figDesc>Att. BBox. Ml. Ins. Cate. Obj. #Training #Testing Loc. Det. Cls. WS. InSeg.</figDesc><table><row><cell></cell><cell>Statistics</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Annotations</cell><cell></cell><cell>Data Split</cell><cell></cell><cell>Tasks</cell><cell></cell></row><row><cell cols="3">Dataset Year #Img. #Cls. CHAMELEON [24] 2018 76 N/A</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0</cell><cell>76</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>CAMO-COCO [25] 2019</cell><cell>2,500</cell><cell>8</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>1,250</cell><cell>1,250</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">COD10K (OUR) 2020 10,000</cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6,000</cell><cell>4,000</cell><cell></cell><cell></cell></row></table><note>.: Attribute label. BBox.: Bounding box label. Ml.: Alpha matting-level annotation [23]. Ins.: Instance-level label. Cate.: Category label. Obj.: Object-level label. Loc.: Location. Det.: Detection. Cls.: Classification. WS.: Weak Supervision. InSeg. Instance Segmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 Results of S ? for each sub-class in our COD10K dataset.</head><label>4</label><figDesc>The best performing method of each category is highlighted in bold.HTC PFANet MRCNN BASNet UNet++ PiCANet MSRCNN PoolNet PSPNet FPN EGNet CPD PraNet SINet</figDesc><table><row><cell>Sub-class</cell><cell>[82]</cell><cell>[81]</cell><cell>[76]</cell><cell>[59]</cell><cell>[78]</cell><cell>[79]</cell><cell>[80]</cell><cell>[89]</cell><cell>[77]</cell><cell>[75]</cell><cell>[12]</cell><cell>[63]</cell><cell>[7] OUR</cell></row><row><cell>Amphibian-Frog</cell><cell cols="2">0.600 0.678</cell><cell>0.664</cell><cell>0.692</cell><cell>0.656</cell><cell>0.687</cell><cell>0.692</cell><cell>0.732</cell><cell cols="5">0.697 0.731 0.745 0.752 0.823 0.837</cell></row><row><cell>Amphibian-Toad</cell><cell cols="2">0.609 0.697</cell><cell>0.666</cell><cell>0.717</cell><cell>0.689</cell><cell>0.714</cell><cell>0.739</cell><cell>0.786</cell><cell cols="5">0.757 0.752 0.812 0.817 0.853 0.870</cell></row><row><cell>Aquatic-BatFish</cell><cell cols="2">0.546 0.746</cell><cell>0.634</cell><cell>0.749</cell><cell>0.626</cell><cell>0.624</cell><cell>0.637</cell><cell>0.741</cell><cell cols="5">0.724 0.764 0.707 0.761 0.879 0.873</cell></row><row><cell>Aquatic-ClownFish</cell><cell cols="2">0.547 0.519</cell><cell>0.509</cell><cell>0.464</cell><cell>0.548</cell><cell>0.636</cell><cell>0.571</cell><cell>0.626</cell><cell cols="5">0.531 0.730 0.632 0.646 0.707 0.787</cell></row><row><cell>Aquatic-Crab</cell><cell cols="2">0.543 0.661</cell><cell>0.691</cell><cell>0.643</cell><cell>0.630</cell><cell>0.675</cell><cell>0.634</cell><cell>0.727</cell><cell cols="5">0.680 0.724 0.760 0.753 0.792 0.815</cell></row><row><cell>Aquatic-Crocodile</cell><cell cols="2">0.546 0.631</cell><cell>0.660</cell><cell>0.599</cell><cell>0.602</cell><cell>0.669</cell><cell>0.646</cell><cell>0.743</cell><cell cols="5">0.636 0.687 0.772 0.761 0.806 0.825</cell></row><row><cell>Aquatic-CrocodileFish</cell><cell cols="2">0.436 0.572</cell><cell>0.558</cell><cell>0.475</cell><cell>0.559</cell><cell>0.373</cell><cell>0.479</cell><cell>0.693</cell><cell cols="5">0.624 0.515 0.709 0.690 0.669 0.746</cell></row><row><cell>Aquatic-Fish</cell><cell cols="2">0.488 0.622</cell><cell>0.597</cell><cell>0.625</cell><cell>0.574</cell><cell>0.619</cell><cell>0.680</cell><cell>0.703</cell><cell cols="5">0.650 0.699 0.717 0.778 0.784 0.834</cell></row><row><cell>Aquatic-Flounder</cell><cell cols="2">0.403 0.663</cell><cell>0.539</cell><cell>0.633</cell><cell>0.569</cell><cell>0.704</cell><cell>0.570</cell><cell>0.782</cell><cell cols="5">0.646 0.695 0.798 0.774 0.835 0.889</cell></row><row><cell>Aquatic-FrogFish</cell><cell cols="2">0.595 0.768</cell><cell>0.650</cell><cell>0.736</cell><cell>0.671</cell><cell>0.670</cell><cell>0.653</cell><cell>0.719</cell><cell cols="5">0.695 0.807 0.806 0.730 0.781 0.894</cell></row><row><cell>Aquatic-GhostPipefish</cell><cell cols="2">0.522 0.690</cell><cell>0.556</cell><cell>0.679</cell><cell>0.651</cell><cell>0.675</cell><cell>0.636</cell><cell>0.717</cell><cell cols="5">0.709 0.744 0.759 0.763 0.784 0.817</cell></row><row><cell cols="3">Aquatic-LeafySeaDragon 0.460 0.576</cell><cell>0.442</cell><cell>0.481</cell><cell>0.523</cell><cell>0.499</cell><cell>0.500</cell><cell>0.547</cell><cell cols="5">0.563 0.507 0.522 0.534 0.587 0.670</cell></row><row><cell>Aquatic-Octopus</cell><cell cols="2">0.505 0.708</cell><cell>0.598</cell><cell>0.644</cell><cell>0.663</cell><cell>0.673</cell><cell>0.720</cell><cell>0.779</cell><cell cols="5">0.723 0.760 0.810 0.812 0.896 0.887</cell></row><row><cell>Aquatic-Pagurian</cell><cell cols="2">0.427 0.578</cell><cell>0.477</cell><cell>0.607</cell><cell>0.553</cell><cell>0.624</cell><cell>0.583</cell><cell>0.657</cell><cell cols="5">0.608 0.638 0.683 0.611 0.615 0.698</cell></row><row><cell>Aquatic-Pipefish</cell><cell cols="2">0.510 0.553</cell><cell>0.531</cell><cell>0.557</cell><cell>0.550</cell><cell>0.612</cell><cell>0.566</cell><cell>0.625</cell><cell cols="5">0.642 0.632 0.681 0.704 0.769 0.781</cell></row><row><cell>Aquatic-ScorpionFish</cell><cell cols="2">0.459 0.697</cell><cell>0.482</cell><cell>0.686</cell><cell>0.630</cell><cell>0.605</cell><cell>0.600</cell><cell>0.729</cell><cell cols="5">0.649 0.668 0.730 0.746 0.766 0.808</cell></row><row><cell>Aquatic-SeaHorse</cell><cell cols="2">0.566 0.656</cell><cell>0.581</cell><cell>0.664</cell><cell>0.663</cell><cell>0.623</cell><cell>0.657</cell><cell>0.698</cell><cell cols="5">0.687 0.715 0.750 0.765 0.810 0.823</cell></row><row><cell>Aquatic-Shrimp</cell><cell cols="2">0.500 0.574</cell><cell>0.520</cell><cell>0.631</cell><cell>0.586</cell><cell>0.574</cell><cell>0.546</cell><cell>0.605</cell><cell cols="5">0.591 0.667 0.647 0.669 0.727 0.735</cell></row><row><cell>Aquatic-Slug</cell><cell cols="2">0.493 0.581</cell><cell>0.368</cell><cell>0.492</cell><cell>0.533</cell><cell>0.460</cell><cell>0.661</cell><cell>0.732</cell><cell cols="5">0.547 0.664 0.777 0.774 0.701 0.729</cell></row><row><cell>Aquatic-StarFish</cell><cell cols="2">0.568 0.641</cell><cell>0.617</cell><cell>0.611</cell><cell>0.657</cell><cell>0.638</cell><cell>0.580</cell><cell>0.733</cell><cell cols="5">0.722 0.756 0.811 0.787 0.779 0.890</cell></row><row><cell>Aquatic-Stingaree</cell><cell cols="2">0.519 0.721</cell><cell>0.670</cell><cell>0.618</cell><cell>0.571</cell><cell>0.569</cell><cell>0.709</cell><cell>0.733</cell><cell cols="5">0.616 0.670 0.741 0.754 0.704 0.815</cell></row><row><cell>Aquatic-Turtle</cell><cell cols="2">0.364 0.686</cell><cell>0.594</cell><cell>0.658</cell><cell>0.565</cell><cell>0.734</cell><cell>0.762</cell><cell>0.757</cell><cell cols="5">0.664 0.745 0.752 0.786 0.773 0.760</cell></row><row><cell>Flying-Bat</cell><cell cols="2">0.589 0.652</cell><cell>0.611</cell><cell>0.623</cell><cell>0.557</cell><cell>0.638</cell><cell>0.679</cell><cell>0.725</cell><cell cols="5">0.657 0.714 0.765 0.784 0.817 0.847</cell></row><row><cell>Flying-Bee</cell><cell cols="2">0.578 0.579</cell><cell>0.628</cell><cell>0.547</cell><cell>0.588</cell><cell>0.616</cell><cell>0.679</cell><cell>0.670</cell><cell cols="5">0.655 0.665 0.737 0.709 0.763 0.777</cell></row><row><cell>Flying-Beetle</cell><cell cols="2">0.699 0.741</cell><cell>0.693</cell><cell>0.810</cell><cell>0.829</cell><cell>0.780</cell><cell>0.796</cell><cell>0.860</cell><cell cols="5">0.808 0.848 0.830 0.887 0.890 0.903</cell></row><row><cell>Flying-Bird</cell><cell cols="2">0.591 0.628</cell><cell>0.680</cell><cell>0.627</cell><cell>0.643</cell><cell>0.674</cell><cell>0.681</cell><cell>0.735</cell><cell cols="5">0.696 0.708 0.763 0.785 0.822 0.835</cell></row><row><cell>Flying-Bittern</cell><cell cols="2">0.639 0.621</cell><cell>0.703</cell><cell>0.650</cell><cell>0.673</cell><cell>0.741</cell><cell>0.704</cell><cell>0.785</cell><cell cols="5">0.701 0.751 0.802 0.838 0.827 0.849</cell></row><row><cell>Flying-Butterfly</cell><cell cols="2">0.653 0.692</cell><cell>0.697</cell><cell>0.700</cell><cell>0.725</cell><cell>0.714</cell><cell>0.762</cell><cell>0.777</cell><cell cols="5">0.736 0.758 0.816 0.818 0.871 0.883</cell></row><row><cell>Flying-Cicada</cell><cell cols="2">0.640 0.682</cell><cell>0.620</cell><cell>0.729</cell><cell>0.675</cell><cell>0.691</cell><cell>0.708</cell><cell>0.781</cell><cell cols="5">0.744 0.733 0.820 0.812 0.845 0.883</cell></row><row><cell>Flying-Dragonfly</cell><cell cols="2">0.472 0.679</cell><cell>0.624</cell><cell>0.712</cell><cell>0.670</cell><cell>0.694</cell><cell>0.682</cell><cell>0.695</cell><cell cols="5">0.681 0.707 0.761 0.779 0.779 0.837</cell></row><row><cell>Flying-Frogmouth</cell><cell cols="2">0.684 0.766</cell><cell>0.648</cell><cell>0.828</cell><cell>0.813</cell><cell>0.722</cell><cell>0.773</cell><cell>0.883</cell><cell cols="5">0.741 0.795 0.901 0.928 0.927 0.941</cell></row><row><cell>Flying-Grasshopper</cell><cell cols="2">0.563 0.671</cell><cell>0.651</cell><cell>0.689</cell><cell>0.656</cell><cell>0.692</cell><cell>0.666</cell><cell>0.734</cell><cell cols="5">0.710 0.740 0.773 0.779 0.821 0.833</cell></row><row><cell>Flying-Heron</cell><cell cols="2">0.563 0.579</cell><cell>0.629</cell><cell>0.598</cell><cell>0.670</cell><cell>0.647</cell><cell>0.699</cell><cell>0.718</cell><cell cols="5">0.654 0.743 0.783 0.786 0.810 0.823</cell></row><row><cell>Flying-Katydid</cell><cell cols="2">0.540 0.661</cell><cell>0.593</cell><cell>0.657</cell><cell>0.653</cell><cell>0.659</cell><cell>0.615</cell><cell>0.696</cell><cell cols="5">0.687 0.709 0.730 0.739 0.802 0.809</cell></row><row><cell>Flying-Mantis</cell><cell cols="2">0.527 0.622</cell><cell>0.569</cell><cell>0.618</cell><cell>0.614</cell><cell>0.629</cell><cell>0.603</cell><cell>0.661</cell><cell cols="5">0.658 0.670 0.696 0.690 0.749 0.775</cell></row><row><cell>Flying-Mockingbird</cell><cell cols="2">0.641 0.550</cell><cell>0.622</cell><cell>0.593</cell><cell>0.636</cell><cell>0.596</cell><cell>0.664</cell><cell>0.670</cell><cell cols="5">0.674 0.683 0.721 0.737 0.788 0.838</cell></row><row><cell>Flying-Moth</cell><cell cols="2">0.583 0.720</cell><cell>0.726</cell><cell>0.737</cell><cell>0.707</cell><cell>0.685</cell><cell>0.747</cell><cell>0.783</cell><cell cols="5">0.753 0.798 0.833 0.854 0.878 0.917</cell></row><row><cell>Flying-Owl</cell><cell cols="2">0.625 0.671</cell><cell>0.705</cell><cell>0.656</cell><cell>0.657</cell><cell>0.718</cell><cell>0.710</cell><cell>0.781</cell><cell cols="5">0.712 0.750 0.793 0.809 0.837 0.868</cell></row><row><cell>Flying-Owlfly</cell><cell cols="2">0.614 0.690</cell><cell>0.524</cell><cell>0.669</cell><cell>0.633</cell><cell>0.580</cell><cell>0.599</cell><cell>0.778</cell><cell cols="5">0.583 0.743 0.782 0.756 0.758 0.863</cell></row><row><cell>Other-Other</cell><cell cols="2">0.571 0.613</cell><cell>0.603</cell><cell>0.593</cell><cell>0.638</cell><cell>0.653</cell><cell>0.675</cell><cell>0.687</cell><cell cols="5">0.671 0.665 0.725 0.700 0.777 0.779</cell></row><row><cell>Terrestrial-Ant</cell><cell cols="2">0.506 0.516</cell><cell>0.508</cell><cell>0.519</cell><cell>0.523</cell><cell>0.585</cell><cell>0.538</cell><cell>0.552</cell><cell cols="5">0.572 0.564 0.627 0.605 0.676 0.669</cell></row><row><cell>Terrestrial-Bug</cell><cell cols="2">0.578 0.681</cell><cell>0.682</cell><cell>0.687</cell><cell>0.686</cell><cell>0.701</cell><cell>0.691</cell><cell>0.743</cell><cell cols="5">0.710 0.799 0.799 0.803 0.828 0.856</cell></row><row><cell>Terrestrial-Cat</cell><cell cols="2">0.505 0.585</cell><cell>0.591</cell><cell>0.557</cell><cell>0.562</cell><cell>0.608</cell><cell>0.613</cell><cell>0.669</cell><cell cols="5">0.624 0.634 0.682 0.678 0.745 0.772</cell></row><row><cell>Terrestrial-Caterpillar</cell><cell cols="2">0.517 0.643</cell><cell>0.569</cell><cell>0.691</cell><cell>0.636</cell><cell>0.581</cell><cell>0.575</cell><cell>0.638</cell><cell cols="5">0.640 0.685 0.684 0.704 0.729 0.776</cell></row><row><cell>Terrestrial-Centipede</cell><cell cols="2">0.432 0.573</cell><cell>0.476</cell><cell>0.485</cell><cell>0.496</cell><cell>0.554</cell><cell>0.629</cell><cell>0.703</cell><cell cols="5">0.561 0.536 0.727 0.643 0.704 0.762</cell></row><row><cell>Terrestrial-Chameleon</cell><cell cols="2">0.556 0.651</cell><cell>0.627</cell><cell>0.653</cell><cell>0.619</cell><cell>0.619</cell><cell>0.632</cell><cell>0.695</cell><cell cols="5">0.659 0.673 0.713 0.732 0.789 0.804</cell></row><row><cell>Terrestrial-Cheetah</cell><cell cols="2">0.536 0.649</cell><cell>0.699</cell><cell>0.624</cell><cell>0.603</cell><cell>0.662</cell><cell>0.598</cell><cell>0.717</cell><cell cols="5">0.720 0.667 0.732 0.769 0.800 0.826</cell></row><row><cell>Terrestrial-Deer</cell><cell cols="2">0.530 0.581</cell><cell>0.610</cell><cell>0.564</cell><cell>0.558</cell><cell>0.600</cell><cell>0.623</cell><cell>0.650</cell><cell cols="5">0.644 0.660 0.667 0.670 0.719 0.757</cell></row><row><cell>Terrestrial-Dog</cell><cell cols="2">0.572 0.560</cell><cell>0.596</cell><cell>0.536</cell><cell>0.559</cell><cell>0.574</cell><cell>0.614</cell><cell>0.608</cell><cell cols="5">0.588 0.613 0.607 0.648 0.666 0.707</cell></row><row><cell>Terrestrial-Duck</cell><cell cols="2">0.530 0.535</cell><cell>0.557</cell><cell>0.539</cell><cell>0.524</cell><cell>0.558</cell><cell>0.619</cell><cell>0.582</cell><cell cols="5">0.602 0.548 0.598 0.682 0.742 0.746</cell></row><row><cell>Terrestrial-Gecko</cell><cell cols="2">0.485 0.674</cell><cell>0.662</cell><cell>0.725</cell><cell>0.683</cell><cell>0.705</cell><cell>0.606</cell><cell>0.733</cell><cell cols="5">0.724 0.747 0.789 0.771 0.833 0.848</cell></row><row><cell>Terrestrial-Giraffe</cell><cell cols="2">0.469 0.628</cell><cell>0.697</cell><cell>0.620</cell><cell>0.611</cell><cell>0.701</cell><cell>0.635</cell><cell>0.681</cell><cell cols="5">0.718 0.722 0.747 0.776 0.809 0.784</cell></row><row><cell>Terrestrial-Grouse</cell><cell cols="2">0.704 0.760</cell><cell>0.726</cell><cell>0.721</cell><cell>0.774</cell><cell>0.805</cell><cell>0.780</cell><cell>0.879</cell><cell cols="5">0.803 0.806 0.904 0.919 0.888 0.921</cell></row><row><cell>Terrestrial-Human</cell><cell cols="2">0.530 0.629</cell><cell>0.608</cell><cell>0.613</cell><cell>0.549</cell><cell>0.577</cell><cell>0.658</cell><cell>0.697</cell><cell cols="5">0.636 0.665 0.708 0.700 0.765 0.817</cell></row><row><cell>Terrestrial-Kangaroo</cell><cell cols="2">0.482 0.586</cell><cell>0.599</cell><cell>0.467</cell><cell>0.548</cell><cell>0.588</cell><cell>0.571</cell><cell>0.644</cell><cell cols="5">0.630 0.623 0.650 0.620 0.798 0.816</cell></row><row><cell>Terrestrial-Leopard</cell><cell cols="2">0.617 0.647</cell><cell>0.742</cell><cell>0.616</cell><cell>0.640</cell><cell>0.652</cell><cell>0.673</cell><cell>0.736</cell><cell cols="5">0.720 0.704 0.744 0.791 0.791 0.823</cell></row><row><cell>Terrestrial-Lion</cell><cell cols="2">0.534 0.634</cell><cell>0.695</cell><cell>0.599</cell><cell>0.660</cell><cell>0.656</cell><cell>0.658</cell><cell>0.720</cell><cell cols="5">0.714 0.663 0.754 0.751 0.805 0.813</cell></row><row><cell>Terrestrial-Lizard</cell><cell cols="2">0.579 0.629</cell><cell>0.634</cell><cell>0.635</cell><cell>0.633</cell><cell>0.656</cell><cell>0.627</cell><cell>0.710</cell><cell cols="5">0.702 0.716 0.744 0.777 0.804 0.830</cell></row><row><cell>Terrestrial-Monkey</cell><cell cols="2">0.423 0.693</cell><cell>0.724</cell><cell>0.593</cell><cell>0.611</cell><cell>0.730</cell><cell>0.663</cell><cell>0.792</cell><cell cols="5">0.678 0.614 0.709 0.699 0.851 0.888</cell></row><row><cell>Terrestrial-Rabbit</cell><cell cols="2">0.504 0.657</cell><cell>0.685</cell><cell>0.634</cell><cell>0.635</cell><cell>0.721</cell><cell>0.731</cell><cell>0.794</cell><cell cols="5">0.722 0.758 0.789 0.806 0.829 0.843</cell></row><row><cell>Terrestrial-Reccoon</cell><cell cols="2">0.451 0.525</cell><cell>0.536</cell><cell>0.461</cell><cell>0.482</cell><cell>0.702</cell><cell>0.723</cell><cell>0.643</cell><cell cols="5">0.532 0.592 0.691 0.659 0.781 0.766</cell></row><row><cell>Terrestrial-Sciuridae</cell><cell cols="2">0.533 0.612</cell><cell>0.638</cell><cell>0.573</cell><cell>0.608</cell><cell>0.693</cell><cell>0.661</cell><cell>0.745</cell><cell cols="5">0.725 0.721 0.775 0.757 0.810 0.842</cell></row><row><cell>Terrestrial-Sheep</cell><cell cols="2">0.434 0.451</cell><cell>0.721</cell><cell>0.410</cell><cell>0.482</cell><cell>0.467</cell><cell>0.763</cell><cell>0.660</cell><cell cols="5">0.466 0.430 0.489 0.487 0.481 0.500</cell></row><row><cell>Terrestrial-Snake</cell><cell cols="2">0.544 0.590</cell><cell>0.586</cell><cell>0.603</cell><cell>0.567</cell><cell>0.614</cell><cell>0.597</cell><cell>0.714</cell><cell cols="5">0.695 0.652 0.738 0.788 0.771 0.831</cell></row><row><cell>Terrestrial-Spider</cell><cell cols="2">0.528 0.594</cell><cell>0.593</cell><cell>0.594</cell><cell>0.580</cell><cell>0.621</cell><cell>0.572</cell><cell>0.650</cell><cell cols="5">0.649 0.651 0.685 0.687 0.740 0.771</cell></row><row><cell>Terrestrial-StickInsect</cell><cell cols="2">0.473 0.548</cell><cell>0.486</cell><cell>0.526</cell><cell>0.535</cell><cell>0.600</cell><cell>0.491</cell><cell>0.578</cell><cell cols="5">0.607 0.629 0.616 0.647 0.660 0.696</cell></row><row><cell>Terrestrial-Tiger</cell><cell cols="2">0.489 0.583</cell><cell>0.576</cell><cell>0.555</cell><cell>0.573</cell><cell>0.563</cell><cell>0.565</cell><cell>0.638</cell><cell cols="5">0.602 0.599 0.647 0.621 0.690 0.703</cell></row><row><cell>Terrestrial-Wolf</cell><cell cols="2">0.472 0.574</cell><cell>0.602</cell><cell>0.535</cell><cell>0.534</cell><cell>0.568</cell><cell>0.621</cell><cell>0.650</cell><cell cols="5">0.656 0.651 0.704 0.662 0.737 0.749</cell></row><row><cell>Terrestrial-Worm</cell><cell cols="2">0.485 0.652</cell><cell>0.596</cell><cell>0.642</cell><cell>0.628</cell><cell>0.558</cell><cell>0.651</cell><cell>0.692</cell><cell cols="5">0.629 0.684 0.763 0.670 0.724 0.806</cell></row></table><note>Fig. 16. Comparison of our SINet and three top-performing baselines, including (d) SINet cvpr</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 Structure</head><label>5</label><figDesc></figDesc><table /><note>-measure (S ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 Ablation studies for each component on three test datasets.</head><label>6</label><figDesc>For details please refer to Section 5.3.</figDesc><table><row><cell>Decoder</cell><cell>TEM</cell><cell>GRA</cell><cell>CHAMELEON [24]</cell><cell>CAMO-Test [25]</cell><cell>COD10K-Test (OUR)</cell></row><row><cell>No.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>PD NCD Sy. Conv. Asy. Conv, Reverse Group Size S ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>0, 0} {32; 8; 1} 0.884 0.940 0.811 0.033 0.812 0.869 0.730 0.073 0.812 0.884 0.679 0.039 #2 {1, 0, 0} {32; 8; 1} 0.881 0.934 0.799 0.034 0.820 0.877 0.740 0.071 0.813 0.884 0.673 0.038 #3 {1, 0, 0} {32; 8; 1} 0.887 0.934 0.813 0.033 0.811 0.867 0.731 0.074 0.815 0.888 0.680 0.036</figDesc><table><row><cell>#4</cell><cell cols="2">{1, 0, 0} {32; 8; 1}</cell><cell>0.888 0.944 0.818 0.030 0.810 0.866 0.730 0.073 0.814 0.883 0.678 0.037</cell></row><row><cell>#5</cell><cell cols="2">{0, 0, 0} {32; 8; 1}</cell><cell>0.886 0.942 0.814 0.031 0.814 0.873 0.739 0.073 0.814 0.887 0.682 0.037</cell></row><row><cell>#6</cell><cell cols="2">{1, 1, 0} {32; 8; 1}</cell><cell>0.879 0.928 0.794 0.035 0.820 0.877 0.738 0.071 0.807 0.878 0.661 0.040</cell></row><row><cell>#7</cell><cell cols="2">{1, 1, 1} {32; 8; 1}</cell><cell>0.886 0.939 0.812 0.031 0.817 0.875 0.736 0.073 0.810 0.884 0.670 0.037</cell></row><row><cell>#8</cell><cell>{1, 0, 0}</cell><cell>{1; 1; 1}</cell><cell>0.888 0.940 0.812 0.031 0.819 0.877 0.741 0.072 0.814 0.887 0.681 0.037</cell></row><row><cell>#9</cell><cell>{1, 0, 0}</cell><cell>{8; 8; 8}</cell><cell>0.886 0.943 0.814 0.032 0.816 0.872 0.738 0.074 0.815 0.886 0.682 0.037</cell></row><row><cell>#10</cell><cell cols="3">{1, 0, 0} {32; 32; 32} 0.884 0.944 0.810 0.033 0.819 0.876 0.738 0.071 0.813 0.884 0.675 0.037</cell></row><row><cell>#11</cell><cell cols="2">{1, 0, 0} {1; 8; 32}</cell><cell>0.883 0.940 0.812 0.032 0.811 0.869 0.734 0.073 0.815 0.887 0.679 0.036</cell></row><row><cell>#OUR</cell><cell cols="2">{1, 0, 0} {32; 8; 1}</cell><cell>0.888 0.942 0.816 0.030 0.820 0.882 0.743 0.070 0.815 0.887 0.680 0.037</cell></row><row><cell>5 ?</cell><cell>5 ?</cell><cell></cell><cell></cell></row><row><cell>4 ?</cell><cell>4 ?</cell><cell></cell><cell></cell></row><row><cell>3 ?</cell><cell>3 ?</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Guolei Sun and Jianbing Shen for insightful feedback. This research was supported by the National Key Research and Development Program of China under Grant No. 2018AAA0100400, NSFC (61922046), and S&amp;T innovation project from Chinese Ministry of Education. <ref type="figure">Fig. 27</ref>. Ground-truths of the images presented in <ref type="figure">Fig. 1</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Camouflaged object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2777" to="2787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Disruptive coloration and background pattern matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Cuthill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maddocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>P?rraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Troscianko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="issue">7029</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Camouflaging an object from many viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Flint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2782" to="2789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Animal camouflage: current issues and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merilaita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">1516</biblScope>
			<biblScope unit="page" from="423" to="427" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9404" to="9413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Camouflage and visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tolhurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pizlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">1516</biblScope>
			<biblScope unit="page" from="449" to="461" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image. Comput. Comput. Assist. Interv</title>
		<editor>Med</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inf-Net: Automatic COVID-19 Lung Infection Segmentation from CT Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Camouflage images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="51" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with deep learning: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3212" to="3232" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<title level="m">Salient objects in clutter</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Egnet:edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generic object recognition by inference of 3-d volumetric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Object Categorization: Comput. Hum. Vis. Perspect</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instance-level salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7469" to="7478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Animal camouflage analysis: Chameleon database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Skurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdulameer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>B?aszczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Depta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kozie?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anabranch network for camouflaged object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page" from="45" to="56" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Highly efficient salient object detection with 100k parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Boundary-aware segmentation network for mobile and web applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sant&amp;apos;anna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Su&amp;apos;arez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04704</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Concealing-coloration in the Animal Kingdom: An Exposition of the Laws of Disguise Through Color and Pattern: Being a Summary of Abbott H. Thayer&apos;s Discoveries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Thayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Thayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
			<publisher>Macmillan Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adaptive coloratcottion in animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Cott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1940" />
			<publisher>Methuen &amp; Co., Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mutual graph learning for camouflaged object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simultaneously localize, segment and rank the camouflaged objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aixuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Camouflaged object segmentation with distraction mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Segmentation-based deep-learning approach for surface-defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?ela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Skvar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Manuf</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="759" to="776" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An end-to-end steel surface defect detection approach via fusing multiple hierarchical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1493" to="1504" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pganet: Pyramid feature fusion and global context attention network for automated surface defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Industr. Inform</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep polarization cues for transparent object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Taamazyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8602" to="8611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transcut: Transparent object segmentation from a light-field image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-I</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3442" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Segmenting transparent objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting video saliency: A large-scale benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4894" to="4903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking rgb-d salient object detection: Models, data sets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="720" to="736" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards highresolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Camouflage, detection and identification of moving targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Cuthill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Shohet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Scott-Samuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Royal Soc. B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="issue">1758</biblScope>
			<biblScope unit="page">20130064</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1269" to="1277" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Progressively guided alternate refinement network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="520" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">F3Net: Fusion, Feedback and Focus for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Jittor: a novel deep learning framework with meta-operators and unified graph execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cognitive vision inspired object segmentation metric and loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SCIENTIA SINICA Informationis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>in chinese</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">D. Learn. Med. Image Anal</title>
		<imprint>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3085" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Structure-measure: A New Way to Evaluate Foreground Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Guiding model search using segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1417" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A fully convolutional neural network for wood defect location and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="123" to="453" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Review of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for covid-19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Features and objects: The fourteenth bartlett memorial lecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. J. Exp. Psychol. (Hove)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="237" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Guided search 2.0 a revised model of visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon. Bull. Rev</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="202" to="238" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Miththanthaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10875</idno>
		<title level="m">Transparent object tracking benchmark</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Selfsupervised learning of audio-visual objects from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04237</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Betrayed by motion: Camouflaged object discovery via motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lamdouar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Rgb-t salient object detection via fusing multi-level cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3321" to="3335" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Re-thinking co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Group collaborative learning for co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Camouflaged object detection and tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Image Graph</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">2050028</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Relative saliency and ranking: Models, metrics, data and benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi-Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu-Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13306</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Siamese network for rgb-d salient object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Uncertainty inspired rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Global2local: Efficient structure search for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Representative batch normalization with feature calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
