<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Target Candidate Association to Keep Track of What Not to Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mayer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
								<address>
									<postCode>D-ITET</postCode>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
								<address>
									<postCode>D-ITET</postCode>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danda</forename><surname>Pani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
								<address>
									<postCode>D-ITET</postCode>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paudel</forename><surname>Luc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
								<address>
									<postCode>D-ITET</postCode>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
								<address>
									<postCode>D-ITET</postCode>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Target Candidate Association to Keep Track of What Not to Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The presence of objects that are confusingly similar to the tracked target, poses a fundamental challenge in appearance-based visual tracking. Such distractor objects are easily misclassified as the target itself, leading to eventual tracking failure. While most methods strive to suppress distractors through more powerful appearance models, we take an alternative approach.</p><p>We propose to keep track of distractor objects in order to continue tracking the target. To this end, we introduce a learned association network, allowing us to propagate the identities of all target candidates from frameto-frame. To tackle the problem of lacking groundtruth correspondences between distractor objects in visual tracking, we propose a training strategy that combines partial annotations with self-supervision. We conduct comprehensive experimental validation and analysis of our approach on several challenging datasets. Our tracker sets a new state-of-the-art on six benchmarks, achieving an AUC score of 67.1% on LaSOT [24] and a +5.8% absolute gain on the OxUvA long-term dataset <ref type="bibr" target="#b53">[54]</ref>. The code and trained models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generic visual object tracking is one of the fundamental problems in computer vision. The task involves estimating the state of the target object in every frame of a video sequence, given only the initial target location. Most prior research has been devoted to the development of robust appearance models, used for locating the target object in each frame. The two currently dominating paradigms are Siamese networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42]</ref> and discriminative appearance modules <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. While the former employs a template matching in a learned feature space, the latter constructs an appearance model through a discriminative learning formulation. Although these approaches have demonstrated promising performance in recent years, they are effectively limited by the quality and discriminative power of the appearance model.  <ref type="figure">Figure 1</ref>. Visualization of the proposed target candidate association network used for tracking. For each target candidate ( ) we extract a set of features such as score, position and appearance in order to associate candidates across frames. The proposed target association network then allows to associate these candidates ( ) with the detected distractors ( ) and the target object ( ) of the previous frame. Lines connecting circles represent associations.</p><p>As one of the most challenging factors, co-occurrence of distractor objects similar in appearance to the target is a common problem in real-world tracking applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b59">60]</ref>. Appearance-based models struggle to identify the sought target in such cases, often leading to tracking failure. Moreover, the target object may undergo a drastic appearance change over time, further complicating the discrimination between target and distractor objects. In certain scenarios, e.g., as visualized in <ref type="figure">Fig. 1</ref>, it is even virtually impossible to unambiguously identify the target solely based on appearance information. Such circumstances can only be addressed by leveraging other cues during tracking, for instance the spatial relations between objects. We therefore set out to address problematic distractors by exploring such alternative cues.</p><p>We propose to actively keep track of distractor objects in order to ensure more robust target identification. To this end, we introduce a target candidate association network, that matches distractor objects as well as the target across frames. Our approach consists of a base appearance tracker, from which we extract target candidates in each frame. Each candidate is encoded with a set of distinctive features, consisting of the target classifier score, location, and appearance. The encodings of all candidates are jointly processed by a graph-based candidate embedding network. From the resulting embeddings, we compute the association scores between all candidates in subsequent frames, allowing us to keep track of the target and distractor objects over time. In addition, we estimate a target detection confidence, used to increase the robustness of the target classifier.</p><p>While associating target candidates over time provides a powerful cue, learning such a matching network requires tackling a few key challenges. In particular, generic visual object tracking datasets only provide annotations of one object in each frame, i.e., the target. As a result, there is a lack of ground-truth annotations for associating distractors across frames. Moreover, the definition of a distractor is not universal and depends on the properties of the employed appearance model. We address these challenges by introducing an approach that allows our candidate matching network to learn from real tracker output. Our approach exploits the single target annotations in existing tracking datasets in combination with a self-supervised strategy. Furthermore, we actively mine the training dataset in order to retrieve rare and challenging cases, where the use of distractor association is important, in order to learn a more effective model. Contributions: In summary, our contributions are as follows: (i) We propose a method for target candidate association based on a learnable candidate matching network. (ii) We develop an online object association method in order to propagate distractors and the target over time and introduce a sample confidence score to update the target classifier more effectively during inference. (iii) We tackle the challenges with incomplete annotation by employing partial supervision, self-supervised learning, and sample-mining to train our association network. (iv) We perform comprehensive experiments and ablative analyses by integrating our approach into the tracker SuperDiMP <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3]</ref>. The resulting tracker KeepTrack sets a new state-of-the-art on six tracking datasets, obtaining an AUC of 67.1% on La-SOT <ref type="bibr" target="#b23">[24]</ref> and 69.7% on UAV123 <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Discriminative appearance model based trackers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b16">17]</ref> aim to suppress distractors based on their appearance by integrating background information when learning the target classifier online. While often increasing robustness, the capacity of an online appearance model is still limited. A few works have therefore developed more dedicated strategies of handling distractors. Bhat et al. <ref type="bibr" target="#b3">[4]</ref> combine an appearance based tracker and an RNN to propagate information about the scene across frames. It internally aims to track all regions in the scene by maintaining a learnable state representation. Other methods exploit the existence of distractors explicitly in the method formulation. DaSiamRPN <ref type="bibr" target="#b72">[73]</ref> handles distractor objects by subtracting corresponding image features from the target tem-plate during online tracking. Xiao et al. <ref type="bibr" target="#b59">[60]</ref> use the locations of distractors in the scene and employ hand crafted rules to classify image regions into background and target candidates on each frame. SiamRCNN <ref type="bibr" target="#b54">[55]</ref> associates subsequent detections across frames using a hand-crafted association score to form short tracklets. In contrast, we introduce a learnable network that explicitly associates target candidates from frame-to-frame. Zhang et al. <ref type="bibr" target="#b70">[71]</ref> propose a tracker inspired by the multi object tracking (MOT) philosophy of tracking by detection. They use the top-k predicted bounding boxes for each frame and link them between frames by using different features. In contrast, we omit any hand crafted features but fully learn to predict the associations using self-supervision.</p><p>Many online trackers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref> employ a memory to store previous predictions to fine-tune the tracker. Typically the oldest sample is replaced in the memory and an agebased weight controls the contribution of each sample when updating the tracker online. Danelljan et al. <ref type="bibr" target="#b15">[16]</ref> propose to learn the tracking model and the training sample weights jointly. LTMU <ref type="bibr" target="#b11">[12]</ref> combines an appearance based tracker with a learned meta-updater. The goal of the meta-updater is to predict whether the employed online tracker is ready to be updated or not. In contrast, we use a learned target candidate association network to compute a confidence score and combine it with sample age to manage the tracker updates.</p><p>The object association problem naturally arises in MOT. The dominant paradigm in MOT is tracking-bydetection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b67">68]</ref>, where tracking is posed as the problem of associating object detections over time. The latter is typically formulated as a graph partitioning problem. Typically, these methods are non-causal and thus require the detections from all frames in the sequence. Furthermore, MOT typically focuses on a limited set of object classes <ref type="bibr" target="#b18">[19]</ref>, such as pedestrians, where strong object detectors are available. In comparison we aim at tracking different objects in different sequences solely defined by the initial frame. Furthermore, we lack ground truth correspondences of all distractor objects from frame to frame whereas the ground-truth correspondences of different objects in MOT datasets are typically provided <ref type="bibr" target="#b18">[19]</ref>. Most importantly, we aim at associating target candidates that are defined by the tracker itself, while MOT methods associate all detections that correspond to one of the sought objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe our tracking approach, which actively associates distractor objects and the sought target across multiple frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>An overview of our tracking pipeline is shown in <ref type="figure">Fig. 2</ref>. We use a base tracker with a discriminative appearance </p><formula xml:id="formula_0">{h i } N i=1</formula><formula xml:id="formula_1">s {z 0 i } N 0 i=1 {h 0 i } N 0 i=1 {z i } N i=1 {v i } N i=1 {v 0 i } N 0 i=1 {(c 0 i , s 0 i , f 0 i )} N 0 i=1 {(c i , s i , f i )} N i=1 Object Association Module ? ? o 0 O 0? Backbone s 0 O ? Base Tracker</formula><p>Target Candidate Extraction Target Candidate Association Network Time <ref type="figure">Figure 2</ref>. Overview of the entire online tracking pipeline, processing the previous and current frames jointly to predict the target object.</p><p>model and internal memory. In particular, we adopt the Su-perDiMP <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> tracker, which employs the target classifier in DiMP <ref type="bibr" target="#b2">[3]</ref> and the probabilistic bounding-box regression from <ref type="bibr" target="#b17">[18]</ref>, together with improved training settings. We use the base tracker to predict the target score map s for the current frame and extract the target candidates v i by finding locations in s with high target score. Then, we extract a set of features for each candidate. Namely: target classifier score s i , location c i in the image, and an appearance cue f i based on the backbone features of the base tracker. Then, we encode this set of features into a single feature vector z i for each candidate. We feed these representations and the equivalent ones of the previous framealready extracted beforehand -into the candidate embedding network and process them together to obtain the enriched embeddings h i for each candidate. These feature embeddings are used to compute the similarity matrix S and to estimate the candidate assignment matrix A between the two consecutive frames using an optimal matching strategy.</p><p>Once having the candidate-to-candidate assignment probabilities estimated, we build the set of currently visible objects in the scene O and associate them to the previously identified objects O , i.e., we determine which objects disappeared, newly appeared, or stayed visible and can be associated unambiguously. We then use this propagation strategy to reason about the target object? in the current frame. Additionally, we compute the target detection confidence ? to manage the memory and control the sample weight, while updating the target classifier online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Problem Formulation</head><p>Let the set of target candidates, which includes distractors and the sought target, be V = {v i } N i=1 , where N de-notes the number of candidates present in each frame. We define the target candidate sets V and V corresponding to the previous and current frames, respectively. We formulate the problem of target candidate association across two subsequent frames as, finding the assignment matrix A between the two sets V and V. If the target candidate v i corresponds to v j then A i,j = 1 and A i,j = 0 otherwise. In practice, a match may not exist for every candidate. Therefore, we introduce the concept of dustbins, which is commonly used for graph matching <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b19">20]</ref> to actively handle the non-matching vertices. The idea is to match the candidates without match to the dustbin on the missing side. Therefore, we augment the assignment matrix A by an additional row and column representing dustbins. It follows that a newly appearing candidate v j -which is only present in the set V -leads to the entry A N +1,j = 1. Similarly, a candidate v i that is no longer available in the set V results in A i,N +1 = 1. To solve the assignment problem, we design a learnable approach that predicts the matrix A. Our approach first extracts a representation of the target candidates, which is discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Target Candidate Extraction</head><p>Here, we describe how to detect and represent target candidates and propose a set of features and their encoding. We define the set of target candidates V as all unique coordinates c i that correspond to a local maximum with minimal score in the target score map s. Thus, each target candidate v i and its coordinate c i need to fulfill the following two constraints,</p><formula xml:id="formula_2">? max (s, c i ) = 1 and s(c i ) ? ?,<label>(1)</label></formula><p>where ? max returns 1 if the score at c i is a local maximum of s or 0 otherwise, and ? denotes a threshold. This definition allows us to build the sets V and V, by retrieving the local maxima of s and s with sufficient score value. We use the max-pooling operation in a 5 ? 5 local neighbourhood to retrieve the local maxima of s and set ? = 0.05. For each candidate we build a set of features inspired by two observations. First, we notice that the motion of the same objects from frame to frame is typically small and thus similar locations and similar distances between different objects. Therefore, the position c i of a target candidate forms a strong cue. In addition, we observe only small changes in appearance for each object. Therefore, we use the target classifier score s i = s(c i ) as another cue. In order to add a more discriminative appearance based feature f i = f (c i ), we process the backbone features (used in the baseline tracker) with a single learnable convolution layer. Finally, we build a feature tuple for each target candidate as (s i , f i , c i ). These features are combined in the following way,</p><formula xml:id="formula_3">z i = f i + ?(s i , c i ), ?v i ? V,</formula><p>where ? denotes a Multi-Layer Perceptron (MLP), that maps s and c to the same dimensional space as f i . This encoding permits jointly reasoning about appearance, target similarity, and position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Candidate Embedding Network</head><p>In order to further enrich the encoded features and in particular to facilitate extracting features while being aware of neighbouring candidates, we employ a candidate embedding network. On an abstract level, our association problem bares similarities with the task of sparse feature matching. In order to incorporate information of neighbouring candidates, we thus take inspiration from recent advances in this area. In particular, we adopt the SuperGlue <ref type="bibr" target="#b50">[51]</ref> architecture that establishes the current state-of-the-art in sparse feature matching. Its design allows to exchange information between different nodes, to handle occlusions, and to estimate the assignment of nodes across two images. In particular, the features of both frames translate to nodes of a single complete graph with two types of directed edges: 1) self edges within the same frame and 2) cross edges connecting only nodes between the frames. The idea is then to exchange information either along self or cross edges. The adopted architecture <ref type="bibr" target="#b50">[51]</ref> uses a Graph Neural Network (GNN) with message passing that sends the messages in an alternating fashion across self or cross edges to produce a new feature representation for each node after every layer. Moreover, an attention mechanism computes the messages using self attention for self edges and cross attention for cross edges. After the last message passing layer a linear projection layer extracts the final feature representation h i for each candidate v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Candidate Matching</head><p>To represent the similarities between candidates v i ? V and v j ? V, we construct the similarity matrix S. The sought similarity is measured using the scalar product: S i,j = h i , h j , for feature vectors h i and h j corresponding to the candidates v i and v j .</p><p>As previously introduced, we make use of the dustbinconcept <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b50">51]</ref> to actively match candidates that miss their counterparts to the so-called dustbin. However, a dustbin is a virtual candidate without any feature representation h i . Thus, the similarity score is not directly predictable between candidates and the dustbin. A candidate corresponds to the dustbin, only if its similarity scores to all other candidates are sufficiently low. In this process, the similarity matrix S represents only an initial association prediction between candidates disregarding the dustbins. Note that a candidate corresponds either to an other candidate or to the dustbin in the other frame. When the candidates v i and v j are matched, both constraints N i=1 A i,j = 1 and N j=1 A i,j = 1 must be satisfied for one-to-one matching. These constraints however, do not apply for missing matches since multiple candidates may correspond to the same dustbin. Therefore, we make use of two new constraints for dustbins. These constraints for dustbins read as follows: all candidates not matched to another candidate must be matched to the dustbins. Mathematically, this can be expressed as, j A N +1,j = N ? M and i A i,N +1 = N ? M , where M = (i?N ,j?N ) A i,j represents the number of candidate-to-candidate matching. In order to solve the association problem, using the discussed constraints, we follow Sarlin et al. <ref type="bibr" target="#b50">[51]</ref> and use the Sinkhorn <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b9">10]</ref> based algorithm therein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Learning Candidate Association</head><p>Training the embedding network that parameterizes the similarity matrix used for optimal matching requires ground truth assignments. Hence, in the domain of sparse keypoint matching, recent learning based approaches leverage large scale datasets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51]</ref> such as MegaDepth <ref type="bibr" target="#b45">[46]</ref> or Scan-Net <ref type="bibr" target="#b10">[11]</ref>, that provide ground truth matches. However, in tracking such ground truth correspondences (between distractor objects) are not available. Only the target object and its location provide a single ground truth correspondence. Manually annotating correspondences for distracting candidates, identified by a tracker on video datasets, is expensive and may not be very useful. Instead, we propose a novel training approach that exploits, (i) partial supervision from the annotated target objects, and (ii) self-supervision by artificially mimicking the association problem. Our approach requires only the annotations that already exist in standard tracking datasets. The candidates for matching are obtained by running the base tracker on the given training dataset.</p><p>Partially Supervised Loss: For each pair of consecutive frames, we retrieve the two candidates corresponding to the annotated target, if available. This correspondence forms a partial supervision for a single correspondence while all other associations remain unknown. For the retrieved candidates v i and v j , we define the association as a tuple (l , l) = (i, j). Here, we also mimic the association for redetections and occlusions by occasionally excluding one of the corresponding candidates from V or V. We replace the excluded candidate by the corresponding dustbin to form the correct association for supervision. More precisely, the simulated associations for redetection and occlusion are expressed as, (l , l) = (N + 1, j) and (l , l) = (i, N + 1), respectively. The supervised loss, for each frame-pairs, is then given by the negative log-likelihood of the assignment probability,</p><formula xml:id="formula_4">L sup = ? log A l ,l .<label>(2)</label></formula><p>Self-Supervised Loss: To facilitate the association of distractor candidates, we employ a self-supervision strategy. The proposed approach first extracts a set of candidates V from any given frame. The corresponding candidates for matching, say V, are identical to V but we augment its features. Since the feature augmentation does not affect the associations, the initial ground-truth association set is given by</p><formula xml:id="formula_5">C = {(i, i)} N i=1 .</formula><p>In order to create a more challenging learning problem, we simulate occlusions and redetections as described above for the partially supervised loss. Note that the simulated occlusions and redetections change the entries of V, V , and C. We make use of the same notations with slight abuse for simplicity. Our feature augmentation involves, randomly translating the location c i , increasing or decreasing the score s i , and transforming the given image before extracting the visual features f i . Now, using the simulated ground-truth associations C, our self-supervised loss is given by,</p><formula xml:id="formula_6">L self = (l ,l)?C ? log A l ,l .<label>(3)</label></formula><p>Finally, we combine both losses as L tot = L sup + L self . It is important to note that the real training data is used only for the former loss function, whereas synthetic data is used only for the latter one. Data Mining: Most frames contain a candidate corresponding to the target object and are thus applicable for supervised training. However, a majority of these frames are not very informative for training because they contain only a single candidate with high target classifier score and correspond to the target object. Conversely, the dataset contains adverse situations where associating the candidate corresponding to the target object is very challenging. Such situations include sub-sequences with different number of candidates, with changes in appearance or large motion between frames. Thus, sub-sequences where the appearance model either fails and starts to track a distractor or when the tracker is no longer able to detect the target with sufficient confidence are valuable for training. However, such failure cases are rare even in large scale datasets. Similarly, we prefer frames with many target candidates when creating synthetic sub-sequences to simultaneously include candidate associations, redetections and occlusions. Thus, we mine the training dataset using the dumped predictions of the base tracker to use more informative training samples.</p><p>Training Details: We first retrain the base tracker Su-perDiMP without the learned discriminative loss parameters but keep everything else unchanged. We split the LaSOT training set into a train-train and a train-val set. We run the base tracker on all sequences and save the search region and score map for each frame on disk. We use the dumped data to mine the dataset and to extract the target candidates and its features. We freeze the weights of the base tracker during training of the proposed network and train for 15 epochs by sampling 6400 sub-sequences per epoch from the traintrain split. We sample real or synthetic data equally likely. We use ADAM <ref type="bibr" target="#b36">[37]</ref> with learning rate decay of 0.2 every 6th epoch with a learning rate of 0.0001. We use two GNN Layers and run 10 Sinkhorn iterations. Please refer to the supplementary (Sec. A) for additional details about training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Object Association</head><p>This part focuses on using the estimated assignments (see Sec. 3.5) in order to determine the object correspondences during online tracking. An object corresponds either to the target or a distractor. The general idea is to keep track of every object present in each scene over time. We implement this idea with a database O, where each entry corresponds to an object o that is visible in the current frame. <ref type="figure" target="#fig_1">Fig. 3</ref> shows these objects as circles. An object disappears from the scene if none of the current candidates is associated with it, e.g., in <ref type="figure" target="#fig_1">Fig. 3</ref> the purple and pink objects ( , ) no longer correspond to a candidate in the last frame. Then, we delete this object from the database. Similarly, we add a new object to the database if a new target candidate appears ( , , ). When initializing a new object, we assign it a new object-id (not used previously) and the score s i . In <ref type="figure" target="#fig_1">Fig. 3</ref> object-ids are represented using colors. For objects that remain visible, we add the score s i of the corresponding candidate to the history of scores of this object. Furthermore, we delete the old and create a new object if the candidate correspondence is ambiguous, i.e., the assignment probability is smaller than ? = 0.75.</p><p>If associating the target object? across frames is unambiguous, it implies that one object has the same object-id as the initially provided object? init . Thus, we return this object as the selected target. However, in real world scenarios the target object gets occluded, leaves the scene or associating the target object is ambiguous. Then, none of  the candidates corresponds to the sought target and we need to redetect. We redetect the target if the candidate with the highest target classifier score achieves a score that exceeds the threshold ? = 0.25. We select the corresponding object as the target as long as no other candidate achieves a higher score in the current frame. Then, we switch to this candidate and declare it as target if its score is higher than any score in the history (of the currently selected object). Otherwise, we treat this object as a distractor for now, but if its score increases high enough, we will select it as the target object in the future. Please refer to the supplementary material (Sec. B) for the detailed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Memory Sample Confidence</head><p>While updating the tracker online is often beneficial, it is disadvantageous if the training samples have a poor quality. Thus, we describe a memory sample confidence score, that we use to decide which sample to keep in the memory and which should be replaced when employing a fixed size memory. In addition, we use the score to control the contribution of each training sample when updating the tracker online. In contrast, the base tracker replaces frames using a first-in-first out policy if the target was detected and weights samples during inference solely based on age.</p><p>First, we define the training samples in frame k as (x k , y k ). We assume a memory size m that stores samples from frame k ? {1, . . . , t}, where t denotes the current frame number. The online loss then given by,</p><formula xml:id="formula_7">J(?) = ?R(?) + t k=1 ? k ? k Q(?; x k , y k ),<label>(4)</label></formula><p>where Q denotes the data term, R the regularisation term, ? is a scalar and ? represents appearance model weights.</p><p>The weights ? k ? 0 control the impact of the sample from frame k, i.e., a higher value increases the influence of the corresponding sample during training. We follow other appearance based trackers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref> and use a learning parameter ? ? [0, 1] in order to control the weights ? k = (1??)? k+1 , such that older samples achieve a smaller value and their impact during training decreases. In addition, we propose a second set of weights ? k that represent the confidence of the tracker that the predicted label y k is correct. Instead of removing the oldest samples to keep the memory fixed <ref type="bibr" target="#b2">[3]</ref>, we propose to drop the sample that achieves the smallest score ? k ? k which combines age and confidence. Thus, if t &gt; n we remove the sample at position k = argmin 1?k?n ? k ? k by setting ? k = 0. This means, that if all samples achieve similar confidence the oldest is replaced, or that if all samples are of similar age the least confident sample is replaced. We describe the extraction of the confidence weights as,</p><formula xml:id="formula_8">? t = ? ?, if? =? init ?, otherwise,<label>(5)</label></formula><p>where ? = max i s t i denotes the maximum value of the target classifier score map of frame t. For simplicity, we assume that ? ? [0, 1]. The condition? =? init is fulfilled if the currently selected object is identical to the initially provided target object, i.e., both objects share the same object id. Then, it is very likely, that the selected object corresponds to the target object such that we increase the confidence using the square root function that increases values in the range [0, 1). Hence, the described confidence score combines the confidence of the target classifier with the confidence of the object association module, but fully relies on the target classifier once the target is lost. Inference details: We propose KeepTrack and the speed optimized KeepTrackFast. We use the SuperDiMP parameters for both trackers but increase the search area scale from 6 to 8 (from 352 to 480 in image space) for KeepTrack. For the fast version we keep the original scale but reduce the number of bounding box refinement steps from 10 to 3. In addition, we skip running the association module if only one target caidndate with a high score is present in the previous and current frame. Overall, both trackers follow the target longer until it is lost such that small search areas occur frequently. Thus, we reset the search area to its previous size if it was drastically decreased before the target was lost, to facilitate redetections. Please refer to the supplementary (Sec. B) for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed tracking architecture on seven benchmarks. Our approach is implemented in Python using PyTorch. On a single Nvidia GTX 2080Ti GPU, KeepTrack and KeepTrackFast achieve 18.3 and 29.6 FPS, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>We perform an extensive analysis of the proposed tracker, memory sample confidence, and training losses. Online tracking components: We study the importance of memory sample confidence, the search area protocol, and target candidate association of our final method KeepTrack.</p><p>In Tab. 1 we analyze the impact of successively adding each component, and report the average of five runs on the NFS <ref type="bibr" target="#b26">[27]</ref>, UAV123 <ref type="bibr" target="#b48">[49]</ref> and LaSOT <ref type="bibr" target="#b23">[24]</ref> datasets. The first row reports the results of the employed base tracker. First, we add the memory sample confidence approach (Sec. 3.8), observe similar performance on NFS and UAV but a significant improvement of 1.5% on LaSOT, demonstrating its potential for long-term tracking. With the added robustness, we next employ a larger search area and increase it if it was drastically shrank before the target was lost. This leads to a fair improvement on all datasets. Finally, we add the target candidate association network, which provides substantial performance improvements on all three datasets, with a +1.3% AUC on LaSOT. These results clearly demonstrate the power of the target candidate association network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training:</head><p>In order to study the effect of the proposed training losses, we retrain the target candidate association network either with only the partially supervised loss or only the self-supervised loss. We report the performance on LaSOT <ref type="bibr" target="#b23">[24]</ref> in Tab. 2. The results show that each loss individually allows to train the network and to outperform the baseline without the target candidate association network (no TCA), whereas, combining both losses leads to the best tracking results. In addition, training the network with the combined loss but without data-mining decreases the tracking performance. Memory management: We not only use the sample con- fidence to manage the memory but also to control the impact of samples when learning the target classifier online. In Tab. 3, we study the importance of each component by adding one after the other and report the results on La-SOT <ref type="bibr" target="#b23">[24]</ref>. First, we use the sample confidence scores only to decide which sample to remove next from the memory. This, already improves the tracking performance. Reusing these weights when learning the target classifier as described in Eq. (4) increases the performance again. To suppress the impact of poor-quality samples during online learning, we ignore samples with a confidence score bellow 0.5. This leads to an improvement on LaSOT. The last row corresponds to the used setting in the final proposed tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">State-of-the-art Comparison</head><p>We compare our approach on seven tracking benchmarks. The same settings and parameters are used for all datasets. In order to ensure the significance of the results, we report the average over five runs on all datasets unless the evaluation protocol requires otherwise. We recompute the results of all trackers using the raw predictions if available or otherwise report the results given in the paper. LaSOT <ref type="bibr" target="#b23">[24]</ref>: First, we compare on the large-scale La-SOT dataset (280 test sequences with 2500 frames in average) to demonstrate the robustness and accuracy of the proposed tracker. The success plot in <ref type="figure">Fig. 4a</ref> shows the overlap precision OP T as a function of the threshold T . Trackers are ranked w.r.t. their area-under-the-curve (AUC) score, denoted in the legend. Tab. 4 shows more results including precision and normalized precision. KeepTrack and KeepTrackFast outperform the recent trackers AlphaRefine <ref type="bibr" target="#b61">[62]</ref>, TransT <ref type="bibr">[7]</ref> and TrDiMP <ref type="bibr" target="#b56">[57]</ref> by a large margin and the base tracker SuperDiMP by 4.0% or 3.7% in AUC. The improvement in OP T is most prominent for thresholds T &lt; 0.7, demonstrating the superior robustness of our tracker. In Tab. 5, we further perform an apple-to-apple comparison with KYS <ref type="bibr" target="#b3">[4]</ref>, LTMU <ref type="bibr" target="#b11">[12]</ref>, AlphaRefine <ref type="bibr" target="#b61">[62]</ref> and TrDiMP <ref type="bibr" target="#b56">[57]</ref>, where all methods use SuperDiMP as base tracker. We outperform the best method on each metric, achieving an AUC improvement of 1.8%. LaSOTExtSub <ref type="bibr" target="#b22">[23]</ref>: We evaluate our tracker on the  recently published extension subset of LaSOT. LaSO-TExtSub is meant for testing only and consists of 15 new classes with 10 sequences each. The sequences are long (2500 frames on average), rendering substantial challenges. <ref type="figure">Fig. 4b</ref> shows the success plot, that is averaged over 5 runs. All results, except ours and SuperDiMP, are obtained from <ref type="bibr" target="#b22">[23]</ref>, e.g., DaSiamRPN <ref type="bibr" target="#b72">[73]</ref>, SiamRPN++ <ref type="bibr" target="#b41">[42]</ref> and SiamMask <ref type="bibr" target="#b57">[58]</ref>. Our method achieves superior results, outperforming LTMU by 6.8% and SuperDiMP by 3.5%.</p><p>OxUvALT <ref type="bibr" target="#b53">[54]</ref>: The OxUvA long-term dataset contains 166 test videos with average length 3300 frames. Trackers are required to predict whether the target is present or absent in addition to the bounding box for each frame. Trackers are ranked by the maximum geometric mean (MaxGM) of the true positive (TPR) and true negative rate (TNR). We use the proposed confidence score and set the threshold for target presence using the separate dev. set. Tab. 6 shows the results on the test set, which are obtained through the evaluation server. KeepTrack sets the new state-of-the-art in terms of MaxGM by achieving an improvement of 5.8% compared to the previous best method and exceed the result of the base tracker SuperDiMP by 6.1%. VOT2019LT <ref type="bibr" target="#b39">[40]</ref>/VOT2020LT <ref type="bibr" target="#b37">[38]</ref>: The dataset for both VOT <ref type="bibr" target="#b40">[41]</ref> long-term tracking challenges contains 215,294 frames divided in 50 sequences. Trackers need to predict a confidence score that the target is present and the bounding box for each frame. Trackers are ranked by the F-score, evaluated for a range of confidence thresholds. We compare with the top methods in the challenge <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38]</ref>, as well as more recent methods. As shown in Tab. 7, our tracker achieves the best result in terms of F-score and outperforms the base tracker SuperDiMP by 4.0% in F-score. UAV123 <ref type="bibr" target="#b48">[49]</ref>: This dataset contains 123 videos and is designed to assess trackers for UAV applications. It contains small objects, fast motions, and distractor objects. Tab. 8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel tracking pipeline employing a learned target candidate association network in order to track both the target and distractor objects. This approach allows us to propagate the identities of all target candidates throughout the sequence. In addition, we propose a training strategy that combines partial annotations with selfsupervision. We do so to compensate for lacking groundtruth correspondences between distractor objects in visual tracking. We conduct comprehensive experimental validation and analysis of our approach on seven generic object tracking benchmarks and set a new state-of-the-art on six.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>In this supplementary material, we first provide details about training in Sec. A and about inference in Sec. B. We then report a more detailed analysis of out method in Sec. C and provide more detailed results of the experiments shown in the main paper in Sec. D.</p><p>Furthermore, we want to draw the attention of the reader to the compiled video available on the project page at https://github.com/visionml/pytracking. The video provides more visual insights about our tracker and compares visually with the baseline tracker Su-perDiMP. It shows tracking of distractors and indicates the same object ids in consecutive frames with agreeing color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training</head><p>First, we describe the training data generation and sample selection to train the network more effectively. Then, we provide additional details about the training procedure such as training in batches, augmentations and synthetic sample generation. Finally, we briefly summarize the employed network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Data-Mining</head><p>We use the LaSOT <ref type="bibr" target="#b23">[24]</ref> training set to train our target candidate association network. In particular, we split the 1120 training sequences randomly into a train-train (1000 sequences) and a train-val (120 sequences) set. We run the base tracker on all sequences and store the target classifier score map and the search area on disk for each frame. During training, we use the score map and the search area to extract the target candidates and its features to provide the data to train the target candidate association network.</p><p>We observed that many sequences or sub-sequences contain mostly one target candidate with a high target classifier score. Thus, in this cases target candidate association is trivial and learning on these cases will be less effective. Conversely, tracking datasets contain sub-sequences that are very challenging (large motion or appearance changes or many distractors) such that trackers often fail. While these sub-sequences lead to a more effective training they are rel-  <ref type="table">Table 9</ref>. Categories and specifications for each frame in the training dataset used for data-mining.</p><p>atively rare such that we decide to actively search the training dataset. First, we assign each frame to one of six different categories. We classify each frame based on four observations about the number of candidates, their target classifier score, if one of the target candidates is selected as target and if this selection is correct, see Tab. 9. A candidate corresponds to the annotated target object if the spatial distance between the candidate location and center coordinate of the target object is smaller than a threshold.</p><p>Assigning each frame to the proposed categories, we observe, that the dominant category is D (70%) that corresponds to frames with a single target candidate matching the annotated target object. However, we favour more challenging settings for training. In order to learn distractor associations using self supervision, we require frames with multiple detected target candidates. Category H (18.4%) corresponds to such frames where in addition the candidate with the highest target classifier score matches the annotated target object. Hence, the base tracker selects the correct candidate as target. Furthermore, category G corresponds to frames where the base tracker was no longer able to track the target because the target classifier score of the corresponding candidate fell bellow a threshold. We favour these frames during training in order to learn continue tracking the target even if the score is low.</p><p>Both categories J and K correspond to tracking failures of the base tracker. Whereas in K the correct target is detected but not selected, it is not detected in frames of category J. Thus, we aim to learn from tracking failures in order to train the target candidate association network such that it learns to compensate for tracking failures of the base tracker and corrects them. In particular, frames of category K are important for training because the two candidates with highest target classifier score no longer match such that the network is forced to include other cues for matching. We use frames of category J because frames where the object is visible but not detected contain typically many distractors such that these frames are suitable to learn distractor associations using self-supervised learning.</p><p>To summarize, we select only frames with category H, K, J for self-supervised training and sample them with a ratio of 2 : 1 : 1 instead of 10 : 2 : 1 (ratio in the dataset). We ignore frames from category D during self-supervised training because we require frames with multiple target candidates. Furthermore, we select sub-sequences of two consecutive frames for partially supervised training. We choose challenging sub-sequences that either contain many distractors in each frame (HH, 350k) or sub-sequences where the base tracker fails and switches to track a distractor (HK, 1001) or where the base tracker is no longer able to identify the target with high confidence (HG, 1380). Again we change the sampling ratio from approximately 350 : 1 : 1 to 10 : 1 : 1 during training. We change the sampling ration in order to use failure cases more often during training than they occur in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training Data Preparation</head><p>During training we use two different levels of augmentation. First, we augment all features of target candidate to enable self-supervised training with automatically produced ground truth correspondences. In addition, we use augmentation to improve generalization and overfitting of the network.</p><p>When creating artificial features we randomly scale each target classifier score, randomly jitter the candidate location within the search area and apply common image transformations to the original image before extracting the appearance based features for the artificial candidates. In particular, we randomly jitter the brightness, blur the image and jitter the search area before cropping the image to the search area.</p><p>To reduce overfitting and improve the generalization, we randomly scale the target candidate scores for synthetic and real sub-sequences. Furthermore, we remove candidates from the sets V and V randomly in order to simulate newly appearing or disappearing objects. Furthermore, to enable training in batches we require the same number of target candidates in each frame. Thus, we keep the five candidates with the highest target classifier score or add artificial peaks at random locations with a small score such that five candidates per frame are present. When computing the losses, we ignore these artificial candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Architecture Details</head><p>We use the SuperDiMP tracker <ref type="bibr" target="#b12">[13]</ref> as our base tracker. SuperDiMP employs the DiMP <ref type="bibr" target="#b2">[3]</ref> target classifier and the probabilistic bounding-box regression of PrDiMP <ref type="bibr" target="#b17">[18]</ref>, together with improved training settings. It uses a ResNet-50 <ref type="bibr" target="#b32">[33]</ref> pretrained network as backbone feature extractor. We freeze all parameters of SuperDiMP while training the target candidate association network. To produce the visual features for each target candidate, we use the third layer ResNet-50 features. In particular, we obtain a 29?29?1024 feature map and feed it into a 2 ? 2 convolutional layer which produces the 30 ? 30 ? 256 feature map f . Note, that the spatial resolution of the target classifier score and feature map agree such that extracting the appearance based features f i for each target candidate v i at location c i is simplified.</p><p>Furthermore, we use a four layer Multi-Layer Perceptron (MLP) to project the target classifier score and location for each candidate in the same dimensional space as f i . We use the following MLP structure: 3 ? 32 ? 64 ? 128 ? 256 with batch normalization. Before feeding the candidate locations into the MLP we normalize it according to the image  We follow Sarlin et al. <ref type="bibr" target="#b50">[51]</ref> when designing the candidate embedding network. In particular, we use self and cross attention layers in an alternating fashion and employ two layers of each type. In addition, we append a 1 ? 1 convolutional layer to the last cross attention layer. Again, we follow Sarlin et al. <ref type="bibr" target="#b50">[51]</ref> for optimal matching and reuse their implementation of the Sinkhorn algorithm and run it for 10 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inference</head><p>In this section we provide the detailed algorithm that describes the object association module (Sec. 3.7 in the paper). Furthermore, we explain the idea of search area rescaling at occlusion and how it is implemented. We conclude with additional inference details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Object Association Module</head><p>Here, we provide a detailed algorithm describing the object association module presented in the main paper, see Alg. 1. It contains target candidate to object association and the redetection logic to retrieve the target object after it was lost.</p><p>First, we will briefly explain the used notation. Each object can be modeled similar to a class in programming.</p><p>Thus, each object o contains attributes that can be accesses using the "." notation. In particular (o j ).s returns the score attribute s of object o j . In total the object class contains two attribute: the list of scores s and the object-id id. Both setting and getting the attribute values is possible.</p><p>The algorithm requires the following inputs: the set of target candidates V, the set of detected objects O and the object selected as target? in the previous frame. First, we check if a target candidate matches with any previously detected object and verify that the assignment probability is higher than a threshold ? = 0.75. If such a match exists, we associate the candidate to the object and append its target classifier score to the scores and add the object to the set of currently visible object O. If a target candidate matches none of the previously detected objects, we create a new object and add it to O. Hence, previously detected objects that miss a matching candidate are not included in O. Once, all target candidates are associated to an already existing or newly created object. We check if the object previously selected as target is still visible in the current scene and forms the new target?. After the object was lost it is possible that the object selected as target is in fact a distractor. Thus, we select an other object as target if this other object achieves a higher target classifier score in the current frame than any score the currently selected object achieved in the past. Furthermore, if the object previously selected as target object is no longer visible, we try to redetect it by checking if the object with highest target classifier score in the current frame achieves a score higher than a threshold ? = 0.25. If the score is high enough, we select this object as the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Search Area Rescaling at Occlusion</head><p>The target object often gets occluded or moves out-ofview in many tracking sequences. Shortly before the target is lost the tracker typically detects only a small part of the target object and estimates a smaller bounding box than in the frames before. The used base tracker SuperDiMP employs a search area that depends on the currently estimated bounding box size. Thus, a partially visible target object causes a small bounding box and search area. The problem of a small search area is that it complicates redetecting the target object, e.g., the target reappears at a slightly different location than it disappeared and if the object then reappears outside of the search area redetection is prohibited. Smaller search areas occur more frequently when using the target candidate association network because it allows to track the object longer until we declare it as lost.</p><p>Hence, we use a procedure to increase the search area if it decreased before the target object was lost. First, we store all search are resolutions during tracking in an list a as long as the object is detected. If the object was lost k frames ago, we compute the new search area by averaging the last k entries of a larger than the search area at occlu- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Inference Details</head><p>In contrast to training, we use all extracted target candidates to compute the candidate associations between consecutive frames. In order to save computations, we extract the candidates and features only for the current frame and cache the results such that they can be reused when computing the associations in the next frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.1 KeepTrack Settings</head><p>We use the same settings as for SuperDiMP but increase the search area scale from 6 to 8 leading to a larger search are (from 352 ? 352 to 480 ? 480) and to a larger target score map (from 22 ? 22 to 30 ? 30). In addition, we employ the aforementioned search area rescaling at occlusion and skip running the target candidate association network if only one target candidates with high target classifier score is detected in the current and previous frame, in order to save computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 KeepTrackFast Settings</head><p>We use the same settings as for SuperDiMP. In particular, we keep the search area scale and target score map resolution the same to achieve a faster run-time. In addition, we reduce the number of bounding box refinement steps from 10 to 3 which reduces the bounding box regression time significantly. Moreover, we double the target candidate extraction threshold ? to 0.  <ref type="figure">Figure 5</ref>. Success and normalized precision plots on LaSOT <ref type="bibr" target="#b23">[24]</ref>. Our approach outperforms all other methods by a large margin in AUC, reported in the legend.  <ref type="bibr" target="#b23">[24]</ref> test set in terms of overall AUC score. The average value over 5 runs is reported for our approach. The symbol ? marks results that were produced by Fan et al. <ref type="bibr" target="#b23">[24]</ref> otherwise they are obtained directly from the official paper. neglegt local maxima with low target classifier scores and thus leads to less frames with multiple detected candidates. Hence, KeepTrackFast runs the target candidate association network less often than KeepTrack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Detailed Analysis</head><p>In addition to the ablation study presented in the main paper (Sec. 4.1) we provide more settings in order to assess the contribution of each component better. In particular, we split the term search area adaptation into larger search area and search area rescaling. Where larger search area refers to a search area scale of 8 instead of 6 and a search area resolution of 480 instead of 352 in the image domain. Tab. 10 shows all the results on NFS <ref type="bibr" target="#b26">[27]</ref>, UAV123 <ref type="bibr" target="#b48">[49]</ref> and La-SOT <ref type="bibr" target="#b23">[24]</ref>. We run each experiment five times and report the average. We conclude that both search area adaptation techniques improve the tracking quality but we achieve the best results on all three datasets when employing both at the same time. Furthermore, we evaluate the target candidate association network with different numbers of Sinkhorn it-erations and with different number of GNN layers of the embedding network or dropping it at all, see Tab. 11. We conclude, that using the target candidate association network even without any GNN layers outperforms the baseline on all three datasets. In addition, using either two or nine GNN layers improves the performance even further on all datasets. We achieve the best results when using nine GNN layers and 50 Sinkhorn iterations. However, using a large candidate embedding network and a high number of Sinkhorn iterations reduces the run-time of the tracker to 12.7 FPS. Hence, using only two GNN layers and 10 Sinkhorn iterations results in a negligible decrease of 0.1 on UAV123 and LaSOT but accelerates the run-time by 44%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments</head><p>We provide more details to complement the state-of-theart comparison performed in the paper. And provide results for the VOT2018LT <ref type="bibr" target="#b37">[38]</ref>   <ref type="figure">Figure 6</ref>. Success and normalized precision plots on LaSOTExtSub <ref type="bibr" target="#b22">[23]</ref>. Our approach outperforms all other methods by a large margin. <ref type="figure">Figure 7</ref>. Failure Cases: a very challenging case is when a distractor crosses the target's location, since positional information is then of limited use. The box represents the ground truth bounding box of the target object, where green indicates the the selected target candidates corresponds to the sought target and red indicates that the tracker selected a candidate corresponding to a distractor object.</p><formula xml:id="formula_9">Time #134 #135 #140 #142 #143 #76 #77 #87 #92 #93</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. LaSOT and LaSOTExtSub</head><p>In addition to the success plot, we provide the normalized precision plot on the LaSOT <ref type="bibr" target="#b23">[24]</ref> test set (280 videos) and LaSOTExtSub <ref type="bibr" target="#b23">[24]</ref> test set (150 videos). The normalized precision score NPr D is computed as the percentage of frames where the normalized distance (relative to the target size) between the predicted and ground-truth target center location is less than a threshold D. NPr D is plotted over a range of thresholds D ? [0, 0.5]. The trackers are ranked using the AUC, which is shown in the legend. Figs. 5b and 6b show the normalized precision plots. We compare with state-of-the-art trackers and report their success (AUC) in Tab. 12 and where available we show the raw results in <ref type="figure">Fig. 5</ref>. In particular, we use the raw results provided by the authors except for DaSiamRPN <ref type="bibr" target="#b72">[73]</ref>, GlobaTrack <ref type="bibr" target="#b34">[35]</ref>, SiamRPN++ <ref type="bibr" target="#b41">[42]</ref> and SiamMask <ref type="bibr" target="#b57">[58]</ref> such results were not provided such that we use the raw results produced by Fan et al. <ref type="bibr" target="#b23">[24]</ref>. Thus, the exact results for these methods might be different in the plot and the  <ref type="table" target="#tab_2">Table 13</ref>. Comparison with state-of-the-art on the OTB-100 <ref type="bibr" target="#b58">[59]</ref>, NFS <ref type="bibr" target="#b26">[27]</ref> and UAV123 <ref type="bibr" target="#b48">[49]</ref> datasets in terms of overall AUC score. The average value over 5 runs is reported for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. VOT2018LT [39]</head><p>Next, we evaluate our tracker on the 2018 edition of the VOT <ref type="bibr" target="#b40">[41]</ref> long-term tracking challenge. We compare with the top methods in the challenge <ref type="bibr" target="#b38">[39]</ref>, as well as more recent methods. The dataset contains 35 videos with 4200 frames per sequence on average. Trackers are required to predict a confidence score that the target is present in addition to the bounding box for each frame. Trackers are ranked by the F-score, evaluated for a range of confidence thresholds. As shown in Tab. 14, our tracker achieves the best results in all three metrics and outperforms the base tracker SuperDiMP by almost 10% in F-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Speed Analysis</head><p>Our method adds an overhead of 19.3ms compared to the baseline tracker. Whereas target candidate extraction is <ref type="table">Table 14</ref>. Results on the VOT2018LT dataset <ref type="bibr" target="#b38">[39]</ref> in terms of F-Score, Precision and Recall. required for every frame, running the target candidate association network is only required if more than one candidate is detected. Candidate extraction is relatively fast and takes only 2.7 ms while performing candidate association requires 16.6 ms. Where computing the candidate embedding takes 10.0 ms, running the Sinkhorn algorithm for 10 iterations takes 3.5 ms and object association 3.1 ms. Thus, we achieve an average run-time of 18.3 FPS for KeepTrack and 29.6 FPS for KeepTrackFast when using SuperDiMP as base tracker. We report this timings on a singe Nvidia GTX 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Number of Candidates and Time Complexity</head><p>In practice, we found the number of target candidates to vary from 0 up to 15 in exceptional cases. For frames with less than 2 candidates, we naturally do not need to apply our association module. Further, we observed no measurable increase in run-time from 2 to 15 candidates, due to the effective parallelization. Therefore, we do not explicitly limit the number of detected candidates.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Failure Cases</head><p>While KeepTrack is particularly powerful when distractor objects appear in the scene, it also fails to track the target object in complex scenes, such as the examples shown in <ref type="figure">Fig. 7</ref>.</p><p>The top row shows such a challenging case, where the target object is the right hand of the person on the right (indicated by [ ] or [ ]). KeepTrack manages to individually track all hands ( , , ) visible in the search area until frame number 134. In the next frame, both hands of the person are close and our tracker only detects one candidate for both hands ( ). Thus, the tracker assigns the target id to the remaining candidate. The tracker detects two candidates ( , ) as soon as both hands move apart in frame 143. However, now it is unclear which hand is the sought target. If two objects approach each other it is unclear whether they cross each other or not. In this scenario positional information is of limited use. Hence, deeper understanding of the scene and the target object seems necessary to mitigate such failure cases.</p><p>The bottom row in <ref type="figure">Fig. 7</ref> shows a similar failure case where again a distractor object ( ) crosses the target's ( ) location. This time, the tracker fails to extract the candidate corresponding to the target from frame number 77 on wards. The tracker detects that the target candidate previously assumed to represent the target has vanished but the remaining distractor object ( ) achieves such a high target score that the tracker reconsiders its previous target selection and continues tracking the distractor object ( ) instead. Thus, the tracker continues tracking the distractor object even if the a target candidate for the sought target ( ) appears (frame number 93).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Attributes</head><p>Tabs. 15 and 16 show the results of various trackers including KeepTrack and KeepTrackFast based on different sequence attributes. We observe that both trackers are superior to other trackers on UAV123 for most attributes. In particular, we outperform the runner-up by a large margin in terms of AUC on the sequences corresponding to the following attributes: Aspect Ratio Change (+2.5/2.6%), Full Occlusion (+1.8/1.9%), Partial Occlusion (+2.5/2.3%), Background Clutter (+1.5/1.3%), Illumination Variation (+1.7/1.5%), Similar Object (+1.8/1.1%). Especially, the superior performance on sequences with the attributes Full Occlusion, Partial Occlusion, Background Clutter and Similar Object clearly demonstrates that KeepTrack mitigates the harmful effect of distractors and allows to track the target object longer and more frequently than other trackers. <ref type="figure">Fig. 8a</ref> shows a similar picture: KeepTrack is the most robust tracker but others achieve a higher bounding box regression accuracy. In addition, Tab. 15 reveals that Keep-Track achieves the highest (red) or second-highest (blue) AUC on the sequences corresponding to each attribute except for the attribute Out-of-View.</p><p>The attribute-based analysis on LaSOT allows similar observations. In particular, KeepTrack and KeepTrack-Fast outperform all other trackers by a large margin in AUC on the sequences corresponding oth the following attributes: Partial Occlusion (+1.8/1.5%), Background Clutter (+2.3/1.2, Viewpoint Change (+1.6/2.3%), Full Occlusion (+2.7/1.8%), Fast Motion (4.1/3.5%), Out-of-View (+1.9/1.2%), Low Resolution (+3.4/3.4%). Moreover, the superior performance is even clearer when comparing to the base tracker SuperDiMP, e.g., and improvement of +6.0/5.2% for Full Occlusion or +7.0/6.4% for Fast Motion. KeepTrack achieves the highest AUC score for every attributed except two where KeepTrackFast achieves slightly higher scores. Again, the best performance on sequences with attributes such as Background Clutter or Full Occlusion clearly demonstrates the effectiveness of our proposed target and distractor association strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visual comparison of the base tracker and our tracker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 3 : 5 : 8 : 9 : 12 :</head><label>1358912</label><figDesc>Object Association Algorithm. Require: Set of target candidates V Require: Set of objects of previous frame O Require: Target object? 1:O = {}, N = |V| 2: for i = 1, . . . , N do if matchOf(v i ) = dustbin &amp;&amp; p(v i ) ? ? then 4: v j ? matchOf(v i ) (o j ).s ? concat((o j ).s, [s i ]) o i ? newObject(getNewId(), [s i ]) O ? O ? {o i } 10: if? = none and? .id ? {o.id | o ? O} then 11:? = getObjectById(O,? .id) for i = 1, . . . , N do 13: if max(?.s) &lt; (o i ).s[?1] then 14:? = o i 15: else 16: i = argmax i {(o i ).s[?1]) | o i ? O} 17: if (o i ).s[?1] ? ? then 18:? = o i 19:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>else 20 :</head><label>20</label><figDesc>? = none 21: return?, O size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 3 .</head><label>13</label><figDesc>Impact of each component in terms of AUC (%) on three datasets. The first row corresponds to our SuperDiMP baseline. Analysis of our memory weighting component on LaSOT.</figDesc><table><row><cell cols="4">Memory Sample Search area Target Candidate</cell><cell></cell></row><row><cell>Confidence</cell><cell cols="5">Adaptation Association Network NFS UAV123 LaSOT</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>64.7 68.0</cell><cell>65.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>65.2 69.1</cell><cell>65.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.4 69.7</cell><cell>67.1</cell></row><row><cell>Data-mining</cell><cell></cell><cell>n.a.</cell><cell></cell><cell>-</cell></row><row><cell cols="2">LaSOT, AUC (%)</cell><cell>65.8</cell><cell>66.0 66.9</cell><cell>66.8</cell><cell>67.</cell></row></table><note>Loss no TCA L sup L self L sup + L self L sup + L self1 Table 2. Analysis on LaSOT of association learning using different loss functions with and without data-mining.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Results on the LaSOT [24] test set. All trackers use the same base tracker SuperDiMP [13].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Results on the OxUvALT<ref type="bibr" target="#b53">[54]</ref> test set in terms of TPR, TNR, and the max geometric mean (MaxGM) of TPR and TNR.</figDesc><table><row><cell></cell><cell cols="2">Keep Keep</cell><cell></cell><cell>Super</cell><cell>Siam</cell><cell></cell><cell>DM</cell><cell></cell><cell>Global</cell><cell></cell><cell>Siam</cell></row><row><cell></cell><cell cols="12">Track Track LTMU DiMP R-CNN TACT Track SPLT Track MBMD FC+R TLD</cell></row><row><cell></cell><cell></cell><cell>Fast</cell><cell>[12]</cell><cell>[13]</cell><cell>[55]</cell><cell>[9]</cell><cell>[71]</cell><cell>[63]</cell><cell>[35]</cell><cell>[69]</cell><cell>[54]</cell><cell>[36]</cell></row><row><cell>TPR</cell><cell>80.6</cell><cell>82.7</cell><cell>74.9</cell><cell>79.7</cell><cell>70.1</cell><cell>80.9</cell><cell>68.6</cell><cell>49.8</cell><cell>57.4</cell><cell>60.9</cell><cell cols="2">42.7 20.8</cell></row><row><cell>TNR</cell><cell>81.2</cell><cell>77.2</cell><cell>75.4</cell><cell>70.2</cell><cell>74.5</cell><cell>62.2</cell><cell>69.4</cell><cell>77.6</cell><cell>63.3</cell><cell>48.5</cell><cell cols="2">48.1 89.5</cell></row><row><cell cols="2">MaxGM 80.9</cell><cell>79.9</cell><cell>75.1</cell><cell>74.8</cell><cell>72.3</cell><cell>70.9</cell><cell>68.8</cell><cell>62.2</cell><cell>60.3</cell><cell>54.4</cell><cell cols="2">45.4 43.1</cell></row><row><cell></cell><cell cols="2">Keep Keep</cell><cell></cell><cell></cell><cell cols="2">Mega</cell><cell></cell><cell cols="2">RLT Super</cell><cell>Siam</cell><cell></cell></row><row><cell></cell><cell cols="12">Track Track LT DSE LTMU B track CLGS DiMP DiMP DW LT ltMBNet</cell></row><row><cell></cell><cell></cell><cell cols="4">Fast [40, 38] [12, 38]</cell><cell cols="3">[38] [40, 38] [38]</cell><cell cols="3">[13] [40, 38]</cell><cell>[38]</cell></row><row><cell cols="2">Precision 72.3</cell><cell>70.6</cell><cell>71.5</cell><cell cols="2">70.1</cell><cell>70.3</cell><cell>73.9</cell><cell>65.7</cell><cell>67.6</cell><cell>69.7</cell><cell></cell><cell>64.9</cell></row><row><cell>Recall</cell><cell>69.7</cell><cell>68.0</cell><cell>67.7</cell><cell cols="2">68.1</cell><cell>67.1</cell><cell>61.9</cell><cell>68.4</cell><cell>66.3</cell><cell>63.6</cell><cell></cell><cell>51.4</cell></row><row><cell>F-Score</cell><cell>70.9</cell><cell>69.3</cell><cell>69.5</cell><cell cols="2">69.1</cell><cell>68.7</cell><cell>67.4</cell><cell>67.0</cell><cell>66.9</cell><cell>66.5</cell><cell></cell><cell>57.4</cell></row><row><cell cols="13">Table 7. Results on the VOT2019LT [40]/VOT2020LT [38] dataset</cell></row><row><cell cols="8">in terms of F-Score, Precision and Recall.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Keep Keep</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Super Pr STM Siam Siam</cell><cell></cell></row><row><cell></cell><cell cols="12">Track Track CRACT TrDiMP TransT DiMP DiMP Track AttN R-CNN KYS DiMP</cell></row><row><cell></cell><cell></cell><cell>Fast</cell><cell>[25]</cell><cell>[57]</cell><cell>[7]</cell><cell cols="5">[13] [18] [26] [66] [55]</cell><cell>[4]</cell><cell>[3]</cell></row><row><cell cols="3">UAV123 69.7 69.5</cell><cell>66.4</cell><cell>67.5</cell><cell cols="6">69.1 68.1 68.0 64.7 65.0 64.9</cell><cell>-</cell><cell>65.3</cell></row><row><cell cols="3">OTB-100 70.9 71.2</cell><cell>72.6</cell><cell>71.1</cell><cell cols="8">69.4 70.1 69.6 71.9 71.2 70.1 69.5 68.4</cell></row><row><cell>NFS</cell><cell cols="2">66.4 65.3</cell><cell>62.5</cell><cell>66.2</cell><cell cols="3">65.7 64.7 63.5</cell><cell>-</cell><cell>-</cell><cell cols="3">63.9 63.5 62.</cell></row></table><note>0 Table 8. Comparison with state-of-the-art on the OTB-100 [59], NFS [27] and UAV123 [49] datasets in terms of AUC score.shows the results, where the entries correspond to AUC for OP T over IoU thresholds T . Our method sets a new state- of-the-art with an AUC of 69.7%, exceeding the perfor- mance of the recent trackers TransT [7] and TrDiMP [57] by 0.6% and 2.2% in AUC. OTB-100 [59]: For reference, we also evaluate our tracker on the OTB-100 dataset consisting of 100 sequences. Sev- eral trackers achieve tracking results over 70% in AUC, as shown in Tab. 8. So do KeepTrack and KeepTrackFast that perform similarly to the top methods, with a 0.8% and 1.1% AUC gain over the SuperDiMP baseline. NFS [27]: Lastly, we report results on the 30 FPS ver- sion of the Need for Speed (NFS) dataset. It contains fast motions and challenging distractors. Tab. 8 shows that our approach sets a new state-of-the-art on NFS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 10. Impact of each component in terms of AUC (%) on three datasets. The first row corresponds to our SuperDiMP baseline.</figDesc><table><row><cell cols="5">Memory Larger Search Candidate</cell><cell></cell></row><row><cell cols="2">Memory Search</cell><cell cols="3">Area Association</cell><cell></cell></row><row><cell cols="5">Confidence Area Rescaling Network</cell><cell cols="2">NFS UAV123 LaSOT</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>64.4 68.2</cell><cell>63.5</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>64.7 68.0</cell><cell>65.0</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>65.3 68.4</cell><cell>65.5</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>64.7 68.4</cell><cell>65.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>65.2 69.1</cell><cell>65.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.4 69.7</cell><cell>67.1</cell></row><row><cell cols="3">Num GNN Num Sinkhorn</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layers</cell><cell cols="2">iterations</cell><cell cols="3">NFS UAV123 LaSOT</cell><cell>FPS</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>65.2</cell><cell>69.1</cell><cell>65.8</cell><cell>-</cell></row><row><cell>0</cell><cell></cell><cell>50</cell><cell>65.9</cell><cell>69.2</cell><cell>66.6</cell><cell>-</cell></row><row><cell>2</cell><cell></cell><cell>10</cell><cell>66.4</cell><cell>69.7</cell><cell>67.1</cell><cell>18.3</cell></row><row><cell>9</cell><cell></cell><cell>50</cell><cell>66.4</cell><cell>69.8</cell><cell>67.2</cell><cell>12.7</cell></row><row><cell cols="7">Table 11. Impact of each component of the Target Candidate As-</cell></row><row><cell cols="7">sociation Network in terms of AUC (%) on three datasets.</cell></row><row><cell cols="7">sion. We average at most over 30 previous search areas to</cell></row><row><cell cols="7">compute the new one. If the target object was not redetected</cell></row><row><cell cols="7">within these 30 frames with keep the search area fixed until</cell></row><row><cell>redetection.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 .</head><label>12</label><figDesc>Comparison with state-of-the-art on the LaSOT</figDesc><table><row><cell cols="2">Keep Keep Alpha</cell><cell></cell><cell>Siam</cell><cell>Tr</cell><cell cols="2">Super STM</cell><cell>Pr</cell><cell>DM</cell><cell></cell><cell></cell><cell></cell><cell>Siam</cell></row><row><cell cols="13">Track Track Refine TransT R-CNN DiMP Dimp Track DiMP Track TLPG TACT LTMU DiMP Ocean AttN</cell></row><row><cell></cell><cell>Fast [62]</cell><cell>[7]</cell><cell>[55]</cell><cell>[57]</cell><cell>[13]</cell><cell>[26]</cell><cell cols="2">[18] [71] [44]</cell><cell>[9]</cell><cell>[12]</cell><cell>[3]</cell><cell>[70]</cell><cell>[66]</cell></row><row><cell>LaSOT 67.1</cell><cell cols="2">66.8 65.3 64.9</cell><cell>64.8</cell><cell>63.9</cell><cell>63.1</cell><cell>60.6</cell><cell cols="2">59.8 58.4 58.1</cell><cell>57.5</cell><cell>57.2</cell><cell>56.9</cell><cell>56.0</cell><cell>56.0</cell></row><row><cell></cell><cell>Siam Siam</cell><cell>PG</cell><cell cols="2">FCOS Global</cell><cell></cell><cell cols="3">DaSiam Siam Siam</cell><cell cols="3">Siam Retina Siam</cell></row><row><cell cols="13">CRACT FC++ GAT NET MAML Track ATOM RPN BAN CAR CLNet RPN++ MAML Mask ROAM++ SPLT</cell></row><row><cell>[25]</cell><cell>[61] [30]</cell><cell>[47]</cell><cell>[56]</cell><cell>[35]</cell><cell>[14]</cell><cell>[73]  ?</cell><cell cols="2">[8] [31] [21]</cell><cell>[42]  ?</cell><cell cols="2">[56] [58]  ?</cell><cell>[64]</cell><cell>[63]</cell></row><row><cell>LaSOT 54.9</cell><cell cols="2">54.4 53.9 53.1</cell><cell>52.3</cell><cell>52.1</cell><cell>51.5</cell><cell>51.5</cell><cell cols="2">51.4 50.7 49.9</cell><cell>49.6</cell><cell>48.0</cell><cell>46.7</cell><cell>44.7</cell><cell>42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>table, because we show in the table the reported result the corresponding paper. Similarly, we obtain all results on LaSOTExtSub directly from Fan et al. [23] except the result of SuperDiMP that we produced.</figDesc><table><row><cell></cell><cell>100</cell><cell>Success plot</cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell>Success plot</cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell>Success plot</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Overlap Precision [%]</cell><cell>0 0 20 40 60</cell><cell>0.2 KeepTrack [69.7] 0.4 Overlap threshold 0.6 KeepTrackFast [69.5] TransT [69.1] PrDiMP50 [68.0] SuperDiMP [67.7] TrDiMP [67.5] STMTrack [65.7] DiMP50 [65.3] SiamRPN++ [65.2] ATOM [64.2] DaSiamRPN [57.7] UPDT [54.5] ECO [53.2] CCOT [51.3]</cell><cell>0.8</cell><cell>1</cell><cell>Overlap Precision [%]</cell><cell>0 0 20 40 60</cell><cell>0.2 KeepTrackFast [71.2] 0.4 Overlap threshold 0.6 KeepTrack [70.9] TrDiMP [70.7] UPDT [70.4] SuperDiMP [70.1] SiamRPN++ [69.6] PrDiMP50 [69.6] ECO [69.1] DiMP50 [68.4] CCOT [68.2] DaSiamRPN [65.8] ATOM [66.3] MDNet [67.8]</cell><cell>0.8</cell><cell>1</cell><cell>Overlap Precision [%]</cell><cell>0 0 20 40 60</cell><cell>0.2 KeepTrack [66.4] 0.4 Overlap threshold 0.6 TrDiMP [66.2] TransT [65.3]</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>(a) UAV123</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) OTB-100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 16 .</head><label>16</label><figDesc>LaSOT attribute-based analysis. Each column corresponds to the results computed on all sequences in the dataset with the corresponding attribute.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was partly supported by the ETH Z?rich Fund (OK), Siemens Smart Infrastructure, the ETH Future Computing Laboratory (EFCL) financed by a gift from Huawei Technologies, an Amazon AWS grant, and an Nvidia hardware grant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. UAV123, OTB-100 and NFS</head><p>We provide the success plot over the 123 videos of the UAV123 dataset <ref type="bibr" target="#b48">[49]</ref> in <ref type="figure">Fig. 8a</ref>, the 100 videos of the OTB-100 dataset <ref type="bibr" target="#b58">[59]</ref> in <ref type="figure">Fig. 8b</ref> and the 100 videos of the NFS dataset <ref type="bibr" target="#b26">[27]</ref> in <ref type="figure">Fig. 8c</ref>. We compare with state-of-theart trackers SuperDiMP <ref type="bibr" target="#b12">[13]</ref>, PrDiMP50 <ref type="bibr" target="#b17">[18]</ref>, UPDT <ref type="bibr" target="#b4">[5]</ref>, SiamRPN++ <ref type="bibr" target="#b41">[42]</ref>, ECO <ref type="bibr" target="#b14">[15]</ref>, DiMP <ref type="bibr" target="#b2">[3]</ref>, CCOT <ref type="bibr" target="#b16">[17]</ref>, MD-Net <ref type="bibr" target="#b49">[50]</ref>, ATOM <ref type="bibr" target="#b13">[14]</ref>, and DaSiamRPN <ref type="bibr" target="#b72">[73]</ref>. Our method provides a significant gain over the baseline SuperDiMP on UAV123 and NFS and performs among the top methods on OTB-100. Tab. 13 shows additional results on UAV123, OTB-100 and NFS in terms of success (AUC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops (ECCVW)</title>
		<meeting>the European Conference on Computer Vision Workshops (ECCVW)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10-01" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Know your surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Braso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Siamese box adaptive network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zedu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual tracking by tridentalign and context embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-performance longterm tracking with meta-updater</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06-02" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">PyTracking: Visual tracking library based on PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<ptr target="https://github.com/visionml/pytracking" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Accessed: 1/08/2020. 2, 3, 7, 8, 13, 15</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ATOM: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ECO: efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>H?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06-02" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motchallenge: A benchmark for single-camera multiple target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clnet: A compact latent network for fast adjusting siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">D2-net: A trainable cnn for joint description and detection of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality large-scale single object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juehuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06-01" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cract: Cascaded regressionalign-classification for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12483</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stmtrack: Template-free visual tracking with space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph convolutional tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph attention tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Siamcar: Siamese fully convolutional classification and regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How to train your energy-based model for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fredrik K Gustafsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Globaltrack: A simple and strong baseline for long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020-02-08" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking-learning-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdenek</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The eighth visual object tracking vot2020 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni-Kristian</forename><surname>K?m?r?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka?ehovin</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Luke?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Drbohlav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Fern?ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops</title>
		<meeting>the European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<publisher>ECCVW</publisher>
			<date type="published" when="2020-08" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pfugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops (ECCVW)</title>
		<meeting>the European Conference on Computer Vision Workshops (ECCVW)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The seventh visual object tracking vot2019 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jir?</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni-Kristian</forename><surname>K?m?r?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Drbohlav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jani</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>K?pyl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fern?ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), October 2019</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), October 2019</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka?ehovin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06-01" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tlpg-tracker: Joint learning of target localization and proposal generation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linglong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Autotrack: Towards high-performance visual tracking for uav with automatic spatio-temporal regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangqiang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Megadepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pg-net: Pixel to global matching network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaonong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Object tracking using spatio-temporal networks for future prediction location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoteng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiubao</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016-10-02" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Concerning nonnegative matrices and doubly stochastic matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Knopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="348" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Long-term tracking in the wild: a benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Siam R-CNN: Visual tracking by redetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06-02" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tracking by instance detection: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Transformer meets tracker: Exploiting temporal context for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distractor-supported single target tracking in extremely cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rustam</forename><surname>Stolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alevs</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020-02" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Alpha-refine: Boosting tracking performance by precise bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">skimming-perusal&apos; tracking: A framework for real-time and robust long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Roam: Recurrently optimizing tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multiple target tracking using spatio-temporal markov chain monte carlo data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">MEEM: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning regression and verification networks for long-term visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-08-07" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Distractor-aware fast tracking via dynamic convolutions and mot philosophy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning feature embeddings for discriminant model based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09-01" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keeptrackfast</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">OTB-100 [59] and NFS [27] datasets in terms of overall AUC score, reported in the legend</title>
	</analytic>
	<monogr>
		<title level="m">Keep Keep Tr Super Pr Siam STM Siam Retina FCOS Track Track DiMP TransT DiMP DiMP R-CNN Track DiMP KYS RPN++ ATOM UPDT MAML MAML Ocean STN Fast</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
	<note>Figure 8. Success plots on the UAV123 [49</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auto</forename><surname>Siam Siam Siam Siam Siam Dasiam Track</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ban Car Eco Dcfst Pg-Net Cract Gct Gat</forename><surname>Clnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tlpg</forename><surname>Attn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fc++</forename><surname>Mdnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ccto</forename><surname>Rpn Ecohc</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>45</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">UAV123 attribute-based analysis in terms of AUC score. Each column corresponds to the results computed on all sequences in the dataset with the corresponding attribute. Illumination Partial Motion Camera Background Viewpoint Scale Full Fast Low Aspect Variation Occlusion Deformation Blur Motion Rotation Clutter Change Variation Occlusion Motion Out-of-View Resolution Ration Change Total</title>
	</analytic>
	<monogr>
		<title level="m">Table 15</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
