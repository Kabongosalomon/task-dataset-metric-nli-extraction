<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
							<email>yanxu@link.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
							<email>zhangguofeng@cad.zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>6-DoF object pose estimation from a monocular image is challenging, and a post-refinement procedure is generally needed for high-precision estimation. In this paper, we propose a framework based on a recurrent neural network (RNN) for object pose refinement, which is robust to erroneous initial poses and occlusions. During the recurrent iterations, object pose refinement is formulated as a nonlinear least squares problem based on the estimated correspondence field (between a rendered image and the observed image). The problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm enabling endto-end training. The correspondence field estimation and pose refinement are conducted alternatively in each iteration to recover the object poses. Furthermore, to improve the robustness to occlusion, we introduce a consistencycheck mechanism based on the learned descriptors of the 3D model and observed 2D images, which downweights the unreliable correspondences during pose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and YCB-Video datasets validate the effectiveness of our method and demonstrate state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>6-DoF object pose estimation is of crucial importance in various applications, including augmented reality, robotic manipulation, and autonomous driving. Influenced by varying illuminations and occlusions, appearances of the differently posed objects may vary significantly from different views, which poses great challenges for 6-DOF object pose estimation from a single color image.</p><p>The recent top-performing methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59]</ref> additionally include a pose refinement procedure which substantially improves the performance. Some of these frameworks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53]</ref> rely on depth sensors and refine the poses with the ICP algorithm <ref type="bibr" target="#b1">[2]</ref>. To avoid the expensive depth sensor, Li et al. <ref type="bibr" target="#b22">[23]</ref> and Manhardt et al. <ref type="bibr" target="#b26">[27]</ref> pioneered the RGB-based pose refinement. During refinement, these methods first render a reference color image according to K. <ref type="bibr">Lin</ref>   <ref type="figure">Figure 1</ref>. The basic idea. (a) Before refinement, a reference image is rendered according to the object initial pose (shown in a fused view). (b) Our RNN-based framework recurrently refines the object pose based on the estimated correspondence field between the reference and target images. The pose is optimized to be consistent with the reliable correspondence estimations highlighted by the similarity score map (built from learned 3D-2D descriptors) via differentiable LM optimization. (c) The output refined pose.</p><p>the coarse pose estimate. This rendered image along with the observed image is then fed to a CNN to directly predict the residual pose for refining the coarse pose <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b58">59]</ref>. While these methods perform well in ideal scenarios based on massive training data, the pose regression becomes less stable in practice. More recently, Iwase et al. <ref type="bibr" target="#b19">[20]</ref> formulated the object pose refinement as an optimization problem based on feature alignment, and reported significant performance improvements. In their work, the encoded features of a 3D model by a neural network are projected to the 2D image plane according to the pose parameters. Thereafter, the pose optimization is conducted by aligning the projected features with the observed target image features.</p><p>As the pose optimization depends on the gradients from the pixel-level feature differences, the feature alignment based methods are only applicable to small inter-frame pose variations <ref type="bibr" target="#b57">[58]</ref> and are not quite robust with erroneous initial poses. Moreover, Iwase et al. <ref type="bibr" target="#b19">[20]</ref> still have a limited design for occlusion handling, which might limit the deployment scope.</p><p>In this work, we propose a recurrent object pose refine-ment framework, dubbed RNNPose, which is robust to erroneous initial poses and occlusions. The overall pipeline is illustrated in <ref type="figure">Fig. 1</ref>. Before refinement, a reference image of the object is rendered according to the initial pose estimation. Our refinement module refines the initial pose based on this rendered image and the observed image. To increase the tolerance to erroneous initial poses, our refinement is conducted within a recurrent framework, where the pose optimization is formulated as a non-linear least squares problem based on estimated correspondence fields. In each recurrent iteration, the dense correspondences between the rendered image and observed image are estimated, and the object pose is then optimized to be consistent with the correspondence field estimation. The architecture of our correspondence estimation is inspired by the recent optical flow estimation techniques <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref>, which is integrated with our pose optimization recurrently. To suit our task where unpatterned objects and illumination variations are ubiquitous, we further include a correspondence field rectification step in each recurrent iteration based on the currently optimized pose. The inconsistent correspondences are rectified by enforcing rigid-transformation constraints. The rectified correspondence field is also used to initialize the next recurrent iteration to improve the robustness further. For occlusion handling, we introduce a 3D-2D hybrid network trained with a contrastive loss, which generates distinctive point-wise descriptors for the 3D object model and observed 2D images. A similarity score is constructed for each estimated correspondence pair based on the learned descriptors, with which to downweight the unreliable correspondences during pose optimization. The pose optimization is conducted by a differentiable Levenberg-Marquardt (LM) algorithm (sharing the ideas of <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>) for end-toend training.</p><p>Our contributions are three-fold: 1) We propose an RNN-based 6-DoF pose refinement framework that is robust to large initial pose errors and occlusions. During recurrent iterations, the pose optimization is formulated as a non-linear least squares problem based on the estimated correspondence field. Meanwhile, the correspondence field is also being rectified and improved by the optimized pose for robustness. 2) To handle the occlusions, a 3D-2D hybrid network is introduced to learn point-wise descriptors which are used to downweight unreliable correspondence estimations during pose optimization. 3) We achieve new state-of-the-art performances on LINEMOD, Occlusion LINEMOD, and YCB-Video datasets. Our code is public at https://github.com/DecaYale/RNNPose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>6-DoF object pose estimation. 6-DoF object pose estimation systems (usually going after an object detector <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b59">60]</ref>) aim to estimate the 3-DoF orientations and 3-DoF locations of rigid objects. The boom of deep learning has significantly improved object pose estimation in recent years. Some methods proposed to directly regress object poses from monocular color images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref> or with the aid from depth sensors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. They leveraged CNNs' regression ability to directly map the observed images to object poses. More recently, correspondence-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b44">45]</ref> become more popular. They employed CNNs to estimate the corresponding 3D model point for each observed object pixel, and then solve for poses with PnP <ref type="bibr" target="#b9">[10]</ref>. These methods may estimate the object's bounding box corners <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref>, predict dense 2D-3D correspondence maps <ref type="bibr" target="#b32">[33]</ref> or vote the keypoints by all object pixels <ref type="bibr" target="#b33">[34]</ref>. More recently, EPOS <ref type="bibr" target="#b14">[15]</ref> proposed to handle symmetric objects by segmenting 3D models into patches and estimating the patch centers. The above direct object pose estimation methods usually become less stable when varying illuminations and occlusions exist. Several methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59]</ref> hence conducted pose refinement based on the estimated coarse initial pose above, which achieved significant performance gains. Some of these methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53]</ref> relied on depth data from costly sensors and utilized ICP to align the known object model to the observed depth image. While <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b58">59]</ref> first rendered a 2D object image according to the initial pose and then compared the rendered image with the observed image via a CNN to estimate the residual pose. These RGB-based methods are especially attractive due to their economical nature. However, most of these methods need massive training data and are not quite robust in practical scenarios. Moreover, they need a cumbersome CNN for pose regression, which sacrifices efficiency. Iwase et al. <ref type="bibr" target="#b19">[20]</ref> proposed to alleviate such dilemma by reusing the images features extracted by CNN and attained real-time processing. Concretely, they employed the CNN as an image feature encoder, based on which to formulate a nonlinear optimization problem to align the features from the inference and target images for pose refinement inspired by BA-Net <ref type="bibr" target="#b41">[42]</ref>. Though efficient, their formulation is built upon overlapped object regions across the reference image and target image, which may thus be less stable with erroneous initial pose inputs. The previous work <ref type="bibr" target="#b0">[1]</ref> proposed to refine the pose based on the correspondences, but their method is still limited to ideal scenarios.</p><p>Non-linear least squares optimization with deep learning. Non-linear least squares optimization algorithms, such as Gauss-Newton <ref type="bibr" target="#b29">[30]</ref> and Levenberg-Marquardt <ref type="bibr" target="#b30">[31]</ref>, are widely used in computer vision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54]</ref>, given their efficient and effective nature. Recently, the differentiability of the optimization algorithm itself has been widely studied and several works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref> have included the differentiable optimization algorithm during the network training for localization systems and visual SLAMs. These inspire our formulation for object pose refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given an observed object image I obs , an initial object pose estimate P init and the object's CAD model M as inputs, a 6-DoF pose refinement system aims to further im-  prove the object pose estimation. In this paper, we propose a recurrent pose refinement method, dubbed RNNPose, which is robust to erroneous initial poses and occlusions. Our method is based on a rendering pipeline and may have several rendering cycles as illustrated by <ref type="figure" target="#fig_0">Fig. 2</ref>. At the beginning of the first rendering cycle, a reference image I ref is rendered with the object's CAD model according to its initial pose P init (estimated by any direct methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b52">53]</ref>). Then, the rendered reference image, the observed target image, and the vertices of the CAD model are encoded as high-dimensional features which will be used to estimate the correspondences (between the rendered image and observed image) in the follow-up pose refinement module. The pose refinement module constitutes our major contribution, where we formulate an optimization problem based on the correspondence estimations. We integrate correspondence field estimation and pose refinement into a recurrent framework for robustness and efficiency. To handle occlusions, we generate point-wise distinctive descriptors for the 3D object model and observed images with a 3D-2D hybrid network, with which to downweight the unreliable correspondences during pose optimization. After every several recurrent iterations, the reference image I ref is re-rendered with the currently optimized pose to decrease the pose gap to the target for the next cycle.</p><formula xml:id="formula_0">Correspondence Field Estimation ! ! $ ! Rectified Correspondence Field ! ! " $ = $ # $%$! Vertices GRU ! ? !"# ! ? ! Differentiable</formula><p>In the ensuing subsections, we will detail the feature extraction (Sec. 3.1), recurrent pose refinement (Sec. 3.2), and the loss functions for training (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">2D-3D Feature Encoding and Rendering</head><p>The rendered reference image I ref and observed target image I obs first need to be encoded into high-dimensional feature maps F ref and F obs for the follow-up feature correlation volume construction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>. The correlation volume encodes the appearance similarities between image pixels, which is essential for correspondence reasoning. In our work, we adopt several residual blocks <ref type="bibr" target="#b11">[12]</ref> for image feature encoding, and the pair-wise correlations of the encoded features are calculated to create a global correlation volume. The global correlation volume will be frequently queried for correspondence field estimation in the followup pose refinement module.</p><p>Besides the pair-wise correlation volume, popular dense correspondence estimation methods also incorporate context features of the reference image for guidance. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, to better encode the geometric contexts, unlike previous methods encoding the context features from 2D images, we directly encode the features from 3D object point clouds with a 3D context feature encoder based on KPConv <ref type="bibr" target="#b45">[46]</ref>. The point-wise geometric features are then rendered as a 2D context feature map F ctx according to the initial object pose estimation. Here, we adopt a differentiable renderer <ref type="bibr" target="#b35">[36]</ref> for feature rendering to enable geometric feature learning. We empirically found that encoding the context features from point clouds brings more robustness. Besides, the vertex features only need to be extracted once per object model and archived for inference after training, which is quite efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recurrent Correspondence Field Estimation and 6-DoF Pose Refinement</head><p>Based on the constructed correlation volume and encoded context features, we propose a 6-DoF object pose refinement system by integrating the correspondence estimation and pose optimization as a recurrent framework. The correspondence field estimation and pose optimization rely on each other and improve recurrently for robust pose refinement. The basic pipeline is illustrated in the pose refinement module in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Correspondence Field Estimation</head><p>For correspondence field estimation, we adopt a network architecture similar to RAFT <ref type="bibr" target="#b43">[44]</ref> but make major modifications to suit our task, i.e., including the 3D context feature encoding (Sec. 3.1) and correspondence rectification (Sec. 3.2.2). At the beginning of each recurrent iteration, for each pixel of the reference image, we first look up and collect (from the global correlation volume) its correlation values with the candidate pixels in the target image. The candidate pixels are within a square local window centered at the estimated correspondences from the previous iteration. The collected correlations are then reshaped as a local correlation volume (a 2D map) spatially aligned with the reference image. In the first iteration, we use an allzeros correspondence field to bootstrap correlation candidate identification, while in the later iterations, the rectified correspondence field (to be elaborated in Sec. 3.2.2) is used.</p><p>After the correlation lookup, the collected local correlation volume, the rectified correspondence field, and the previously encoded context feature map F ctx are concatenated as inputs to a GRU network to estimate the correspondence field? t for the current (t-th) recurrent iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">6-DoF Pose Refinement</head><p>Basic Formulation. Given a reference image (with depth map) and a target image, the ground-truth correspondence field of the reference image can be derived based on the ground-truth residual pose ?P gt point-wisely:</p><formula xml:id="formula_1">C(x i ; ?P gt ) = ?(?P gt ? ?1 (x i , z i )),<label>(1)</label></formula><p>where C(x i ; ?P gt ) ? R 2 denotes the ground-truth correspondence field value of point x i , and z i denotes the associated rendered depth value. Here, ?(?) and ? ?1 (?; z i ) are the projection (3D-to-2D) and inverse projection (2Dto-3D) functions of a pinhole camera model. To estimate the residual pose, we take the correspondence field? t estimated by the GRU as an approximation of its ground-truth, i.e.,? t (x i ) ? C(x i ; ?P gt ), and push the correspondence field derived by the pose argument ?P, i.e., C(x i ; ?P), close to the GRU's estimation by optimizing ?P. In this way, the residual pose parameter ?P will approximate the ground-truth ?P gt after the optimization. The specific formulation is a non-linear least squares problem and the objective function is expressed as</p><formula xml:id="formula_2">E(?) = M i=1 (? t (x i ) ? C(x i ; ?)) T (? t (x i ) ? C(x i ; ?)),</formula><p>(2) where the residual pose argument ?P is parameterized as its minimal representation ? ? se(3) (of the associated Lie-algebra) during optimization.? t is the GRU-estimated correspondence field at the t-th recurrent iteration, and C(x i ; ?) denotes the correspondence of point x i derived with the pose parameter argument ?, and M is the total number of object points in the rendered reference image. Handling Unreliable Correspondences with Similarity Scores. The formulation of Eq. <ref type="formula">(2)</ref> is based on an impractical assumption that the correspondence field? t can be reliably estimated for all foreground regions, which is extremely difficult considering ubiquitous occlusions. We further propose to incorporate a consistency-check mechanism to downweight the unreliable values in? t during pose optimization. To model the reliability of estimated correspondence, one option is to adopt a forward-and-backward consistency check <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>. However, the bidirectional consistency check doubles the computational cost, and the domain gap between the rendered images and the real images increases the learning difficulty.</p><p>We therefore propose a descriptor-based consistency check to alleviate the dilemma. The basic idea is to represent the 3D object model M and the observed 2D target image I obs as two sets of distinctive descriptors point-wisely via a 3D-2D hybrid network (with KPConvs <ref type="bibr" target="#b45">[46]</ref> and a keypoint description net <ref type="bibr" target="#b7">[8]</ref> as backbones). The corresponding descriptors of the object model and object images are enforced to be similar, while the non-corresponding descriptors are enforced to be dissimilar (by training with a contrastive descriptor loss function being described in Sec. 3.3). The learned 3D model descriptors are rendered as 2D feature maps, denoted as D M , according to the object pose of the reference image for fast indexing. The encoded target image descriptor map is denoted as D I .</p><p>With these high-dimensional distinctive descriptors, for each estimated correspondence pair (x i ,? t (x i )), we measure its reliability according to the similarity between their associated 3D and 2D descriptors (d i M , d i I ). d i M and d i I here are collected from the above descriptor maps:</p><formula xml:id="formula_3">d i M = D M (x i ) and d i I = D I (? t (x i )),</formula><p>where bilinear interpolation may be applied for non-inetger correspondence coordinates. The reliability of this correspondence pair is modeled with a similarity score:</p><formula xml:id="formula_4">w i = exp ? |1 ? d i T M d i I | ? ,<label>(3)</label></formula><p>where ? is a learnable parameter (initialized with 1) adjusting the sharpness. The similarity scores are used as the weights of the Mahalanobis distance measurements in Eq. <ref type="formula">(2)</ref>, which effectively downweight unreliable correspondences during optimization. By introducing a diagonal weighting matrix w i = w i 0 0 w i , the weighted version of Eq. (2) is written as</p><formula xml:id="formula_5">E(?) = M i=1 (? t (x i ) ? C(x i ; ?)) T w i (? t (x i ) ? C(x i ; ?)</formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(4)</head><p>The pose optimization is thus formulated a?</p><formula xml:id="formula_6">? = arg min ? E(?),<label>(5)</label></formula><p>where the pose parameter ? ? se <ref type="formula" target="#formula_4">(3)</ref> is optimized by minimizing the objective function defined by Eq. (4). Differentiable Residual Pose Optimization. We solve the non-linear least squares problem (Eq. (5)) with Levenberg-Marquardt (LM) algorithm. For the optimization in the tth recurrent iteration, the pose parameter is initialized with the estimated pose from the previous iteration i.e., ? 0 = log(?P t?1 ). Continuing from the parameter ? p?1 of the previous LM iteration, the left-multiplied increment ?? p is computed by</p><formula xml:id="formula_7">?? p = (J T WJ + ?I) ?1 J T Wr(? p?1 ),<label>(6)</label></formula><p>with which we update the parameter as ? p ? ?? p ? ? p?1 , to approach the optimal solution. Here, J = ? ?r ?? is the Jacobian matrix containing the derivative of the stacked residual vector r = (r 1 , r 2 , ..., r 2M ) T (established from Eq. (4)) with regard to a left-multiplied increment. We unroll the parameter update procedure and make the LM optimization layer differentiable to enable end-to-end network training. The differentiable optimization procedure enhances the feature learning for correspondence field estimation, which is essential to high performance. After LM optimization, the residual pose of the t-th recurrent iteration is estimated as ?P t = exp(?), where? denotes the optimized parameter after several updates with Eq. <ref type="bibr" target="#b5">(6)</ref>. Correspondence Field Rectification. The erroneous initial poses usually produce large offsets between the rendered reference object and the observed object, which poses challenges for correspondence estimation. Moreover, unlike the standard scenarios of optical flow estimation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>, unpatterned objects and varying illuminations are ubiqui-tous in our task, which further increases the difficulty. Considering the optimized pose by Eq. (5) is mainly supported by the reliable correspondence estimations with our weighting mechanism Eq. (3), we rectify the correspondence field as? ? t (x) = ?(?P t ? ?1 (x; z)) based on the currently optimized pose ?P t . The rectification enforces the rigidtransformation constraints among the correspondence field, which improves the overall correspondence quality for the correlation volume lookup in the following recurrent iteration. A toy example is shown in <ref type="figure" target="#fig_1">Fig. 3</ref> for better understanding. Object Pose Estimation Update. After every N recurrent iterations, the residual pose is estimated as ?P N by the RNN. We update the object pose estimation with the estimated residual pose ?P N asP ? ?P N P init , and we re-render the reference image I ref based on this updated pose to start the next N-recurrent-iteration refinement, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. We refer to the N-recurrent-iteration refinement as a rendering cycle, and the initial pose P init for the next cycle is set toP accordingly. The performance and efficiency with different rendering cycles and recurrent iterations will be discussed in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Functions</head><p>Model Alignment Loss. To supervise the residual pose estimations {?P t |t = 1 . . . N } generated in each rendering cycle (including N recurrent iterations), we apply these residual poses as the left-multiplied increments to the initial pose P init , having the corresponding object pose estimations {P t |t = 1 . . . N }, whereP t = ?P t P init . Thereafter, we adopt a 3D model alignment loss to supervise these pose estimations for each rendering cycle:</p><formula xml:id="formula_8">L ma = N t=1 ||P t X model ? P gt X model || 1 ,<label>(7)</label></formula><p>whereP t is the object pose estimation mentioned above and P gt denotes the ground-truth pose. Here, X model ? R 4?M contains homogeneous coordinates of the M model points. This loss function encourages the pose estimation to be close to the ground-truth so that the transformed model points can be well aligned. Correspondence Loss. We adopt L1 loss <ref type="bibr" target="#b43">[44]</ref> for correspondence field supervision, where the ground-truth correspondence fields are derived with Eq. (1) based on groundtruth poses. Descriptor Loss. We use circle loss L cir <ref type="bibr" target="#b39">[40]</ref> as the contrastive loss to supervise the point-wise descriptor learning of the 3D object model and the target images for similarity score calculation Eq. (3). Concretely, we view the target image I obs as two parts, i.e., the foreground region (object region) denoted as f g(I obs ) and the background region denoted as bg(I obs ). For each foreground descriptor d i I ? f g(I obs ), we first find a set of its corresponding 3D descriptors {d j M } + of object model via KNN  searching (see supplementary materials for details). Then, d i I ? f g(I obs ) is enforced to be similar to {d j M } + and dissimilar to the remaining non-corresponding descriptors {d k M } ? with circle loss L cir <ref type="bibr" target="#b39">[40]</ref>, which is expressed as</p><formula xml:id="formula_9">L cir (d i I , {d j M } + , {d k M } ? ).</formula><p>Moreover, for background descriptors d i I ? bg(I obs ) , we constrain them to be similar to each other in the background, while to be dissimilar to the foreground descriptor set f g(I obs ) with loss L cir (d i I , bg(I obs ), f g(I obs )). Traversing all target image descriptors d i I , the descriptor loss is calculated as</p><formula xml:id="formula_10">L d = d i I ?f g(I obs ) Lcir(d i I , {d j M }+, {d k M }?) + d i I ?bg(I obs )</formula><p>Lcir(d i I , bg(I obs ), f g(I obs )) <ref type="bibr" target="#b7">(8)</ref> to supervise the descriptor learning. With the contrastive learning, the corresponding 2D-3D descriptors would be similar while the noncorresponding ones would be dissimilar, which provides the foundation for unreliable correspondence handling with similarity scores Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Implementation Details. We train all of our networks endto-end using the Adam <ref type="bibr" target="#b21">[22]</ref> optimizer with an initial learning rate of 10 ?4 and adjust it with a cosine annealing strategy. The weights of model alignment loss L ma and descriptor loss L d are set to 1, while the correspondence loss weight is set to 0.5. During training, we conduct 3 rendering cycles, each of which performs 4 recurrent refinement iterations for pose refinement. All our models are trained agnostic to the initial pose sources where disturbed ground-truth poses are taken as initial poses for training following <ref type="bibr" target="#b22">[23]</ref>. For testing, we conduct the same numbers of rendering cycles and refinement iterations as those during training for most experiments if without further declaration, though more iterations could produce better results. Please refer to the supplementary materials for more details.</p><p>Datasets. We evaluate our method on three datasets, including LINEMOD <ref type="bibr" target="#b13">[14]</ref>, Occlusion LINEMOD <ref type="bibr" target="#b2">[3]</ref> and YCB-Video <ref type="bibr" target="#b52">[53]</ref>. LINEMOD is a standard benchmark for 6D object pose estimation. This dataset contains texture-less objects in cluttered scenes captured with challenging illuminance variations. The Occlusion LINEMOD is a subset of LINEMOD dataset with additional annotations for occluded objects, which is suitable for testing the robustness to severe occlusions. Besides, the YCB-Video dataset contains the images of the YCB object set <ref type="bibr" target="#b3">[4]</ref> where strong occlusions, clutters are exhibited. It includes more than 110k real images captured for 21 objects with or without textures. We follow similar conventions in data processing and synthetic data generation as the previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>. For the initial poses, we mainly rely on PoseCNN <ref type="bibr" target="#b52">[53]</ref> and PVNet <ref type="bibr" target="#b33">[34]</ref>, two typical direct estimation methods, following <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b19">[20]</ref>. We also create a set of extremely erroneous initial poses by adding random Gaussian noise to the original initial pose estimations to evaluate the robustness to large initial poses errors. Evaluation Metrics. We evaluate our method with the metrics ADD-(S) <ref type="bibr" target="#b13">[14]</ref> and AUC of ADD(-S) <ref type="bibr" target="#b52">[53]</ref>. For the ADD-(S) metric, the mean distance between the model points transformed with the pose estimation and the groundtruth is calculated. With the standard ADD-(S) metric, if the mean distance is less than 10% of the model diameter, the pose estimation is regarded as correct. In some of our experiments, we also test the performances when the threshold is set to 2% or 5% of the model diameter for stricter testing. For symmetric objects, the mean distance is computed based on closest point distances <ref type="bibr" target="#b13">[14]</ref>. When evaluating on the YCB-Video dataset, we also compute the AUC (Area Under Curve) of ADD(-S) by varying the distance threshold from 0 cm to 10 cm following <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We conduct a thorough ablation study on LINEMOD and Occlusion LINEMOD datasets to evaluate the effectiveness of the components in our framework. Correspondence Field Supervision. We first remove the correspondence loss to verify the influence of correspon- <ref type="table">Table 1</ref>. (a) Ablation study on LINEMOD dataset. (b) Validation of effectiveness of similarity score on the Occlusion LINEMOD dataset with the ADD(-S) metric. For more detailed comparison, the evaluations with different thresholds of ADD(-S) metric are conducted, i.e., 2%, 5% and 10% of the model diameter denoted as 0.01d, 0.05d, 0.1d respectively.  dence field quality on the pose estimation. The results 'w/o correspondence loss' in <ref type="table">Table 1</ref> correspond to this ablation study, and the performance degrades significantly. Since our pose optimization is directly based on the correspondence field estimation, solid supervision on correspondence field estimation is essential to the overall system. Effectiveness of the Pose Supervision and End-to-end Learning. We further remove the supervision to the pose estimation by setting the weight of the model alignment loss L ma to 0. This is equivalent to adopting a typical nondifferentiable LM optimizer because no gradient is backpropagated through the LM layer during training. It can be found that the object pose can still be reasonably estimated (denoted as 'w/o L ma ' in <ref type="table">Table 1</ref>), but with humble performance, especially with stricter evaluation criteria, i.e., by setting a smaller threshold 0.01d or 0.05d. The performance degradation reflects the importance of end-to-end pose learning. The differentiable LM layer enables the pose supervision to affect the feature learning for more robust correspondence field estimation, which is essential to our formulation. Correspondence Field Rectification. Another key procedure in our recurrent pose refinement is the correspondence field rectification. To validate the effectiveness, we ablate this step and directly use the correspondence esti-mation? t from the GRU as the initialization for the next iteration (denoted as 'w/o? ? t rect.' in <ref type="table">Table 1</ref>). We find that the performance drops significantly compared with our full framework, especially on more strict metrics, i.e., 0.01d  <ref type="figure">Figure 6</ref>. Robustness comparison with RePOSE by degrading the initial poses (from PVNet <ref type="bibr" target="#b33">[34]</ref>) with Gaussian noise on LINEMOD dataset. and 0.05d. This phenomenon demonstrates that the corrected correspondence field with the rigid-transformation constraints from the optimized pose can facilitate the refinement in the following iterations. 3D Context Encoder. To verify the effectiveness of our 3D context encoder, we test the system without the context encoder (denoted as 'w/o 3D context F ctx ') or with a commonly used 2D context encoder (denoted as 'w/ 2D context'). The performances of these two versions both degrade compared to that with a 3D context encoder. The degradation not only reveals the importance of context information as indicated by previous works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref>, but also proves that our 3D context encoder is a more effective choice than the 2D counterpart in our task. We reckon that the more robust performance may be attributed to the finer granularity of dense 3D point cloud features (compared with the lowresolution 2D image features). The finer-granularity features could provide more detailed geometric contexts. Similarity Scores for Occlusion Handling. In <ref type="table">Table 1</ref>(b), we evaluate the effectiveness of similar scores in occlusion handling on the Occlusion LINEMOD dataset. The version 'w/ similarity score' performs better for severely occluded objects. By including similarity scores during pose optimization, flawed correspondence estimations in the occluded unreliable regions are effectively downweighted. Some similarity score map examples are exhibited in <ref type="figure" target="#fig_2">Fig. 4</ref> for better understanding. Recurrent Iterations vs Rendering Cycles. The number of refinement iterations affects the system performance, especially when erroneous initial pose estimations exist. We analyze the performances with different recurrent iterations and rendering cycles in <ref type="figure" target="#fig_3">Fig. 5. From Fig. 5(a)</ref>, it can <ref type="table">Table 3</ref>. The comparison of estimation accuracy with competitive direct methods (PoseCNN <ref type="bibr" target="#b51">[52]</ref>, PVNet <ref type="bibr" target="#b33">[34]</ref> and HybridPose <ref type="bibr" target="#b37">[38]</ref>) and refinement methods (DPOD <ref type="bibr" target="#b58">[59]</ref>, DeepIM <ref type="bibr" target="#b22">[23]</ref> and RePOSE <ref type="bibr" target="#b19">[20]</ref>) on LINEMOD dataset in terms of the ADD(-S) metric.  be found that, by solely increasing the recurrent iterations while rendering the reference object image only once, we have achieved a high accuracy of 96.05% which is comparable to RePOSE <ref type="bibr" target="#b19">[20]</ref>. If conducting refinement with more recurrent iterations and rendering cycles, steady improvements are reported, which reflect good convergence of our method. To further validate the robustness to erroneous initial poses, we add Gaussian noise to the initial poses. Specifically, we randomly disturb translation components and rotation Euler angles with Gaussian noise. For the rotation, we add angular noise with standard deviation (STD, denoted as ? r ) of 10 ? in all three axes. For the translational disturbance, we apply noise with a STD of 15 cm along the z axis (the axis perpendicular to the image plane) and STDs of 3 cm in x and y directions ( 1 5 ?) considering current methods usually have larger variances on depth estimations. From <ref type="figure" target="#fig_3">Fig. 5(b)</ref>, we find that the necessity of recurrent refinement becomes more noticeable.</p><p>Though more rendering cycles bring performance gains as well, the extra costs are significant, since most of the input features need re-encoding. Based on the runtime analysis <ref type="table">(Table 2)</ref>, increasing the recurrent iterations is more economical for better performance as only the CF (correspondence field) estimation, pose optimization and CF rectification modules are activated for a recurrent iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art Methods</head><p>We compare with the cutting-edge methods on LINEMOD, Occlusion LINEMOD, and YCB-Video.</p><p>For the LINEMOD dataset, we compare with the recent pose refinement methods RePOSE <ref type="bibr" target="#b19">[20]</ref>, DPOD <ref type="bibr" target="#b58">[59]</ref> and DeepIM <ref type="bibr" target="#b22">[23]</ref> as well as some direct estimation baselines <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53]</ref>. <ref type="table">Table 3 contains the comparison results   Table 5</ref>. Comparison with the refinement methods based on single images on the YCB-Video dataset. The performance of our initial pose generator, i.e., PoseCNN, is also included. and we achieve a state-of-the-art performance. Interestingly, we achieve slightly better average performance when using PoseCNN <ref type="bibr" target="#b52">[53]</ref> as the initial pose generator rather than the PVNet <ref type="bibr" target="#b33">[34]</ref>, although the pose accuracy of PVNet is much better as exhibited in <ref type="table">Table 3</ref>. This phenomenon reveals the good tolerance of our system to erroneous initial poses. To test our robustness to even larger initial pose errors, we add random Gaussian pose noises to the initial rotation and translation components separately for accuracy evaluation similar to those in Sec. 4.2. <ref type="figure">Fig. 6</ref> plots the accuracy variations w.r.t. the disturbance magnitudes. Our method exhibits strong robustness and works reasonably even with extremely noisy initial poses. We also conduct comparisons on Occlusion LINEMOD. As shown in <ref type="table" target="#tab_5">Table 4</ref>, we outperform the cutting-edge method <ref type="bibr" target="#b19">[20]</ref> by a significant margin (51.6 ? 60.65), which manifests the system robustness to occlusions. We visualize some of our pose estimates from severely occluded images in the first row of <ref type="figure" target="#fig_2">Fig. 4</ref>, where the initial poses from PVNet are disturbed with Gaussian noise like before (? t = 15 cm, ? r = 10 ? ) to pose more challenges. It is shown that our system is capable of handling large initial pose errors even in highly occluded scenarios.</p><p>Our additional evaluation on the YCB-Video dataset uses PoseCNN as the pose initializer, following the settings of RePOSE <ref type="bibr" target="#b19">[20]</ref>. We compare with the refinement methods based on monocular color images. Our system still performs well on this large-scale complex dataset. We consistently improve the initial poses provided by PoseCNN <ref type="bibr" target="#b52">[53]</ref>, and outperform the cutting-edge pose refinement method RePOSE in both metrics, as shown in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Limitations</head><p>We have presented a recurrent framework for 6-DOF object pose refinement. A non-linear least squares problem is formulated for pose optimization based on the estimated correspondence field between the rendered image and observed image. Descriptor-based consistency checking is included to downweight unreliable correspondences for occlusion handling. Our method performs robustly against erroneous pose initializations and severe occlusions, which achieves state-of-the-art performances on public datasets.</p><p>One limitation of our method is that the trained model is object-specific similar to many other works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b58">59]</ref>. Although for a novel object, only the pose refinement module needs further finetuning, the limited generalization ability to unknown objects is still undeniable. More detailed discussions are in the supplementary material. In the future, we plan to extend our method to handle unknown objects for better generality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed method. For pose refinement, a reference image I ref is rendered with the object CAD model and its initial pose Pinit. The image feature encoding module encodes the rendered image I ref and the observed image I obs to feature maps and build a 4D global correlation volume. In parallel, the 3D context feature encoding module encodes the 3D model geometry and render the 3D features to a 2D context feature map Fctx according to the initial pose estimation. During pose refinement, the correspondence field Ct and the residual pose ?Pt are alternately estimated in a recurrent framework. After the LM pose optimization, correspondence field estimation?t is rectified as? ? t by enforcing rigid-transformation constraints with the currently optimized pose ?Pt to further improve next-iteration estimations. After N recurrent iterations, the reference image I ref is re-rendered with the current pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>With the rectified correspondences, the related local correlation windows are accordingly shifted to better locations, which improves the estimation in the next recurrent iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of our pose estimations (first row) on Occlusion LINEMOD dataset and the similarity score maps (second row) for downweighting unreliable correspondences during pose optimization. For pose visualization, the white boxes represent the erroneous initial poses, the red boxes are estimated by our algorithm and the ground-truth boxes are in blue. Here, the initial poses for pose refinement are originally from PVNet<ref type="bibr" target="#b33">[34]</ref> but added with significant disturbances for robustness testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 &amp;</head><label>5</label><figDesc>Table2. Left: ADD(-S) accuracies w.r.t. different recurrent iterations and rendering cycles on LINEMOD. (a) Results based on the initial poses from PoseCNN [52] (b) Results based on the disturbed PoseCNN poses (with Gaussian noise ?t = 15cm, ?r = 10 ? ). Right: runtime analysis of individual modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and H. Li are the co-corresponding authors.</figDesc><table><row><cell></cell><cell></cell><cell>Object</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Reference Image (Rendered )</cell><cell cols="2">Target Image (Observed)</cell></row><row><cell>(a) Observed Image &amp; Rendered Object (c) Pose Result</cell><cell>Re-rendering</cell><cell cols="2">2 2 Recurrent Pose-derived Correspondence Field (?; ) Correspondence Field Estimation Estimated Correspondence Field Similarity Score Map LM Optim. Optimized Pose Iterations</cell><cell>Field Rectification Correspondence</cell></row><row><cell>(shown as a</cell><cell></cell><cell></cell><cell></cell></row><row><cell>green mask)</cell><cell></cell><cell cols="2">(b) Recurrent Pose Refinement</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Context Feature Encoding Image Feature Encoding Pose Refinement</head><label></label><figDesc></figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Output</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Template CAD Model ? Renderer 3D Initial Pose $%$! $%$! = $</cell><cell>2D Feat. Encoder 2D Feat. Encoder 3D Context Encoder Sharing</cell><cell>Renderer ?,? Initial Pose Vertex Features H!?W! H " ?W " $%$!</cell><cell>H"</cell><cell>W"</cell><cell>H!? !</cell><cell>$ ! = arg min 0 Local W! H! Window Size</cell><cell>$ !23 Correspondence Differentiable LM Optimization ! ! Field Rectification</cell><cell>Rendering Cycles</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">$%$! = $</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>36.03 70.35 4.40 35.76 74.51 14.86 50.48 80.10 12.19 52.22 82.33 18.76 57.14 88.19 Benchvise 31.60 87.08 99.32 58.14 94.30 99.71 79.56 98.72 100.0 72.26 99.13 100.0 75.26 98.37 99.81 75.17 98.25 100.0 Camera 19.37 70.89 94.90 45.13 82.31 95.95 56.72 90.09 97.91 53.63 90.69 98.73 56.90 91.68 97.78 55.39 89.12 98.04 Can 8.95 77.88 96.83 32.65 86.71 98.76 47.13 94.37 99.31 53.25 95.28 99.80 53.21 95.62 99.72 54.53 94.69 99.31 Cat 4.59 28.39 71.64 25.24 62.60 92.81 31.74 75.76 97.98 32.34 74.55 96.71 36.81 79.15 98.55 36.43 74.85 96.41 Driller 40.25 84.04 92.57 49.88 88.50 98.22 59.81 96.43 99.70 60.46 95.34 99.70 60.69 95.54 99.41 62.44 95.44 99.70 Duck 5.62 22.44 69.08 16.66 47.46 79.69 19.18 55.68 87.01 16.71 57.37 85.92 25.19 63.62 88.01 25.82 61.13 89.30 Eggbox 43.45 89.81 99.65 46.40 87.12 98.12 52.64 83.45 97.65 50.05 81.03 95.59 54.51 86.38 96.36 59.06 93.80 99.53 Glue 44.08 93.57 69.83 10.67 52.84 92.29 51.83 93.95 99.87 55.12 94.40 99.52 54.14 95.71 99.87 60.14 95.56 99.71 Holep. 6.26 15.89 51.95 31.55 65.55 95.04 32.81 70.22 96.53 24.26 66.51 93.91 20.61 56.03 91.04 35.68 75.26 97.43 Iron 42.33 96.09 99.08 52.14 95.48 99.69 62.46 97.32 99.59 63.74 97.45 100.00 63.07 98.24 100.0 68.03 98.16 100.0 .81 95.36 36.55 82.74 98.13 39.66 82.06 97.26 42.68 83.85 98.39 42.30 83.95 98.39 Average 21.85 60.83 85.27 33.72 73.35 93.47 44.68 82.25 96.00 44.82 82.93 95.93 47.44 83.91 96.17 50.39 85.56 97.37 Cat 1.60 11.71 27.97 1.60 11.63 29.15 Driller 13.76 52.47 78.42 14.58 59.80 88.14 Duck 0.18 11.31 47.77 0.26 11.13 49.17 Eggbox 2.98 25.96 61.28 4.94 38.47 66.98 Glue 6.98 35.22 65.01 10.52 40.97 63.79 Holep. 0.08 18.33 59.83 0.42 21.42 62.76 Average 4.13 27.21 58.05 5.02 31.15 60.65</figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell></row><row><cell cols="4">w/o correspondence loss 0.02d 0.05d 0.1d 1.29 17.60 61.23 8.65 Lamp Object 0.02d 0.05d 0.1d 0.02d 0.05d 0.1d 0.02d 0.05d w/o Lma w/o? ? t rect. w/o 3D context Fctx 0.1d Ape 33.87 88.57 98.98 30.18 81.90 99.17 45.95 94.75 99.81 46.35 93.76 99.81 60.71 94.43 99.00 61.32 94.91 99.81 w/ 2D context Full(Ours) 0.02d 0.05d 0.1d 0.02d 0.05d 0.1d Phone 2.33 18.53 55.59 31.08 72(b) w/o similarity score w/ similarity score (Ours) Object 0.02d 0.05d 0.1d 0.02d 0.05d 0.1d Ape 0.17 8.97 38.63 0.09 9.74 37.18 Can 7.29 53.69 85.50 7.79 56.01 88.07</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>Module</cell><cell>Runtime (ms)</cell></row><row><cell></cell><cell cols="2">Ref. Image Rendering</cell><cell>8.88</cell></row><row><cell></cell><cell cols="2">3D Context Encoding (run once per sequence)</cell><cell>35.20</cell></row><row><cell></cell><cell cols="2">3D Feat. Rendering (context&amp;descriptor)</cell><cell>5.39</cell></row><row><cell></cell><cell cols="2">Image Feat. Encoding</cell><cell>6.39</cell></row><row><cell></cell><cell cols="2">2D-3D Hybrid Net (2D part)</cell><cell>2.99</cell></row><row><cell></cell><cell cols="2">CF Estimation</cell><cell>6.21</cell></row><row><cell></cell><cell></cell><cell>Pose Optim.</cell><cell>6.23</cell></row><row><cell></cell><cell cols="2">CF Rectification</cell><cell>1.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Accuracy comparison with the state of the art on OC-CLUSION LINEMOD dataset in terms of the ADD(-S) metric.</figDesc><table><row><cell cols="8">Object PoseCNN [52] PVNet [34] HybridPose [38] GDR-Net [51] DPOD [59] RePOSE [20] Ours</cell></row><row><cell>Ape</cell><cell>9.60</cell><cell>15.8</cell><cell>20.9</cell><cell>39.3</cell><cell>-</cell><cell>31.1</cell><cell>37.18</cell></row><row><cell>Can</cell><cell>45.2</cell><cell>63.3</cell><cell>75.3</cell><cell>79.2</cell><cell>-</cell><cell>80.0</cell><cell>88.07</cell></row><row><cell>Cat</cell><cell>0.93</cell><cell>16.7</cell><cell>24.9</cell><cell>23.5</cell><cell>-</cell><cell>25.6</cell><cell>29.15</cell></row><row><cell>Driller</cell><cell>41.4</cell><cell>65.7</cell><cell>70.2</cell><cell>71.3</cell><cell>-</cell><cell>73.1</cell><cell>88.14</cell></row><row><cell>Duck</cell><cell>19.6</cell><cell>25.2</cell><cell>27.9</cell><cell>44.4</cell><cell>-</cell><cell>43.0</cell><cell>49.17</cell></row><row><cell>Eggbox</cell><cell>22.0</cell><cell>50.2</cell><cell>52.4</cell><cell>58.2</cell><cell>-</cell><cell>51.7</cell><cell>66.98</cell></row><row><cell>Glue</cell><cell>38.5</cell><cell>49.6</cell><cell>53.8</cell><cell>49.3</cell><cell>-</cell><cell>54.3</cell><cell>63.79</cell></row><row><cell>Holep.</cell><cell>22.1</cell><cell>39.7</cell><cell>54.2</cell><cell>58.7</cell><cell>-</cell><cell>53.6</cell><cell>62.76</cell></row><row><cell>Average</cell><cell>24.9</cell><cell>40.8</cell><cell>47.5</cell><cell>53.0</cell><cell>47.3</cell><cell>51.6</cell><cell>60.65</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Geometric correspondence fields: Learned differentiable rendering for 3d pose refinement in the wild</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Least-squares fitting of two 3-d point sets. IEEE Transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Somani Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blostein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="698" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 international conference on advanced robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end learnable geometric vision by backpropagating pnp optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8100" to="8109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning canonical shape space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11973" to="11982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="224" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Epos: Estimating 6d pose of objects with symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11703" to="11712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single-stage 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vs-net: Voting with segmentation for visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangbang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6101" to="6111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Shun Iwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rawal</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Khirodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Yokota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Repose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00633</idno>
		<title level="m">Real-time iterative rendering and refinement for 6d object pose estimation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7678" to="7687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bundle-adjusting neural radiance fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision<address><addrLine>Barf</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5741" to="5751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4571" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="800" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramid r-cnn: Towards better performance and adaptability for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2723" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Statistical data analysis: A practical guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Meloun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Militk?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Woodhead Publishing Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The levenberg-marquardt algorithm: implementation and theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mor?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1978" />
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Maria Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7668" to="7677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4561" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08501,2020.3</idno>
		<title level="m">Accelerating 3d deep learning with pytorch3d</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Back to the feature: Learning robust camera localization from pixels to pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajaykumar</forename><surname>Unagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mans</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3247" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="431" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ba-net: Dense bundle adjustment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepv2d: Video to depth with differentiable structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision, ECCV 2020</title>
		<imprint>
			<publisher>Springer Science and Business Media Deutschland GmbH</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gn-net: The gauss-newton loss for multiweather relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lukas Von Stumberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qadeer</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="890" to="897" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Morefusion: Multi-object reasoning for 6d pose estimation from volumetric fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14540" to="14549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="16611" to="16621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems XIV</title>
		<editor>Hadas Kress-Gazit, Siddhartha S. Srinivasa, Tom Howard, and Nikolay Atanasov</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust self-supervised lidar odometry via representative structure discovery and 3d inherent error modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pdnet: Towards better one-stage object detection with prediction decoupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13876</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improving visual grounding with visuallinguistic verification and iterative reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Segvoxelnet: Exploring semantic context and depth-aware features for 3d vehicle detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2274" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The advantages of a joint direct and indirect vslam in ar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Younes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ssn: Shape signature networks for multi-class object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="581" to="597" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
