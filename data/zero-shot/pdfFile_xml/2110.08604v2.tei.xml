<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Implicit Sentiment Learning via Local Sentiment Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4QF</postCode>
									<settlement>Exeter</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
							<email>k.li@exeter.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4QF</postCode>
									<settlement>Exeter</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Implicit Sentiment Learning via Local Sentiment Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent well-known works demonstrate encouraging progress in aspect-based sentiment classification (ABSC), while implicit aspect sentiment modelling is still a problem that has to be solved. Our preliminary study shows that implicit aspect sentiments usually depend on adjacent aspects' sentiments, which indicates we can extract implicit sentiment via local sentiment dependency modeling. We formulate a local sentiment aggregation paradigm (LSA) based on empirical sentiment patterns (SP) to address sentiment dependency modelling. Compared to existing methods, LSA is an efficient approach that learns the implicit sentiments in a local sentiment aggregation window, which tackles the efficiency problem and avoids the token-node alignment problem of syntaxbased methods. Furthermore, we refine a differential weighting method based on gradient descent that guides the construction of the sentiment aggregation window. According to experimental results, LSA is effective for all objective ABSC models, attaining state-of-the-art performance on three public datasets. LSA is an adaptive paradigm and is readily to be adapted to existing models, and we release the code to offer insight to improve existing ABSC models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It has been recognized that there are many aspects that contain human-aware implicit aspect sentiments that are difficult for models to extract <ref type="bibr" target="#b2">(de Kauter, Breesch, and Hoste 2015;</ref><ref type="bibr" target="#b8">Li and Lu 2019;</ref><ref type="bibr" target="#b0">Cai, Xia, and Yu 2021;</ref><ref type="bibr" target="#b10">Li et al. 2021b;</ref><ref type="bibr" target="#b28">Zhuang et al. 2022</ref>). For instance, in an ABSC example: "this laptop has a lot of storage, and so does the battery capacity", the customer praised both storage and battery capacity, while no explicit sentiment description of the aspect battery capacity is available in this example. <ref type="table">Table 1</ref> shows other examples of implicit sentiment that are hard to learn by existing models. To address the problem of implicit sentiment modeling, prominent studies on ABSC <ref type="bibr" target="#b17">(Pontiki et al. 2014</ref><ref type="bibr" target="#b16">(Pontiki et al. , 2015</ref><ref type="bibr" target="#b15">(Pontiki et al. , 2016</ref> exploit the syntax tree to explore potential sentiment dependency between aspects <ref type="bibr" target="#b24">(Zhang, Li, and Song 2019;</ref><ref type="bibr" target="#b6">Huang and Carley 2019;</ref><ref type="bibr" target="#b14">Phan and Ogunbona 2020)</ref>. However, previous approaches based on syntax trees may encounter token-node dis-alignment caused by different tokenization strategies between syntax parsing tools and Copyright ? 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the pretrained language models. For example, spaCy 1 , one of the most popular syntax tree parsing tool, tokenizes "I don't like it" into {I, do, n't, like, it}, while the BERT (Devlin et al. 2019) 2 tokenizes it into {i, don, ', t, like, it}. The red tokens indicate the tokenization differences. In this case, many other tokenization differences result in incorrect syntax trees. Another inevitable problem is that syntax tree-based methods are resource-intense, requiring additional budgets to parse syntax tree before training. <ref type="table">Table 1</ref>: Several ABSC examples from different domains that contains implicit sentiment. The aspects in italic and bold denotes the dependent aspect and target aspect, respectively. The aspects in green and red contains positive and negative sentiment polarity, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No.</head><p>Example Domain 1 Not only was the food outstanding, Restaurant but also the coffee and juice! 2</p><p>The servers always surprise us Restaurant with a different starter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>The speakers of this TV sucks! TV Just like its screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>If you have no idea about the camera Camera , think about its good quality ! Our study shows that implicit aspect sentiments mainly exist between adjacent aspects, which means we can model implicit aspect sentiment by exploiting adjacent aspects' sentiment information. To leverage the sentiment information of the adjacent aspects to extract implicit sentiment, we introduce sentiment patterns (refer to Section 2) to guide the modeling of implicit sentiment. More specifically, we propose the local sentiment aggregation (LSA) paradigm based on sentiment patterns for implicit sentiment modeling. Compared to syntax tree-based models, LSA only needs to handle the sentiment dependency in a local sentiment aggregation window, alleviating the computation budgets and improving modeling efficiency.</p><p>Furthermore, we intend to let LSA know to what extent the contribution of adjacent aspect's sentiment information in the sentiment aggregation window should be measured. Hence, we propose the differential weighted sentiment aggregation window, which adopts learnable weights based on gradient descent to control the contribution of the adjacent aspect's sentiment. In this way, LSA learns how to build a better sentiment aggregation window.</p><p>To evaluate the generalization ability of LSA, we propose three variants of local sentiment aggregation in Section 3. The experimental results show that LSA is efficient and achieves state-of-the-art performance (e.g., up to 86.31% and 90.86% average accuracy on the Laptop14 dataset and the Restaurant14 dataset, respectively) compared to existing models. We also conduct ablation experiments on three public ABSC datasets to evaluate the efficacy of differential weighting.</p><p>The main contributions 3 of this work are as follows: ? We regard implicit sentiment learning in ABSC as a local sentiment aggregation problem. Our experimental results indicate that non-syntax-based methods could achieve better performance than existing syntax-based methods. ? To validate LSA is a general paradigm for ABSC, we propose three local sentiment aggregation strategies for local sentiment aggregation. The experimental results show that LSA is effective for all LSA aspect-based features and outperforms state-of-the-art methods with higher efficiency. ? We enable LSA to auto-optimise sentiment aggregation window construction by proposing the differential weighing strategies, which allows LSA learn to optimise the contribution of adjacent sentiment by gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sentiment Pattern</head><p>Cozy atmosphere, good food and service, good place to meet friends for dinner and a drink. We first introduce the concept of sentiment pattern, the motivation of local sentiment aggregation. To extract implicit aspect sentiment, we investigate the popular ABSC datasets and find that users generally organize their opinions according to specific 'patterns', which presents a hopeful implicit aspect sentiment learning insight. We demonstrate the 'patterns'in <ref type="figure" target="#fig_0">Fig. 1</ref>, which shows an example of aspect-based sentiment classification. In <ref type="figure" target="#fig_0">Fig. 1</ref>, the 'atmosphere', 'food' and 'service' contain positive sentiments, while the 'dinner' and 'drink' contain neutral sentiments. We introduce two sentiment patterns 4 based on <ref type="figure" target="#fig_0">Fig. 1</ref> in the following subsections.</p><p>3 Code and datasets are available in supplementary materials. <ref type="bibr">4</ref> We account sentiment coherency for users who may recollect an aspect with the same sentiment as the previous one, resulting in heuristic thinking. Sentiment Cluster: Aspects with similar sentiment polarity probably cluster in the context. The first pattern sentiment cluster is coarse-grained, which indicates the adjacent aspects could be in a sentiment cluster. We mine the sentiment clusters for five public ABSC datasets to provide feasible support for sentiment pattern modeling. <ref type="table" target="#tab_0">Table 2</ref> shows the number of different-sized clusters. However, the sentiment cluster is still complicated to model; As an alternative, we propose a fine-grained sentiment pattern, sentiment coherency, to perform adjacent sentiment aggregation. Sentiment Coherency: It is feasible to extract implicit sentiment based on adjacent sentiment aggregation. The fine-grained sentiment coherency finally provides a clue for local sentiment aggregation to extract implicit sentiment. In the existing works, the sentiment of each aspect is modeled separately, leading to inevitable sentiment coherency information loss. On the one hand, local sentiment aggregation can be utilized to extract implicit sentiment. On the other hand, sentiment coherency is able to smooth the sentiment prediction probabilities and eliminate misclassifications caused by occasional noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section introduces the concepts of sentiment window construction and LSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Sentiment Aggregation</head><p>In order to aggregate local sentiments in the context, we need to obtain the local sentiment information of each aspect and build a sentiment aggregation window, which will be clarified in Section 3. In other words, the sentiment aggregation window concatenates the feature representation of the aspect's local sentiment information (i.e., aspect feature in the following sections). We propose three ways, LSA P , LSA T and LSA S , to construct a sentiment aggregation window. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the architecture of LSA P , while <ref type="figure" target="#fig_2">Fig. 3</ref> shows the architecture of LSA T and LSA S . The difference between LSA T and LSA S is the feature representation of local sentiment information. Sentence Pair-based Aspect Feature A straightforward way to obtain aspect features is to utilize the BERT-SPC input format <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref>, which appends the aspect to the context to learn aspect features. For example, let</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect Feature Learning</head><formula xml:id="formula_0">W = [CLS], {w c i } n i=1 , [SEP ], {w a j } m j=1 , [SEP ] be the BERT-SPC format input, i ? [1, n] and j ? [1, m],</formula><p>where w c i and w a j denotes the token in the context and the aspect, respectively. A PLM (e.g., BERT) can learn the aspect feature because the duplicated aspects will get more attention in the self-attention mechanism <ref type="bibr" target="#b20">(Vaswani et al. 2017)</ref>. As it is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we simply apply the sentiment aggregation to BERT-SPC-based aspect features. Note that we deploy a self-attention encoder before each linear layer to activate hidden states. Local Context-based Aspect Feature The second implementation of LSA is LSA T . The local context-based aspect feature is derived by position-wise weighting the global context feature, where the weights are calculated using the relative distance of token-aspect pairs. Let W = {w c 1 , w c 2 , . . . , w c n } be the tokens after tokenization. We calculate the position weight for token w c i as follows:</p><formula xml:id="formula_1">H * w c i := H c w c i d w c i ? ? 1 ? d w c i ?? n ? H c w c i d w c i &gt; ? ,<label>(1)</label></formula><p>where H *</p><formula xml:id="formula_2">w c i and H c w c i , i ? [1, n]</formula><p>, are the hidden states at the position of w c i in the aspect feature and global context feature, respectively; d w c i is the relative distance between w c i and the aspect. We concatenate H * w c i to obtain the aspect feature H * . ? = 3 is a fixed distance threshold. If d w c i ? ?, the H c w c i will be preserved; otherwise the it decays according to d w c i . In equation <ref type="formula" target="#formula_1">(1)</ref>, the relative distance d w c i between w c i and the aspect is obtained by:</p><formula xml:id="formula_3">d w c i := m j=1 |p c i ? p a j | m ,<label>(2)</label></formula><p>where p c i and p a j are the positions of the w c i and j-th token in the aspect; As it shows in <ref type="figure" target="#fig_2">Fig. 3</ref>, we take the global context feature as supplementary feature to learn aspect sentiments.</p><p>Syntactical Local Context-based Aspect Feature The final variant of LSA is LSA S , which adopts the syntax-tree based local context feature to construct sentiment aggregation window. The distance between the context word w c i and the aspect can be calculated according to the shortest node distance between w c i and the aspect in the syntax tree. To leverage the syntactical information without directly modeling the syntax tree, LSA S calculates the average node distance between w c i and the aspect:</p><formula xml:id="formula_4">d w c i = m i=j dist(w c i , w a j ) m ,<label>(3)</label></formula><p>where dist denotes the shortest distance between the node of w c i and the node of w a j in the syntax tree; the calculation of H * w c i follows LSA T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Aggregation Window</head><p>The sentiment aggregation window is composed of knearest aspect feature vectors. Because most of the clusters is small, we only consider k = 1 in this work: Aggregation Window Padding We need to pad the sentiment aggregation window for those with no adjacent aspects. <ref type="figure" target="#fig_3">Fig. 4</ref> shows three padding strategies. Rather than zero vectors, we pad the window using the targeted aspect's feature, which emphasizes the targeted aspect's local sentiment feature and avoids degenerating model's performance. Case #1 means there is only one aspect in the context, in this case we triple the targeted aspect's feature to build the sentiment aggregation window. Case #2 and Case #3 copy the targeted aspect's feature to the left and right slots in the window, respectively.</p><formula xml:id="formula_5">H o aw := [{H l k }; H t ; {H r k }],<label>(4)</label></formula><formula xml:id="formula_6">H o := W o H o aw + b o ,<label>(5)</label></formula><p>Differential Weighted Aggregation It is natural to realize that the importance of sentiment information from different side may be different. Hence, we propose differential weighted aggregation (DWA) to control the contribution of sentiment information of the adjacent aspects from different sides. We initialize learnable ? * l and ? * r to adjust the contribution of adjacent aspect's features and obtain the differential weighted sentiment aggregation window as follows:</p><formula xml:id="formula_7">H o dwa := [? * l {H l k }; H t ; ? * r {H r k }],<label>(6)</label></formula><p>where H o dwa is the aggregated hidden state leanrned by differential weighted aggregation window. We initialize ? * l and ? * r to 1 and optimize them based on gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Layer</head><p>For sentence pair-based sentiment aggregation, we simply apply pooling and softmax to predict the likelihood of sentiment. For the local context feature-based sentiment aggregation, we follow the original claim to combine the global context feature and feature learned to predict sentiment polarity as follows:</p><formula xml:id="formula_8">H out := W d [H o ; H c ] + b d ,<label>(7)</label></formula><p>where H out is the output hidden state; H o and H c are the features extracted by a PLM (e.g., DeBERTa), we use the feature of the first token (a.k.a, head pool), to classify sentiments:?</p><formula xml:id="formula_9">:= exp(h head ) C 1 exp(h head ) ,<label>(8)</label></formula><p>where h head is the head-pooled feature; C is the number of polarity categories; W d ? R 1?C , b d ? R C are the trainable weights and biases.? is the predicted sentiment polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Detail</head><p>We conduct experiments based on multiple PLMs 5 . The LSA based on different PLMs are denoted as LSA-BERT, LSA-RoBERTa, LSA-DeBERTa, etc. LSA-X represents the LSA based on the large version of PLM. . We train LSA using the AdamW optimizer with the crossentropy loss function:</p><formula xml:id="formula_10">L = ? C 1 y i log y i + ? ? 2 + ? * {? * l , ? * r } 2 ,<label>(9)</label></formula><p>where ? is the L 2 regularization parameter; ? is parameter set of the model. Since we employ gradient-based optimization for ? * l and ? * r , we also apply a L 2 regularization with ? * for ? * l and ? * r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments Datasets</head><p>To evaluate the efficacy of the local sentiment aggregation paradigm, we conducted experiments on three popular and public datasets 6 : the Laptop14 and Restaurant14 datasets from SemEval-2014 Task4 <ref type="bibr" target="#b17">(Pontiki et al. 2014)</ref>, and the MAMS dataset from , respectively. We adopt the widely used Accuracy (Acc) and macro F1 as evaluation metrics; we report the average metrics obtained in five runs with different random seeds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter Setting</head><p>We fine-tune LSA using the following hyper-parameters which are obtained by grid searching.</p><p>? We set k = 1 in sentiment aggregation window construction. ? The learning rate for pre-trained models (e.g., BERT and DeBERTa) is 2 ? 10 ?5 . ? The learning rates for ? * l and ? * r are both 0.01. ? The batch size and maximum text modeling length are 16 and 80, respectively. ? The L 2 regularization parameters ? and ? * are both 10 ?5 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Models</head><p>We compare the performance of LSA with state-of-the-art baselines (many of these methods are syntax-based methods). For the details, please refer to the original publication. SK-GCN-BERT ) is a GCN-based model which exploits syntax and commonsense to learn sentiment information. DGEDT-BERT <ref type="bibr" target="#b18">(Tang et al. 2020</ref>) is a dual-transformerbased network enhanced by a dependency graph. SDGCN-BERT (Zhao, Hou, and Wu 2020) is a GCN-based model that can capture the sentiment dependencies between aspects. Dual-GCN <ref type="bibr" target="#b9">(Li et al. 2021a</ref>) is a novel GCN-based model which aims at improving the learning ability of syntax and semantic features. RGAT-,PWCN-,ASGCN-RoBERTa are the models improved by <ref type="bibr" target="#b1">Dai et al. (2021)</ref>, which use RoBERTa to induce syntax trees. Compared to spaCy-based syntax trees, the induced trees align with the tokenization strategy of RoBERTa. TGCN-BERT (Tian, Chen, and Song 2021) is a type-aware GCN that measures the importance of each edge in the syntax structure graph by attention mechanism. SARL-RoBERTa  employs adversarial training to eliminate sentiment bias. It applies the spanbased dependency to align the aspects and opinion words. <ref type="table" target="#tab_3">Table 4</ref> shows the experimental performance of LSA models and their counterparts. Overall, LSA and LSA-X models obtain state-of-the-art performance on all datasets without any syntax information 7 , which indicates implicit sentiment modeling is significant in existing ABSC datasets. Besides, we find that syntax tree modeling, which has been widely studied in sentiment dependency modeling, is not significant better than non-syntactical methods for implicit sentiment extraction. For example, LSA T and LSA T -X outperform LSA S and LSA S -X in many scenarios, and the global IQR of LSA T -X is smaller than other models. LSA P and LSA P -X are not as efficient as other LSA models(? 3 times slower) because they have to learn aspect features separately, while we can reuse the global context feature for aspectfocused feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performance</head><p>Compared to LSA, syntax-based sentiment dependency is one of the prominent strategies to address implicit sentiment learning in recent ABSC studies. However, the low quality of syntax tree and the alignment problem between the tree node and tokenization node have to be resolved properly (which will be discussed in Section 4). Some recent works are devoted to refining the quality of the syntax structure (TGCN (Tian, Chen, and Song 2021), etc.) or improving the ability of dependency parsing (Dual-GCN <ref type="bibr" target="#b9">(Li et al. 2021a</ref>), etc.), while the techniques in these works are not easy to be adapted to other model architectures. Meanwhile, <ref type="bibr" target="#b1">Dai et al. (2021)</ref> argue that the existing methods of syntax tree extraction are disappointing and propose to induce the tree structure by fine-tuning the PLM. Although, the performance of ASGCN-RoBERTa, RGAT-RoBERTa, and PWCN-RoBERTa shows hopeful improvement, their resource occupation is tremendous compared to other models.</p><p>Since LSA outperforms these three models on three datasets, it is hard to argue that the inducing syntax trees are necessary given its complexity. SARL-RoBERTa tries to alleviate the sentiment bias and align the sentiment prediction with opinion terms extraction, but it cannot be fairly compared with LSA. Because it reports the best performance in ten runs while we report the average results. In conclusion, the experimental results of LSA models show a hopeful insight for implicit sentiment extraction. <ref type="table">Table 5</ref>: The cases for implicit sentiment learning based on LSA. The target aspects are denoted in bold and the underlined words indicates the difference between two sentences. "Pos", "Neg" and "Neu" represent positive, negative and neutral, respectively.  <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Questions</head><p>According to the experimental results in <ref type="table">Table 5</ref>, we prove LSA's ability to learn implicit sentiment in ABSC.</p><p>RQ2: Does gradient-based aggregation window optimization improve implicit sentiment learning? We find that ? l and ? r are dependent to specific dataset and hyperparameters, so we propose the differential weighted window construction. The first attempt at building the differential sentiment aggregation window is setting static weights ? l and ? r for window components (See Section 7 for details). We hope that ? l and ? r are adaptive, i.e., the model can automatically find the best weights. Thereafter, we use learnable weights ? * l and ? * r to guide the window construction, and we add them to the objective function to optimize them by gradient descent. We initialize ? * l and ? * r to 1. 8 <ref type="table" target="#tab_5">Table 6</ref> shows the experimental results of LSA without DWA models, which indicate a consistent performance drop of while the DWA is ablated. But we also find the performance drop in several scenarios, but it is acceptable as a trade-off for auto-optimization of ? * l and ? * r . Moreover, <ref type="figure" target="#fig_4">Fig. 5</ref> shows the trajectory of ? * l and ? * r during training process. It can be observed that the contribution of side aspects increases rapidly in the beginning and then decreases during the training process. There is generally a small difference between ? * l and ? * r .</p><p>In summary, We observe approximately 0.2% ? 0.5% improvement in most scenarios compared to LSA without DWA.</p><p>RQ3: Can LSA improve existing models' performance?</p><p>We perform comparison experiments on several popular pre-trained language models (including BERT, RoBERTa, and DeBERTa), and the experimental results are shown in <ref type="table" target="#tab_5">Table 6</ref>. Compared with LSA-RoBERTa, LSA-DeBERTa achieves a more significant performance improvement. For LSA-BERT, both Accuracy and F1 achieved an absolute improvement ranging from 0.5% ? 1%. Based on DWA-based LSA, a significant improvement in all datasets has been obtained compared to the prototype PLMs, especially the LSA based on DeBERTa. However, We find that DWA fails to improve the DeBERTa-Large and RoBERTa-Large models noticeably. We believe this problem probably results from the inevitable redundant features in the sentiment aggregation window; the gradient optimization of ? * l and ? * r slows the learning process of the large pretrained models and interferes with the feature representation learning of PLM.</p><p>In a nutshell, LSA is a paradigm rather than a complex network structure, which means LSA is extensible and flexible. In this case, LSA can improve existing methods by simply applying local sentiment aggregation based on their original architecture.</p><p>RQ4: Is LSA effective compared to syntax-based methods in implicit sentiment learning? Recent studies on ABSC implicit sentiment learning are in favor of syntaxbased modeling. However, our study indicates that LSA works better than syntax-based methods in most situations. From the results shown in <ref type="table" target="#tab_3">Table 4</ref>, we can see that our proposed LSA paradigm, which does not consider modeling syntax information (except for LSA S and LSA S -X), consistently outperforms those syntax-based models. Besides, compared to the syntax tree-based LSA S , LSA T and LSA P obtain better performance than LSA S on three datasets. On the other hand, some glitches remain in syntax-based methods. Firstly, these methods usually encounter the token-node alignment problem which has not been settled. Secondly, syntax-based learning in ABSC is inefficient due to additional adjacent matrix modeling. Although <ref type="bibr" target="#b1">Dai et al. (2021)</ref> propose to use a PLM to alleviate the alignment problem, it still requires a startup cost of fine-tuning a tree inducer. Ta-  ble 7 9 shows the resource occupation of LSA; it is observed that the syntax structure-based models introduce extra time and resource occupations.</p><p>In conclusion, LSA is a hopeful paradigm for aspect's implicit sentiment extraction in ABSC. <ref type="table">Table 7</ref>: The resources occupation of state-of-the-art ABSC models. "Proc.T." and "Add.S." indicate the dataset pre-processing time (sec.) and additional storage occupation (MB), respectively. " * " represents non-syntax tree based models, and " ? " indicates our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laptop14</head><p>Restaurant14 Proc.T. Add.S. Proc.T. Add.S. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-BASE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Compared to coarse-grained implicit sentiment research <ref type="bibr" target="#b2">(de Kauter, Breesch, and Hoste 2015;</ref><ref type="bibr" target="#b26">Zhou et al. 2021;</ref><ref type="bibr" target="#b11">Liao et al. 2022;</ref><ref type="bibr" target="#b28">Zhuang et al. 2022)</ref>, the aspect's implicit sentiment learning in ABSC is still a challenging problem in recent years. Cai, Xia, and Yu (2021) formulate a quadruple extraction task (i.e., aspect, category, opinion and sentiment) which aims to model implicit sentiments and opinions. However, this method is not easy to be adapted to existing datasets because the existing datasets only have sentiment annotations. <ref type="bibr" target="#b13">Murtadha et al. (2022)</ref> propose a unified framework, that crafts auxiliary sentences, to help implicit aspect extraction and sentiment analysis. <ref type="bibr" target="#b10">Li et al. (2021b)</ref> present a supervised contrastive pre-training mechanism to align the representation of implicit sentiment and explicit sentiment. This method achieves hopeful performance improvement in both implicit and explicit sentiment learning. However, it is enhanced by fine-tuning on a largescale sentiment-annotated corpus from in-domain language resources, which may be inefficient and resource-intense. On the other hand, syntax-based sentiment dependency modeling can be used to model related aspects' sentiment. For example, <ref type="bibr" target="#b24">(Zhang, Li, and Song 2019;</ref><ref type="bibr" target="#b27">Zhou et al. 2020;</ref><ref type="bibr" target="#b19">Tian, Chen, and Song 2021;</ref><ref type="bibr" target="#b9">Li et al. 2021a;</ref><ref type="bibr" target="#b1">Dai et al. 2021)</ref> explore the effectiveness of syntax-structure in ABSC, which alleviates implicit sentiment modeling failure. However, the node alignment problem introduced by different implementations of syntax parsing methods are still critical problems have to be solved. For refining the quality of syntax structure, <ref type="bibr" target="#b19">Tian, Chen, and Song (2021)</ref> employ the type-aware GCN to distinguish different relations in the graph and achieve promising performance. <ref type="bibr">9</ref> The experiments are based on RTX2080 GPU, AMD R5-3600 CPU with PyTorch 1.9.0. The original size of the Laptop14 and Restaurant14 datasets are 336kb and 492kb, respectively.</p><p>To enhance GCN models in dependency parsing, <ref type="bibr" target="#b9">Li et al. (2021a)</ref> propose SynGCN and SemGCN for different dependency information. TGCN alleviates dependency parsing errors and shows considerable improvement compared to previous GCN-based models. However, the new techniques in these papers are not easy to transfer. <ref type="bibr" target="#b1">Dai et al. (2021)</ref> propose employing the pre-trained RoBERTa model to induce trees for ABSC, which solves the node alignment problem. However, the efficiency of inducing trees should be improved. Compared to related works, LSA avoids the efficiency trap of syntax modeling by eliminating structure information, and it achieves state-of-the-art performance on three public datasets. Furthermore, LSA is readily to be adapted to existing methods because it is a transferable paradigm that only aggregate adjacent aspect's sentiment information before output layers with other modules keep intact at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Implicit sentiment learning in ABSC has been an difficult problem. We find the implicit aspect sentiments in ABSC can be modeled according to some empirical setniment patterns. Hence, we propose LSA which apply local sentiment aggregation to learn implicit sentiment in ABSC. Moreover, we also propose differential weighting for sentiment aggregation window to control the contribution of adjacent aspect's sentiment information. Our experimental results on three mostly used ABSC datasets indicate LSA's efficacy. Moreover, compared to existing methods, LSA achieves state-of-the-art performance without loss of transferability and simplicity. Our work shows that local sentiment aggregation is a promising method to prompt ABSC implicit sentiment learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiment of Static Weighted Sentiment Aggregation</head><p>Besides the dynamic sentiment window differential weighting, we also try static weight to control the contribution of adjacent aspects' sentiment information. We first initialize ? l , ? ? [0, 1]), for the left-adjacent aspects, while ? r = 1?? l . In this case, a greater ? l means more importance of the left-adjacent aspect's feature and vice versa. However, it is difficult to search for the optimal static weights for many scenarios via gird search; we even found that the performance trajectory is non-convex while ? l ? [0, 1], indicating the ? l on a dataset will be difficult to reuse on another dataset. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the performance curve of LSAbased on DeBERTa under different ? l . In other words, static differential weighting is inefficient and unstable. We recommend applying an automatic weights search to find a better construction strategy for the sentiment window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiment of Simplified Sentiment Aggregation Window</head><p>To investigate the necessity of bidirectional aggregation, we assess the effectiveness of the streamlined aggregation window. We simply concatenate the left or right adjacent aspect's feature with the targeted aspect's feature and then change the output layer to accommodate the new feature dimension of the simplified aggregation window. <ref type="table">Table 8</ref> shows the experimental results. From the performance comparison of simplified aggregation, we observe that the full LSA is optimal in most situations, despite the underlying PLM or training dataset. Moreover, to our surprise, LSA with "RA" outperforms LSA with "LA" in some situations. However, this is slightly different from the conclusion in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Additional Experiments on Twitter Dataset</head><p>We conduct the experiments on the Twitter dataset, the experimental results are shown in <ref type="table" target="#tab_8">Table 9</ref>. <ref type="table">Table 8</ref>: The average performance deviation of ablated LSA baselines. "LA" and "RA" indicates the simplified aggregating window constructed only exploits the left-adjacent aspect or right-adjacent aspect, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laptop14</head><p>Restaurant14 Acc F1 Acc F1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Limitation of Local Sentiment Aggregation</head><p>Although LSA achieves impressive performance for multiple-aspects situations, e.g., SemEval-2014 datasets. However, while being applied in mono aspect situations, LSA degenerates to be equivalent to a prototype model, e.g., the local context focus model. For LSA S , an additional limitation comes from the quality of the syntax tree. Because we use spaCy to extract the syntax tree follow in previous works, the alignment problem of tree nodes and tokens remains. Which leads to many distance calculation errors for local context extraction. Considering the LSA T and LSA P are ? 10 times faster in the data pre-processing phase, they would be better choices than LSA S in most situations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example to visualize of the sentiment cluster and sentiment coherency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>To comprehensively validate LSA's efficacy, we use the following local sentiment feature representation in LSA: a) Sentence pair-based (BERT-SPC) based aspect feature<ref type="bibr" target="#b3">(Devlin et al. 2019</ref>) (employed in LSA P ) b) Local context focus (LCF) based aspect feature<ref type="bibr" target="#b23">(Yang et al. 2021</ref>) (employed in LSA T ) c) Syntactical local context focus (LCFS) aspect feature (Phan and Ogunbona 2020) (employed in LSA S ) The local sentiment aggregation paradigm based on BERT-SPC, denoted as LSA P . "SA" indicates the selfattention encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The local sentiment aggregation paradigm based on LCF/LCFS, denoted asLSA T and LSA S . "SA" indicates the self-attention encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Window padding strategies for different situations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of performance under differential weighted sentiment window construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of performance under static differential weighting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>The number of aspect in sentiment clusters with different sizes.</figDesc><table><row><cell>Dataset</cell><cell>1</cell><cell>2</cell><cell cols="2">Cluster Size 3</cell><cell>4</cell><cell>?5</cell><cell>Sum</cell></row><row><cell>Laptop14</cell><cell>791</cell><cell cols="2">799</cell><cell cols="3">468 294 614</cell><cell>2966</cell></row><row><cell cols="8">Restaurant14 1318 1050 667 479 1214 4728</cell></row><row><cell cols="2">Restaurant15 617</cell><cell cols="2">406</cell><cell cols="3">229 163 326</cell><cell>1741</cell></row><row><cell cols="2">Restaurant16 836</cell><cell cols="2">539</cell><cell cols="3">314 210 462</cell><cell>2361</cell></row><row><cell>MAMS</cell><cell cols="7">6463 2583 1328 746 1397 12517</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where H o aw is the feature representation learned by local sentiment aggregation; ";" denotes vector concatenation. {H l k } and {H r k } are the k nearest left and right adjacent aspect features, respectively. H t is the targeted aspect feature. H o is the representation learned by sentiment aggregation window, and W o and b o are the trainable weights and biases.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The statistics of three datasets used in our experiments. Note that the existing ABSC datasets generally have no validation sets in previous research except for the MAMS dataset.</figDesc><table><row><cell>Datasets</cell><cell cols="6">Positive Training Testing Training Testing Training Testing Negative Neutral</cell></row><row><cell>Laptop14</cell><cell>994</cell><cell>341</cell><cell>870</cell><cell>128</cell><cell>464</cell><cell>169</cell></row><row><cell>Restaurant14</cell><cell>2164</cell><cell>728</cell><cell>807</cell><cell>196</cell><cell>637</cell><cell>196</cell></row><row><cell>MAMS</cell><cell>3379</cell><cell>400</cell><cell>2763</cell><cell>329</cell><cell>5039</cell><cell>607</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The overall performance of LSA models on the three public datasets, and the best results are heightened in bold font. Numbers in parentheses are IQRs.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Laptop14</cell><cell cols="2">Restaurant14</cell><cell>MAMS</cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>SK-GCN-BERT (Zhou et al. 2020)</cell><cell></cell><cell>79.00</cell><cell>75.57</cell><cell>83.48</cell><cell>75.19</cell><cell>-</cell><cell>-</cell></row><row><cell>SDGCN-BERT (Zhao, Hou, and Wu 2020)</cell><cell></cell><cell>81.35</cell><cell>78.34</cell><cell>83.57</cell><cell>76.47</cell><cell>-</cell><cell>-</cell></row><row><cell>DGEDT-BERT (Tang et al. 2020)</cell><cell></cell><cell>79.80</cell><cell>75.60</cell><cell>86.30</cell><cell>80.00</cell><cell>-</cell><cell>-</cell></row><row><cell>DualGCN-BERT (Li et al. 2021a)</cell><cell></cell><cell>81.80</cell><cell>78.10</cell><cell>87.13</cell><cell>81.16</cell><cell>-</cell><cell>-</cell></row><row><cell>ASGCN-RoBERTa Dai et al. (2021) RGAT-RoBERTa Dai et al. (2021) PWCN-RoBERTa Dai et al. (2021) TGCN-BERT (Tian, Chen, and Song 2021)</cell><cell>Baselines</cell><cell>83.33 83.33 84.01 80.88</cell><cell>80.32 79.95 81.08 77.03</cell><cell>86.87 87.52 87.35 86.16</cell><cell>80.59 81.29 80.85 79.95</cell><cell>---83.38</cell><cell>---82.77</cell></row><row><cell>SARL-RoBERTa  ? (Wang et al. 2021) RoBERTa (Liu et al. 2019)  ? DeBERTa (He, Gao, and Chen 2021)  ?</cell><cell></cell><cell cols="6">85.42 82.76(0.63) 79.73(0.77) 87.77(1.61) 82.10(2.01) 83.83(0.49) 83.29(0.50) 82.97 88.21 82.44 --82.76(0.31) 79.45(0.60) 88.66(0.35) 83.06(0.29) 83.06(1.24) 82.52(1.25)</cell></row><row><cell>LSA P -RoBERTa LSA T -RoBERTa LSA S -RoBERTa</cell><cell>LSA</cell><cell cols="6">83.39(0.35) 80.47(0.44) 88.04(0.62) 82.96(0.48) 83.37(0.31) 83.78(0.29) 83.44(0.56) 80.47(0.71) 88.30(0.37) 83.09(0.45) 83.31(0.41) 84.60(0.22) 83.23(0.44) 80.30(0.68) 88.48(0.52) 83.81(0.62) 83.58(0.39) 83.78(0.24)</cell></row><row><cell>LSA P -DeBERTa LSA T -DeBERTa LSA S -DeBERTa</cell><cell>LSA</cell><cell cols="6">84.33(0.55) 81.46(0.77) 89.91(0.09) 84.90(0.45) 83.91(0.31) 83.31(0.21) 84.80(0.39) 82.00(0.43) 89.91(0.40) 85.05(0.85) 84.28(0.32) 83.70(0.47) 84.17(0.08) 81.23(0.27) 89.64(0.66) 84.53(0.79) 83.61(0.30) 83.07(0.28)</cell></row><row><cell>LSA P -X-DeBERTa LSA</cell><cell>LSA-X</cell><cell cols="6">86.00(0.07) 83.10(0.30) 90.27(0.61) 85.51(0.48) 82.78(0.96) 81.99(0.86)</cell></row></table><note>T -X-DeBERTa 86.31(0.20) 83.93(0.27) 90.86(0.18) 86.26(0.22) 84.21(0.42) 83.72(0.46) LSAS -X-DeBERTa 86.21(0.52) 83.97(0.64) 90.33(0.37) 85.55(0.46) 84.68(0.67) 84.12(0.64) ? indicates the results are the best performance in multiple runs, while other methods report the average performance; ? indicates the experimental results of the models implemented by us.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>RQ1: Does LSA learn to extract implicit sentiment? To figure out whether LSA is capable of learning implicit sentiment, we conduct experiments and show several cases inTable 5. Overall, our observation indicates that LSA outperforms its base PLM model, which also accounts for the performance improvement of LSA in three public ABSC datasets. For example, the DeBERTa in example #4 gives two inference errors while all LSA based on the DeBERTa model yields correct results. Meanwhile, the experimental results of LSA P BERT and LSA S BERT output fewer wrong inferences. Besides, LSA P , LSA T and LSA S show promising robustness in dealing with perturbed examples containing implicit sentiments. Although it is hard to list more examples, all the experiments show consistent observation compared to</figDesc><table><row><cell>No.</cell><cell>Domain</cell><cell>Example</cell><cell>Model</cell><cell>Sentiment</cell></row><row><cell>1</cell><cell>Restaurant</cell><cell>Not only was the food outstanding, but also the coffee and juice! Not only was the food terrible, but also the coffee and juice!</cell><cell>LSAP -BERT LSAP -BERT</cell><cell>Pos(Pos) , Pos(Pos) Neg(Neg) , Neu(Neg)</cell></row><row><cell>2</cell><cell>Restaurant</cell><cell>The servers always surprise us with a different starter. The servers always temporize us with a different starter.</cell><cell>LSAS-BERT LSAS-BERT</cell><cell>Pos(Pos) Neg(Neg)</cell></row><row><cell>3</cell><cell>TV</cell><cell>The speakers of this TV is great! Just like its screen. The speakers of this TV sucks! Just like its screen.</cell><cell>LSAT -DeBERTa LSAT -DeBERTa</cell><cell>Pos(Pos) Neg(Neg)</cell></row><row><cell>4</cell><cell>Camera</cell><cell>If you are worry about usability, think about the quality ! If you are worry about usability, think about it good quality !</cell><cell>DeBERTa DeBERTa</cell><cell>Neu(Pos) Pos(Pos)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The performance of LSA based on different PLMs. The best experimental results are heightened in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Laptop14</cell><cell cols="2">Rest14</cell><cell cols="2">MAMS</cell></row><row><cell>Model</cell><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>BERT-BASE RoBERTa-BASE</cell><cell>Base</cell><cell>80.36(0.78) 82.76(0.63)</cell><cell>77.04(0.71) 79.73(0.77)</cell><cell>86.34(0.18) 87.77(1.61)</cell><cell>80.01(0.28) 82.10(2.01)</cell><cell>82.52(1.13) 83.83(0.49)</cell><cell>81.87(1.23) 83.29(0.50)</cell></row><row><cell>DeBERTa-BASE</cell><cell></cell><cell>82.76(0.31)</cell><cell>79.45(0.60)</cell><cell>88.66(0.35)</cell><cell>83.06(0.29)</cell><cell>83.06(1.24)</cell><cell>82.52(1.25)</cell></row><row><cell>LSA P -BERT</cell><cell></cell><cell>80.67(0.47)</cell><cell>77.20(0.69)</cell><cell>86.43(0.13)</cell><cell>80.71(0.47)</cell><cell>83.58(0.56)</cell><cell>83.00(0.55)</cell></row><row><cell>LSA T -BERT</cell><cell></cell><cell>80.72(0.31)</cell><cell>77.16(0.27)</cell><cell>87.53(0.58)</cell><cell>81.85(0.69)</cell><cell>83.03(0.34)</cell><cell>82.34(0.42)</cell></row><row><cell>LSA S -BERT LSA P -RoBERTa LSA T -RoBERTa</cell><cell>w/o DWA</cell><cell>80.62(0.55) 82.55(0.78) 82.76(0.55)</cell><cell>76.89(0.44) 79.93(0.83) 80.08(0.44)</cell><cell>86.70(0.62) 87.68(0.48) 87.59(1.03)</cell><cell>81.11(0.79) 82.46(0.65) 82.02(1.29)</cell><cell>82.41(1.35) 83.31(0.47) 83.53(0.45)</cell><cell>81.71(1.45) 82.90(0.62) 82.92(0.32)</cell></row><row><cell>LSA S -RoBERTa LSA P -DeBERTa</cell><cell>LSA</cell><cell>82.92(0.39) 84.27(0.47)</cell><cell>80.10(0.57) 81.38(0.23)</cell><cell>88.21(0.89) 89.60(0.51)</cell><cell>82.32(0.78) 84.90(0.49)</cell><cell>83.95(0.34) 84.06(0.08)</cell><cell>83.30(0.54) 83.57(0.18)</cell></row><row><cell>LSA T -DeBERTa</cell><cell></cell><cell>84.27(0.31)</cell><cell>81.18(0.29)</cell><cell>89.79(0.71)</cell><cell>84.88(1.13)</cell><cell>83.01(0.86)</cell><cell>82.53(0.92)</cell></row><row><cell>LSA S -DeBERTa</cell><cell></cell><cell>83.91(0.78)</cell><cell>81.24(1.01)</cell><cell>89.73(0.46)</cell><cell>84.71(0.55)</cell><cell>83.31(0.41)</cell><cell>82.80(0.58)</cell></row><row><cell>LSA P -BERT</cell><cell></cell><cell>81.35(0.63)</cell><cell>77.79(0.48)</cell><cell>87.23(0.22)</cell><cell>81.06(0.67)</cell><cell>83.13(0.30)</cell><cell>82.53(0.44)</cell></row><row><cell>LSA T -BERT</cell><cell></cell><cell>81.35(0.39)</cell><cell>78.43(0.52)</cell><cell>87.32(0.22)</cell><cell>81.86(0.20)</cell><cell>83.51(0.26)</cell><cell>82.90(0.28)</cell></row><row><cell>LSA S -BERT</cell><cell></cell><cell>81.03(0.31)</cell><cell>77.45(0.37)</cell><cell>87.41(0.40)</cell><cell>81.52(0.49)</cell><cell>83.23(0.56)</cell><cell>82.68(0.52)</cell></row><row><cell>LSA P -RoBERTa LSA T -RoBERTa</cell><cell>LSA</cell><cell>83.39(0.35) 83.44(0.56)</cell><cell>80.47(0.44) 80.47(0.71)</cell><cell>88.04(0.62) 88.30(0.37)</cell><cell>82.96(0.48) 83.09(0.45)</cell><cell>83.37(0.31) 83.31(0.41)</cell><cell>83.78(0.29) 83.60(0.22)</cell></row><row><cell>LSA S -RoBERTa</cell><cell></cell><cell>83.23(0.44)</cell><cell>80.30(0.68)</cell><cell>88.48(0.52)</cell><cell>83.81(0.62)</cell><cell>83.58(0.39)</cell><cell>83.78(0.24)</cell></row><row><cell>LSA P -DeBERTa</cell><cell></cell><cell>84.33(0.55)</cell><cell>81.46(0.77)</cell><cell>89.91(0.09)</cell><cell>84.90(0.45)</cell><cell>83.91(0.31)</cell><cell>83.31(0.21)</cell></row><row><cell>LSA T -DeBERTa</cell><cell></cell><cell>84.80(0.39)</cell><cell>82.00(0.43)</cell><cell>89.91(0.40)</cell><cell>85.05(0.85)</cell><cell>84.28(0.32)</cell><cell>83.70(0.47)</cell></row><row><cell>LSA S -DeBERTa</cell><cell></cell><cell>84.17(0.08)</cell><cell>81.23(0.27)</cell><cell>89.64(0.66)</cell><cell>84.53(0.79)</cell><cell>83.61(0.30)</cell><cell>83.07(0.28)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>The performance of LSA models on the Twitter datasets, and the best results are heightened in bold. Numbers in parentheses denote IQR.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Twitter</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>F1</cell></row><row><cell>LSAP -DeBERTa LSAT -DeBERTa LSAS-DeBERTa</cell><cell>LSA</cell><cell cols="2">76.91(0.36) 75.90(0.41) 76.61(0.20) 76.12(0.27) 76.61(0.52) 75.84(0.64)</cell></row><row><cell>LSAP -X-DeBERTa LSAT -X-DeBERTa LSAS-X-DeBERTa</cell><cell>LSA-X</cell><cell cols="2">76.81(0.76) 76.09(0.50) 77.17(0.71) 76.45(0.65) 77.06(0.26) 76.23(0.29)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We implement LSA based on the transformers: https://github. com/huggingface/transformers 6 We also evaluate LSA on the Twitter (Dong et al. 2014) dataset and report the experimental results in Section 9. However, differing to the common datasets, all the examples in Twitter dataset only contain one aspect, which degenerates LSA's perforamnce. The processed datasets are available with the code in supplementary materials.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Except for LSAS, which utilizes the syntax tree to calculate the token distances. But it avoids structural modeling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">we do not use random initialization because it will be slow to optimize initial ? * l and ? * r that are close to 0.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="340" to="350" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<date type="published" when="2021-06-06" />
			<biblScope unit="page" from="1816" to="1829" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finegrained analysis of explicit and implicit sentiment in financial news articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>De Kauter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Breesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4999" to="5010" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Burstein, J.</editor>
		<editor>Doran, C.</editor>
		<editor>and Solorio, T.</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Recursive Neural Network for Targetdependent Twitter Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
		<respStmt>
			<orgName>The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2111.09543</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="5468" to="5476" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="6279" to="6284" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Explicit and Implicit Structures for Targeted Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="5477" to="5487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual Graph Convolutional Networks for Aspectbased Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<editor>Zong, C.</editor>
		<editor>Xia, F.</editor>
		<editor>Li, W.</editor>
		<editor>and Navigli, R.</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6319" to="6329" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Moens, M.</editor>
		<editor>Huang, X.</editor>
		<editor>Specia, L.</editor>
		<editor>and Yih, S. W.</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Virtual Event / Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="246" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic commonsense knowledge fused method for Chinese implicit sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">102934</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning in Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2203.11702</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Jurafsky, D.</editor>
		<editor>Chai, J.</editor>
		<editor>Schluter, N.</editor>
		<editor>and Tetreault, J. R.</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3211" to="3220" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SemEval-2016 Task 5: Aspect Based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Kotelnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M J</forename><surname>Zafra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eryigit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<editor>Bethard, S.</editor>
		<editor>Cer, D. M.</editor>
		<editor>Carpuat, M.</editor>
		<editor>Jurgens, D.</editor>
		<editor>Nakov, P.</editor>
		<editor>and Zesch, T.</editor>
		<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 12: Aspect Based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015</title>
		<editor>Cer, D. M.</editor>
		<editor>Jurgens, D.</editor>
		<editor>Nakov, P.</editor>
		<editor>and Zesch, T.</editor>
		<meeting>the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-04" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SemEval-2014 Task 4: Aspect Based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<editor>Nakov, P.</editor>
		<editor>and Zesch, T.</editor>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>SemEval@COLING; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dependency Graph Enhanced Dual-transformer Structure for Aspectbased Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Jurafsky, D.</editor>
		<editor>Chai, J.</editor>
		<editor>Schluter, N.</editor>
		<editor>and Tetreault, J. R.</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6578" to="6588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06-06" />
			<biblScope unit="page" from="2910" to="2922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11-20" />
			<biblScope unit="page" from="3002" to="3012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multi-task learning model for Chinese-oriented aspect polarity classification and aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">419</biblScope>
			<biblScope unit="page" from="344" to="356" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="4567" to="4577" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling sentiment dependencies with graph convolutional networks for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">105443</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implicit Sentiment Analysis with Event-centered Text Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Moens, M.</editor>
		<editor>Huang, X.</editor>
		<editor>Specia, L.</editor>
		<editor>and Yih, S. W.</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-07-11" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="6884" to="6893" />
		</imprint>
		<respStmt>
			<orgName>Virtual Event / Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SK-GCN: Modeling Syntax and Knowledge via Graph Convolutional Network for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page">106292</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Implicit sentiment analysis based on multi-feature neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="635" to="644" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
