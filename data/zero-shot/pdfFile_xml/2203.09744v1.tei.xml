<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihuang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
							<email>cslzhang@comp.polyu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
							<email>xjia@dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptive semantic segmentation aims to learn a model with the supervision of source domain data, and produce satisfactory dense predictions on unlabeled target domain. One popular solution to this challenging task is self-training, which selects high-scoring predictions on target samples as pseudo labels for training. However, the produced pseudo labels often contain much noise because the model is biased to source domain as well as majority categories. To address the above issues, we propose to directly explore the intrinsic pixel distributions of target domain data, instead of heavily relying on the source domain. Specifically, we simultaneously cluster pixels and rectify pseudo labels with the obtained cluster assignments. This process is done in an online fashion so that pseudo labels could co-evolve with the segmentation model without extra training rounds. To overcome the class imbalance problem on long-tailed categories, we employ a distribution alignment technique to enforce the marginal class distribution of cluster assignments to be close to that of pseudo labels. The proposed method, namely Class-balanced Pixel-level Self-Labeling (CPSL), improves the segmentation performance on target domain over state-of-the-arts by a large margin, especially on long-tailed categories. The source code is available at https://github.com/lslrh/CPSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a fundamental computer vision task, which aims to make dense semantic-level predictions on images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53]</ref>. It is a key step in numerous applications, including autonomous driving, human-machine interaction, and augmented reality, to name a few. In the past few years, the rapid development of deep Convolutional Neural Networks (CNNs) has boosted semantic segmentation significantly in terms of accuracy and efficiency. However, the performance of deep models trained in one * Corresponding Author domain often drops largely when they are applied to unseen domains. For example, in autonomous driving the segmentation model is confronted with great challenges when weather conditions are changing constantly <ref type="bibr" target="#b55">[56]</ref>. A natural way to improve the generalization ability of segmentation model is to collect data from as many scenarios as possible. However, it is very costly to annotate pixel-wise labels for a large amount of images <ref type="bibr" target="#b10">[11]</ref>. More effective and practical approaches are required to address the domain shifts of semantic segmentation.</p><p>Unsupervised Domain Adaptation (UDA) provides an important way to transfer the knowledge learned from one labeled source domain to another unlabeled target domain. For example, we can collect many synthetic data whose dense annotations are easy to get by using game engines such as GTA5 <ref type="bibr" target="#b35">[36]</ref> and SYNTHIA <ref type="bibr" target="#b36">[37]</ref>. Then the question turns to how to adapt the model trained from a labeled synthetic domain to an unlabeled real image domain. Most previous works of UDA bridge the domain gap by aligning data distributions at the image level <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>, feature level <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref> or output level <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref>, through adversarial training or auxiliary style transfer networks. However, these techniques will increase the model complexity and make the training process unstable, which impedes their reproducibility and robustness.</p><p>Another important approach is self-training <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, which alternatively generates pseudo labels by selecting high-scoring predictions on target domain and provides supervision for the next round of training. Though these methods have produced promising performance, there are still some major limitations. On one hand, the segmentation model tends to be biased to source domain so that the pseudo labels produced on target domain are error-prone; on the other hand, highly-confident predictions may only provide very limited supervision information for the model training. To solve these issues, some methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> have been proposed to produce more accurate and informative pseudo labels. For example, instead of using the classifier trained on source domain to generate pseudo labels, Zhang et al. <ref type="bibr" target="#b50">[51]</ref> assigned pseudo labels to pixels based on their distances to the category prototypes. These prototypes, however, were built in source domain and usually deviated much from the target domain. ProDA <ref type="bibr" target="#b49">[50]</ref> leveraged the feature distances from prototypes to perform online rectification, but it was challenging to construct prototypes for long-tailed categories, which often led to unsatisfactory performance.</p><p>Different from previous self-training methods which use classifier-based noisy pseudo labels for supervision, in this paper we propose to perform online pixel-level self-labeling via clustering on target domain, and use the resulting soft cluster assignments to correct pseudo labels. Our idea comes from the fact that pixel-wise cluster assignments could reveal the intrinsic distributions of pixels in target domain, and provide useful supervision for model training. Compared to conventional label generation methods that are often biased towards source domain, cluster assignment in target domain is more reliable as it explores inherent data distribution. Considering that the classes of segmentation dataset are highly imbalanced (please refer to <ref type="figure" target="#fig_0">Fig. 2</ref>), we employ a distribution alignment technique to enforce the class distribution of cluster assignments to be close to that of pseudo labels, which is more favorable to class-imbalanced dense prediction tasks. The proposed Class-balanced Pixellevel Self-Labeling (CPSL) module works in a plug-andplay fashion, which could be seamlessly incorporated into existing self-training framework for UDA. The major contributions of this work are summarized as follows:</p><p>? A pixel-level self-labeling module is developed for domain adaptive semantic segmentation. We cluster pixels in an online fashion and simultaneously rectify pseudo labels based on the resulting cluster assignments.</p><p>? A distribution alignment technique is introduced to align the class distribution of cluster assignments to that of pseudo labels, aiming to improve the performance over long-tailed categories. A class-balanced sampling strategy is adopted to avoid the dominance of majority categories in pseudo label generation.</p><p>? Extensive experiments demonstrate that the proposed CPSL module improves the segmentation performance on target domain over state-of-the-arts by a large margin. It especially shows outstanding results on long-tailed classes such as "motorbike", "train", "light", etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Segmentation. The goal of semantic segmentation is to segment an image into regions of different semantic categories. While the Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b27">[28]</ref> have greatly boosted the performance of semantic segmentation, they have relatively small receptive field to explore visual context. Many later works focus on how to enlarge the receptive field of FCNs to model longrange context dependencies of images, such as dilated convolution <ref type="bibr" target="#b7">[8]</ref>, multi-layer feature fusion <ref type="bibr" target="#b26">[27]</ref>, spatial pyramid pooling <ref type="bibr" target="#b52">[53]</ref> and variants of non-local blocks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. However, directly applying these models to unseen domains will induce poor segmentation performance because of their weak generalization ability. Therefore, many domain adaptation techniques have been proposed to improve model generalization ability on new domains.</p><p>Domain Adaptation for Semantic Segmentation. Recently, many works have been proposed to bridge the domain gap and improve the adaptation performance. The most representative ones are adversarial training-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, which aim to align different domains on intermediate features or network predictions. Style transfer-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref> minimize domain gap at the image level. For example, Chang et al. <ref type="bibr" target="#b5">[6]</ref> proposed to disentangle an image into domaininvariant structures and domain-specific textures for image translation. The training process of these models is rather complex since multiple networks, such as discriminators or style transfer networks, have to be trained concurrently. Another important technique for UDA is selftraining <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57]</ref>, which iteratively generates pseudo labels on target data for model update. Zou et al. <ref type="bibr" target="#b54">[55]</ref> proposed a class-balanced self-training method for domain adaption of semantic segmentation. To reduce the noise in pseudo labels, Zou et al. <ref type="bibr" target="#b56">[57]</ref> further proposed a confidence regularized self-training method, which treated pseudo labels as trainable latent variables. Lian et al. <ref type="bibr" target="#b25">[26]</ref> constructed a pyramid curriculum for exploring various properties about the target domain. Zhang et al. <ref type="bibr" target="#b50">[51]</ref> enforced category-aware feature alignment by choosing the prototypes of source domain as guided anchors. ProDA <ref type="bibr" target="#b49">[50]</ref> went further by employing the feature distances from each pixel to prototypes to correct pseudo labels pre-computed by the source model. These methods, however, neglect either the pixel-wise intrinsic structures or inherent class distribution of target domain images, tending to be biased to source domain or majority classes.</p><p>Clustering-based Representation Learning. Our work is also related to clustering-based methods <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58]</ref>. Caron et al. <ref type="bibr" target="#b3">[4]</ref> iteratively performed k-means on latent representations and used the produced cluster assignments to update network parameters. Recently, Asano et al. <ref type="bibr" target="#b2">[3]</ref> cast the cluster assignment problem as an optimal transport problem which can be solved efficiently through a fast variant of the Sinkhorn-Knopp algorithm. SwAV <ref type="bibr" target="#b4">[5]</ref> performed clustering while enforcing consistency among the cluster assignments of different augmentations of the same image. In this paper, we extend self-labeling from image-level classification to pixel-level semantic segmentation. In addition, different from Asano et al. <ref type="bibr" target="#b2">[3]</ref> and Caron et al. <ref type="bibr" target="#b3">[4]</ref>, we com- <ref type="figure">Figure 1</ref>. The framework of Class-balanced Pixel-level Self-Labeling (CPSL). The model contains a main segmentation network fSEG and its momentum-updated version f SEG . The f SEG is followed by a self-labeling head fSL and its momentum version f SL , which projects pixel-wise feature embedding into a class probability vector. The pixel-level self-labeling module produces soft cluster assignment PSL to gradually rectify soft pseudo label PST. Then the segmentation loss L t SEG is computed between the prediction map P and the rectified pseudo label? t . To train the self-labeling head, we randomly sample pixels from each image, and use the memory bank M, which contains previous batches of pixel features, to augment the current batch. Then we compute the optimal transport assignment Qaug over the augmented data by enforcing class balance, and use the assignment of current batch Qcur to compute the self-labeling loss LSL.</p><p>pute cluster assignments in an online fashion, making our method scalable to dense pixel-wise prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Framework</head><p>In the setting of unsupervised domain adaptation for semantic segmentation, we are provided with a set of labeled data in source domain D S = {(X s n , Y s n )} N S n=1 , where X s n is the source image with label Y s n and N S is the number of images, as well as a set of N T unlabeled images X t n in target domain D T = {X t n } N T n=1 . Both domains share the same C classes. Our goal is to learn a model by using the labeled source data in D S and unlabeled target data in D T , which could perform well on unseen test data in the target domain.</p><p>The overall framework of our proposed CPSL is shown in <ref type="figure">Fig. 1</ref>. We propose a pixel-level self-labeling module (highlighted in the green color box) to explore the intrinsic pixel-wise distributions of the target domain data via clustering, and to reduce the noise in pseudo labels. Before the training, we first generate a soft pseudo label map P ST ? R H?W ?C for each target domain image by a warmed-up model that is pre-trained on the source domain data. The obtained P ST is usually error-prone because of the large domain shift. Therefore, in the training process, we rectify P ST incrementally with the soft cluster assignment, denoted by P SL ? R H?W ?C . Specifically, the rectification of P ST is conducted as follows:</p><formula xml:id="formula_0">Y t,(c) n,i = ? ? ? 1, if c = argmax c * (P (c * ) SL,n,i ? P (c * ) ST,n,i ) 0, otherwise ,<label>(1)</label></formula><p>where? t,(c) n,i denotes the c-th element of rectified pseudo label at the i-th pixel of target image X t n . P (c * ) SL,n,i represents the probability that the i-th pixel of X t n belongs to the c *th category. Eq. 1 has a similar formulation to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref>, where P SL can be regarded as the weight map to modulate the softmax probability map P ST . The cluster assignment P SL exploits the inherent data distribution of target domain, thus it is highly complementary to the classifier-based pseudo label P ST which heavily relies on source domain.</p><p>We define the segmentation loss on target domain, denoted by L t SEG , as the pixel-level cross-entropy loss between the segmentation probability map P n ? R H?W ?C and the rectified pseudo label? t n of target image X t n :</p><formula xml:id="formula_1">L t SEG = ? N T n=1 H?W i=1 C c=1? t,(c) n,i log P (c) n,i .<label>(2)</label></formula><p>In addition, the loss on source domain, denoted by L s SEG , can be defined as the standard pixel-wise cross-entropy on the labeled images:</p><formula xml:id="formula_2">L s SEG = ? N S n=1 H?W i=1 C c=1 Y s,(c) n,i log P (c) n,i .<label>(3)</label></formula><p>Then the total segmentation loss L SEG is obtained as the sum of them: L SEG = L t SEG + L s SEG . In the following subsections, we will explain in detail the design of our CPSL module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Online Pixel-Level Self-Labeling</head><p>Pixel-Level Self-Labeling. Conventional self-training based methods usually use a model pre-trained on source domain to produce pseudo labels, which often contain much noise <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57]</ref>. To clean the pseudo labels, we propose to perform pixel-level self-labeling via clustering on target domain and use the obtained cluster assignments to rectify the pseudo labels. The basic motivation is that pixel-wise clustering could reveal the intrinsic structures of target domain data, and it is complementary to the classifier trained on source domain data. Thus, cluster assignments could provide extra supervision for training a domain adaptive segmentation model.</p><p>Specifically, we first extract features from an input image to obtain Z ? R H?W ?D and normalize it with z i = zi ||zi||2 , where z i is the i-th feature vector of Z with length D. Then we randomly sample a group of pixels? = [z 1 , ? ? ? , z M ] from each image, and pass them through a self-labeling head f SL . Finally, we obtain their class probability vector? P = [p 1 , ? ? ? , p M ] by taking a softmax operation:</p><formula xml:id="formula_3">p (c) m = exp( 1 ? f (c) SL (z m )) c exp( 1 ? f (c ) SL (z m )) , c ? {1, ? ? ? , C},<label>(4)</label></formula><p>where f</p><formula xml:id="formula_4">(c) SL (z m ) is the c-th element of the output of z m from self-labeling head. p (c)</formula><p>m denotes the probability that the m-th pixel belongs to the c-th category. ? is a temperature parameter. Considering there is no ground truth label available for target data, we train the head f SL through a self-labeling mechanism <ref type="bibr" target="#b2">[3]</ref> with the following objective function:</p><formula xml:id="formula_5">LSL = ? 1 M M m=1 C c=1 q (c) m log p (c) m s.t. Q ? Q, with Q := {Q ? R C?M + |Q1M = r, Q T 1C = h}.<label>(5)</label></formula><p>The above formula is an instance of the optimal transport problem <ref type="bibr" target="#b12">[13]</ref>, where Q = 1 M [q 1 , ? ? ? , q M ] is a transport assignment and it is restricted to be a probability matrix by satisfying the constraint Q. 1 C and 1 M denote the vectors of ones with dimension C and M , respectively. r and h are the marginal projections of Q onto its rows and columns, respectively.</p><p>By formulating the cluster assignment problem as an optimal transport problem, the optimization of Eq. 5 with respect to variable Q can be solved efficiently by the iterative Sinkhorn-Knopp algorithm <ref type="bibr" target="#b12">[13]</ref>. The optimal solution is obtained by:</p><formula xml:id="formula_6">Q * = diag(?) exp( f SL (?) ? ) diag(?),<label>(6)</label></formula><p>where ? ? R C and ? ? R M are two renormalization vectors which can be computed efficiently in linear time even for dense prediction tasks. ? is a temperature parameter. Then by fixing label assignment Q, the self-labeling head f SL is updated by minimizing L SL with respect toP , which is the same as training with cross-entropy loss.</p><p>Weight Initialization. We use the soft cluster assignment P SL to rectify the classifier-based pseudo label P ST . However, the clustering categories usually mismatch those of the classifier, resulting in performance degradation. To overcome this issue, we initialize the weight of self-labeling head f SL with category prototypes. Specifically, we compute the prototypes [z 1 , ? ? ? ,z C ] for each category through:</p><formula xml:id="formula_7">z c = 1 |? c | N T n=1 H?W i=1 Y (c) ST,n,i ? z n,i ,<label>(7)</label></formula><p>where |? c | denotes the number of pixels belonging to the c-th category in all images. Y ST is the hard version of P ST . Then the self-labeling process can be regarded as assigning pixels to different prototypes. In this way, the clustering categories are able to match classification categories.</p><p>Online Cluster Assignment. Different from Asano et al. <ref type="bibr" target="#b2">[3]</ref>, where the assignment Q is computed over the full dataset, we conduct online clustering on data batches during training. Considering that the number of samples in a mini-batch is often too small to cover all categories, and the class distribution varies largely across different batches, we augment the features? with a memory bank M, which is updated on-the-fly, to reduce the randomness of sampling. Specifically, throughout the training process, we maintain a queue of 65,536 pixel features from previous batches in M. In each iteration, we compute the optimal transport assignment on the augmented data Z aug , denoted by Q aug , but only the assignment of current batch, denoted by Q cur , is used to compute the self-labeling loss L SL . In this way, we could alternatively update the self-labeling head f SL and use it to generate more accurate cluster assignment P SL online. Hence, the pseudo labels will be improved incrementally by the resulting cluster assignments, and the noise will be gradually reduced without extra rounds of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Class-Balanced Self-Labeling</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, there exists severe class-imbalance in current semantic segmentation datasets. Some long-tailed classes have very limited pixels (e.g., "traffic light", "sign"), and some classes only appear in a few images (e.g., "motorbike", "train"). Such a problem will make it difficult to train a robust segmentation model, especially for those long-tailed classes. In this work, we propose two techniques to address this issue, i.e., class-balanced sampling and distribution alignment.</p><p>Class-Balanced Sampling. We randomly sample pixels from each image, which makes the class distribution of data in memory bank M approach to that of the whole dataset. In order to make sure that the pixels of long-tailed categories can be selected equally, we sample from different categories with the same proportion, i.e., M H?W , where M is the number of pixels to be sampled in each image. For each input image X t n , we first compute its class distribution ? n through</p><formula xml:id="formula_8">? (c) n = 1 H ? W H?W i? t,(c) n,i ,<label>(8)</label></formula><p>where ? (c) n denotes the proportion of pixels belonging to the c-th category in image X t n . Then the number of samples M c for each category c is decided by:</p><formula xml:id="formula_9">M c = M ? ? (c) n .<label>(9)</label></formula><p>If image X t n does not contain certain classes of pixels, we will randomly sample the rest pixels from other categories to make up M samples.</p><p>Distribution Alignment. As discussed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, simultaneously optimizing Q andP in Eq. 5 may lead to degenerated results that all data points are trivially assigned to a single cluster. To avoid this, Asano et al. <ref type="bibr" target="#b2">[3]</ref> constrained that Q should induce an equipartition of the data. However, this constraint is not reasonable and it will degrade the performance if the ground truth class distribution of the data, denoted by ? gt , is not uniform. In the Cityscapes dataset <ref type="bibr" target="#b10">[11]</ref>, for example, the number of pixels of the largest category ("road") is approximately 300 times that of the smallest category ("motorbike").</p><p>To overcome this problem, we propose a novel technique, namely distribution alignment, to align the distribution of cluster assignments to ground truth class distribution ? gt , aiming at partitioning pixels into subsets of unequal sizes. However, ? gt is unknown since the true labels of target domain data are unavailable. Thus we propose to employ the moving average of pseudo labels' class distribution ? pseudo to approximate ? gt . Specifically, we first initialize ? pseudo based on the fixed pseudo labels Y t ST as follows:</p><formula xml:id="formula_10">? (c) pseudo | 0 = 1 N T ? H ? W N T n H?W i Y t,(c) ST,n,i .<label>(10)</label></formula><p>Over the course of training, we compute the class distribution ? n of each image through Eq. 8. Then the class distribution ? pseudo after each training iteration k is updated with a momentum ? ? [0, 1]:</p><formula xml:id="formula_11">? (c) pseudo | k = ?? (c) pseudo | k?1 + (1 ? ?)? (c) n .<label>(11)</label></formula><p>Finally, we enforce the class distribution of cluster assignments, denoted by r in Eq. 5, to be close to ? pseudo :</p><formula xml:id="formula_12">r = ? pseudo , h = 1 M 1 M .<label>(12)</label></formula><p>Our empirical results (please refer to <ref type="figure" target="#fig_4">Fig. 6</ref>) demonstrate that the proposed distribution alignment technique effectively avoids the dominance of majority classes during training. Please refer to Sec. 4.3 for more discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>As shown in <ref type="figure">Fig. 1</ref>, we employ momentum encoder to stabilize the self-labeling process. To further improve the model generalization ability on target domain and alleviate the bias inherited from source domain, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">50]</ref>, we impose consistency regularization on the segmentation network. Specifically, we generate a weakly-augmented image X w and a strongly-augmented image X s from the same input image X, and pass X w through the momentum segmentation network f SEG to generate a probability map P w , which is used to supervise the output P s of stronglyaugmented image X s from f SEG . Then we enforce P w and P s to be consistent via:</p><formula xml:id="formula_13">LREG = N T n=1 H?W i=1</formula><p>( KL (Pw,n,i, Ps,n,i) + KL (Ps,n,i, Pw,n,i)) , <ref type="bibr" target="#b12">(13)</ref> where KL denotes the KL-divergence. P s,n,i and P w,n,i represent the i-th pixel of the segmentation probability maps P s and P w of image X n , respectively.</p><p>The overall loss function is defined as:</p><formula xml:id="formula_14">L TOTAL = L SEG + ? 1 L SL + ? 2 L REG ,<label>(14)</label></formula><p>where ? 1 and ? 2 are trade-off parameters. L SL and L REG are complementary to each other. The former uses pixellevel cluster assignment P SL to rectify the pseudo label P ST , which effectively dilutes the bias to source domain, while the latter improves model generalization ability by applying data augmentations on inputs and consistency regularization on outputs.  <ref type="table">Table 2</ref>. Experimental results on the SYNTHIA ? Cityscapes adaptation task. The top score is highlighted in bold font.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Implementation Details. We implement the segmentation model with DeepLabv2 <ref type="bibr" target="#b7">[8]</ref> and employ ResNet-101 <ref type="bibr" target="#b15">[16]</ref> as the backbone, which is pre-trained on ImageNet. The segmentation model is warmed up by applying adversarial training like <ref type="bibr" target="#b38">[39]</ref>. The input images are randomly cropped to 896?512, and the batch size is set as 4. We employ a series of data augmentations such as RandAugment <ref type="bibr" target="#b11">[12]</ref>, Cutout <ref type="bibr" target="#b13">[14]</ref>, CutMix <ref type="bibr" target="#b48">[49]</ref>, and add photometric noise, including color jitter, random blur, etc. SGDM is used as the optimizer. The initial learning rate of segmentation model and self-labeling head are set to 10 ?4 and 5 ? 10 ?4 , which decay exponentially with power 0.9. The weight decay and momentum are set to 2 ? 10 ?4 and 0.9, respectively. The trade-off parameters ? 1 , ? 2 and the temperature parameters ? , ? are empirically set to 0.1, 5, 0.08, and 0.05, respectively. The length of memory bank is set to 65, 536 and we sample 512 pixels per image for clustering (M = 512), that is, there are 128 images in the memory bank. For the momentum networks, the momentum is set to 0.999. Our model is trained with four Tesla V100 GPUs on PyTorch. Datasets. Following <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, we adopt two synthetic datasets (GTA5 <ref type="bibr" target="#b35">[36]</ref>, SYNTHIA <ref type="bibr" target="#b36">[37]</ref>) and one real dataset (Cityscapes <ref type="bibr" target="#b10">[11]</ref>) in the experiments. The GTA5 dataset contains 24,966 images with resolution 1914?1052. The corresponding dense annotations are generated by game engine. The SYNTHIA dataset contains 9,400 images of 1280?760 pixels and it has 16 common categories with Cityscapes, which contains 2,975 training images and 500 validation images of resolution 2048?1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with State-of-the-Arts</head><p>We name the proposed method as Class-balanced Pixellevel Self-Labeling (CPSL). Following <ref type="bibr" target="#b49">[50]</ref>, after the training converges, we also conduct two more knowledge distillation rounds to transfer the knowledge to a student model pre-trained in a self-supervised manner, and the resulting model is called "CPSL+distill". We compare our models with representative and state-of-the-art methods, which can be categorized to two main groups: adversarial trainingbased methods, including AdaptSeg <ref type="bibr" target="#b38">[39]</ref>, CyCADA <ref type="bibr" target="#b16">[17]</ref>, FADA <ref type="bibr" target="#b41">[42]</ref>, ADVENT <ref type="bibr" target="#b40">[41]</ref>, and self-training based methods, including CBST <ref type="bibr" target="#b54">[55]</ref>, IAST <ref type="bibr" target="#b30">[31]</ref>, CAG UDA <ref type="bibr" target="#b50">[51]</ref>, ProDA <ref type="bibr" target="#b49">[50]</ref>, SAC <ref type="bibr" target="#b0">[1]</ref>. Following previous works, the results on validation set are reported in terms of category-wise Intersection over Union (IoU) and mean IoU (mIoU). GTA5?Cityscapes. The results on GTA5?Cityscapes task are reported in Tab. 1. Our CPSL achieves the best IoU score on 7 out of 19 categories, and it achieves the highest mIoU score, outperforming the second best method  ProDA <ref type="bibr" target="#b49">[50]</ref> by a large margin of 2.0. This can be attributed to the exploration of inherent data distribution of target domain, which provides extra supervision for training. By applying knowledge distillation, there is a further performance gain of 5.1, achieving 60.8 mIoU, which is by far the new state-of-the-art. It is worth mentioning that our method performs especially well on long-tailed categories, such as "pole", "light", "train", and "motor". For example, ProDA fails on the small class "train" due to the difficulties in constructing prototypes for long-tailed categories. By applying distribution alignment, CPSL alleviates the class-imbalance problem, attaining 24.9 IoU on "train" without sacrificing the performance on other categories. SYNTHIA?Cityscapes. This adaptation task is more challenging than the previous one because of the large domain gap. The mIoUs over 13 classes (mIoU 13 ) and 16 classes (mIoU <ref type="bibr" target="#b15">16</ref> ) are reported in Tab. 2. Our model still achieves significant improvements over competing methods on this task. Specifically, CPSL achieves the mIoU of 54.4 and 61.7 over 16 and 13 categories, surpassing the second best method SAC [1] by 1.8 and 2.4, respectively. This owes to the fact that CPSL reduces the label noise and calibrates the bias to source domain. The results are further improved to 57.9 and 65.3 in terms of mIoU after distillation. Among all the 16 categories, our method tops over six of them, especially on the hardest categories, such as "light", "motorbike", "bike", and so on. Qualitative Results. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the qualitative seg- mentation results of our method and ProDA [50] on GTA5?Cityscapes task. As can be seen, our method improves the performance on long-tailed classes substantially, e.g. "pole", "light", "bus", thanks to the class-balanced sampling and distribution alignment techniques. ProDA <ref type="bibr" target="#b49">[50]</ref> does not perform well on these categories since it does not explicitly enforce class balance in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussions</head><p>Ablation Study. We conduct ablation studies on the GTA5?Cityscapes task to investigate the role of each component in CPSL. For the convenience of expression, we abbreviate 'self-labeling', 'self-training', 'class balance', 'weight initialization', 'data augmentation', and 'momentum encoder' with 'SL', 'ST', 'CB', 'Init', 'Aug', 'Mom'. Tab. 3 shows the corresponding results by switching off each component. We have the following observations. First, removing the SL component leads to a drop of 7.9 in mIoU, while disabling CB component leads to a drop of 3.9 in mIoU. This demonstrates they play key roles in improving the segmentation performance by exploring the intrinsic data structures of target domain images. Second, training without the pseudo labels produced by ST causes a significant drop of 16.3 in mIoU. This is not surprising because simultaneously updating network parameters and generating pseudo labels will lead to a degenerate solution <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. Third, randomly initializing the self-labeling head (w/o Init) results in a decline of 5.8 in mIoU, which is attributed to the mismatch between clustering and classifi-  Unequal Partition Constraint. To further analyze the effect of unequal partition on class-imbalanced dataset, we plot the curves of mIoU and MPA scores with different partition constraints in <ref type="figure" target="#fig_2">Fig. 4</ref>, where a huge gap can be observed in terms of mIoU. However, equal partition slightly outperforms unequal partition in terms of MPA. This is not surprising because many pixels belonging to large categories are assigned to small categories under the equal partition constraint, largely improving pixel accuracy of small classes without influencing much large classes. Thus the MPA score is improved. More details can be found in the supplemental files.</p><p>Self-Training (ST) vs. Self-Labeling (SL). We explore the complementarity of label assignments produced by ST and SL, and visualize the results in <ref type="figure" target="#fig_3">Fig. 5</ref>. One can draw a conclusion that the integration of ST and SL in our CPSL leads to better results than any one of them. Specifically, ST performs better on large categories which are easy to transfer, such as "sky" and "building", while SL has advantages on small categories such as "light" and "pole". Therefore, the pixels that are wrongly classified in one view will be corrected in another view. The Effect of Distribution Alignment. We compare the class distributions of labels produced by CPSL and conventional self-training (ST). As illustrated in <ref type="figure" target="#fig_4">Fig. 6</ref>, the results of ST mismatch heavily to ground truth (GT). Its predictions are biased towards majority categories, e.g. 'road' and 'building', ignoring small categories such as 'train', 'sign' and 'bike'. CPSL calibrates the bias and produces a class distribution closer to GT. This demonstrates that CPSL can capture the inherent class distribution of target domain and avoids gradual dominance of majority classes. Parameter Sensitivity Analysis. In Tab. 4, we evaluate the segmentation performance on GTA5?Cityscapes task with different number of samples per image. Our method is robust to this parameter within a wide range. More analyses can be found in supplemental materials. Limitation. Although the proposed CPSL alleviates the bias to source domain with the self-labeling assignment, it still relies on the self-training based pseudo labels, which may lead to confirmation bias. We consider to develop a fully clustering-based assignment method in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a plug-and-play module, namely Classbalanced Pixel-level Self-Labeling (CPSL), which could be seamlessly incorporated into self-training pipelines to improve the domain adaptive semantic segmentation performance. Specifically, we conducted pixel-level clustering online and used the resulting cluster assignments to rectify pseudo labels. On one hand, the label noise was reduced and the bias to source domain was calibrated by exploring pixel-level intrinsic structures of target domain images. On the other hand, CPSL captured inherent class distribution of target domain, which effectively avoided gradual dominance of majority classes. Both the qualitative and quantitative analyses demonstrated that CPSL outperformed the existing state-of-the-arts by a large margin. In particular, it achieved great performance gains on long-tailed classes without sacrificing the performance on other categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplemental Materials</head><p>In this supplemental file, we provide the following materials:</p><p>? The training procedure of CPSL;</p><p>? The definition of mean pixel accuracy (MPA) (referring to Sec4.3-Unequal partition constraint in the main paper);</p><p>? Ablation studies in terms of per-category IoU (referring to Sec4.3-Ablation study in the main paper);</p><p>? Comparisons on the training process on the GTA5?Cityscapes task;</p><p>? More parameter sensitivity analyses (referring to Sec4.3-Parameter sensitivity analysis in the main paper);</p><p>? More qualitative results (referring to Sec4.2-Qualitative results in the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Algorithm</head><p>The training procedure of our CPSL is summarized in Algorithm. 1. For detailed equations and loss functions, please refer to our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Mean pixel accuracy (MPA)</head><p>Denoting by C the number of classes, by p ij the number of pixels which belong to the i-th class but are wrongly classified into the j-th class, and by p ii the number of pixels which belong to the i-th class and are accurately classified into the i-th class, the pixel accuracy (PA) of the i-th class is defined as:</p><formula xml:id="formula_15">P A = p ii C j=1 p ij .<label>(15)</label></formula><p>Then the mean pixel accuracy (MPA) is defined as:</p><formula xml:id="formula_16">M P A = 1 C C i=1 p ii C j=1 p ij .<label>(16)</label></formula><p>As discussed in Sec. 4.3 of our manuscript, under the constraint of equal partition, many pixels belonging to large categories are assigned to small categories, largely improving the pixel accuracy of small classes. However, this constraint has very small influences on large categories because these categories contain a great number of pixels. Therefore, the MPA is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation study</head><p>We only reported the mIoU scores in Tab.3 of the main paper. Here we present in Tab. 5 the per-class IoU scores of ablation studies. Note that "w/o CB" denotes that we do not Compute the self-labeling loss L SL through Eq. 5 using the cluster assignment of current batch Q cur ; <ref type="bibr" target="#b14">15</ref> Train the self-labeling head f SL using loss L SL . <ref type="bibr" target="#b15">16</ref> Update the momentum self-labeling head f SL in an EMA manner; <ref type="bibr" target="#b16">17</ref> Pass X t n through f SEG and f SL to obtain self-labeling assignment P SL ; <ref type="bibr" target="#b17">18</ref> Use P SL to rectify P ST and obtain the rectified pseudo labels? t n through Eq. 1;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Update f SEG using loss L t SEG ; <ref type="bibr" target="#b19">20</ref> Update the momentum segmentation model f SEG in an EMA manner. employ the class-balanced sampling techniques, and constrain that Q should induce an equipartition of data rather than an unequal partition. One can see that this leads to a degradation of 3.9 in terms of mIoU, demonstrating that the equal partition is not reasonable when the class distribution of data is highly imbalanced.   <ref type="table">Table 5</ref>. Ablation studies on the key components of CPSL in terms of per-category IoU. The top score is highlighted in bold font.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Training process of CPSL and ProDA</head><p>To further highlight the improvement of CPSL during training, we plot the curves of mIoU and MPA scores on the GTA5?Cityscapes task in <ref type="figure" target="#fig_6">Fig. 7</ref>. A large performance improvement of CPSL over ProDA can be observed in terms of both mIoU and MPA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Parameter analysis</head><p>Tab. 6 and Tab. 7 show the segmentation results by using different self-labeling loss weight ? 1 and consistency regularization loss weight ? 2 , respectively. One can see that our method is insensitive to these two parameters. Tab. 8 shows the effect of temperature ? . We employ the cluster assignment P SL as a weight map to online modulate the softmax probability of pseudo labels P ST , where the temperature ? controls the modulation intensity. When ? ? 0, the modulation intensity increases so that the rectified pseudo label? Y t will rely heavily on P SL . When ? ? ?, the modulation intensity decreases so that the rectified pseudo label? Y t will rely heavily on P ST .   <ref type="table">Table 8</ref>. The influence of temperature parameter ? . 6.6. Qualitative results PSL vs. CPSL. To better illustrate the performance of our method, we implement a variant of CPSL without class-balanced training, i.e., purely Pixel-level Self-Labeling (PSL). The qualitative results of PSL and CPSL are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. Overall, CPSL is capable of producing more accurate segments across various scenes. Specifically, our method performs better on long-tailed categories, e.g. "bus", "bicycle", "person", "light". Compared to PSL, the segment boundaries of CPSL tend to be clearer and closer to object boundaries, such as "bicycle" and "person". Besides, it is noteworthy that PSL wrongly classifies the "road" class into the "sidewalk" class in a large area, which is attributed to the equipartition constraint applied on cluster assignments. This constraint is not useful and would even degrade the performance if the real class distribution is not uniform. However, this issue is solved by aligning class distribution of cluster assignments to that of pseudo labels.  Comparisons with state-of-the-arts. As in <ref type="figure" target="#fig_1">Fig. 3</ref> of the main manuscript, we compare our CPSL with other stateof-the-art methods. Here we provide more visualization results in <ref type="figure" target="#fig_3">Fig. 9 -Fig. 15</ref>. Our method performs better on long-tailed categories, such as "person", "pole", "traffic light", "bus", and "rider".      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The class distribution of the Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results of our method and ProDA [50] on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The mIoU and mean pixel accuracy (MPA) scores evaluated on the validation set with equal/unequal partition constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The complementarity between label assignments produced by self-training (ST) and self-labeling (SL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The comparisons of class distributions on Cityscapes dataset. 'GT' denotes the ground truth class distribution. 'ST' and 'CPSL' denote the class distributions of pseudo labels produced by self-training and class-balanced pixel-level self-labeling. cation categories. Fourth, Aug and Mom components bring an improvement of 1.7 and 1.1 in mIoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>The mIoU (left) and MPA (right) scores evaluated on the validation set during the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>51.4 54.2 55.7 54.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results of PSL and CPSL on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative comparison of different methods on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Qualitative comparison of different methods on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Qualitative comparison of different methods on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Qualitative comparison of different methods on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>Qualitative comparison of different methods on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Qualitative comparison of different methods on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Qualitative comparison of different methods on the GTA5?Cityscapes task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>] 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 CyCADA [17] 86.7 35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19.0 65.0 12.0 28.6 4.5 31.1 42.0 42.7 ADVENT [41] 89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.5 CBST [56] 91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9 FADA [42] 92.5 47.5 85.1 37.6 32.8 33.4 33.8 18.4 85.3 37.7 83.5 63.2 39.7 87.5 32.9 47.8 1.6 34.9 39.5 49.2 CAG UDA [51] 90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.43.0 32.3 43.7 51.3 42.8 85.4 37.6 81.1 69.5 30.0 88.1 44.1 59.9 24.9 47.2 48.4 55.7 ProDA+distill 87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4 57.5 CPSL+distill 92.3 59.9 84.9 45.7 29.7 52.8 61.5 59.5 87.9 41.5 85.0 73.0 35.5 90.4 48.7 73.9 26.3 53.8 53.9 60.8 Experimental results on the GTA5 ? Cityscapes adaptation task. The top score is highlighted in bold font. ProDA+distill 87.8 45.7 84.6 37.1 0.6 44.0 54.6 37.0 88.1 84.4 74.2 24.3 88.2 51.1 40.5 45.6 62.0 55.5 CPSL+distill 87.2 43.9 85.5 33.6 0.3 47.7 57.4 37.2 87.8 88.5 79.0 32.0 90.6 49.4 50.</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sideway</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege</cell><cell>terrace</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="21">AdaptSeg [392 50.2</cell></row><row><cell>FDA [48]</cell><cell cols="20">92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.4 50.5</cell></row><row><cell>PIT [30]</cell><cell cols="20">87.5 43.4 78.8 31.2 30.2 36.3 39.3 42.0 79.2 37.1 79.3 65.4 37.5 83.2 46.0 45.6 25.7 23.5 49.9 50.6</cell></row><row><cell>IAST [31]</cell><cell cols="20">93.8 57.8 85.1 39.5 26.7 26.2 43.1 34.7 84.9 32.9 88.0 62.6 29.0 87.3 39.2 49.6 23.2 34.7 39.6 51.5</cell></row><row><cell>ProDA [50]</cell><cell cols="20">91.5 52.4 82.9 42.0 35.7 40.0 44.4 43.3 87.0 43.8 79.5 66.5 31.4 86.7 41.1 52.5 0.0 45.4 53.8 53.7</cell></row><row><cell cols="4">CPSL (ours) 91.7 52.9 83.6 Method road sideway building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell></cell><cell>bus</cell><cell>motor</cell><cell>bike</cell><cell cols="3">mIoU 13 mIoU 16</cell></row><row><cell cols="4">AdaptSeg [39] 79.2 37.2 78.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="11">9.9 10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3</cell><cell>45.9</cell><cell></cell><cell>-</cell></row><row><cell cols="8">ADVENT [41] 85.6 42.2 79.7 8.7 0.4 25.9 5.4</cell><cell cols="10">8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0</cell><cell>48.0</cell><cell></cell><cell>41.2</cell></row><row><cell>CBST [56]</cell><cell cols="17">68.0 29.9 76.3 10.8 1.4 33.9 22.8 29.5 77.6 78.3 60.6 28.3 81.6 23.5 18.8 39.8</cell><cell>48.9</cell><cell></cell><cell>42.6</cell></row><row><cell cols="18">CAG UDA [51] 84.7 40.8 81.7 7.8 0.0 35.1 13.3 22.7 84.5 77.6 64.2 27.8 80.9 19.7 22.7 48.3</cell><cell>51.5</cell><cell></cell><cell>44.5</cell></row><row><cell>PIT [30]</cell><cell cols="17">83.1 27.6 81.5 8.9 0.3 21.8 26.4 33.8 76.4 78.8 64.2 27.6 79.6 31.2 31.0 31.3</cell><cell>51.8</cell><cell></cell><cell>44.0</cell></row><row><cell>FADA [42]</cell><cell cols="17">84.5 40.1 83.1 4.8 0.0 34.3 20.1 27.2 84.8 84.0 53.5 22.6 85.4 43.7 26.8 27.8</cell><cell>52.5</cell><cell></cell><cell>45.2</cell></row><row><cell>FDA [48]</cell><cell cols="3">79.3 35.0 73.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="11">19.9 24.0 61.7 82.6 61.4 31.1 83.9 40.8 38.4 51.1</cell><cell>52.5</cell><cell></cell><cell>-</cell></row><row><cell>PyCDA [26]</cell><cell cols="17">75.5 30.9 83.3 20.8 0.7 32.7 27.3 33.5 84.7 85.0 64.1 25.4 85.0 45.2 21.2 32.0</cell><cell>53.3</cell><cell></cell><cell>46.7</cell></row><row><cell>IAST [31]</cell><cell cols="17">81.9 41.5 83.3 17.7 4.6 32.3 30.9 28.8 83.4 85.0 65.5 30.8 86.5 38.2 33.1 52.7</cell><cell>57.0</cell><cell></cell><cell>49.8</cell></row><row><cell>SAC [1]</cell><cell cols="17">89.3 47.2 85.5 26.5 1.3 43.0 45.5 32.0 87.1 89.3 63.6 25.4 86.9 35.6 30.4 53.0</cell><cell>59.3</cell><cell></cell><cell>52.6</cell></row><row><cell>ProDA [50]</cell><cell cols="17">87.1 44.0 83.2 26.9 0.7 42.0 45.8 34.2 86.7 81.3 68.4 22.1 87.7 50.0 31.4 38.6</cell><cell>58.5</cell><cell></cell><cell>51.9</cell></row><row><cell>CPSL (ours)</cell><cell cols="16">87.3 44.4 83.8 25.0 0.4 42.9 47.5 32.4 86.5 83.3 69.6 29.1 89.4 52.1 42.6</cell><cell>54.1</cell><cell>61.7</cell><cell></cell><cell>54.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">8 59.8</cell><cell>65.3</cell><cell></cell><cell>57.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Training Procedure of CPSL Input : Training data D S = {(X s n , Y s n )} N S n=1 and D T = {X t n } N T n=1 ; Output: The output model f SEG ; 1 Generate soft pseudo labels P ST with the warmed-up model; 2 Initialize the weight of f SL and f SL with the prototypes [z 1 , ? ? ? ,z C ] for each category computed by Eq. 7; 3 for i = 1 to max epochs do Train the model f SEG using loss L s SEG ;</figDesc><table><row><cell>4</cell><cell>for n = 1 to N S do</cell><cell></cell></row><row><cell>5</cell><cell cols="2">Get source image X s n ;</cell></row><row><cell>7 8</cell><cell cols="3">Get target image X t n ; Extract features from X t n to obtain</cell></row><row><cell></cell><cell cols="3">Z ? R H?W ?D and normalize it with</cell></row><row><cell></cell><cell>z i = zi ||zi||2 ;</cell><cell></cell></row><row><cell>11</cell><cell cols="3">for k = 1 to sinkhorn iterations do</cell></row><row><cell>12</cell><cell>Q  *  aug =</cell><cell></cell></row><row><cell></cell><cell>diag(?) exp(</cell><cell>fSL(Zaug) ?</cell><cell>) diag(?);</cell></row></table><note>69 Sample a group of pixels? = [z 1 , ? ? ? , z M ] from Z randomly;10 Augment the features? with a memory bank M and obtain Z aug = [?; M];13 end 14</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>CB 91.7 51.3 84.0 33.9 24.3 42.5 43.3 49.0 81.5 29.1 75.8 67.0 28.5 87.7 34.3 63.3 20.1 36.0 40.5 51.8 -3.9 w/o Init 89.6 56.1 80.0 40.3 36.7 43.7 45.9 39.6 86.2 39.8 81.9 66.7 24.8 89.0 45.4 50.Aug 90.6 45.5 83.8 41.4 33.0 44.3 52.0 42.0 86.4 40.2 81.6 68.4 28.9 88.0 42.8 58.5 14.9 40.0 47.1 54.2 -1.5 w/o Mom 92.6 53.7 84.1 41.7 36.6 44.8 50.6 41.7 86.2 40.5 79.6 68.2 26.6 87.4 37.4 55.9 19.3 43.1 47.5 54.6 -1.1 CPSL 91.7 52.9 83.6 43.0 32.3 43.7 51.3 42.8 85.4 37.6 81.1 69.5 30.0 88.1 44.1 59.9 24.9 47.2 48.4 55.7 -</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sideway</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege</cell><cell>terrace</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell><cell>?</cell></row><row><cell>w/o SL</cell><cell cols="20">91.9 56.3 82.9 35.9 30.2 37.5 37.4 32.9 85.3 39.2 77.8 51.2 18.6 84.7 37.8 44.6 1.0 20.2 42.7 47.8</cell><cell>-7.9</cell></row><row><cell>w/o ST</cell><cell cols="21">82.4 39.0 70.5 30.5 16.0 24.1 39.6 37.0 77.8 24.2 78.7 28.5 18.7 75.7 9.2 36.1 4.1 22.9 36.5 39.4 -16.3</cell></row><row><cell cols="20">w/o 8 0.0 31.4 9.3</cell><cell>49.9</cell><cell>-5.8</cell></row><row><cell>w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>The influence of parameter ?1. The influence of parameter ?2.</figDesc><table><row><cell>? 2</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell></row><row><cell cols="6">mIoU 55.5 55.7 55.2 54.7 54.4</cell></row><row><cell>?</cell><cell></cell><cell cols="3">0.05 0.08 0.1 0.15</cell><cell></cell></row><row><cell cols="5">mIoU 52.8 55.7 55.3 53.6</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Labelling unlabelled videos from scratch with multi-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4660" to="4671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selflabelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Y M Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 : Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Po</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1900" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning by neighbourhood discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12975" to="12984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">T-svdnet: Exploring high-order prototypical correlations for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9991" to="10000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A nonadversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-domain semantic segmentation via domain-invariant interactive relation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pixmatch: Unsupervised domain adaptation via pixelwise consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Kyriazi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun K</forename><surname>Manrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12435" to="12445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lowshot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (14)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring cross-image pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7303" to="7313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">G?khan</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;16 Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6509" to="6518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation. 1, 2, 3, 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13049</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cliquecnn: Deep unsupervised exemplar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel?ngel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Tikhoncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3846" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
