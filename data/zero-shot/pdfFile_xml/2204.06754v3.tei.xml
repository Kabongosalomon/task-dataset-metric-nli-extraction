<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RecurSeed and EdgePredictMix: Single-stage Learning is Sufficient for Weakly-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Jo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In-Jae</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Medical AI Research Center</orgName>
								<orgName type="institution">Samsung Medical Center</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sungkyunkwan University School of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Seoul, Korea</roleName><forename type="first">Ogq</forename><surname>Gyn</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Electronics</orgName>
								<address>
									<settlement>Suwon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RecurSeed and EdgePredictMix: Single-stage Learning is Sufficient for Weakly-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although weakly-supervised semantic segmentation using only image-level labels (WSSS-IL) is potentially useful, its low performance and implementation complexity still limit its application. The main causes are (a) non-detection and (b) false-detection phenomena: (a) The class activation maps refined from existing WSSS-IL methods still only represent partial regions for large-scale objects, and (b) for small-scale objects, over-activation causes them to deviate from the object edges. We propose RecurSeed which alternately reduces non-and false detections through recursive iterations, thereby implicitly finding an optimal junction that minimizes both errors. We also propose a novel data augmentation (DA) approach called EdgePredictMix, which further expresses an object's edge by utilizing the probability difference information between adjacent pixels in combining the segmentation results, thereby compensating for the shortcomings when applying the existing DA methods to WSSS. We achieved new state-of-the-art performances on both the PASCAL VOC 2012 and MS COCO 2014 benchmarks (VOC val 74.4%, COCO val 46.4%). The code is available at https://github.com/shjo-april/RecurSeed_ and_EdgePredictMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RRM CAM</head><p>SSSS AFA RS+EPM (Ours) <ref type="figure">Figure 1</ref>: Comparison of localization maps produced by using conventional CAMs, existing single-stage frameworks (SLFs), and our SLF. Our results represent both foreground and background regions more accurately than other localization maps through a recursive learning strategy of SLF that minimizes both non-and false-detection of the foreground.</p><p>advanced MLFs employ external saliency supervision with image-level class labels (i.e., beyond the scope of WSSS-IL) to detect prominent foreground objects. However, image saliency ignores non-salient foreground objects (e.g., small or low-contrast objects), hindering the broadening of various datasets <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. Although a single-stage learning framework (SLF) solves these limitations of an MLF, the performances of existing SLFs <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref> are generally still far behind those of recent MLFs. To compensate for these limitations, some researchers <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b19">19]</ref> have applied a conditional random field (CRF) in the training loop of SLF. However, a CRF significantly increases the training time as a fatal drawback. To refine the initial CAM with minimal computational complexity, Araslanov and Roth <ref type="bibr" target="#b20">[20]</ref> proposed an alternative called pixel-adaptive mask refinement (PAMR). Despite advances in post-processing algorithms such as a CRF, a limitation remains regarding an insufficient spread covering the object, resulting in many FNs in current studies. The self-correlation map generation (SCG) proposed by Pan et al. <ref type="bibr" target="#b24">[23]</ref> complements the insufficient spread phenomenon by leveraging the second-order correlation of the feature maps. Although the intrinsic properties of an SCG, considering the high-order correlation, effectively reduce the FNs of the CAM on large-scale objects (e.g., ImageNet and  CUB-200-2011), they also tend to increase the number of false positives (FPs) on middle-or small-scale objects (e.g., PASCAL VOC and MS COCO). These mismatching issues of FNs and FPs on WSSS-IL are called FPN-WSSS-IL.</p><p>Meanwhile, data augmentation (DA) is a simple method that significantly improves data efficiency. Notable examples include regional-level augmentation methods, such as Cutout <ref type="bibr" target="#b25">[24]</ref> and CutMix <ref type="bibr" target="#b26">[25]</ref>, which cut and paste randomly selected rectangular regions of the images, or SaliencyGrafting (SG) <ref type="bibr" target="#b27">[26]</ref>, CDA [27], and ClassMix <ref type="bibr" target="#b29">[28]</ref>, which directly utilize predicted masks from the model for the mix. However, in the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is an essential task in computer vision, including object tracking and medical image analysis. Most approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> employ pixel-wise annotations that require labor-intensive and timeconsuming tasks. In addition, image-and pixel-level annotations have been reported <ref type="bibr" target="#b3">[4]</ref> to require 20 and 239 seconds per image, respectively. To alleviate human labor, numerous efforts have been made to develop weakly supervised semantic segmentation (WSSS), to employ weak labels such as image-level class labels <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7]</ref>, scribbles <ref type="bibr" target="#b8">[8]</ref>, points <ref type="bibr" target="#b3">[4]</ref>, and bounding boxes <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>. We concentrate on WSSS through image-level class labels (WSSS-IL) as the most economical among the aforementioned weak supervisions.</p><p>Most WSSS-IL approaches rely on a class activation map (CAM) <ref type="bibr" target="#b11">[11]</ref>, which is designed to visualize only the most discriminative part of an object, causing many false negatives (FNs). Existing studies on WSSS-IL employ single-or multi-stage learning frameworks to alleviate this drawback. Although the latest major approaches <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref> to WSSS-IL depend on a multi-stage learning framework (MLF), MLFs require additional training stages for distinct purposes, causing sophisticated training procedures. Furthermore, WSSS-IL setup, the simple synthesis without any refinements using predicted masks inevitably accelerates the ambiguity of mixed results due to insufficient spread. We named this issue for incomplete recognition of the object boundary region in the existing DA approaches as IBDA-WSSS-IL.</p><p>We propose RecurSeed (RS) and EdgePredictMix (EPM) to resolve FPN-WSSS-IL and IBDA-WSSS-IL simultaneously. RS is a novel approach that recursively rectifies the initial CAM by leveraging both SCG and PAMR beyond their original purpose of post-processing. EPM is the first DA approach to resolve IBDA-WSSS-IL, which remedies the uncertainty of mixed results by reviving edge information of target objects. <ref type="figure">Figure 1</ref> presents several examples of PASCAL VOC 2012 train set, showing the quality of our SLF method compared with the conventional CAM and other SLF approaches. Their main characteristics and contributions are summarized as follows.</p><p>? We newly apply recursion to the combination of post-processing SCG/PAMR to mitigate FPN-WSSS-IL. Because SCG discovers the spatial information in the the second-order based correlation features, it reduces the FNs by covering the sufficient foreground of an object that the CAM does not capture. By contrast, PAMR quickly reduces the FPs by capturing the exact boundary of the initial seed. Accordingly, RS recursively transfers the knowledge of the seed refinement result considering the high-order correlation to the network; it integrates the SCG and PAMR recursively to exploit both advantages making a seed that minimizes both the FP and FN. This substantially improves (e.g., by 11.4%) the performance limitations of existing studies, the recursion based on the first-order correlation <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b13">13]</ref>, by distillating more accurate information to the network.</p><p>? To address the IBDA-WSSS-IL, we propose a novel edge refinement (EP) technique suitable for mixing augmentation (EPM). Our EPM refines predicted masks before mask-to-mask synthesis by exploiting the absolute and relative per-pixel probability information. This diversity gain allowed for restoring the mask's boundary more accurately, maximizing the mixing effect and significantly improving the WSSS-IL performance.</p><p>? Our SLF, composed of RS and EPM, recorded the latest WSSS-IL performance of mIoU 70.6% on the PASCAL VOC 2012 test set, whereas existing SLFs did not exceed 70.0%. This result implies that our SLF suffices to accomplish advanced performance on WSSS-IL without the complicated learning configuration (i.e., MLF). Furthermore, the straightforward extension of our SLF to MLF using a conventional method (RW) also achieved the state-of-the-art performance of mIoU 74.4% on the PASCAL VOC 2012 validation set (i.e., improved by more than 3.5% over recent MLFs), validating the scalability of the proposed SLF as MLF. We also supported the superiority by proving that the proposed SLF or MLF showed the latest performance even in MS COCO data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 Weakly-supervised semantic segmentation</head><p>The dominant approaches for WSSS-IL follow a multi-stage learning procedure: producing an initial CAM, generating pseudo masks, and training the segmentation model. In particular, gold standards employed region growing <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b23">22]</ref> or random walk (RW) <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b32">31]</ref> to produce pseudo labels from the initial seeds (e.g., CAM). Most studies <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b37">36]</ref> concentrated on enhancing the quality of the initial seed (e.g., CAM). FickleNet <ref type="bibr" target="#b6">[6]</ref>, Puzzle-CAM <ref type="bibr" target="#b38">[37]</ref>, and L2G <ref type="bibr" target="#b14">[14]</ref> enlarged seed regions by expanding the CAMs through the patch-based dropout principle. AdvCAM <ref type="bibr" target="#b7">[7]</ref> and RIB <ref type="bibr" target="#b33">[32]</ref> proposed anti-adversarial frameworks <ref type="table">Table 1</ref>: Differentiation in the proposed EPM and the existing DA methods: CutMix <ref type="bibr" target="#b26">[25]</ref>, SG <ref type="bibr" target="#b27">[26]</ref>, CDA <ref type="bibr" target="#b28">[27]</ref>, ClassMix <ref type="bibr" target="#b29">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Properties</head><p>CutMix SG CDA ClassMix EPM Use predicted mask in mix Use mixed mask in learning Consider mix in WSSS Refine predicted mask to manipulate the CAMs by exploiting images or fully trained weights. MCIS <ref type="bibr" target="#b39">[38]</ref>, EDAM <ref type="bibr" target="#b34">[33]</ref>, and RCA <ref type="bibr" target="#b12">[12]</ref> capture information from several images by reflecting cross-image semantic similarities and differences. To mitigate the FN of CAM, MCTformer <ref type="bibr" target="#b40">[39]</ref> proposed a transformer-based architecture by exploiting global information. Our method is a model-agnostic learning technique without structural dependencies such as transformers, and achieves the latest performance even on a traditional convolutional neural network-based backbone with fewer parameters. C 2 AM <ref type="bibr" target="#b41">[40]</ref>, ADEHE <ref type="bibr" target="#b42">[41]</ref>, and SANCE <ref type="bibr" target="#b43">[42]</ref> were independent studies to eliminate an erroneous region in pseudo masks. All aforementioned studies did not consider a recursion process unlike our RS.</p><p>Similar to the proposed RS, SEAM <ref type="bibr" target="#b30">[29]</ref>, CPN <ref type="bibr" target="#b31">[30]</ref>, and SIPE <ref type="bibr" target="#b13">[13]</ref> also consider the recursion process but they only applied it to the pseudo label obtained by considering the first-order correlation (e.g., a pixel correlation module), causing insufficient improvement of FN. By contrast, our recursion in RS enables the reduction of both FP and FN by considering the higher-order correlation and its subsequent refinement by the simple combination of SCG and PAMR, respectively.</p><p>For the simplicity and usability of SLF, EM-Adapt <ref type="bibr" target="#b17">[17]</ref> first developed an SLF of WSSS-IL by applying an expectation-maximization. RRM <ref type="bibr" target="#b19">[19]</ref> and SSSS <ref type="bibr" target="#b20">[20]</ref> proposed an SLF by refining the naive CAM with the CRF (or PAMR) in the training loop, increasing the FN. AFA <ref type="bibr" target="#b44">[43]</ref> integrated the standard transformer into SLF to exploit the inherent advantage of the multi-head attention. Despite these attempts towards advancement, recent SLF approaches <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b44">43]</ref> have been getting less attention lately, owing to the significant gaps between the SLF and MLF. Our study also belongs to an SLF but surprisingly mitigates FPN-WSSS-IL by utilizing the recursive update and a novel edge refinement in mixing augmentation. When extending the proposed SLF to MLF (e.g., applying RW), our approach improved over recent WSSS methods by a large margin (+3.5%).</p><p>AuxSegNet <ref type="bibr" target="#b45">[44]</ref>, W-OoD <ref type="bibr" target="#b46">[45]</ref>, and CLIMS <ref type="bibr" target="#b35">[34]</ref> employed additional supervison, such as external saliency module and datasets, to remedy the shortcomings of the initial CAM. Our results achieved state-of-the-art performance without the use of such additional supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data augmentation</head><p>CutMix <ref type="bibr" target="#b26">[25]</ref> crops a random region of an image and pastes it on another. Saliency Grafting (SG) <ref type="bibr" target="#b27">[26]</ref> reflects the degree of occlusion in mixed class labels according to the saliency map generated from the CAM to prevent misleading labels. CDA <ref type="bibr" target="#b28">[27]</ref> is the first DA technique for improving the WSSS performance. ClassMix <ref type="bibr" target="#b29">[28]</ref> mixes unlabeled images with decoder outputs by training the part of pixel-wise annotations.</p><p>Previous works, including all WSSS studies, employed affine transformation <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b36">35]</ref>, cropping <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b14">14]</ref>, and mixing <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref> for data augmentation. To the best of our knowledge, our EPM only considered an edge refining method to enhance the quality of pseudo labels for data augmentation. Precisely, our EP method compensated uncertain regions by leveraging the edge information derived from indistinct cues of predicted masks. <ref type="table">Table 1</ref> compares the proposed EPM with the other related DA works in terms of critical properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><formula xml:id="formula_0">max(SC 1 l , SC 2 l ),</formula><p>and applies HSC to obtain the refined CAM as M scg = SCG(M cam ). In addition, the proposed RS includes the SCG in the training loop to maximize the effectiveness of the SCG. However, the SCG is designed to reduce the FNs of the CAM only at the inference time. We additionally reported the effects of the high-order feature correlation for SCG in Appendix.</p><formula xml:id="formula_1">M scg = SCG(M cam ) = ReLU (K scg ({M cam } &gt;? h ) ? K scg ({M cam } &lt;? l ),</formula><p>where {M cam } &gt;? h denotes the set of (i, j) indices satisfying M cam [i, j] &gt; ? h and K scg ({M cam } &gt;? h ) denotes the average activation maps generated from each pixel in {M cam } &gt;? h based on the HSC,</p><formula xml:id="formula_2">K scg ({M } ?? ) = 1 |{M } ?? | (i,j)?{M } ?? HSC[i, j, :, :].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PAMR</head><p>Pixel-adaptive mask refinement (PAMR) proposed by Araslanov and Roth <ref type="bibr" target="#b20">[20]</ref> iteratively refines the CAM M cam as M pamr = G T (M cam ) by exploiting the image pixel-level affinity matrix (? i,j,k,n ) (k,n)?N (i,j) for (i, j) ? W = {1 : h, 1 : w} of all pixels:</p><formula xml:id="formula_3">M pamr = P AM R(M cam ; W) = G T (M cam ),<label>(1)</label></formula><formula xml:id="formula_4">where G 0 (M cam ) = M cam , G t (M cam )[i, j] = (k,n)?N (i,j) ? i,j,k,n , ? i,j,k,n = exp[k(i,j,k,n)] (q,r)?N (i,j) exp[k(i,j,q,r)]</formula><p>. Here,k(?) is the average affinity value k(?) across the RGB channels, k(i, j, k, n) = ?|I i,j ? I k,n |/? 2 i,j denotes the normalized distance between the i, j-th and k, n-th pixel values, I i,j denotes the i, j-th pixel value of the original image I, and ? i,j denotes the standard deviation of the image intensity computed locally for the affinity kernel. In addition, G T (M cam )[i, j] for (i, j) / ? W is set to zero. Compared with existing methods (e.g., pixel adaptive convolution (PAC) <ref type="bibr" target="#b47">[46]</ref> or CRF <ref type="bibr" target="#b48">[47]</ref>), PAMR effectively reduces the computational complexity by narrowing the affinity kernel computation ? i,j,k,n to regions of contiguous pixels (k, n) ? N (?, ?) rather than all pixels {1 : h, 1 : w}.  <ref type="figure">Figure 2</ref>: Overview of the proposed SLF with RS and EPM. Based on extracted features from an encoder, the classifier and decoder produce the CAMs (M cam ) in <ref type="bibr" target="#b7">(7)</ref> and fine-grained segmentation masks (M dec ) in (9), respectively. The proposed RS provides further refined CAMs (M rs ) in (2) and pseudo masks (M seg ) in (10) by making the combination result of PAMR and SCG fed back to CAMs. The encoder and decoder recursively update with them in L cls (3), L seg (4), and L rec <ref type="bibr" target="#b5">(5)</ref>. EPM synthesizes two images (I mix ), mixed CAMs (M mix rs ), and pseudo masks (M mix seg ) refined by EP in <ref type="bibr" target="#b11">(11)</ref> to improve WSSS performance further. Finally, the classifier and decoder produce the predictions (M mix cam , M mix seg ) from a mixed image and update with them in L mix seg <ref type="bibr" target="#b16">(16)</ref> and L mix rec (17).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>The proposed method has parallel classification and segmentation branches. We propose two novel components relevant to our task: (i) RS and (ii) EPM. RS recursively updates the initial CAM with SCG and PAMR to discover an unseen foreground not detected in the previous iteration. We then apply the CertainFilter (CF), commonly used in WSSS-IL, like regular training. EPM mixes two images and pseudo masks refined by EP, which disentangles foreground and background regions by using edge information in the per-pixel class probability domain, thereby leading to sample diversification and significantly improving performance for WSSS. The overall framework is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RecurSeed</head><p>We observed that the complementary integration of the SCG and PAMR effectively moderated the FPN-WSSS-IL. In addition, the performance of the SCG depends highly on feature maps owing to its ability to update the CAM from SCG and PAMR recursively. In this section, we introduce RS M rs as follows:</p><formula xml:id="formula_5">For the t-th epoch, where t ? {0 : T }, M rs (t) = P AM R(SCG(M cam (t)); W),<label>(2)</label></formula><p>...  where M cam (0) is the initial CAM, M rs (t) denotes its refined result through RS, and W is set as the active region of SCG(M cam (t)). In particular, we improved the result of M cam (0) recursively by training the network such that the CAM updated at the next step M cam (1) becomes the result M rs (0), aiming at M rs (t) ? M cam (t + 1) for every epoch t ? {0 : T }. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the proposed RS gradually updates the initial CAM to remedy the shortcomings of the SCG and PAMR in <ref type="bibr" target="#b1">(2)</ref>. To achieve the objective of M rs (t) ? M cam (t + 1), we minimize the reconstruction loss in (4) (i.e., in decoder domain) and (5) (i.e., in encoder domain) along with the classification loss in (3), and the total net parameter ? t is updated at step t as follows:</p><formula xml:id="formula_6">? t = ? t?1 ? ? ? ?? E I L cls ? cls (? ), Y ; ? ?=?? ,? =t?1 (3) ? ? ? ?? E I L seg M dec (? ), M seg (? ); ? ?=?? ,? =t?1 (4) ? ? ? ?? E I L rec M cam (? ), M rs (? ); ? ?=?? ,? =t?1 ,<label>(5)</label></formula><p>where I = {I 1 , I 2 , ..., I B } denotes a mini-batch of size B, Y denotes their truth class labels,</p><formula xml:id="formula_7">f enc (t) = E ? enc t (I),<label>(6)</label></formula><formula xml:id="formula_8">M cam (t) = A ? cls t (f enc (t)), (7) Y cls (t) = ?(GAP (M cam (t))),<label>(8)</label></formula><formula xml:id="formula_9">M dec (t) = D ? dec t (f enc (t)),<label>(9)</label></formula><p>and</p><formula xml:id="formula_10">M seg (t) = CF (M rs (t)) (10) = ? ? ? ? ? ? ? argmax c?C M c rs (t)[:, i, j] if max c?C M c rs (t)[:, i, j] &gt; ? f g , 0 if max c?C M c rs (t)[:, i, j] &lt; ? bg , 255</formula><p>otherwise.</p><p>where 0 and 255 represent the background class and the ignored class respectively. As shown in <ref type="figure">Figure 2</ref>, our network consists of an encoder E(?) and a decoder D(?) with outputs of f enc (t) in <ref type="formula" target="#formula_7">(6)</ref> andM dec (t) in <ref type="formula" target="#formula_9">(9)</ref>, respectively. The refined CAMM cam (t) is then obtained as <ref type="formula">(7)</ref> by adding layer A(?), scaling the number of channels to the number of classes. We apply global average pooling (GAP) and sigmoid ? to estimate the class labels as? cls (t) in <ref type="bibr" target="#b8">(8)</ref>. Following the common practice, we employ the multi-label soft margin loss for the classification loss L cls , the commonly used cross-entropy loss for segmentation loss L seg , and the L1 loss for the reconstruction loss L rec . Our network narrows the gap between the decoder outputM dec (t) (or CAMM cam (t)) and RS M rs (t) through (4) (or (5)), leading to indirect (or direct) improvements in the CAMM cam (t) in the next step as the latent feature of the decoder. For stable training, CF is applied to generate the pseudo label M t cl = CF (M rs (t)) in (10) and to avoid the influence of uncertain labels. Through this process, CF filters certain and uncertain regions of the RS, thereby enhancing the reliability of the learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EdgePredictMix</head><p>We proposed a novel DA method, EPM, to moderate the IBDA-WSSS-IL. EPM is composed of refining the mask and synthesizing refined masks. The first step of EPM is to perform the mask refinement by using absolute and relative per-pixel probability values. We name this mask refinement EP, whose detailed process is as follows: suppose that there is an uncertain region given through thresholding of absolute probabilities per pixel. We utilize separate information on the relative difference between adjacent per-pixel probability values by deriving an edge from the mask and obtaining superpixels from the edge. We then remedy the uncertainty of low absolute probabilities by singling out the most dominant class within each superpixel, resulting in a boundary-aware mask. To extract the edge and superpixels, we simply used conventional algorithms named Canny <ref type="bibr" target="#b49">[48]</ref> and Connected-component labeling (CCL) <ref type="bibr" target="#b50">[49]</ref>, respectively. The second step of EPM is to blend two EP-refined masks (and their corresponding original images). EPM is illustrated in <ref type="figure">Figure 4</ref> and formulated in the following steps for two arbitrary indices i, j ? {1 : B} in a mini-batch.</p><p>The first step of EPM is to supplement the uncertain mask produced from the model by EP so that the decoder output M i dec (t) in the current epoch t is converted to M i ep (t) as in <ref type="bibr" target="#b11">(11)</ref>. Then, we extract the union : 0%</p><p>:93%</p><p>: 7%</p><p>: 0%</p><p>:100%</p><p>: 0%</p><p>:100%</p><p>: 0%</p><p>:15%</p><p>: 0%</p><p>:85%</p><p>:80%</p><p>:18%</p><p>: 2%</p><p>:100%</p><p>: 0%</p><p>: 0%</p><p>... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Certain Mask Uncertain Mask</head><p>Ours (EPM) ClassMix <ref type="figure">Figure 4</ref>: (a) Illustration of EP. We create a superpixel separated from the edge between class probabilities (e.g., simply by using Canny and CCL) and newly define the superpixel-based label to correct the original label in the uncertain area. (b) The proposed EPM compares to the conventional mixing method (ClassMix). Our EP further corrects for uncertain regions of pseudo masks by superpixel-based relabeling so that unlike ClassMix, our EPM synthesizes this refined image/mask, resulting in better WSSS performance. of all EP-refined foregrounds for image I i as M i f g in <ref type="bibr" target="#b12">(12)</ref>.</p><formula xml:id="formula_11">M i dec (t) = P AM R D ? dec t (E ? enc t (I i )); W M i ep (t) = EP (M i dec (t))<label>(11)</label></formula><formula xml:id="formula_12">R i c (t) = {(k, n) | M i ep (t)[c, k, n] &gt; ? f g } M i f g = 1 ? c?C R i c (t) ,<label>(12)</label></formula><p>where 1 denotes the element-wise indicator operator and W is set as the active region of D ? dec t (E ? enc t (I i )). The second step is pasting certain regions M i f g of image I i and its corresponding EP-refined-mask set M i ep (t) (or its RS seed as encoder output M i rs (t)) onto image I j and its EP-refined-mask set M j ep (t) (or its RS seed M j rs (t)), respectively, as shown in I i?j in <ref type="bibr" target="#b13">(13)</ref> and M rs i?j (t) in (14) (or M cam i?j (t) in <ref type="formula" target="#formula_3">(15)</ref>). In other words, I i?j , M seg i?j (t), and M rs i?j (t) correspond to the (EP-based) synthesizing results in the original image, decoder, and encoder, respectively.</p><formula xml:id="formula_13">I i?j = I i M i f g + I j (1 ? M i f g ) (13) M seg i?j (t) = M i ep (t) M i f g + M j ep (t) (1 ? M i f g ) (14) M rs i?j (t) = M i rs (t) M i f g + M j rs (t) (1 ? M i f g )<label>(15)</label></formula><p>We then make the network train the mixed images and labels as follows:</p><formula xml:id="formula_14">? t = ? t?1 ? ( * ) ? ? ? ?? E I L mix seg M mix dec (? ), M mix seg (? ); ? ?=?? ,? =t?1 ,<label>(16)</label></formula><formula xml:id="formula_15">? ? ? ?? E I L mix rec M mix cam (? ), M mix rs (? ); ? ?=?? ,? =t?1 ,<label>(17)</label></formula><p>where ( * ) refers to all terms in <ref type="formula">(3)</ref>, <ref type="formula">(4)</ref>, and <ref type="formula" target="#formula_6">(5)</ref> and</p><formula xml:id="formula_16">I mix = I i?j j ? unif({1 : B}/i) for i ? {1 : B} M mix seg (t) = M seg i?j (t) I i?j ? I mix (18) M mix dec (t) = D ? dec t (E ? cam t (I mix )) (19) M mix rs (t) = M rs i?j (t) I i?j ? I mix (20) M mix cam (t) = A ? cls t (E ? cam t (I mix ))<label>(21)</label></formula><p>Using the proposed EPM, we obtained another mini-batch I mix of B mixed images by selecting each sample in the mini-batch I, choosing another sample at random from the remaining samples, and mixing them. The pseudo masks and the decoder outputs for I mix at epoch t are given by M mix seg (t) in (18) andM mix dec (t) in <ref type="bibr" target="#b19">(19)</ref>, respectively, through the additional optimizations specified as follows: The first loss term in (16) (or second loss term in <ref type="formula" target="#formula_3">(17)</ref>) corresponds to the cross-entropy loss (or the reconstruction loss) that makes the decoder outputM mix dec (t) (or the encoder outputM mix cam (t)) reproduce the mixed mask refined by EP M mix seg (t) (or the mixed RS M mix rs (t)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluated our method on the PASCAL VOC 2012 <ref type="bibr" target="#b15">[15]</ref> and MS COCO 2014 datasets <ref type="bibr" target="#b16">[16]</ref>. Similar to Ahn and Kwak <ref type="bibr" target="#b5">[5]</ref>, Ahn et al. <ref type="bibr" target="#b32">[31]</ref>, Wang et al. <ref type="bibr" target="#b30">[29]</ref>, Lee et al. <ref type="bibr" target="#b7">[7]</ref>, Jo and Yu <ref type="bibr" target="#b51">[50]</ref>, we used the mIoU to characterize our experiments. Implementation details are given in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with state-of-the-art approaches</head><p>When comparing SLFs or MLFs using the same supervision, the proposed method outperformed the other methods even if only RS was used, regardless of PASCAL VOC 2012 or MS COCO 2014. Furthermore, if additionally considering the EPM, the performance is consistently further enhanced than the proposed one using only the RS, thereby proving the effectiveness of both RS and EPM. Specifically, our single-and multi-stage (e.g., using RW) methods achieved mIoUs of 69.5% and 74.4% on the PASCAL VOC 2012 val set, improving mIoUs by 2.5% and 5.6% compared to the latest studies, respectively. In particular, our single-stage method (test mIoU 70.6) performs similar to or better than  <ref type="bibr" target="#b19">[19]</ref> WR38 I 62.6 62.9 -SSSS CVPR'20 <ref type="bibr" target="#b20">[20]</ref> WR38 I 62.7 64.3 -AFA CVPR'22 <ref type="bibr" target="#b44">[43]</ref> MiT  <ref type="bibr" target="#b6">[6]</ref> R101 I+S 64.9 65.3 -NSRM CVPR'21 <ref type="bibr" target="#b52">[51]</ref> R101 I+S 68.3 68.5 -AuxSegNet ICCV'21 <ref type="bibr" target="#b45">[44]</ref> WR38 I+S 69.0 68.6 33.9 EDAM CVPR'21 <ref type="bibr" target="#b34">[33]</ref> R101 I+S 70.9 70.6 -DRS AAAI'21 <ref type="bibr" target="#b53">[52]</ref> R101 I+S 71.2 71.4 -CLIMS CVPR'22 <ref type="bibr" target="#b35">[34]</ref> R50 I+D 69.3 68.7 -W-OoD CVPR'22 <ref type="bibr" target="#b46">[45]</ref> R101 I+D 70. recent single-or multi-stage methods using the same supervision, even though we use a backbone with fewer parameters (i.e., R50). This indicates for the first time through the proposed method that the latest performance can be achieved only with a single-stage (without additional stages such as RW).</p><p>In contrast with PASCAL VOC 2012, MS COCO 2014 is more challenging because it has four times categories and requires to detection of non-salient foreground objects. As shown in the last column of <ref type="table" target="#tab_0">Table  2</ref>, even for MS COCO 2014, our single-and multi-stage methods achieved new state-of-the-art mIoUs of 42.2% and 46.4%, improving mIoUs by 3.3% and 1.7% compared to the latest studies, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis</head><p>In this section, we empirically study the proposed RS and EPM. We also refer to Appendix for additional studies.  Existing SCG and PAMR studies, which use a fully trained network, were individually developed for the post-processing method on the CAM. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the simple integration of SCG and PAMR (i.e., w. SCG and PAMR but w.o. RS) shows a marginal improvement, but it fails to reduce FP and FN simultaneously. By contrast, we observed a decrease in both FP and FN when applying our RS. This indicates that RS makes both SCG and PAMR being utilized uniquely as recursively together to mitigate FPN-WSSS-IL successfully. Therefore, the proposed RS achieved higher performance than the simple integration, verifying the validity and novelty of the proposed recursive update in RS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Novelty of EPM.</head><p>The previous DA methods rely on unsuitable seeds (e.g., CAM) or ideal annotations (e.g., pixel-wise annotations). Nevertheless, we applied the proposed RS to pre-process our EPM and other DA methods for a fair comparison. In <ref type="table" target="#tab_4">Table 4</ref>, CutMix <ref type="bibr" target="#b26">[25]</ref>, SaliencyGrafting <ref type="bibr" target="#b27">[26]</ref>, and CDA <ref type="bibr" target="#b28">[27]</ref> improved F 1 with mixed class labels but decreased approximately 2% of mIoU. ClassMix <ref type="bibr" target="#b29">[28]</ref> showed a marginal improvement despite the WSSS setting. However, mixing predicted masks without the mask refinement leads to an insufficient improvement. Our EPM not only trains mixed masks (like ClassMix) but also further refines pseudo masks by leveraging the reconstituted edge from uncertain regions, thereby resulting in the highest WSSS performance (mIoU 75.2%) compared to other DA techniques (the best mIoU 71.2%). For simplicity, we did not use the mixed class loss but observed that the F 1 performance was similar to the existing DA methods, proving its justification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Importance of the proposed components.</head><p>To clarify the effect of the proposed RS and EPM, we conducted ablation studies related to loss functions and EP on our single-stage network. In <ref type="table" target="#tab_5">Table 5</ref>, we report different combinations as we add or remove  components. First, as shown in row 3, we observe that training both the encoder and decoder with our RS significantly improves the decoder's performance and brings an explicit gain for the CAM as the recursively advanced features lead to better pseudo masks in the next step. Second, in rows 5 and 7, training mixed results without EP shows a certain level of improvements and outperforms ClassMix <ref type="bibr" target="#b29">[28]</ref> as the model only trained reliable labels from CF. We present samples of qualitative segmentation results on PASCAL VOC 2012 and MS COCO 2014 sets in Appendix. We achieved the best performance (the last row) when applying essential components together with EP by exploiting absolute and relative per-pixel probability information. In addition, RS with EP (row 4) achieved 71.2% of Decoder. The result shows that EP itself achieved meaningful improvement even without mixing augmentation. Therefore, we demonstrate the novelty and validity of the proposed components (i.e., RS and EPM).</p><p>Additional comparison with existing MLFs using RW. In <ref type="table" target="#tab_6">Table 6</ref>, we compared the performance of the proposed SLF with existing MLFs <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b37">36]</ref> by extending it to MLF with the same configuration of RW. Surprisingly, in all cases before or after the application of RW, our method outperforms the related works by achieving mIoUs of 75.2% and 76.7% without and with RW, respectively, exhibiting the performance improvement of at least 13% and 6% compared to previous state-of-the-art methods (PPC <ref type="bibr" target="#b36">[35]</ref>: 61.5% and RIB <ref type="bibr" target="#b33">[32]</ref>: 70.6%). This also proves the superiority of the proposed RS and EPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study, we proposed RS and EPM to improve the WSSS-IL performance, achieving new state-of-the-art performances on the PASCAL VOC 2012 and MS COCO 2014 benchmarks. Furthermore, because RS and EPM are learning methods generally applicable to SLFs consisting of an arbitrary encoder and decoder, one can upgrade this backbone to achieve a higher SLF performance. We also expect higher MLF performance can be derived by performing multi-stage extension from the proposed SLF to the latest techniques other than RW or applying our RS and EPM to the individual learning of any recent MLF. From this perspective, we expect that our single-and multi-stage methods have high utility and scalability in weakly or semi-supervised tasks. Implementation details. To train and evaluate the PASCAL VOC 2012 set, we extended our SLF model to an MLF by applying an RW <ref type="bibr" target="#b32">[31]</ref> to generate pseudo masks from the SLF results and build the MLF by applying ResNet101 with DeepLabv3+ <ref type="bibr" target="#b1">[2]</ref>. In the case of MS COCO 2014, we did not apply an RW because our SLF results sufficiently cover foregrounds on MS COCO 2014, even without using an RW. We also applied CF with ? f g = 0.55 and ? bg = 0.10. To train an SLF, we employ ResNet50 <ref type="bibr" target="#b61">[60]</ref> and add DeepLabv3+ as in Araslanov and Roth <ref type="bibr" target="#b20">[20]</ref>. For testing, we adopted a multi-scale strategy, and CRF <ref type="bibr" target="#b48">[47]</ref> with the hyperparameters suggested in Chen et al. <ref type="bibr" target="#b0">[1]</ref>. We used all hyperparameters when training the MS COCO 2014 dataset, the same as the PASCAL VOC 2012 dataset. For training, we used a stochastic gradient descent (SGD) optimizer with weight decay 4e ?5 . The initial learning rate was set to 0.1, and it decayed polynomially at a rate of 0.9. The training images were augmented by random scaling, horizontal flipping, color jittering, and random cropping to a pixel resolution of 512 ? 512. We used a single RTX A6000 GPU to conduct all experiments and implemented our method in PyTorch. In addition, our method takes less than 24 hours for training the PASCAL VOC 2012. The proposed modules are only used for trianing. Thus, ours has the same runtime as other SLF methods <ref type="bibr" target="#b20">[20]</ref> in evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional analysis B.1 Effect of joint consideration for RecurSeed and EdgePredictMix</head><p>Quantitative results. To show the effects of RS and EPM, we conducted ablation studies with and without RS and EPM individually on our single-stage network. Without RS and EPM, training is conducted only by minimizing the classification loss L cls . <ref type="table" target="#tab_7">Table 7</ref> compares the network's CAM, SCG, and decoder results for up to 30 epochs. For the PASCAL VOC 2012 and MS COCO 2014 datasets, we observed an additional improvement in the mIoU of CAM and SCG results by approximately 3%-5% with each sequential application of RS and EPM, thereby demonstrating the individual necessity and effectiveness of RS and EPM.</p><p>In addition, we verified the validity of the proposed decoder by showing that its results were consistently better than those of CAM and SCG by more than approximately 5%.</p><p>Hyperparameter analysis. To validate the effects of the proposed RS and EPM, we evaluated our singlestage results for each epoch, as shown in <ref type="figure" target="#fig_4">Figure 5(a)</ref>. Initially, the mIoU increased steeply through the RS between the 5th and 10th epochs. However, the mIoU then saturated between the 10th and 15th epochs. To address this saturation, we applied EPM after the 15th epoch.</p><p>Here, ? f g and ? bg in (10) control the foreground and background regions. Although we observed a sufficiently high mIoU above a certain level irrespective of changes of ? f g and ? bg in <ref type="figure" target="#fig_4">Figures 5(b)-(c)</ref>, the best performance can be found through their adjustment, proving the validity of two thresholds.</p><p>Qualitative results. <ref type="figure" target="#fig_5">Figure 6</ref> illustrates the results produced by our single-stage method (including CAM, RS, and Decoder). We then generated pseudo masks from the decoder output by using CF. For the initial training step shown in <ref type="figure" target="#fig_5">Figure 6</ref>(a), CAM and SCG cover the most discriminative part of an object. In addition, the decoder output from the segmentation branch fails to detect all of the foregrounds owing to insufficient training steps. After we apply RS to train the segmentation branch ( <ref type="figure" target="#fig_5">Figures 6(b)-(c)</ref>), the predicted masks were better than those at the initial step (t = 1). As the learning steps progress through EPM ( <ref type="figure" target="#fig_5">Figure 6(d)</ref>), the CAM and SCG represent more integral regions of an object, and the segmentation branch produces an accurate seed. Specifically, we secured the data diversity and increased the mIoU by more than 5%. The qualities of pseudo masks (Decoder+CF) are improved progressively in <ref type="figure" target="#fig_5">Figures 6(a)-(d)</ref>, and the final mask in <ref type="figure" target="#fig_5">Figure 6(d)</ref> is then close to the ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Effect of RecurSeed</head><p>Usage of high-order feature correlation in SCG. In <ref type="table" target="#tab_8">Table 8</ref>, we show the SCG results obtained by varying the layer combination in the HSC. For each combination, we displayed the mIoU of both cases, i.e., the first case in which the SCG result is generated from the raw CAM (i.e., t = 1) and the second case in which the SCG result is updated by the proposed RS (i.e., t = 30). In both cases, we observed that combining more layers in HSC sequentially improves the mIoU (i.e., from 49.7% to 58.0% for t = 1, and 67.5% to 69.0% for t = 30). From this observation, we used all layers from layer1 to layer5 for training the RS. In particular, the proposed RS and EPM consistently improve the performance under all combinations of layers (e.g., the performance improves by 11%, from 58.0% to 69.0%, when combining all layers), validating the usefulness of our method.</p><p>In <ref type="table" target="#tab_9">Table 9</ref>, we add both FN and FP results into <ref type="table" target="#tab_8">Table 8</ref>. Note that the closer the network layer is to the input, the more discriminative feature maps are extracted. Therefore, using high-order features (i.e., adding low layer maps L4-L1) reduces FP as it focuses on the target's discriminative area; however, it has the side effect of increasing FN in general. If RS is not used, this trend appears as shown in <ref type="table" target="#tab_8">Table 8</ref>, but if RS is used, FP decreased (0.232 ? 0.210) without increasing FN (0.161 ? 0.141). As RS repeatedly gives diverse discriminative information to the network, the network can ideally accumulate all this information, thus compensating for the weakness of increasing FN. As such, RS does not increase FN but does not play a key role in reducing FN. However, it can be observed in <ref type="table" target="#tab_9">Table 9</ref> that FN is also significantly reduced (0.148 ? 0.133) when EPM is added to RS. The reason is that EPM can effectively reduce FN by letting the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional results</head><p>Quantitative results. We show the per-class segmentation results from the both PASCAL VOC 2012 and MS COCO 2014 datasets in <ref type="table" target="#tab_10">Tables 10, 11</ref>, and 12. These quantitative results indicate that our method outperforms existing state-of-the-art methods in most class categories.</p><p>Qualitative results. We present additional examples of qualitative segmentation results produced by our method on both the PASCAL VOC 2012 and MS COCO 2014 val sets in <ref type="figure">Figures 7 and 8</ref>, respectively. These results show that our method not only performs well for different complex scenes, small objects, or multiple instances but also can achieve a satisfactory segmentation performance for various challenging scenes. We also visualize more examples of attention maps for each step (t = 1, 5, 15, 30) on the PASCAL VOC 2012 and MS COCO 2014 train sets, as shown in <ref type="figure">Figures 9 and 10</ref>, respectively. The raw CAMs without RS (i.e., t = 1) only focus on the local discriminative parts for large-scale objects, such as the head and hands of people or wheels of the vehicles. However, when applying the proposed RS (i.e., t = 5, 15), our single-stage results cover more object regions, including those less discriminative regions for large-scale objects, and also capture the exact boundaries of small-scale objects (i.e., decreasing the FP). Furthermore, with more training steps, including the proposed EPM (i.e., t = 30), our final results produce more accurate boundaries. This demonstrates the sequential improvements through the proposed RS and EPM.             <ref type="figure">Figure 9</ref>: Visualization of attention maps with recursive improvements on the PASCAL VOC 2012 train set.</p><formula xml:id="formula_17">DSRG - - - - - - - - - - - - - - - - - - - - - 61.</formula><formula xml:id="formula_18">- - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_19">- - - - - - - - - - - - - - - - - - - - - 68.3 EDAM - - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_20">- - - - - - - - - - - - - - - - - - - - - 68.4 DRS - - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_21">- - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_22">- - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_23">- - - - - - - - - - - - - - - - - - - - - 68.5 EDAM - - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_24">- - - - - - - - - - - - - - - - - - - - - 68.2 DRS - - - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of the quality of pseudo masks produced from CAM, simple post-processing on CAM (PAMR or SCG), and the proposed RecurSeed (RS). Unlike the simple post-processing schemes for CAM, the proposed RS recursively updates the CAM, thereby taking advantage of both the strengths of SCG (i.e., reducing FN) and PAMR (i.e., reducing FP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(a) Effects of RS and EPM per epoch. (b) Effect of foreground threshold ? f g . (c) Effect of background threshold ? bg .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of attention maps and pseudo masks with recursive improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Qualitative segmentation results of PASCAL VOC 2012 val set. Qualitative segmentation results of MS COCO 2014 val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of WSSS methods in terms of mIoU (%) on PASCAL VOC 2012 and COCO 2014. * and ? indicate the backbone of VGG-16 and ResNet-50, respectively. Sup., supervision; I, image-level class labels; S, the saliency supervision; D, the external dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Sup.</cell><cell>VOC val test</cell><cell>COCO val</cell></row><row><cell>Single stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EM ICCV'15 [17]</cell><cell>VGG16</cell><cell>I</cell><cell>38.2 39.6</cell><cell>-</cell></row><row><cell>RRM AAAI'20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Element-wise component analysis on PASCAL VOC 2012 train set. With RS, SCG and SCG+PAMR denote M rs (t) in (2) by removing and preserving operator P AM R(?) throughout the training, respectively. RS indicates t = 1 ( ) or 30 ( ). * denotes the decoder map result.</figDesc><table><row><cell>RS SCG PAMR</cell><cell>mIoU</cell><cell>FP</cell><cell>FN</cell></row><row><cell></cell><cell>58.0</cell><cell>0.268</cell><cell>0.165</cell></row><row><cell></cell><cell>59.3</cell><cell>0.225 (? 0.043)</cell><cell>0.194 (? 0.029)</cell></row><row><cell></cell><cell>65.2</cell><cell>0.216</cell><cell>0.143</cell></row><row><cell></cell><cell>65.9</cell><cell>0.210 (? 0.006)</cell><cell>0.141 (? 0.002)</cell></row><row><cell></cell><cell>*67.4</cell><cell>*0.196</cell><cell>*0.141</cell></row><row><cell></cell><cell>*70.7</cell><cell>*0.171 (? 0.025)</cell><cell>*0.134 (? 0.007)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison with the proposed EPM and existing mixing methods in terms of F 1 (%) and mIoU (%) on PASCAL VOC 2012 train set. * denotes our implementation for a fair comparison.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>F 1</cell><cell>mIoU</cell></row><row><cell>RecurSeed</cell><cell>R50</cell><cell>94.7</cell><cell>70.7</cell></row><row><cell>RecurSeed + *CutMix [25]</cell><cell>R50</cell><cell>95.6</cell><cell>68.5</cell></row><row><cell>RecurSeed + *SaliencyGrafting [26]</cell><cell>R50</cell><cell>96.8</cell><cell>68.6</cell></row><row><cell>RecurSeed + *CDA [27]</cell><cell>R50</cell><cell>96.0</cell><cell>69.0</cell></row><row><cell>RecurSeed + *ClassMix [28]</cell><cell>R50</cell><cell>94.6</cell><cell>71.2</cell></row><row><cell>RecurSeed + EdgePredictMix</cell><cell>R50</cell><cell>95.2</cell><cell>75.2</cell></row><row><cell>5.2.1 Novelty of RS.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effect of key components in terms of mIoU (%) on PASCAL VOC 2012 train set.</figDesc><table><row><cell>L cls</cell><cell>Lseg</cell><cell>Lrec</cell><cell>L mix seg</cell><cell>L mix rec</cell><cell>EP</cell><cell cols="2">CAM Decoder</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.9</cell><cell>17.4</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.6</cell><cell>65.8</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.9</cell><cell>70.7</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59.0</cell><cell>71.7</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.0</cell><cell>73.5</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60.8</cell><cell>74.5</cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.9</cell><cell>73.6</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63.4</cell><cell>75.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>mIoUs (%) of single-stage results produced from a trained network (Seed) and pseudo masks generated by RW on PASCAL VOC 2012 train set.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone Seed</cell><cell>RW</cell></row><row><cell>SEAM [29]</cell><cell>WR38</cell><cell>55.4</cell><cell>63.6</cell></row><row><cell>IRNet [31]</cell><cell>R50</cell><cell>48.8</cell><cell>66.3</cell></row><row><cell>CSE [54]</cell><cell>WR38</cell><cell>56.0</cell><cell>66.9</cell></row><row><cell>CDA [27]</cell><cell>R50</cell><cell>50.8</cell><cell>67.7</cell></row><row><cell>CPN [30]</cell><cell>WR38</cell><cell>57.4</cell><cell>67.8</cell></row><row><cell>CONTA [58]</cell><cell>R50</cell><cell>48.8</cell><cell>67.9</cell></row><row><cell>AMR [56]</cell><cell>R50</cell><cell>56.8</cell><cell>69.7</cell></row><row><cell>AdvCAM [7]</cell><cell>R50</cell><cell>55.6</cell><cell>69.9</cell></row><row><cell>PPC [35]</cell><cell>WR38</cell><cell>61.5</cell><cell>70.1</cell></row><row><cell>RIB [32]</cell><cell>R50</cell><cell>56.5</cell><cell>70.6</cell></row><row><cell>Ours (RS)</cell><cell>R50</cell><cell>70.7</cell><cell>74.8</cell></row><row><cell>Ours (RS+EPM)</cell><cell>R50</cell><cell>75.2</cell><cell>76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>mIoUs (%) of CAM, SCG, and the prediction from the segmentation branch (Decoder) on PASCAL VOC 2012 and MS COCO 2014 train images. RS, RecurSeed; EPM, EdgePredictMix.</figDesc><table><row><cell>Dataset</cell><cell>RS EPM</cell><cell cols="3">CAM SCG Decoder</cell></row><row><cell></cell><cell></cell><cell>47.4</cell><cell>58.0</cell><cell>17.4</cell></row><row><cell>VOC</cell><cell></cell><cell>57.9</cell><cell>65.9</cell><cell>70.7</cell></row><row><cell></cell><cell></cell><cell>63.4</cell><cell>69.0</cell><cell>75.2</cell></row><row><cell></cell><cell></cell><cell>32.1</cell><cell>37.8</cell><cell>10.6</cell></row><row><cell>COCO</cell><cell></cell><cell>40.0</cell><cell>41.5</cell><cell>47.2</cell></row><row><cell></cell><cell></cell><cell>42.3</cell><cell>43.2</cell><cell>50.3</cell></row><row><cell>A Reproducibility</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Details for datasets. PASCAL VOC 2012 has 21 classes (including a background class) and three subsets</cell></row><row><cell cols="5">(i.e., training (train), validation (val), and test with 1,464, 1,449, and 1,456 images, respectively). Following</cell></row><row><cell>common practice, e.</cell><cell></cell><cell></cell><cell></cell></row></table><note>g., Ahn and Kwak [5], Wang et al. [29], Araslanov and Roth [20], Zhang et al. [19], Lee et al. [7], Zhang et al. [30], we used additional annotations from the Semantic Boundary Dataset [59] to build an augmented dataset with 10,582 images for training. MS COCO 2014 contains 81 classes, including a background class. This dataset contains 82,783 training images and 40,504 validation images. Although the two datasets contain labels for object detection and semantic segmentation, we only used image-level class labels for the WSSS. The results of the PASCAL VOC 2012 val and test sets were obtained from the official PASCAL VOC online evaluation server.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ablation study for each combination of the SCG. Our method boosts the overall performance of SCG applied in various layers. indicates that SCG is applied.</figDesc><table><row><cell>layer5 layer4 layer3 layer2 layer1 ? =1 (w.o. RS and EPM)</cell><cell>? =30 (w. RS and EPM)</cell></row><row><cell>49.7</cell><cell>67.5</cell></row><row><cell>57.0</cell><cell>68.4</cell></row><row><cell>57.7</cell><cell>68.8</cell></row><row><cell>57.9</cell><cell>69.0</cell></row><row><cell>58.0</cell><cell>69.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Extended results of Table 8with mIoU, FP, and FN: w/o RS (t = 1), w/ RS (t = 30), L5 (layer5), and L4-L1 (layers 4+3+2+1).</figDesc><table><row><cell>RS EPM</cell><cell>Low L5</cell><cell>High L4-L1</cell><cell>mIoU</cell><cell>FP</cell><cell>FN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>49.7</cell><cell>0.358</cell><cell>0.159</cell></row><row><cell></cell><cell></cell><cell></cell><cell>58.0</cell><cell cols="2">0.268 (? 0.090) 0.165 (? 0.006)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>61.9</cell><cell>0.232</cell><cell>0.161</cell></row><row><cell></cell><cell></cell><cell></cell><cell>65.9</cell><cell cols="2">0.210 (? 0.022) 0.141 (? 0.020)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.5</cell><cell>0.188</cell><cell>0.148</cell></row><row><cell></cell><cell></cell><cell></cell><cell>69.0</cell><cell>0.187 (? 0.001)</cell><cell>0.133 (? 0.015)</cell></row><row><cell cols="6">network better recognize a detailed region of each object. As a result, both FN and FP achieved the lowest</cell></row><row><cell cols="6">values of 0.133 and 0.187, respectively, when RS and EPM were considered together.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Per-class performance comparisons with WSSS methods in terms of IoUs (%) on the PASCAL VOC 2012 val set.</figDesc><table><row><cell>Method</cell><cell>bkg</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbk</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell>EM-Adapt</cell><cell cols="22">67.2 29.2 17.6 28.6 22.2 29.6 47.0 44.0 44.2 14.6 35.1 24.9 41.0 34.8 41.6 32.1 24.8 37.4 24.0 38.1 31.6 33.8</cell></row><row><cell>MIL-LSE</cell><cell cols="22">79.6 50.2 21.6 40.9 34.9 40.5 45.9 51.5 60.6 12.6 51.2 11.6 56.8 52.9 44.8 42.7 31.2 55.4 21.5 38.8 36.9 42.0</cell></row><row><cell>SEC</cell><cell cols="22">82.4 62.9 26.4 61.6 27.6 38.1 66.6 62.7 75.2 22.1 53.5 28.3 65.8 57.8 62.3 52.5 32.5 62.6 32.1 45.4 45.3 50.7</cell></row><row><cell>TransferNet</cell><cell cols="22">85.3 68.5 26.4 69.8 36.7 49.1 68.4 55.8 77.3 6.2 75.2 14.3 69.8 71.5 61.1 31.9 25.5 74.6 33.8 49.6 43.7 52.1</cell></row><row><cell>CRF-RNN</cell><cell cols="22">85.8 65.2 29.4 63.8 31.2 37.2 69.6 64.3 76.2 21.4 56.3 29.8 68.2 60.6 66.2 55.8 30.8 66.1 34.9 48.8 47.1 52.8</cell></row><row><cell>WebCrawl</cell><cell cols="22">87.0 69.3 32.2 70.2 31.2 58.4 73.6 68.5 76.5 26.8 63.8 29.1 73.5 69.5 66.5 70.4 46.8 72.1 27.3 57.4 50.2 58.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>4 PSA 87.6 76.7 33.9 74.5 58.5 61.7 75.9 72.9 78.6 18.8 70.8 14.1 68.7 69.6 69.5 71.3 41.5 66.5 16.4 70.2 48.7 59.4 FickleNet 89.5 76.6 32.6 74.6 51.5 71.1 83.4 74.4 83.6 24.1 73.4 47.4 78.2 74.0 68.8 73.2 47.8 79.9 37.0 57.3 64.6 64.9 SSDD 89.0 62.5 28.9 83.7 52.9 59.5 77.6 73.7 87.0 34.0 83.7 47.6 84.1 77.0 73.9 69.6 29.8 84.0 43.2 68.0 53.4 64.9 MCIS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>40.0 88.1 52.7 67.3 85.9 80.1 89.2 32.4 78.5 48.7 83.9 81.2 77.3 84.7 52.2 83.3 46.7 51.2 59.4 69.5 Ours (multi-stage, RS) 91.7 85.0 32.5 87.5 48.3 79.4 91.7 83.0 92.5 38.2 88.2 60.8 89.7 86.2 79.9 83.3 56.0 85.3 57.2 55.5 56.3 72.8 Ours (multi-stage, RS+EPM) 92.2 88.4 35.4 87.9 63.8 79.5 93.0 84.5 92.7 39.0 90.5 54.5 90.6 87.5 83.0 84.0 61.1 85.6 52.1 56.2 60.2 74.4</figDesc><table><row><cell></cell><cell></cell><cell>70.4</cell></row><row><cell>CPN</cell><cell></cell><cell>89.9 75.0 32.9 87.8 60.9 69.4 87.7 79.4 88.9 28.0 80.9 34.8 83.4 79.6 74.6 66.9 56.4 82.6 44.9 73.1 45.7 67.8</cell></row><row><cell>RIB</cell><cell></cell><cell>90.3 76.2 33.7 82.5 64.9 73.1 88.4 78.6 88.7 32.3 80.1 37.5 83.6 79.7 75.8 71.8 47.5 84.3 44.6 65.9 54.9 68.3</cell></row><row><cell cols="2">Ours (single-stage, RS)</cell><cell>89.7 80.0 36.1 87.7 40.1 65.2 82.6 75.3 88.6 30.1 74.4 48.9 82.9 79.0 75.0 80.9 46.0 75.7 47.1 52.1 58.3 66.5</cell></row><row><cell>Ours</cell><cell>(single-stage,</cell><cell>91.3 85.7</cell></row><row><cell>RS+EPM)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Per-class performance comparisons with WSSS methods in terms of IoUs (%) on the PASCAL VOC 2012 test set. 58.8 30.5 67.6 24.7 44.7 74.8 61.8 73.7 22.9 57.4 27.5 71.3 64.8 72.4 57.3 37.3 60.4 42.8 42.2 50.6 53.7 WebCrawl 87.2 63.9 32.8 72.4 26.7 64.0 72.1 70.5 77.8 23.9 63.6 32.1 77.2 75.3 76.2 71.5 45.0 68.8 35.5 46.2 49.3 58.7 DSRG</figDesc><table><row><cell>Method</cell><cell>bkg</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbk</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell>EM-Adapt</cell><cell cols="22">76.3 37.1 21.9 41.6 26.1 38.5 50.8 44.9 48.9 16.7 40.8 29.4 47.1 45.8 54.8 28.2 30.0 44.0 29.2 34.3 46.0 39.6</cell></row><row><cell>MIL-LSE</cell><cell cols="22">78.7 48.0 21.2 31.1 28.4 35.1 51.4 55.5 52.8 7.8 56.2 19.9 53.8 50.3 40.0 38.6 27.8 51.8 24.7 33.3 46.3 40.6</cell></row><row><cell>SEC</cell><cell cols="22">83.5 56.4 28.5 64.1 23.6 46.5 70.6 58.5 71.3 23.2 54.0 28.0 68.1 62.1 70.0 55.0 38.4 58.0 39.9 38.4 48.3 51.7</cell></row><row><cell>TransferNet</cell><cell cols="22">85.7 70.1 27.8 73.7 37.3 44.8 71.4 53.8 73.0 6.7 62.9 12.4 68.4 73.7 65.9 27.9 23.5 72.3 38.9 45.9 39.2 51.2</cell></row><row><cell>CRF-RNN</cell><cell>85.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>70.6 31.6 77.2 42.2 68.9 79.1 66.5 74.9 29.6 68.7 56.1 82.1 64.8 78.6 73.5 50.8 70.7 47.7 63.9 51.1 63.7 FickleNet 90.3 77.0 35.2 76.0 54.2 64.3 76.6 76.1 80.2 25.7 68.6 50.2 74.6 71.8 78.3 69.5 53.8 76.5 41.8 70.0 54.2 65.0 SSDD 89.5 71.8 31.4 79.3 47.3 64.2 79.9 74.6 84.9 30.8 73.5 58.2 82.7 73.4 76.4 69.9 37.4 80.5 54.5 65.7 50.3 65.5 MCIS</figDesc><table><row><cell></cell><cell>63.2</cell></row><row><cell>PSA</cell><cell>89.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>70.7 CPN 90.4 79.8 32.9 85.7 52.8 66.3 87.2 81.3 87.6 28.2 79.7 50.1 82.9 80.4 78.8 70.6 51.1 83.4 55.4 68.5 44.6 68.5 RIB 90.4 80.5 32.8 84.9 59.4 69.3 87.2 83.5 88.3 31.1 80.4 44.0 84.4 82.3 80.9 70.7 43.5 84.9 55.9 59.0 47.3 68.6 Ours (single-stage, RS) 89.8 83.8 33.4 87.5 39.8 67.1 85.2 78.5 91.2 29.3 77.9 54.1 84.3 81.7 78.5 78.9 53.4 78.4 53.9 42.2 56.9 67.9 Ours (single-stage, RS+EPM) 91.2 86.8 37.3 80.6 52.1 71.3 87.8 81.3 90.9 32.2 80.4 54.5 86.4 88.6 83.6 80.8 59.0 82.8 54.2 42.6 59.1 70.6 Ours (multi-stage, RS) 91.4 89.5 36.2 88.7 46.2 69.1 93.1 85.7 91.8 34.0 86.7 66.3 89.0 88.8 82.9 81.3 57.3 89.0 57.5 46.9 57.9 72.8 Ours (multi-stage, RS+EPM) 91.9 89.7 37.3 88.0 62.5 72.1 93.5 85.6 90.2 36.3 88.3 62.5 86.3 89.1 82.9 81.2 59.7 89.2 56.2 44.5 59.4 73.6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>Per-class performance comparisons with WSSS methods in terms of IoUs (%) on the MS COCO 2014 val set.</figDesc><table><row><cell>Class</cell><cell cols="4">SEC DSRG Ours (multi-stage, RS) Ours (multi-stage, RS+EPM)</cell><cell>Class</cell><cell cols="4">SEC DSRG Ours (multi-stage, RS) Ours (multi-stage, RS+EPM)</cell></row><row><cell>background</cell><cell>74.3</cell><cell>80.6</cell><cell>82.6</cell><cell>83.6</cell><cell>wine glass</cell><cell>22.3</cell><cell>24.0</cell><cell>37.5</cell><cell>39.8</cell></row><row><cell>person</cell><cell>43.6</cell><cell>-</cell><cell>74.8</cell><cell>74.9</cell><cell>cup</cell><cell>17.9</cell><cell>20.4</cell><cell>39.1</cell><cell>38.9</cell></row><row><cell>bicycle</cell><cell>24.2</cell><cell>30.4</cell><cell>53.1</cell><cell>55.0</cell><cell>fork</cell><cell>1.8</cell><cell>0.0</cell><cell>19.9</cell><cell>4.9</cell></row><row><cell>car</cell><cell>15.9</cell><cell>22.1</cell><cell>48.6</cell><cell>50.1</cell><cell>knife</cell><cell>1.4</cell><cell>5.0</cell><cell>19.9</cell><cell>9.0</cell></row><row><cell>motorcycle</cell><cell>52.1</cell><cell>54.2</cell><cell>72.4</cell><cell>72.9</cell><cell>spoon</cell><cell>0.6</cell><cell>0.5</cell><cell>5.5</cell><cell>1.1</cell></row><row><cell>airplane</cell><cell>36.6</cell><cell>45.2</cell><cell>73.4</cell><cell>76.5</cell><cell>bowl</cell><cell>12.5</cell><cell>18.8</cell><cell>26.8</cell><cell>11.3</cell></row><row><cell>bus</cell><cell>37.7</cell><cell>38.7</cell><cell>71.2</cell><cell>72.5</cell><cell>banana</cell><cell>43.6</cell><cell>46.4</cell><cell>66.4</cell><cell>67.0</cell></row><row><cell>train</cell><cell>30.1</cell><cell>33.2</cell><cell>44.8</cell><cell>47.4</cell><cell>apple</cell><cell>23.6</cell><cell>24.3</cell><cell>43.0</cell><cell>49.2</cell></row><row><cell>truck</cell><cell>24.1</cell><cell>25.9</cell><cell>46.5</cell><cell>46.5</cell><cell>sandwich</cell><cell>22.8</cell><cell>24.5</cell><cell>39.7</cell><cell>33.7</cell></row><row><cell>boat</cell><cell>17.3</cell><cell>20.6</cell><cell>32.1</cell><cell>44.1</cell><cell>orange</cell><cell>44.3</cell><cell>41.2</cell><cell>59.8</cell><cell>62.3</cell></row><row><cell>traffic light</cell><cell>16.7</cell><cell>16.1</cell><cell>23.6</cell><cell>60.8</cell><cell>broccoli</cell><cell>36.8</cell><cell>35.7</cell><cell>46.5</cell><cell>50.4</cell></row><row><cell>fire hydrant</cell><cell>55.9</cell><cell>60.4</cell><cell>79.0</cell><cell>80.3</cell><cell>carrot</cell><cell>6.7</cell><cell>15.3</cell><cell>35.1</cell><cell>35.0</cell></row><row><cell>stop sign</cell><cell>48.4</cell><cell>51.0</cell><cell>79.0</cell><cell>84.1</cell><cell>hot dog</cell><cell>31.2</cell><cell>24.9</cell><cell>49.0</cell><cell>48.3</cell></row><row><cell>parking me-</cell><cell>25.2</cell><cell>26.3</cell><cell>72.2</cell><cell>77.8</cell><cell>pizza</cell><cell>50.9</cell><cell>56.2</cell><cell>69.9</cell><cell>68.6</cell></row><row><cell>ter</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bench</cell><cell>16.4</cell><cell>22.3</cell><cell>40.3</cell><cell>41.2</cell><cell>donut</cell><cell>32.8</cell><cell>34.2</cell><cell>62.6</cell><cell>62.3</cell></row><row><cell>bird</cell><cell>34.7</cell><cell>41.5</cell><cell>65.2</cell><cell>62.6</cell><cell>cake</cell><cell>12.0</cell><cell>6.9</cell><cell>50.7</cell><cell>48.3</cell></row><row><cell>cat</cell><cell>57.2</cell><cell>62.2</cell><cell>79.2</cell><cell>79.2</cell><cell>chair</cell><cell>7.8</cell><cell>9.7</cell><cell>26.9</cell><cell>28.9</cell></row><row><cell>dog</cell><cell>45.2</cell><cell>55.6</cell><cell>73.4</cell><cell>73.3</cell><cell>couch</cell><cell>5.6</cell><cell>17.7</cell><cell>47.0</cell><cell>44.9</cell></row><row><cell>horse</cell><cell>34.4</cell><cell>42.3</cell><cell>74.4</cell><cell>76.1</cell><cell>potted plant</cell><cell>6.2</cell><cell>14.3</cell><cell>20.3</cell><cell>16.9</cell></row><row><cell>sheep</cell><cell>40.3</cell><cell>47.1</cell><cell>76.4</cell><cell>80.0</cell><cell>bed</cell><cell>23.4</cell><cell>32.4</cell><cell>54.8</cell><cell>53.6</cell></row><row><cell>cow</cell><cell>41.4</cell><cell>49.3</cell><cell>78.4</cell><cell>79.3</cell><cell>dining table</cell><cell>0.0</cell><cell>3.8</cell><cell>31.4</cell><cell>24.6</cell></row><row><cell>elephant</cell><cell>62.9</cell><cell>67.1</cell><cell>84.7</cell><cell>85.6</cell><cell>toilet</cell><cell>38.5</cell><cell>43.6</cell><cell>71.1</cell><cell>71.1</cell></row><row><cell>bear</cell><cell>59.1</cell><cell>62.6</cell><cell>84.9</cell><cell>82.9</cell><cell>tv</cell><cell>19.2</cell><cell>25.3</cell><cell>49.5</cell><cell>49.9</cell></row><row><cell>zebra</cell><cell>59.8</cell><cell>63.2</cell><cell>85.2</cell><cell>87.0</cell><cell>laptop</cell><cell>20.1</cell><cell>21.1</cell><cell>57.0</cell><cell>56.6</cell></row><row><cell>giraffe</cell><cell>48.8</cell><cell>54.3</cell><cell>79.8</cell><cell>82.2</cell><cell>mouse</cell><cell>3.5</cell><cell>0.9</cell><cell>7.9</cell><cell>17.4</cell></row><row><cell>backpack</cell><cell>0.3</cell><cell>0.2</cell><cell>21.6</cell><cell>9.4</cell><cell>remote</cell><cell>17.5</cell><cell>20.6</cell><cell>50.3</cell><cell>54.8</cell></row><row><cell>umbrella</cell><cell>26.0</cell><cell>35.3</cell><cell>71.1</cell><cell>73.4</cell><cell>keyboard</cell><cell>12.5</cell><cell>12.3</cell><cell>51.1</cell><cell>48.8</cell></row><row><cell>handbag</cell><cell>0.5</cell><cell>0.7</cell><cell>9.3</cell><cell>4.6</cell><cell>cell phone</cell><cell>32.1</cell><cell>33.0</cell><cell>61.2</cell><cell>60.8</cell></row><row><cell>tie</cell><cell>6.5</cell><cell>7.0</cell><cell>18.0</cell><cell>17.2</cell><cell>microwave</cell><cell>8.2</cell><cell>11.2</cell><cell>47.7</cell><cell>43.6</cell></row><row><cell>suitcase</cell><cell>16.7</cell><cell>23.4</cell><cell>54.6</cell><cell>53.9</cell><cell>oven</cell><cell>13.7</cell><cell>12.4</cell><cell>42.2</cell><cell>38.0</cell></row><row><cell>frisbee</cell><cell>12.3</cell><cell>13.0</cell><cell>55.1</cell><cell>57.7</cell><cell>toaster</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell></row><row><cell>skis</cell><cell>1.6</cell><cell>1.5</cell><cell>9.0</cell><cell>8.2</cell><cell>sink</cell><cell>10.8</cell><cell>17.8</cell><cell>38.8</cell><cell>36.9</cell></row><row><cell>snowboard</cell><cell>5.3</cell><cell>16.3</cell><cell>28.6</cell><cell>24.7</cell><cell>refrigerator</cell><cell>4.0</cell><cell>15.5</cell><cell>59.4</cell><cell>51.8</cell></row><row><cell>sports ball</cell><cell>7.9</cell><cell>9.8</cell><cell>24.5</cell><cell>41.6</cell><cell>book</cell><cell>0.4</cell><cell>12.3</cell><cell>28.6</cell><cell>27.3</cell></row><row><cell>kite</cell><cell>9.1</cell><cell>17.4</cell><cell>31.2</cell><cell>62.6</cell><cell>clock</cell><cell>17.8</cell><cell>20.7</cell><cell>27.8</cell><cell>23.3</cell></row><row><cell>baseball bat</cell><cell>1.0</cell><cell>4.8</cell><cell>1.4</cell><cell>1.5</cell><cell>vase</cell><cell>18.4</cell><cell>23.9</cell><cell>27.1</cell><cell>26.0</cell></row><row><cell>baseball</cell><cell>0.6</cell><cell>1.2</cell><cell>1.1</cell><cell>0.4</cell><cell>scissors</cell><cell>16.5</cell><cell>17.3</cell><cell>46.6</cell><cell>47.1</cell></row><row><cell>glove</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>skateboard</cell><cell>7.1</cell><cell>14.4</cell><cell>30.4</cell><cell>34.8</cell><cell>teddy bear</cell><cell>47.0</cell><cell>46.3</cell><cell>69.2</cell><cell>68.8</cell></row><row><cell>surfboard</cell><cell>7.7</cell><cell>13.5</cell><cell>11.1</cell><cell>17.0</cell><cell>hair drier</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>tennis</cell><cell>9.1</cell><cell>6.8</cell><cell>14.8</cell><cell>9.0</cell><cell>toothbrush</cell><cell>2.8</cell><cell>2.0</cell><cell>23.3</cell><cell>19.7</cell></row><row><cell>racket</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bottle</cell><cell>13.2</cell><cell>22.3</cell><cell>41.3</cell><cell>38.1</cell><cell>mIoU</cell><cell>22.4</cell><cell>26.0</cell><cell>45.8</cell><cell>46.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anti-adversarially manipulated attributions for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4071" to="4080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Regional semantic contrast and aggregation for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09653</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-supervised image-specific prototype exploration for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02909</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">L2g: A simple local-to-global knowledge transfer framework for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03206</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly-and semisupervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining bottom-up, top-down, and smoothness cues for weakly supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3529" to="3538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12765" to="12772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4253" to="4262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unveiling the potential of structure preserving for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingguo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11642" to="11651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno>ArXiv preprint:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonhyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><forename type="middle">Yong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08796</idno>
		<title level="m">Saliency grafting: Innocuous attribution-guided mixup with calibrated label mixing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context decoupling augmentation for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7004" to="7014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Classmix: Segmentationbased data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12275" to="12284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Complementary patch for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<idno>ArXiv preprint:2108.03852</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reducing information bottleneck for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Embedded discriminative attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16765" to="16774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Clims: Cross language image matching for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianxu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4483" to="4492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation by pixel-to-prototype contrast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4320" to="4329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Threshold matters in wsss: Manipulating the activation for the robust and accurate segmentation model against thresholds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongseob</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4330" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Puzzle-CAM: Improved localization via matching partial and full features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In-Jae</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv preprint:2101.11253</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="347" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-class token transformer for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4310" to="4319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">C2am: Contrastive learning of class-agnostic activation map for weakly supervised object localization and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianxu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="989" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive early-learning correction for segmentation from noisy annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2606" to="2616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards noiseless object contours for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16856" to="16865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning affinity from attention: End-to-end weakly-supervised semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixiang</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02664</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Leveraging auxiliary tasks with affinity learning for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<idno>ArXiv preprint:2107.11787</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16897" to="16906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pixel-adaptive convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11166" to="11175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sequential operations in digital picture processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azriel</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">L</forename><surname>Pfaltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="494" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Puzzle-cam: Improved localization via matching partial and full features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In-Jae</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP42928.2021.9506058</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="639" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Non-salient region object mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Sen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2623" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Discriminative region suppression for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1754" to="1761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Railroad is not a train: Saliency as pseudo-pixel supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5495" to="5505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unlocking the potential of ordinary classifier: Class-specific adversarial erasing framework for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeokjun</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Hoon</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehee</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6994" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Class re-activation maps for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="969" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Activation modulation and recalibration scheme for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08996</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Uncertainty estimation via response scaling for pseudo-mask noise mitigation in weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07431</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
