<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FireCaffe: near-linear acceleration of deep neural network training on compute clusters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
							<email>forresti@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
							<email>ashrafkhalid@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
							<email>moskewcz@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">Keutzer</forename><surname>Deepscale</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<title level="a" type="main">FireCaffe: near-linear acceleration of deep neural network training on compute clusters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long training times for high-accuracy deep neural networks (DNNs) impede research into new DNN architectures and slow the development of high-accuracy DNNs. In this paper we present FireCaffe, which successfully scales deep neural network training across a cluster of GPUs. We also present a number of best practices to aid in comparing advancements in methods for scaling and accelerating the training of deep neural networks. The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule. Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train. Our approach has three key pillars. First, we select network hardware that achieves high bandwidth between GPU servers -Infiniband or Cray interconnects are ideal for this. Second, we consider a number of communication algorithms, and we find that reduction trees are more efficient and scalable than the traditional parameter server approach. Third, we optionally increase the batch size to reduce the total quantity of communication during DNN training, and we identify hyperparameters that allow us to reproduce the small-batch accuracy while training with large batch sizes. When training GoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup, respectively, when training on a cluster of 128 GPUs. *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and Motivation</head><p>Since the publication of AlexNet <ref type="bibr" target="#b28">[29]</ref>, a variety of new deep neural network (DNN) architectures such as GoogleNet <ref type="bibr" target="#b40">[41]</ref>, Network-in-Network <ref type="bibr" target="#b31">[32]</ref>, and VGG <ref type="bibr" target="#b38">[39]</ref> have been developed at a rapid pace. This is natural, because with the training and testing dataset fixed (e.g. ImageNet-1k <ref type="bibr" target="#b12">[13]</ref>), it is the DNN architecture that is primarily responsible for improvements in accuracy. In other words, the race for improvements in accuracy in image classification and other contemporary problems of computer science, has become a race in the development of new DNN architectures. So, what is the bottleneck in the development of new architectures?</p><p>In the development of new DNN architectures, as in any human research endeavor, creativity is a key element. However, the impact of architectural variations in DNNs -such as number of layers, filter dimensions, and so forth -can be hard to predict, and experimentation is required to assess their impact. A high-accuracy deep neural network (DNN) model such as GoogLeNet <ref type="bibr" target="#b40">[41]</ref> can take weeks to train on a modern GPU. This is true even when leveraging deep neural network primitives like cuDNN <ref type="bibr">[7]</ref>, maxDNN <ref type="bibr" target="#b29">[30]</ref>, or fbfft <ref type="bibr" target="#b42">[43]</ref> -all of which operate near the theoretical peak computation per second achievable on GPUs. Thus, training time is a key challenge at the root of the development of new DNN architectures. This sentiment was voiced by Jeffrey Dean of Google in his recent keynote address <ref type="bibr" target="#b9">[10]</ref>.</p><p>The four key points that Dean makes are:</p><p>? We [i.e. DNN researchers and users] want results of experiments quickly</p><p>? There is a "patience threshold:" No one wants to wait more than a few days or a week for a result</p><p>? This significantly affects scale of problems that can be tackled</p><p>? We sometimes optimize for experiment turnaround time, rather than absolute minimal system resources for performing the experiment Given the considerable resources available to Google researchers, Dean's comments indicate that simply throwing more computational resources at the problem is not sufficient to solve the DNN training problem. In the following, we will spend a little more time dimensionalizing the current problems with DNN training and the upside potential if these problems can be solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Accelerating DNN Research and Development</head><p>As a particular example of where long training times are limiting the pace of DNN research and productization, consider the following. ImageNet-1k has 1.2 million training images, distributed across 1000 different category labels. From firsthand conversations with engineers and executives, we know that several internet companies have internal databases containing billions of images with hundreds of thousands of different category labels. Due to long training times, these companies are facing serious delays in bringing DNN-based solutions to market. Accelerated DNN training solutions would solve a major pain point for these companies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Real-Time DNN Training</head><p>So far, we have argued how accelerating DNN training would benefit applications where DNNs are in use today. Now, we consider ways in which accelerating DNN training would allow DNN-based techniques to be applied in entirely new ways. There are a number of situations where it is crucial to incorporate new data into a DNN model in real time. For example, reinforcement learning (RL) enables robots to learn things themselves with minimal supervision. A recent study by Levine et al. applied state-of-the-art DNN-based RL techniques to enable a robot to teach itself how to build lego structures and screw on bottle caps <ref type="bibr" target="#b30">[31]</ref>. This technique is effective, and the robot does indeed learn to screw on bottle caps. However, it takes 3-4 hours for the robot to learn to screw on bottle caps, and the majority of this time is spent on DNN training. Faster DNN training would enable this and other reinforcement learning applications to move toward real-time.</p><p>Deep Neural Networks are used for an ever-broadening variety of problems, including classifying <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b23">24]</ref> and detecting <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b16">17]</ref> objects in images, writing sentences about images <ref type="bibr" target="#b14">[15]</ref>, identifying actions in videos <ref type="bibr">[4]</ref>, performing speech recognition <ref type="bibr" target="#b20">[21]</ref>, and gaining semantic understanding of text <ref type="bibr" target="#b32">[33]</ref>. We anticipate that sophisticated reinforcement learning (RL) systems in robotics will eventually leverage all of these modalities, ideally in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Accelerating DNN Training with FireCaffe</head><p>In our work, we focus directly on the problem of DNN training. Since single-GPU efficiency has reached the hard limits of the hardware, the next frontier for accelerating DNN training is to scale it across a compute cluster. In this paper, we present FireCaffe, which scales DNN training across a cluster of 128 GPUs with speedups of more than 40x compared to a single GPU. Our strategy for scaling up DNN training is to focus on reducing communication overhead, and we make a number of design choices toward this goal. For example, we use fast interconnects such as Infiniband or Cray Gemini to accelerate communication among the GPUs. We also show that reduction trees are a faster method for communication than using parameter servers. We map our parallelization strategy to high-accuracy DNN architectures that require less communication.</p><p>The rest of the paper is organized as follows. In Section 2, we describe our choice of hardware for evaluating scalable DNN training, and Section 3 introduces key factors that we will use for analyzing communication among GPU workers. We describe tradeoffs between DNN parallelism strategies in Section 4, and Section 5 explains why certain high-accuracy DNN architectures are particularly amenable to parallelism. In Section 6, we describe our approach to efficiently implementing distributed DNN training. In Section 7, we describe good practices that facilitate the comparison of scalable DNN training techniques and we present our speedups for training the NiN and GoogLeNet architectures on ImageNet. Section 8 describes approaches that are complimentary to FireCaffe for further accelerating DNN training. Finally, we conclude in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hardware for scalable DNN training</head><p>It is both useful and possible to experiment with the scalability of DNN computations using theoretical or scale models. However, demonstration and verification of the correctness and real-world scalability of the proposed FireCaffe system requires using concrete hardware platforms. The speed at which data can be sent between nodes is a key consideration in selecting a hardware platform for scalable DNN training. This is because, the faster the interconnect between nodes is, the more scale we can achieve without being dominated by communication overhead. Hardware manufacturers such as Cray and Mellanox address this by developing high-bandwidth, low-latency interconnects that are substantially faster than typical Ethernet connections.</p><p>For example, the Titan supercomputer at Oak Ridge Leadership Computing Facility (OLCF) has a high bandwidth, low latency Cray Gemini interconnect for communication among servers. The Titan supercomputer has a total of 18000 servers, with one NVIDIA Kepler-based K20x GPU per server <ref type="bibr">[2,</ref><ref type="bibr" target="#b34">35]</ref>. With this in mind, we choose the OLCF Titan supercomputer for tuning and evaluating FireCaffe.</p><p>In this research, we use relatively small slices of the overall capacity of Titan for each training run. The additional computational capacity (?27 PetaFLOPS/s in total) enables us to conduct multiple training runs concurrently, where each training run utilizes 32 to 128 GPUs. When considering 32-node slices of Titan, we found that the interconnect speed (at least for the applications of this work) is similar to that provided by having all nodes in the slice connected to a single Infiniband-class switch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries and terminology</head><p>Deep neural network training is comprised of iterating between two phases: forward and backward propagation. In the forward phase, a batch of data items (e.g. images) is taken from the training set, and the DNN attempts to classify them. Then comes the backward phase, which consists of computing gradients with respect to the weights (?W ) and gradients with respect to the data (?D). The weight gradients are used to update the model's weights. Then, an other forward phase is performed, and so on. We train models using batched stochastic gradient descent (SGD), which is the standard choice for popular DNN models such as GoogLeNet <ref type="bibr" target="#b40">[41]</ref>.</p><p>We now present a few preliminaries, which we will use later in the paper for reasoning about data volume to communicate in distributed DNN training. In Equation 1, we show how to calculate the total size (in bytes) of the weights in all convolutional and fully-connected layers, combined.</p><formula xml:id="formula_0">|W | = #layers L=1 ch L * numF ilt L * f ilterW L * f ilterH L * 4<label>(1)</label></formula><p>where ch is the number of channels, numF ilt is the number of filters, f ilterH is the filter height, and f ilterW is the filter width. Next, Equation 2 expresses the size of activations produced by all layers, combined.</p><formula xml:id="formula_1">|D| = #layers L=1 ch L * numF ilt L * activationW L * activationH L * batch * 4<label>(2)</label></formula><p>where activationH is the activation map height, activationW is the activation width, and batch is the batch size. Note the * 4 in Equations 1 and 2 -this is because a floating-point number is 4 bytes.</p><p>To minimize confusion, we now define some terminology. In our terminology, the each following sets of words are synonyms: (weights = parameters = filters = W ); (nodes = workers = GPU servers). We also sometimes use the terms "activations" and "data" (D) interchangeably. Fully-connected layers are a special case of convolutional layers where f ilterH = activationH and f ilterW = activationW . We define an "epoch" as one pass through the training data. Finally, the word "performance" can be ambiguous, so we write in terms of specific metrics such as "accuracy" and "training time."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Parallelism strategies</head><p>There are two commonly-used methods for parallelizing neural network training across multiple servers: model parallelism (e.g. <ref type="bibr" target="#b44">[45]</ref>) and data parallelism (e.g. <ref type="bibr" target="#b50">[51]</ref>).</p><p>For batched SGD training of DNNs, we define data parallelism as the case where each worker (e.g. GPU) gets a subset of the batch, and then the workers communicate by exchanging weight gradient updates ?W . We define model parallelism as the case where each worker gets a subset of the model parameters, and the workers communicate by exchanging data gradients ?D and exchanging activations D. Note that |W | = |?W | and |D| = |?D|; in other words, the weights and weight gradients are the same size; and the data and data gradients are the same size. Now, to maximize DNN training scalability, our goal is to select a parallelism strategy that requires the lowest possible quantity of communication between servers. The choice of whether it is ideal to use data parallelism, model parallelism, or both depends strongly on the DNN's architectural characteristics. Commonly-used DNN architectures for speech recognition (e.g. <ref type="bibr" target="#b37">[38]</ref>) consist primarily of fully-connected layers, where the activations and parameters have the same spatial resolution (typically 1x1). For typical batch sizes, these fully-connected models often have a similar quantity of weights W and activations D. For example, we observe in <ref type="table" target="#tab_1">Table 1</ref> that this property holds true for the MSFT-Speech DNN architecture <ref type="bibr" target="#b37">[38]</ref>.</p><p>In computer vision, some of the most popular and accurate DNN models (e.g. GoogLeNet <ref type="bibr" target="#b40">[41]</ref>) consist primarily of convolution layers, where the spatial resolution of the filters is smaller than the resolution of the activations. For these convolutional models, data parallelism is typically preferable because it requires less communication -that is, |?W | is much smaller than |?D| at typical batch sizes. Notice that the computer vision DNNs in <ref type="table" target="#tab_1">Table 1</ref>   we enable data parallelism across a cluster of GPUs, and we find that it produces ample speedups for training popular deep convolutional neural network architectures. We illustrate our data parallel approach in <ref type="figure" target="#fig_0">Figure 1</ref>. In this configuration, all the GPUs contain the full DNN model parameters. Each worker (GPU) gets a subset of each batch. The GPUs compute their share of the weight gradients. Once the gradients are calculated locally, they are added together using either a parameter server or a reduction tree communication (described in Section 6.2).  <ref type="bibr" target="#b31">[32]</ref> computer vision 5800MB 30MB 195 6.7TF AlexNet <ref type="bibr" target="#b27">[28]</ref> computer vision 1680MB 249MB 10.2 7.0TF GoogLeNet <ref type="bibr" target="#b40">[41]</ref> computer vision 19100MB 54MB 358 9.7TF VGG-19 <ref type="bibr" target="#b38">[39]</ref> computer vision 42700MB 575MB 71.7 120TF MSFT-Speech <ref type="bibr" target="#b37">[38]</ref> speech recognition 74MB 151MB 0.49 0.00015TF</p><formula xml:id="formula_2">?W i i=0 31 ? conv1 conv2 conv3 softmax D(992:1023) GPU 31 D W D W D W maxpool maxpool avgpool ?W i i=992 1023 ? ?W i i=0 31 ? ?W i i=0 31 ? ?W i i=992 1023 ? ?W i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Choosing DNN architectures to accelerate</head><p>Of the popular deep convolutional neural network architectures for computer vision, some are more amenable to data parallel training than others. One might na?vely assume that DNN models with more parameters would produce higher classification accuracy. To evaluate this assumption, consider <ref type="figure">Figure 2</ref>, where we plot the total size of all parameters in bytes versus top-5 ImageNet accuracy for several popular DNN architectures. Observe that Nerwork-in-Network (NiN) <ref type="bibr" target="#b31">[32]</ref> and AlexNet <ref type="bibr" target="#b28">[29]</ref> have similar accuracy, while NiN has 8x fewer parameters than AlexNet. Likewise, GoogLeNet <ref type="bibr" target="#b40">[41]</ref> and VGG <ref type="bibr" target="#b38">[39]</ref> have similar accuracy, yet GoogLeNet has 10x fewer parameters. In data parallel training, |?W | is the quantity of data sent by each GPU worker, so DNN architectures with fewer parameters require less communication and are more amenable to training at large scale.</p><p>You may wonder, what are the architectural choices that led to NiN and GoogLeNet having 8-10x fewer parameters than AlexNet and VGG? The answer is twofold. First, GoogLeNet and NiN are more judicious in their use of filters with spatial resolution: many of the filters in GoogLeNet and NiN have a resolution of 1x1 instead of 3x3 or larger. Second, while VGG and AlexNet each have more than 150MB of fully-connected layer parameters, GoogLeNet has smaller fully-connected layers, and NiN does not have fully-connected layers.</p><p>In summary, models with fewer parameters are more amenable to scalability in data parallel training, while still delivering high accuracy. Therefore, for the rest of the paper, we focus our efforts on accelerating the training of models with fewer parameters (e.g. NiN and GoogLeNet) while maintaining high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Implementing efficient data parallel training</head><p>Our data-parallel distributed training strategy requires no communication among GPU workers in the forward pass. In the backward pass, a traditional single-GPU implementation (e.g. single-GPU Caffe <ref type="bibr" target="#b26">[27]</ref>) sums the weight gradients over all images in the batch and then uses the weight gradient sum to update the model. <ref type="bibr">1</ref> When we distribute the backward pass over a compute cluster, each GPU worker computes a sum of the weight gradients ( ?W ) for its subset of the batch. Then, we sum the weight gradients across GPUs. This gradient aggregation scheme produces identical numerical results as you would find on a single GPU. Now, our task is to find an efficient way to sum up ?W among GPUs in a compute cluster. We consider two strategies for implementing this gradient aggregation: parameter servers, and reduction trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Parameter server</head><p>One strategy for communicating gradients is to appoint one node as a parameter server. The remaining worker nodes are each assigned a subset of the batch on which to perform forward and backward-propagation. After each backward pass, all the workers send their gradient updates to the parameter server. Then, the parameter server computes the sum of the gradients. Finally, the parameter server sends the summed gradients to the workers, and the workers apply these gradient updates to their local copies of the model. We illustrate the parameter server communication pattern in <ref type="figure" target="#fig_2">Figure 3</ref>(a).</p><p>The logical question here is, what is the communication overhead of a parameter server, and how does that overhead scale as we increase the number of GPU workers? Recall from Section 4 that each GPU worker provides |W | = |?W | bytes of weight gradients (Equation 1), which need to be summed with gradients from all other GPU workers. Now, the bottleneck is is in sending and receiving all the gradients on one parameter server. If there are p GPU workers, the parameter server is responsible for sending and receiving |?W | * p bytes of data. If each node (GPU worker or parameter server) can send and receive data at a rate of BW bytes/s, then we can calculate the minimum communication time as follows:</p><formula xml:id="formula_3">param server communication time = |?W | * p BW (sec)<label>(3)</label></formula><p>In other words, the parameter server's communication time scales linearly as we increase the number of GPU workers; doubling the number of workers leads to at least 2x more communication time per gradient update. We confirm this experimentally in <ref type="figure">Figure 4</ref>.</p><p>For the parameter server experiments in <ref type="figure">Figure 4</ref>, we have implemented a fully synchronous parameter server with the following characteristics. The parameter server is one arbitrarily-selected server in the cluster, while the other servers are workers; the parameter server and worker servers have identical hardware. After each batch, the workers send their weight height: 1  gradients to the parameter server, the parameter server computes the sum, and then the parameter server sends the sum back to the workers.</p><p>There are a number of ways to augment the parameter server for greater scalability. For example, when having a single parameter server became a bottleneck, Microsoft Adam <ref type="bibr" target="#b7">[8]</ref> and Google DistBelief <ref type="bibr" target="#b11">[12]</ref> each defined a pool of nodes that collectively behave as a parameter server. The bigger the parameter server hierarchy gets, the more it looks like a reduction tree. This made us wonder: could we achieve greater scalability if we implement gradient aggregation as a reduction tree?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Reduction tree</head><p>There are various common patterns of communication in parallel programs; among such common patterns, a frequently occurring one is allreduce. This pattern occurs when each worker produces one or more data values that must be globally reduced (generally with a commutative binary element-wise operator) to produce a single result value, and then this single value must be broadcast to all workers before they can continue. In this work, each worker produces a single vector of length |?W | (the gradient updates for that worker), which must be reduced using element-wise vector addition (to sum the perworker gradient updates for each parameter). Since this computation exactly fits the allreduce communication pattern it is convenient to use existing library support for such operations. While there are many possible implementations of allreduce, most share the key property that the time taken to perform the operation scales as the log of the number of workers (at least for large numbers of workers). Intuitively, this is because allreduce algorithms use binomial reduction tree and/or butterfly communication patterns internally <ref type="bibr" target="#b41">[42]</ref>. Out of the possible allreduce implementation strategies, we find that the binomial reduction tree is particularly easy to reason about on a theoretical level. So, for the rest of this section, we focus on allreduce communication implemented with a reduction tree.</p><p>In Figures 3(a) and 3(b), we present the intuition on how parameter servers and reduction trees differ. We might think of a parameter server as a reduction tree with a height of 1 and a branching factor of p. However, many cluster computers and supercomputers have several dimensions of network fabric among nodes (e.g. an N-D Torus), which enable nodes to talk to each other via many different paths. With this in mind, we can sum gradients using a taller reduction tree, where nodes collaboratively sum the gradients. For example, consider a binary communication tree with a branching factor of 2 and a depth of log 2 (p). In this case, the serialized communication is 2log 2 (p); the outer 2 represents the fact that each node receives data from 2 children, and the log 2 (p) is the height of the tree. Therefore, unlike the parameter server model, the reduction tree's communication time is:</p><formula xml:id="formula_4">reduction tree communication time = |?W | * 2log2(p) BW (sec)<label>(4)</label></formula><p>In practice, the base of log(p) depends on the branching factor in the reduction tree, but the basic idea here is straightforward: While the parameter server communication overhead scales linearly with p, reduction tree communication is much more efficient because it scales logarithmically as O(log(p)). We confirm that reduction trees scale more efficiently than parameter servers in <ref type="figure">Figure 4</ref>.</p><p>Measuring communication only (if computation were free) <ref type="figure">Figure 4</ref>. Comparing communication overhead with a parameter server vs. a reduction tree. This is for the Network-in-Network DNN architecture, so each GPU worker contributes 30MB of gradient updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Evaluation of FireCaffe-accelerated training on ImageNet</head><p>In this section, we evaluate how FireCaffe can accelerate DNN training on a cluster of GPUs. We train GoogLeNet <ref type="bibr" target="#b40">[41]</ref> and Network-in-Network <ref type="bibr" target="#b31">[32]</ref> on up to 128 GPU servers in the Titan supercomputer (described in Section 2), leveraging FireCaffe's reduction tree data parallelism (Section 6.2). We begin by describing our evaluation methodology, and then we analyze the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Evaluation Methodology</head><p>We now describe a few practices that aid in comparing advancements in accelerating the training of deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Evaluate the speed and accuracy of DNN training on a publicly-available dataset.</head><p>In a recent study, Azizpour et al. applied DNNs to more than 10 different visual recognition challenge datasets, including human attribute prediction, fine-grained flower classification, and indoor scene recognition <ref type="bibr">[5]</ref>. The accuracy obtained by Azizpour et al. ranged from 56% on scene recognition to 91% on human attribute prediction. As you can see, the accuracy of DNNs and other machine learning algorithms depends highly on the specifics of the application and dataset to which they are applied. Thus, when researchers report improvements in training speed or accuracy on proprietary datasets, there is no clear way to compare the improvements with the related literature. For example, Baidu <ref type="bibr" target="#b45">[46]</ref> and Amazon <ref type="bibr" target="#b39">[40]</ref> recently presented results on accelerating DNN training. Amazon and Baidu 2 each reported their training time numbers on a proprietary dataset, so it's not clear how to compare these results with the related literature. In contrast, we conduct our evaluation on a publicly-available dataset, ImageNet-1k <ref type="bibr" target="#b12">[13]</ref>, which contains more than 1 million training images, and each image is labeled as containing 1 of 1000 object categories. ImageNet-1k is a widely-studied dataset, so we can easily compare our accuracy, training speed, and scalability results with other studies that use this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Report hyperparameter settings such as weight initialization, momentum, batch size, and learning rate.</head><p>Glorot et al. <ref type="bibr" target="#b17">[18]</ref>, Breuel <ref type="bibr">[6]</ref>, and Xu et al. <ref type="bibr" target="#b47">[48]</ref> have each shown that seemingly-subtle hyperparameter settings such as weight initialization can have a big impact on the speed and accuracy produced in DNN training. When training Networkin-Network (NiN) <ref type="bibr" target="#b31">[32]</ref>, we initialize the weights with a gaussian distribution centered at 0, and we set the standard deviation (std) to 0.01 for 1x1 convolution layers, and we use std=0.05 for other layers. For NiN, we initialize the bias terms to a constant value of 0, we set the weight decay to 0.0005, and we set momentum to 0.9. These settings are consistent with the Caffe configuration files released by the NiN authors <ref type="bibr" target="#b31">[32]</ref>.</p><p>Frustratingly, in Google's technical reports on GoogLeNet <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25]</ref>, training details such as batch size, momentum, and learning rate are not disclosed. Fortunately, Wu et al. <ref type="bibr" target="#b46">[47]</ref> and Guadarrama <ref type="bibr" target="#b18">[19]</ref> each reproduced GoogLeNet and released all the details of their training protocols. As in <ref type="bibr" target="#b18">[19]</ref>, we train GoogLeNet with momentum=0.9 and weight decay=0.0002, we use xavier <ref type="bibr" target="#b17">[18]</ref> weight initialization, and we initialize the bias terms to a constant value of 0.2. We will address learning rate and batch size settings in the following sections.</p><p>Given a DNN architecture, there are a number of strategies that can further increase accuracy, albeit at a substantial computational cost. One such strategy is to train multiple independent copies of a DNN architecture (e.g. GoogLeNet), each with a different random number generator seed for initializing the parameters. At test time, these DNNs can be used as an ensemble -that is, all DNNs are run on the test data, and for each test data item, the DNN's classification activations are averaged. For example, using an ensemble of 7 GoogLeNet DNNs, Szegedy et al. achieved a 2 percentage-point accuracy improvement on ImageNet, compared to a single GoogLeNet baseline <ref type="bibr" target="#b40">[41]</ref>. An other such technique is to augment the data by adding deformations or color variations during training and/or testing <ref type="bibr" target="#b45">[46]</ref>. Our focus in this paper is to show speedup on training single models and compare with reported baselines. Hence we avoid using exotic data augmentation or ensembles of multiple DNNs. In our experiments, we resize images to 256x256; at training time we use a 224x224 crop with a randomized offset, and at test time we classify the 224x224 crop in the center of the image; these settings are also commonly used in the AlexNet <ref type="bibr" target="#b28">[29]</ref> and Network-in-Network <ref type="bibr" target="#b31">[32]</ref> DNN architectures.</p><p>3. Measure speedups with respect to a single-server baseline. In order to meaningfully measure how much we have accelerated DNN training by adding more GPUs, we must have a representative baseline, e.g. with a single GPU. When reporting results, we begin by considering time required to train a DNN on single GPU, and we report our multi-GPU speedups with respect to this single-GPU baseline. A recent study by Microsoft <ref type="bibr" target="#b7">[8]</ref> reported training a custom DNN architecture (e.g. not GoogLeNet or NiN) on a cluster of CPU servers. This may sound impressive, but Microsoft did not report the time that the model would take to train on a single server. It could be that Microsoft achieved a 10x speedup by going from 1 server to 10 servers, or the speedup could be 2x -this isn't clear from the information provided in Microsoft's paper. This illustrates the importance of measuring the speed of scalable DNN training systems with respect to a single-server baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Measure accuracy with respect to a single-server baseline.</head><p>In our experience, if hyperparameters such as learning rate and batch size are selected too aggressively, a DNN model may converge quickly, but fall short of the state-of-art accuracy. Therefore, in our experiments, we train multi-GPU models until they reach to the single-GPU accuracy baseline; this validates that we can accelerate DNN training without degrading accuracy. However, in cluster-scale multi-GPU training experiments by Baidu <ref type="bibr" target="#b45">[46]</ref> and Flickr <ref type="bibr" target="#b33">[34]</ref>, the training is stopped prematurely before the DNNs converge. This leaves us wondering whether the Baidu and Flickr multi-GPU training experiments would have reproduced the accuracy produced on a single GPU. To avoid this type of confusion, we evaluate both the speed and accuracy of FireCaffe DNN training with respect to a single-server/single-GPU baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Results: Midsized deep models</head><p>Using the settings described by Krizhevsky <ref type="bibr" target="#b28">[29]</ref>, we find that AlexNet achieves 58.9% top-1 ImageNet-1k accuracy after 100 epochs of training. After just 47 epochs of training, we find that NiN also converges to 58.9% top-1 accuracy. Each training iteration of NiN is more time-consuming than AlexNet, and AlexNet and NiN both take approximately 6 days to converge to this level of accuracy.</p><p>At Google, Krizhevsky developed a scheme for accelerating AlexNet training using multiple GPUs within a single server <ref type="bibr" target="#b27">[28]</ref>. Krizhevsky's strategy uses data parallelism in convolutional layers and model parallelism in fully-connected layers. As we show in <ref type="table" target="#tab_3">Table 2</ref>, Krizhevsky achieves near-linear acceleration up to 8 GPUs, but it has not been shown to scale beyond a single server. For reasons that we don't entirely understand, Krizhevsky's accuracy drops by 1.8 percentage points when doing multi-GPU training <ref type="bibr" target="#b27">[28]</ref>.</p><p>In FireCaffe, we scale NiN training to 32 GPUs, which is the scale at which we find communication time and computation are approximately equal 3 . We begin by using the learning rate and batch size settings that were reported in the Caffe configuration file released by the NiN authors <ref type="bibr" target="#b31">[32]</ref>: For a batch size of 256, we use an initial learning rate of 0.01, and we reduce this by a factor of 10x twice during our training. Using this configuration, we reproduce the single-GPU NiN accuracy in 11 hours (13x speedup) when training on 32 GPUs.</p><p>For a fixed number of epochs, increasing the batch size reduces the number of times we need to communicate weight gradients, thus reducing the overall training time. With this in mind, we now train NiN with a batch size of 1024. <ref type="bibr">4</ref> As in <ref type="bibr" target="#b27">[28]</ref> when we increase the batch size, we increase the learning rate by an equal proportion. For example, when we use a batch size of 1024, we initialize the learning rate to 0.04. In this configuration, we train NiN in just 6 hours (23x speedup) on 32 GPUs. By increasing the batch size to 1024, we achieved a substantial speedup, but this came at the price of reducing the final accuracy by <ref type="bibr">3</ref> 10 of a percentage point. We expect that this 3 10 % of accuracy could be regained at a batch size of 1024while retaining a substantial speed advantage -by training for a few more epochs. Finally, on 128 GPUs, we achieve a 39x speedup over single-GPU training.</p><p>So far, we have compared FireCaffe to the cuda-convnet2 framework from Google <ref type="bibr" target="#b27">[28]</ref>, which runs on a single-server/multi-GPU platform but not in a multi-server distributed platform. In addition to cuda-convnet2, Google has developed the Ten-sorFlow framework <ref type="bibr">[1]</ref>, which also supports single-server/multi-GPU training but not distributed multi-server training. Thus far, Google has not released training speed results for multi-GPU TensorFlow. Twitter <ref type="bibr" target="#b15">[16]</ref> has also experimented with scaling DNN training to 8 GPUs, but speed and accuracy results have not been released. Tencent <ref type="bibr" target="#b50">[51]</ref>, Theano <ref type="bibr" target="#b13">[14]</ref>, and Facebook <ref type="bibr" target="#b48">[49]</ref> have published AlexNet single-server/multi-GPU training times that are slower than Google <ref type="bibr" target="#b27">[28]</ref>. Other than FireCaffe, we have not seen literature on training AlexNet/NiN-scale models in a multi-server/multi-GPU setting. On 32 GPUs, FireCaffe is at least 3x faster to train AlexNet/NiN-scale models than all of the aforementioned results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Results: Ultra-deep models</head><p>Ultra-deep models such as GoogLeNet can produce higher accuracy, but they present an even bigger challenge in terms of training time. Internally, Google has trained GoogLeNet on a cluster of CPU servers, but they have not reported the time required to complete this training <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25]</ref>. Fortunately, Guadarrama reproduced GoogLeNet in Caffe, and he released his GoogLeNet Caffe configuration files <ref type="bibr" target="#b18">[19]</ref>. Guadarrama trained for 64 epochs using a batch size of 32 and an initial learning rate of 0.01, and we use these settings in our single-GPU GoogLeNet training experiments. Instead of occasionally reducing the learning rate by 10x, Guadarrama used a polynomial learning rate -that is, the learning rate is gradually reduced after every iteration of training. More specifically, at a given iteration of training, the learning rate is calculated as initialLearningRate(1 ? iter max iter ) power , and we set power to 0.5 in all of our GoogLeNet training runs. Running this in Caffe on a single-GPU, GoogLeNet takes 21 days to train on ImageNet-1k, producing 68.3% top-1 accuracy and 88.7% top-5 accuracy. This is slightly lower than the 89.9% top-5 single-model accuracy reported by Google <ref type="bibr" target="#b40">[41]</ref>, and it will be interesting to see whether the open-source Caffe community will eventually be able reproduce or surpass Google's GoogLeNet accuracy. Here, we use the single-GPU Caffe GoogLeNet accuracy (88.7% top-5 accuracy) as a baseline, and we aim to reproduce this rapidly on a cluster of GPUs. Now, we consider how to accelerate GoogLeNet training using FireCaffe. We initially tried to run GoogLeNet with a batch size of 32 on a GPU cluster, but there just wasn't enough work per batch to keep a GPU cluster saturated. As we learned earlier in the paper, larger batch sizes lead to less frequent communication and therefore enable more scalability in a distributed setting. When modifying the batch size, Breuel <ref type="bibr">[6]</ref> and Krizhevsky <ref type="bibr" target="#b27">[28]</ref> found that the choice of learning rate is crucial in order to preserve high accuracy. We trained five separate versions of GoogLeNet, each with a different initial learning rate (LR): {0.02, 0.04, 0.08, 0.16, and 0.32}, and all with a batch size of 1024. With LR=0.16 and LR=0.32, GoogLeNet didn't ever learn anything beyond random-chance accuracy on the test set. Using LR=0.02 produced 66.1% top-1 ImageNet-1k accuracy, and LR=0.04 produced 67.2%. Finally, we declare victory with LR=0.08, where we achieved 68.3% accuracy (again, with a batch size of 1024), which matches the accuracy of the baseline that used a batch size of 32. With a batch size of 1024 and a fixed number of epochs, we find that FireCaffe on 32 GPUs can train GoogLeNet 23x faster than a single GPU. When we move from a batch size of 32 with LR=0.01 to a batch size of 1024 with LR=0.08, we find that GoogLeNet takes a few more epochs to converge (72 epochs instead of 64 epochs), so the absolute training speedup is 20x; we show these results in <ref type="table" target="#tab_4">Table 3</ref>. In other words, FireCaffe can train GoogLeNet in 23.4 hours on 32 GPUs, compared to 21 days on a single GPU. Finally, on 128 GPUs, we achieve a 47x speedup over single-GPU GoogLeNet training, while matching the single-GPU accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Complementary approaches to accelerate DNN training</head><p>We have discussed related work throughout the paper, but we now provide a brief survey of additional techniques to accelerate deep neural network training. Several of the following techniques could be used in concert with FireCaffe to further accelerate DNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Accelerating convolution on GPUs</head><p>In the DNN architectures discussed in this paper, more than 90% of the floating-point operations in forward and backward propagation reside in convolution layers, so accelerating convolution is key to getting the most out of each GPU. Recently, a number of techniques have been developed to accelerate convolution on GPUs. Unlike CPUs, NVIDIA GPUs have an inverted memory hierarchy, where the register file is larger than the L1 cache. Volkov and Demmel <ref type="bibr" target="#b43">[44]</ref> pioneered a communication-avoiding strategy to accelerate matrix multiplication on GPUs by staging as much data as possible in registers while maximizing data reuse. Iandola et al. <ref type="bibr" target="#b22">[23]</ref> extended the communication-avoiding techniques to accelerate 2D convolution; and cuDNN <ref type="bibr">[7]</ref> and maxDNN <ref type="bibr" target="#b29">[30]</ref> extended the techniques to accelerate 3D convolution. FireCaffe can be coupled with current and future GPU hardware and convolution libraries for further speedups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Reducing communication among servers</head><p>Reducing the quantity of data communicated per batch is a useful way to increase the speed and scalability of DNN training. There is an inherent tradeoff here: as gradients are more aggressively quantized, training speed goes up, but the model's accuracy may go down compared to a non-quantized baseline. While FireCaffe uses 32-bit floating-point values for weight gradients, Jeffrey Dean stated in a recent keynote speech that Google often uses 16-bit floating-point values for communication between servers in DNN training <ref type="bibr" target="#b10">[11]</ref>. Along the same lines, Wawrzynek et al. used 16-bit weights and 8-bit activations in distributed neural network training <ref type="bibr" target="#b44">[45]</ref>. Going one step further, Seide et al. used 1-bit gradients for backpropagation, albeit with a drop in the accuracy of the trained model <ref type="bibr" target="#b36">[37]</ref>. Finally, a related strategy to reduce communication between servers is to discard (and not communicate) gradients whose numerical values fall below a certain threshold. Amazon presented such a thresholding strategy in a recent paper on scaling up DNN training for speech recognition <ref type="bibr" target="#b39">[40]</ref>. However, Amazon's evaluation uses a proprietary dataset, so it is not clear how this type of thresholding impacts the accuracy compared to a well-understood baseline.</p><p>So far in this section, we have discussed strategies for compressing or quantizing data to communicate in distributed DNN training. There has also been a series of studies on applying dimensionality reduction to DNNs once they have been trained. Jaderberg et al. <ref type="bibr" target="#b25">[26]</ref> and Zhang et al. <ref type="bibr" target="#b49">[50]</ref> both use PCA to compress the weights of DNN models by up to 5x, albeit with a substantial reduction in the model's classification accuracy. Han et al. <ref type="bibr" target="#b19">[20]</ref> use a combination of pruning, quantization, and Huffman encoding to compress the weights of pretrained models by 35x with no reduction in accuracy. Thus far, these algorithms have only been able to accelerate DNNs at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions</head><p>Long training times impose a severe limitation on progress in deep neural network research and productization. Accelerating DNN training has several benefits. First, faster DNN training enables models to be trained on ever-increasing dataset sizes in a tractable amount of time. Accelerating DNN training also enables product teams to bring DNN-based products to market more rapidly. Finally, there are a number of compelling use-cases for real-time DNN training, such as robot selflearning. These and other compelling applications led us to focus on the problem of accelerating DNN training, and our work has culminated in the FireCaffe distributed DNN training system. Our approach to accelerating DNN training at scale has three key pillars. First, we select network hardware that achieves high bandwidth between GPU servers -Infiniband or Cray interconnects are ideal for this. Second, when selecting a communication algorithm, we find that reduction trees are more efficient and scalable than the traditional parameter server approach. Third, we optionally increase the batch size to reduce the total quantity of communication during DNN training, and we identify hyperparameters that allow us to reproduce the small-batch accuracy while training with large batch sizes. These three pillars helped us to achieve a near-linear speedup for a number of leading deep neural network architectures. In particular, we have achieved 39x speedup on NiN training, and a 47x speedup on GoogLeNet training on a 128 GPU cluster.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Data parallel DNN training in FireCaffe: Each worker (GPU) gets a subset of each batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustrating how parameter servers and reduction trees communicate weight gradients. In this figure, we only show the summingup of weight gradients. We distribute the weight gradient sums by going back down the tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>all have this property. In FireCaffe,</figDesc><table><row><cell></cell><cell>GPU 0</cell><cell>W</cell><cell>D</cell><cell>D(0:15)</cell><cell>W</cell><cell>D</cell><cell>D(0:15)</cell><cell>W</cell><cell>D</cell><cell>D(0:15)</cell></row><row><cell></cell><cell></cell><cell cols="2">conv1</cell><cell>maxpool</cell><cell cols="2">conv2</cell><cell>maxpool</cell><cell cols="2">conv3</cell><cell>avgpool</cell><cell>softmax</cell></row><row><cell>sum W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>using a reduction tree</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>update</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>weights</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>DNN architecture</cell><cell>typical use-case</cell><cell>data size |D|</cell><cell>weight size |W |</cell><cell>data/weight ratio</cell><cell>Forward+Backward TFLOPS/batch</cell></row><row><cell>NiN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Volumes of data and computation for four widely-used DNN architectures. The batch size impacts all numbers in this table except for |W |, and we use a batch size of 1024 in this table. Here, TFLOPS is the quantity of computation to perform.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure 2. Deep neural network architectures with more parameters do not necessarily deliver higher accuracy.</figDesc><table><row><cell>GoogLeNet'</cell><cell></cell><cell></cell></row><row><cell>NiN'</cell><cell>AlexNet'</cell><cell>VGG_11' VGG_19'</cell></row><row><cell cols="3">Deep'Neural'Networks'can'achieve'high'</cell></row><row><cell cols="3">accuracy'with'rela8vely'few'parameters'</cell></row><row><cell></cell><cell cols="2">more'scalable'training'</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Accelerating the training of midsized deep models on ImageNet-1k.</figDesc><table><row><cell></cell><cell>Hardware</cell><cell>Net</cell><cell>Epochs</cell><cell>Batch</cell><cell>Initial Learning</cell><cell>Train</cell><cell>Speedup</cell><cell>Top-1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>size</cell><cell>Rate</cell><cell>time</cell><cell></cell><cell>Accuracy</cell></row><row><cell>Caffe [27]</cell><cell>1 NVIDIA K20</cell><cell>AlexNet</cell><cell>100</cell><cell>256</cell><cell>0.01</cell><cell>6.0 days</cell><cell>1x</cell><cell>58.9%</cell></row><row><cell></cell><cell></cell><cell>[29]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Caffe</cell><cell>1 NVIDIA K20</cell><cell>NiN [32]</cell><cell>47</cell><cell>256</cell><cell>0.01</cell><cell>5.8 days</cell><cell>1x</cell><cell>58.9%</cell></row><row><cell>Google cuda-convnet2</cell><cell>8 NVIDIA K20s (1 node)</cell><cell>AlexNet</cell><cell>100</cell><cell>varies</cell><cell>0.02</cell><cell>16 hours</cell><cell>7.7x</cell><cell>57.1%</cell></row><row><cell>[28]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FireCaffe (ours)</cell><cell>32 NVIDIA K20s (Titan</cell><cell>NiN</cell><cell>47</cell><cell>256</cell><cell>0.01</cell><cell>11 hours</cell><cell>13x</cell><cell>58.9%</cell></row><row><cell></cell><cell>supercomputer)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FireCaffe-batch1024</cell><cell>32 NVIDIA K20s (Titan</cell><cell>NiN</cell><cell>47</cell><cell>1024</cell><cell>0.04</cell><cell>6 hours</cell><cell>23x</cell><cell>58.6%</cell></row><row><cell>(ours)</cell><cell>supercomputer)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FireCaffe-batch1024</cell><cell>128 NVIDIA K20s (Titan</cell><cell>NiN</cell><cell>47</cell><cell>1024</cell><cell>0.04</cell><cell>3.6</cell><cell>39x</cell><cell>58.6%</cell></row><row><cell>(ours)</cell><cell>supercomputer)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hours</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Accelerating the training of ultra-deep, computationally intensive models on ImageNet-1k.</figDesc><table><row><cell></cell><cell>Hardware</cell><cell>Net</cell><cell cols="2">Epochs Batch</cell><cell>Initial Learning</cell><cell>Train</cell><cell>Speedup</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>size</cell><cell>Rate</cell><cell>time</cell><cell></cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell>Caffe</cell><cell>1 NVIDIA K20</cell><cell>GoogLeNet</cell><cell>64</cell><cell>32</cell><cell>0.01</cell><cell>21 days</cell><cell>1x</cell><cell>68.3%</cell><cell>88.7%</cell></row><row><cell></cell><cell></cell><cell>[41]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FireCaffe</cell><cell>32 NVIDIA K20s (Titan</cell><cell>GoogLeNet</cell><cell>72</cell><cell>1024</cell><cell>0.08</cell><cell>23.4</cell><cell>20x</cell><cell>68.3%</cell><cell>88.7%</cell></row><row><cell>(ours)</cell><cell>supercomputer)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hours</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FireCaffe</cell><cell>128 NVIDIA K20s (Titan</cell><cell>GoogLeNet</cell><cell>72</cell><cell>1024</cell><cell>0.08</cell><cell>10.5</cell><cell>47x</cell><cell>68.3%</cell><cell>88.7%</cell></row><row><cell>(ours)</cell><cell>supercomputer)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hours</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">However, the data gradients (?D) are not summed up.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Baidu evaluated their training times using proprietary dataset<ref type="bibr" target="#b45">[46]</ref>. Baidu also did some ImageNet experiments, but Baidu did not report the training time on ImageNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">at a batch size of 10244  While keeping a fixed number of epochs. In other words, with a batch size of 1024, we perform 4x fewer training iterations than with a batch size of 256.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research used resources of the Oak Ridge Leadership Facility at the Oak Ridge National Laboratory, which is supported by the Office <ref type="figure">of</ref>  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google Technical Report</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Approaching exascale: application requirements for olcf leadership computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Anantharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Foertter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Joubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wells</surname></persName>
		</author>
		<ptr target="https://www.olcf.ornl.gov/wp-content/uploads/2013/01/OLCF_Requirements_TM_2013_Final1.pdf" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="43" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FireBox: a hardware building block for 2020 warehouse-scale computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio-based multimedia event detection with dnns and sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Deep Vision Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02788</idno>
		<title level="m">The effects of hyperparameters on SGD training of neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">cuDNN: efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Project Adam: building an efficient and scalable deep learning training system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MDB: A memory-mapped database and backend for openldap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LDAPCon</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Keynote: Large scale deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Keynote: Large scale deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU Technology Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2302</idno>
		<title level="m">Theano-based large-scale visual recognition with multiple gpus</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time, content-driven representations at twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU Technology Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bvlc</surname></persName>
		</author>
		<ptr target="https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A deep neural network compression pipeline: Pruning, quantization, huffman encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<title level="m">Densenet: Implementing efficient convnet descriptor pyramids</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Communication-minimizing 2d convolution in gpu registers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheffield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplogo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02131</idno>
		<title level="m">Hitting logo recognition with the deep neural network hammer</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">maxDNN: an efficient convolution kernel for deep learning with maxwell gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.06633</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00702</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
	</analytic>
	<monogr>
		<title level="j">Network in network</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Large scale distributed deep learning on hadoop clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="http://yahoohadoop.tumblr.com/post/129872361846.8" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introducing titan: Advancing the era of accelerated computing</title>
		<idno>2015. 3</idno>
		<ptr target="http://www.olcf.ornl.gov/titan" />
		<imprint/>
	</monogr>
	<note>Oak Ridge Leadership Computing Facility (OLCF)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable distributed dnn training using commodity gpu cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optimization of collective communication operations in mpich</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rabenseifner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7580</idno>
		<title level="m">Fast convolutional nets with fbfft: A gpu performance evaluation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Benchmarking GPUs to tune dense linear algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Volkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spert-ii: A vector microprocessor system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02876</idno>
		<title level="m">Deep image: Scaling up image recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A gpu implementation of googlenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="http://vision.princeton.edu/pvt/GoogLeNet" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5853</idno>
		<title level="m">Multi-gpu training of convnets</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Efficient and accurate approximations of nonlinear convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4229</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mariana: Tencent deep learning platform and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Appendix: Frequently Asked Questions about FireCaffe</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Will my DNN models and network definitions work with FireCaffe? Yes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caffe</forename><surname>I Use</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">What are some good DNN architectures to use with FireCaffe? We offer intuition and analysis of this in Sections 4 and 6, but here we offer a short summary of what you need to know. As a general rule, DNN architectures with fewer parameters train faster in FireCaffe. We have found that NiN [32] and AlexNet [29] provide similar accuracy on ImageNet and other popular data benchmarks. However, NiN has 8x fewer parameters and therefore incurs 8x less communication cost than AlexNet</title>
		<imprint/>
	</monogr>
	<note>If you were planning to use AlexNet, we recommend trying NiN for faster training</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">What design tradeoffs should I consider? If you&apos;re designing your own DNN architectures, do your best to economize on parameters. Also, reducing the convolution and pooling strides may improve accuracy</title>
		<imprint/>
	</monogr>
	<note>I want to design my own DNN architectures that will scale well nicely in FireCaffe while producing good accuracy on my problem. this doesn&apos;t require additional parameters and doesn&apos;t hurt training scalability</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Is it better to use FireCaffe with one server that contains many GPUs, or is it better to distribute the GPUs distributed over many servers? FireCaffe is compatible with both of these scenarios. FireCaffe can even run across many servers that each contain many GPUs. To avoid stragglers during backpropagation, we prefer running FireCaffe on a collection of identical GPUs (e.g. all Titan X or all K80; but preferrably not a mixture of K80 and Titan X). For optimal speed and utilization</title>
		<imprint/>
	</monogr>
	<note>FireCaffe prefers a low-latency interconnect such as Infiniband between servers</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">How does FireCaffe handle the storage and loading of training data? As in Caffe, FireCaffe can ingest LMDB databases [9] of training data. This data format is agnostic to the type of data (e.g. images, audio, text), so long as each training data item consists of a vector (e.g. pixels, audio waveform, text trigram) and a label (e.g. dog or cat). We store the LMDB database of training data on a distributed filesystem that is accessable to all workers. During DNN training</title>
		<imprint/>
	</monogr>
	<note>each worker is responsible for loading its own training data from the distributed filesystem</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Given a specific seed for the random number generator, FireCaffe produces repeatable numerical results, just like ordinary Caffe. In fact, for a model with Dropout disabled, FireCaffe will give you the exact same numerics as ordinary Caffe. With Dropout enabled, FireCaffe handles randomization in a slightly different way than Caffe</title>
	</analytic>
	<monogr>
		<title level="m">Does FireCaffe use nondeterministic techniques such as Hogwild</title>
		<imprint/>
	</monogr>
	<note>but the numerics are still deterministic for FireCaffe with a constant number of GPUs</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">How did FireCaffe get its name? Asanovic and Patterson presented a roadmap for warehouse-scale computing in the year 2020</title>
		<imprint/>
	</monogr>
	<note>which they call FireBox [3</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The grand vision is that a typical large-scale commercial datacenter will have extremely low latency connections within and across racks, likely using photonic networking hardware. Low-latency network hardware is crucial not only for today&apos;s mainstream applications like distributed databases and search engines, but also for emerging applications such as deep neural network training at large scale. We have designed FireCaffe with FireBox-style warehouse-scale computing in mind</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
