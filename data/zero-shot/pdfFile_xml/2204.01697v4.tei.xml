<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaxViT: Multi-Axis Vision Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MaxViT: Multi-Axis Vision Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transformer</term>
					<term>Image classification</term>
					<term>Multi-axis attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of selfattention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to "see" globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. The source code and trained models will be available at https://github.com/google-research/maxvit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (ConvNets) have been the dominant architectural design choice for computer vision <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref> since AlexNet <ref type="bibr" target="#b47">[48]</ref>. Con-vNets continue to excel on numerous vision problems by going deeper <ref type="bibr" target="#b74">[75]</ref>, wider <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b75">76]</ref>, adding dense connections <ref type="bibr" target="#b36">[37]</ref>, efficient separable convolutions <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b69">70]</ref>, atrous convolutions <ref type="bibr" target="#b8">[9]</ref>, using encoder-decoder frameworks <ref type="bibr" target="#b66">[67]</ref>, and even introducing modern micro-design components <ref type="bibr" target="#b56">[57]</ref>. Meanwhile, as inspired by the evolution of self-attention models like Transformers <ref type="bibr" target="#b84">[85]</ref> in natural language processing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b99">100]</ref>, numerous researchers have started to introduce attention arXiv:2204.01697v4 [cs.CV] 9 Sep 2022 0 5 10 15 20 25 30 <ref type="bibr" target="#b34">35</ref>  <ref type="bibr">40 45</ref> FLOPs (G)   mechanisms into vision <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b87">88]</ref>. The Vision Transformer (ViT) <ref type="bibr" target="#b21">[22]</ref> is perhaps the first fully Transformer-based architecture for vision, whereby image patches are simply regarded as sequences of words and a transformer encoder is applied on these visual tokens. When pre-trained on large-scale datasets <ref type="bibr" target="#b72">[73]</ref>, ViT can achieve compelling results on image recognition. However, it has been observed that without extensive pre-training <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b80">81]</ref> ViT underperforms on image recognition. This is due to the strong model capacity of Transformers, that is imbued with less inductive bias, which leads to overfitting. To properly regularize the model capacity and improve its scalability, numerous subsequent efforts have studied sparse Transformer models tailored for vision tasks such as local attention <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b98">99]</ref>. These methods typically reintroduce hierarchical architectures to compensate for the loss of non-locality. The Swin Transformer <ref type="bibr" target="#b55">[56]</ref> is one such successful attempt to modify Transformers by applying self-attention on shifted non-overlapping windows. For the first time, this approach outperformed ConvNets on the ImageNet benchmark with a pure vision Transformer. Despite having more flexibility and generalizability than the full attention used in ViT, window-based attention has been observed to have limited model capacity due to the loss of non-locality, and henceforth scales unfavorably on larger data regimes such as ImageNet-21K and JFT <ref type="bibr" target="#b18">[19]</ref>. However, acquiring global interactions via full-attention at early or high-resolution stages in a hierarchical network is computationally heavy, as the attention operator requires quadratic complexity. How to efficiently incorporate global and local interactions to balance the model capacity and generalizability under a computation budget still remains challenging.</p><p>In this paper, we present a new type of Transformer module, called multi-axis self-attention (Max-SA), that capably serves as a basic architecture component which can perform both local and global spatial interactions in a single block. Compared to full self-attention, Max-SA enjoys greater flexibility and efficiency, i.e., naturally adaptive to different input lengths with linear complexity; in contrast to (shifted) window/local attention, Max-SA allows for stronger model capacity by proposing a global receptive field. Moreover, with merely linear complexity, Max-SA can be used as a general stand-alone attention module in any layer of a network, even in earlier, high-resolution stages.</p><p>To demonstrate its effectiveness and universality, we further design a simple but effective vision backbone called Multi-axis Vision Transformer (MaxViT) by hierarchically stacking repeated blocks composed of Max-SA and convolutions. While our proposed model belongs to the category of hybrid vision Transformers, MaxViT distinguishes from previous approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b93">94]</ref> in that we strive for simplicity, by designing a basic block unifying convolution, local, and global attention, then simply repeating it. Our experiments shows that the MaxViT significantly improves upon state-of-the-art (SOTA) performance under all data regimes for a broad range of visual tasks including classification, object detection and segmentation, image aesthetics assessment, and image generation. Specifically, as <ref type="figure" target="#fig_2">Figure 1</ref> shows, MaxViT outperforms all recent Transformer-based models in regards to both accuracy vs. FLOPs and accuracy vs. parameter curves. Our contributions are: </p><formula xml:id="formula_0">-A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Convolutional networks. Since AlexNet <ref type="bibr" target="#b47">[48]</ref>, convolutional neural networks (ConvNets) have been used as de facto solutions to almost all vision tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b103">104]</ref> before the "Roaring 20s" <ref type="bibr" target="#b56">[57]</ref>. Phenomenal architectural improvements have been made in the past decade: residual <ref type="bibr" target="#b28">[29]</ref> and dense connections <ref type="bibr" target="#b36">[37]</ref>, fully-convolutional networks <ref type="bibr" target="#b57">[58]</ref>, encoder-decoder schemes <ref type="bibr" target="#b66">[67]</ref>, feature pyramids <ref type="bibr" target="#b51">[52]</ref>, increased depths and widths <ref type="bibr" target="#b74">[75]</ref>, spatial-and channelwise attention models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b90">91]</ref>, non-local interactions <ref type="bibr" target="#b87">[88]</ref>, to name a few. A remarkable recent work ConvNeXt <ref type="bibr" target="#b56">[57]</ref> has re-introduced core designs of vision Transformers and shown that a 'modernized' pure ConvNet can achieve performance comparable to Transformers on broad vision tasks. Transformers in vision. Transformers were originally proposed for natural language processing <ref type="bibr" target="#b84">[85]</ref>. The debut of the Vision Transformer (ViT) <ref type="bibr" target="#b21">[22]</ref> in 2020</p><p>showed that pure Transformer-based architectures are also effective solutions for vision problems. The elegantly novel view of ViT that treats image patches as visual words has stimulated explosive research interest in visual Transformers. To account for locality and 2D nature of images, the Swin Transformer aggregates attention in shifted windows in a hierarchical architecture <ref type="bibr" target="#b55">[56]</ref>. More recent works have been focused on improving model and data efficiency, including sparse attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b98">99]</ref>, improved locality <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b100">101]</ref>, pyramidal designs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b96">97]</ref>, improved training strategies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b104">105]</ref>, etc. We refer readers to dedicated surveys <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b43">44]</ref> of vision Transformers for a comprehensive review. Hybrid models. Pure Transformer-based vision models have been observed to generalize poorly due to relatively less inductive bias <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b80">81]</ref>. Vision Transformers also exhibit substandard optimizability <ref type="bibr" target="#b93">[94]</ref>. An intriguingly simple improvement is to adopt a hybrid design of Transformer and convolution layers such as using a few convolutions to replace the coarse patchify stem <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b93">94]</ref>. A broad range of works fall into this category, either explicitly hybridized <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b97">98]</ref> or in an implicit fashion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">56]</ref>. Transformer for GANs. Transformers have also proven effective in generative adversarial networks (GANs) <ref type="bibr" target="#b25">[26]</ref>. TransGAN <ref type="bibr" target="#b39">[40]</ref> built a pure Transformer GAN with a careful design of local attention and upsampling layers, demonstrating effectiveness on small scale datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b46">47]</ref>. GANformer <ref type="bibr" target="#b37">[38]</ref> explored efficient global attention mechanisms to improve on StyleGAN <ref type="bibr" target="#b41">[42]</ref> generator. HiT <ref type="bibr" target="#b102">[103]</ref> presents an efficient Transformer generator based on local-global attention that can scale up to 1K high-resolution image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Inspired by the sparse approaches presented in <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b102">103]</ref>, we introduce a new type of attention module, dubbed blocked multi-axis self-attention (Max-SA), by decomposing the fully dense attention mechanisms into two sparse forms -window attention and grid attention -which reduces the quadratic complexity of vanilla attention to linear, without any loss of non-locality. Our sequential design offers greater simplicity and flexibility, while performing even better than previous methods -each individual module can be used either standalone or combined in any order <ref type="table">(Tables 7-9)</ref>, whereas parallel designs <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b102">103]</ref> offer no such benefits. Because of the flexibility and scalability of Max-SA, we are able to build a novel vision backbone, which we call MaxViT, by simply stacking alternative layers of Max-SA with MBConv <ref type="bibr" target="#b34">[35]</ref> in a hierarchical architecture, as shown in <ref type="figure">Figure 2</ref>.</p><p>MaxViT benefits from global and local receptive fields throughout the entire network, from shallow to deep stages, demonstrating superior performance in regards to both model capacity and generalization abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention</head><p>Self-attention allows for spatial mixing of entire spatial (or sequence) locations while also benefiting from content-dependent weights based on normalized pairwise similarity. The standard self-attention defined in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b84">85]</ref> is location-unaware, </p><formula xml:id="formula_1">+ + Grid Attention Block Attention MBConv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global 7x7</head><p>Local 7x7 Conv 3x3</p><p>MaxT-1 (69M): S1 hidden_size 96; num_blocks [2, 2, 5, 2] MaxT-2 (119M): S1 hidden_size 96; num_blocks [2, 6, 14, 2] MaxT-3 (212M): S1 hidden_size 128; num_blocks [2, 6, 14, 2] MaxT-4 (476M): S1 hidden_size 192; num_blocks [2, 6, 14, 2]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head</head><p>( 1 x 1 ) <ref type="figure">Fig. 2</ref>: MaxViT architecture. We follow a typical hierarchical design of Con-vNet practices (e.g., ResNet) but instead build a new type of basic building block that unifies MBConv, block, and grid attention layers. Normalization and activation layers are omitted for simplicity.</p><p>i.e., non-translation equivariant, an important inductive bias imbued in Con-vNets. Relative self-attention <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b70">71]</ref> has been proposed to improve on vanilla attention by introducing a relative learned bias added to the attention weights, which has been shown to consistently outperform original attention on many vision tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56]</ref>. In this work, we mainly adopt the pre-normalized relative self-attention defined in <ref type="bibr" target="#b18">[19]</ref> as the key operator in MaxViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-axis Attention</head><p>Global interaction is one of the key advantages of self-attention as compared to local convolution. However, directly applying attention along the entire space is computationally infeasible as the attention operator requires quadratic complexity. To tackle this problem, we present a multi-axis approach to decompose the full-size attention into two sparse forms -local and global -by simply decomposing the spatial axes. Let X ? R H?W ?C be an input feature map. Instead of applying attention on the flattened spatial dimension HW , we block the feature into a tensor of shape ( H P ? W P , P ? P, C), representing partitioning into non-overlapping windows, each of size P ? P . Applying self-attention on the local spatial dimension i.e., P ? P , is equivalent to attending within a small window <ref type="bibr" target="#b55">[56]</ref>. We will use this block attention to conduct local interactions.</p><p>Despite bypassing the notoriously heavy computation of full self-attention, local-attention models have been observed to underfit on huge-scale datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. Inspired by block attention, we present a surprisingly simple but effective way to gain sparse global attention, which we call grid attention. Instead of partitioning feature maps using fixed window size, we grid the tensor into the shape (G ? G, H G ? W G , C) using a fixed G ? G uniform grid, resulting in windows  <ref type="figure">Fig. 3</ref>: Multi-axis self-attention (Max-SA) (best viewed in color). An illustration of the multi-axis approach for computing self-attention (window/grid size is 4?4). The block-attention module performs self-attention within windows, while the grid-attention module attends globally to pixels in a sparse, uniform grid overlaid on the entire 2D space, with both having linear complexity against input size, as we use fixed attention footage. The same colors are spatially mixed by the self-attention operation.</p><p>having adaptive size H G ? W G . Employing self-attention on the decomposed grid axis i.e., G ? G, corresponds to dilated, global spatial mixing of tokens. By using the same fixed window and grid sizes (we use P = G = 7 following Swin <ref type="bibr" target="#b55">[56]</ref>), we can fully balance the computation between local and global operations, both having only linear complexity with respect to spatial size or sequence length. Note that our proposed Max-SA module can be a drop-in replacement of the Swin attention module <ref type="bibr" target="#b55">[56]</ref> with exactly the same number of parameters and FLOPs. Yet it enjoys global interaction capability without requiring masking, padding, or cyclic-shifting, making it more implementation friendly, preferable to the shifted window scheme <ref type="bibr" target="#b55">[56]</ref>. For instance, the multi-axis attention can be easily implemented with einops <ref type="bibr" target="#b65">[66]</ref> without modifying the original attention operation (see Appendix). It is worth mentioning that our proposed multi-axis attention (Max-SA) is fundamentally different from the axial-attention models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b85">86]</ref>. Please see Appendix for a detailed comparison. MaxViT block. We sequentially stack the two types of attentions to gain both local and global interactions in a single block, as shown in <ref type="figure">Figure 3</ref>. Note that we also adopt typical designs in Transformers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b55">56]</ref>, including LayerNorm <ref type="bibr" target="#b1">[2]</ref>, Feedforward networks (FFNs) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b55">56]</ref>, and skip-connections. We also add a MB-Conv block <ref type="bibr" target="#b34">[35]</ref> with squeeze-and-excitation (SE) module <ref type="bibr" target="#b35">[36]</ref> prior to the multiaxis attention, as we have observed that using MBConv together with attention further increases the generalization as well as the trainability of the network <ref type="bibr" target="#b93">[94]</ref>. Using MBConv layers prior to attention offers another advantage, in that depthwise convolutions can be regarded as conditional position encoding (CPE) <ref type="bibr" target="#b16">[17]</ref>, making our model free of explicit positional encoding layers. Note that our proposed stand-alone multi-axis attention may be used together or in isolation for different purposes -block attention for local interaction, and grid attention for global mixing. These elements can be easily plugged into many vision architectures, especially on high-resolution tasks that can benefit by global interactions with affordable computation. <ref type="table">Table 1</ref>: MaxViT architecture variants. B and C denotes number of blocks and number of channels for each stage. We set each attention head to 32 for all attention layers. For MBConv, we always use expansion rate 4 and shrinkage rate 0.25 in SE <ref type="bibr" target="#b35">[36]</ref>, following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>. We use two Conv layers in the stem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage</head><p>Size</p><formula xml:id="formula_2">MaxViT-T MaxViT-S MaxViT-B MaxViT-L MaxViT-XL S0: Conv-stem 1 /2 B=2 C=64 B=2 C=64 B=2 C=64 B=2 C=128 B=2 C=192 S1: MaxViT-Block 1 /4 B=2 C=64 B=2 C=96 B=2 C=96 B=2 C=128 B=2 C=192 S2: MaxViT-Block 1 /8 B=2 C=128 B=2 C=192 B=6 C=192 B=6 C=256 B=6 C=384 S3: MaxViT-Block 1 /16 B=5 C=256 B=5 C=384 B=14 C=384 B=14 C=512 B=14 C=768 S4: MaxViT-Block 1 /32 B=2 C=512 B=2 C=768 B=2 C=768 B=2 C=1024 B=2 C=1536</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture Variants</head><p>We designed a series of extremely simple architectural variants to explore the effectiveness of our proposed MaxViT block, as shown in <ref type="figure">Figure 2</ref>. We use a hierarchical backbone similar to common ConvNet practices <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b79">80]</ref> where the input is first downsampled using Conv3x3 layers in stem stage (S0). The body of the network contains four stages (S1-S4), with each stage having half the resolution of the previous one with a doubled number of channels (hidden dimension). In our network, we employ identical MaxViT blocks throughout the entire backbone. We apply downsampling in the Depthwise Conv3x3 layer of the first MBConv block in each stage. The expansion and shrink rates for inverted bottleneck <ref type="bibr" target="#b34">[35]</ref> and squeeze-excitation (SE) <ref type="bibr" target="#b35">[36]</ref> are 4 and 0.25 by default. We set the attention head size to be 32 for all attention blocks. We scale up the model by increasing block numbers per stage B and the channel dimension C. We summarize the architectural configurations of the MaxViT variants in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We validated the efficacy of our proposed model on various vision tasks: Ima-geNet classification <ref type="bibr" target="#b47">[48]</ref>, image object detection and instance segmentation <ref type="bibr" target="#b52">[53]</ref>, image aesthetics/quality assessment <ref type="bibr" target="#b60">[61]</ref>, and unconditional image generation <ref type="bibr" target="#b25">[26]</ref>. More experimental details can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Classification on ImageNet-1K</head><p>ImageNet-1K. We show in <ref type="table" target="#tab_3">Table 2</ref> the performance comparisons on ImageNet-1K classification. Under the basic 224?224 setting, MaxViT outperformed the most recent strong hybrid model CoAtNet by a large margin across the entire FLOPs spectrum, as shown in <ref type="figure" target="#fig_2">Figure 1a</ref>. The MaxViT-L model sets a new performance record of 85.17% at 224 ? 224 training without extra training strategies, outperforming CoAtNet-3 by 0.67%. In regards to throughput-accuracy trade-offs at 224 2 , MaxViT-S obtains 84.45% top-1 accuracy, 0.25% higher than CSWin-B and 0.35% higher than CoAtNet-2 with comparable throughput. When fine-tuned at higher resolutions (384/512), MaxViT continues to deliver high performance compared to strong ConvNet and Transformer competitors: (1) at 384 2 , MaxViT-B attains 86.34% top-1 accuracy, outperforming EfficientNetV2-L by 0.64%; (2) when fine-tuned at 512 2 , our MaxViT-L (212M)   limitations, we leave experiments on billion-parameter-scale models on planetscale datasets (e.g., JFT-3B <ref type="bibr" target="#b101">[102]</ref>) as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Detection and Instance Segmentation</head><p>Setting. We evaluated the MaxViT architectures on the COCO2017 <ref type="bibr" target="#b52">[53]</ref> object bounding box detection and instance segmentation tasks with a two-stage framework <ref type="bibr" target="#b64">[65]</ref>. On the object detection task, a feature-pyramid architecture <ref type="bibr" target="#b51">[52]</ref> was employed to boost different levels of objectiveness. In the instance segmentation task, a well-known Cascade Mask-RCNN framework <ref type="bibr" target="#b27">[28]</ref> was employed. The dataset contains 118K training and 5K validation samples. For all the compared models, the backbones are first pretrained using ImageNet-1K. The pretrained models are then used to finetune on the detection and segmentation tasks. Results on COCO. As shown in Setting. We train and evaluate the MaxViT model on the AVA benchmark <ref type="bibr" target="#b60">[61]</ref> which contains 255K images with aesthetics scores rated by amateur photographers. Similar to <ref type="bibr" target="#b76">[77]</ref>, we split the dataset into 80%/20% training and test sets. We followed <ref type="bibr" target="#b76">[77]</ref> and used the normalized Earth Mover's Distance as our training loss. We trained MaxViT at three different input resolutions: 224 2 , 384 2 and 512 2 , initialized with ImageNet-1K pre-trained weights.</p><p>Results on AVA. To evaluate and compare our model against existing methods, we present a summary of our results in <ref type="table" target="#tab_7">Table 5</ref>. For similar input resolutions, the proposed MaxViT-T model outperforms existing image aesthetic assessment methods. As the input resolution increases, the performance improves, benefiting from its strong non-local capacity. Also, MaxViT shows better linear correlation compared to the SOTA method <ref type="bibr" target="#b42">[43]</ref> which uses multi-resolution inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Image Generation</head><p>Setting. We evaluate the generative ability of MaxViT blocks to generate images of 128x128 resolution on ImageNet-1K. We choose the unconditional image generation to focus on the performance of different generators in GANs. We use the Inception Score (IS) <ref type="bibr" target="#b68">[69]</ref> and the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b31">[32]</ref> as quantitative evaluation metrics. 50,000 samples were randomly generated to calculate the FID and IS scores. We compared MaxViT against HiT <ref type="bibr" target="#b102">[103]</ref>, a SOTA generative Transformer model, which uses attention at low resolutions (e.g., <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b63">64)</ref>, and using implicit neural functions at high resolutions (e.g., 128). By contrast, MaxViT uses the proposed MaxViT block at every resolution. Note that we use an inverse block order (GA-BA-Conv) as we found it to perform better (see <ref type="table" target="#tab_10">Table 8</ref>). Since Batch Normalization <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b102">103]</ref> achieves better results on image generation, we replaced all Layer Norm with Batch Norm under this setting.  Results on ImageNet-1K. The results are shown in <ref type="table" target="#tab_8">Table 6</ref>. Our MaxViT achieved better FID and IS with significantly lower number of parameters. These results demonstrate the effectiveness of MaxViT blocks for generation tasks. More details of the generative experiment can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies.</head><p>In this section, we ablate important design choices in MaxViT on ImageNet-1K image classification. We use the MaxViT-T model trained for 300 epochs by default and report top-1 accuracy on ImageNet-1K. Except for the ablated design choice, we used the same training configurations, unless stated otherwise. Global grid-attention. One of our main contributions is the grid-attention module, which allows for sparse global interactions at linear time, enabling our model to capture global information at all stages. We conducted two ablations to understand its gain: 1) completely removed global attention at each stage; 2) replaced grid attention with block attention to retain the same parameter count and FLOPs. As <ref type="table">Table 7</ref> shows, enabling global attention at earlier stages can further boost performance over using only local attention or convolutions. MBConv layer. We also ablated the usage of MBConv layers in MaxViT by removing all MBConv in each stage. Note that we should also consider the reduction of parameter count and FLOPs when removing the MBConv layers. Plus, Stage 3 has 5 blocks whereas other stages have only 2. As <ref type="table">Table 9</ref> shows, the usage of MBConv layers in MaxViT significantly boosts performance. Block order study. We present three different modules to build the MaxViT block -MBConv, block-, and grid-attention -which captures spatial interactions from local to global. To investigate the most effective way to combine them, we evaluated the MaxViT-T model using all 6 permutations. We always apply downsampling in the first layer, which might cause a minor model size difference. We can observe from <ref type="table" target="#tab_10">Table 8</ref> that placing MBConv before attention layers is <ref type="table">Table 7</ref>: Effects of global gridattention. Ablate-S1 means we remove grid-attention in stage 1 while Replace-S1 means replacing gridattention with block-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Pars. FLOPs Top-1 Acc.    <ref type="table">Table 9</ref>: Ablation of MBConv. Ablate-S1 means we delete MBConv layers in stage 1. Note that the network will also be smaller if we ablate MBConv layers in some stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Pars. FLOPs Top-1 acc.</p><p>MaxViT-T 30.9M 5.6G 83.62 Ablate-S1 30.8M 5.  almost always better than other combinations. The reason might be that it is more suitable to get local features/patterns in early layers, then aggregate them globally, which is aligned with existing hybrid models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b93">94]</ref>, which puts Conv layers in front of attention. In generative experiments (Section 4.4), however, we found the best order to be from global to local: GA-BA-C. We hypothesize that it may be advantageous for generation tasks to first obtain the overall structures correct with global processing blocks (i.e., grid-attention layers), then fill in finer details using local processing blocks (i.e., MBConv).</p><p>Sequential vs. parallel. In our approach, we sequentially stack the multi-axis attention modules following <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b85">86]</ref>, while there also exist other models that adopt a parallel design <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b102">103]</ref>. In this ablation, we compare our sequential Max-SA against parallel branches containing block-and grid-attention respectively. Note that we use an input projection to double the channels, then split the heads to feed the two branches in order to remain similar complexity to MaxViT, and an output projection that reduces the concatenated branches. We did rough parameter tuning and found that an initial learning rate of 10 ?3 per-forms significantly better than 3 ? 10 ?3 for parallel models. We use all the same parameters except the learning rate. As <ref type="table" target="#tab_11">Table 10</ref> shows, our sequential approach remarkably outperforms parallel counterparts with fewer parameters and computation. The reason may be that the parallel designs learn complementary cues with less interactions between them, whereas our sequential stack is able to learn more powerful fusions between local and global layers. Our model scales better than Swin layeout <ref type="bibr" target="#b55">[56]</ref>.</p><p>Vertical layout. We further examine our vertical layout design, i.e., the number of blocks each stage. We compared our design against the choice of Swin/ConvNeXt <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. We change MaxViT-T and -S to blocks B = (2, 2, 6, 2), and MaxViT-B, -L to have blocks B = (2, 2, 18, 2) strictly following the stage ratio of Swin <ref type="bibr" target="#b55">[56]</ref>. It may be seen from Figure 5 that our layout performed comparably to Swin for small models, but scales significantly better for larger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>While recent works in the 2020s have arguably shown that ConvNets and vision Transformers can achieve similar performance on image recognition, our work presents a unified design that takes advantages of the best of both worldsefficient convolution and sparse attention -and demonstrates that a model built on top, namely MaxViT, can achieve state-of-the-art performance on a variety of vision tasks, and more importantly, scale extremely well to massive scale data sizes. Even though we present our model in the context of vision tasks, the proposed multi-axis approach can easily extend to language modeling to capture both local and global dependencies in linear time. We also look forward to studying other forms of sparse attention in higher-dimensional or multi-modal signals such as videos, point clouds, and vision-languages. Societal impact. Investigating the performance and scalability of large model designs would consume considerable computing resources. These efforts can contribute to increased carbon emissions, which could hence raise environmental concerns. However, the proposed model offers strong modular candidates that expand the network's design space for future efforts on automated architectural design. If trained improperly, the proposed model may express bias and fairness issues. The proposed generative model can be abused to generate misleading media and fake news. These issues demand caution in future related research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this Appendix we provide the following material:  <ref type="table" target="#tab_4">(Table 13</ref>), ImageNet-21K and JFT <ref type="table" target="#tab_5">(Table 14)</ref>, as well as more image generation visualizations on ImageNet-1K <ref type="figure" target="#fig_11">(Figure 8</ref>).</p><formula xml:id="formula_3">-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Backbone Details</head><p>MBConv MaxViT leverages the MBConv block <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b78">79]</ref> as the main convolution operator. We also adopt a pre-activation structure <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref> to promote homogeneity between MBConv and Transformer blocks. Specifically, assume x to be the input feature, the MBConv block without downsampling is formulated as:</p><p>x ? x + Proj(SE(DWConv(Conv(Norm(x))))),</p><p>where Norm is BatchNorm <ref type="bibr" target="#b38">[39]</ref>, Conv is the expansion Conv1x1 followed by BatchNorm and GELU <ref type="bibr" target="#b30">[31]</ref> activation, a typical choice for Transformer-based models. DWConv is the Depthwise Conv3x3 followed by BatchNorm and GELU. SE is the Squeeze-Excitation layer <ref type="bibr" target="#b35">[36]</ref>, while Proj is the shrink Conv1x1 to downproject the number of channels. Note that for the first MBConv block in every stage, the downsampling is done by applying stride-2 Depthwise Conv3x3 while the shortcut branch should also apply pooling and channel projection:</p><p>x ? Proj(Pool2D(x)) + Proj(SE(DWConv ? <ref type="figure">(Conv(Norm(x)</ref>)))).</p><p>Relative Attention Relative attention has been explored in several previous studies for both NLP <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b91">92]</ref> and vision <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b83">84]</ref>. Here to simplify the presentation, we present our model using only a single head of the multi-head self-attention. In the actual implementation, we always use multi-head attention with the same head dimension. The relative attention can be defined as:</p><formula xml:id="formula_6">RelAttention(Q, K, V ) = softmax(QK T / ? d + B)V,<label>(3)</label></formula><p>where Q, K, V ? R (H?W )?C are the query, key, and value matrices and d is the hidden dimension. The attention weights are co-decided by a learned static location-aware matrix B and the scaled input-adaptive attention QK T / ? d. Considering the differences in 2D coordinates, the relative position bias B is parameterized by a matrixB ? R (2H?1)(2W ?1) . Following typical practices <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b55">56]</ref>, when fine-tuned at a higher resolution e.g., H ? W , we use bilinear interpolation to map the relative positional bias from R (2H?1)(2W ?1) to R (2H ?1)(2W ?1) . This relative attention benefits from input-adaptivity, translation equivariance, and global interactions, which is a preferred choice over the vanilla self-attention on 2D vision tasks. In our model, all the attention operators use this relative attention defined in Eq. 3 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Axis Attention</head><p>We assume the relative attention operator in Eq. 3 follows the convention for 1D input sequences i.e., always regards the second last dimension of an input (..., L, C) as the spatial axis where L, C represent sequence length and channels. The proposed Multi-Axis Attention can be implemented without modification to the self-attention operation. To start with, we first define the Block(?) operator with parameter P as partitioning the input image/feature x ? R H?W ?C into non-overlapping blocks with each block having size P ? P . Note that after window partition, the block dimensions are gathered onto the spatial dimension (i.e., -2 axis):</p><formula xml:id="formula_7">Block : (H, W, C) ? ( H P ? P, W P ? P, C) ? ( HW P 2 , P 2 , C).<label>(4)</label></formula><p>We denote the Unblock(?) operation as the reverse of the above block partition procedure. Similarly, we define the Grid(?) operation with parameter G as dividing the input feature into a uniform G?G grid, with each lattice having adaptive size H G ? W G . Unlike the block operator, we need to apply an extra Transpose to place the grid dimension in the assumed spatial axis (i.e., -2 axis):</p><formula xml:id="formula_8">Grid : (H, W, C) ? (G ? H G , G ? W G , C) ? (G 2 , HW G 2 , C) ? ( HW G 2 , G 2 , C) swapaxes(axis1=-2,axis2=-3)<label>(5)</label></formula><p>with its inverse operation Ungrid(?) that reverses the gridded input back to the normal 2D feature space.</p><p>To this end, we are ready to explain the multi-axis attention module. Given an input tensor x ? R H?W ?C , the local Block Attention can be expressed as:</p><p>x ? x + Unblock(RelAttention <ref type="figure">(Block(LN(x)</ref>)))</p><formula xml:id="formula_9">x ? x + MLP(LN(x))<label>(6)</label></formula><p>while the global, dilated Grid Attention module is formulated as:</p><p>x ? x + Ungrid(RelAttention <ref type="figure">(Grid(LN(x)</ref>)))</p><p>x ? x + MLP(LN(x))</p><p>where we omit the QKV input format in the RelAttention operation for simplicity. LN denotes the Layer Normalization <ref type="bibr" target="#b1">[2]</ref>, where MLP is a standard MLP network <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b55">56]</ref> consisting of two linear layers: x ? W 2 GELU(W 1 x). Comparison to Axial attention It should be noted that our proposed multi-axis attention (Max-SA) module is completely different from the axial attention proposed in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b85">86]</ref>. As shown in <ref type="figure" target="#fig_8">Figure 6</ref>(a), Axial attention proposes to first apply columnwise attention then row-wise, which achieves a global receptive field with O(N ? N ) complexity (assuming N equals to the number of pixels). On the contrary, our proposed Max-SA shown in <ref type="figure" target="#fig_8">Figure 6</ref>(b) first employs local attention, then sparse global attention, enjoying global receptive fields with only O(N ) linear complexity. Moreover, we deem the proposed Max-SA a more natural approach for vision since the design of attended regions account for the 2D structure of images, e.g., mixing tokens in a spatially-local small window.</p><p>MaxViT Block We demonstrate in Algo. 1 an einops-style pseudocode of the MaxViT block which contains MBConv, block attention, and grid attention. Classification Head Instead of using the [cls] token <ref type="bibr" target="#b21">[22]</ref>, we simply apply global average pooling to the output of the last stage (S4) to obtain the feature representation, followed by the final classification head. Architectural Specifications Finally, we present detailed architectural specifications for the MaxViT model family (T/S/B/L) in <ref type="table" target="#tab_13">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algo. 1 Pseudocode of MaxViT Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Detection and Segmentation Models</head><p>We follow the settings of the cascaded Faster-RCNN <ref type="bibr" target="#b64">[65]</ref> and Mask-RCNN <ref type="bibr" target="#b27">[28]</ref>, but replace the feature extraction backbone with our MaxViT backbone. We also applied FPN <ref type="bibr" target="#b51">[52]</ref> in the feature map generation, where the S2, S3, S4 (multiscale features of targeted resolution 1/8, 1/16, 1/32 in MaxViT, respectively) are used. Then the generated feature maps are fed into the detection head. For fair comparison, we follow the original implementation without adopting any system-level strategies to further boost the final performance, such as the HTC framework <ref type="bibr" target="#b6">[7]</ref>, instaboost <ref type="bibr" target="#b24">[25]</ref>, etc. used in Swin <ref type="bibr" target="#b55">[56]</ref>. We show the results of MaxViT-T/S/B on these two tasks to compare it against recent strong models at similar model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Image Aesthetics Model</head><p>This task requires incorporating both local and global information of an image to accurately predict human perceptual preference. To this end, the model needs to  <ref type="figure">Fig. 7</ref>: Generator architecture using the MaxViT block for the GAN experiment. In every stage, we first use the cross-attention module to let the features attend to the latent embedding projected from the input code, which are then fed into the proposed MaxViT block consisting of grid attention, block attention, and MBConv layer. Note that unlike the main model in Sec. A.1, the order of applying the three layers are reversed: from global to local.</p><p>have the capacity to learn pixel-level quality aspects such as sharpness, noisiness and contrast as well as semantic-level aspects such as composition and depthof-field. We follow <ref type="bibr" target="#b76">[77]</ref> and use the normalized Earth Mover's Distance as our training loss. Given the ground truth and predicted probability mass functions p and p representing the histogram of scores, the normalized Earth Mover's Distance can be expressed as:</p><formula xml:id="formula_11">EMD(p, p) = 1 N N k=1 |CDF p (k) ? CDF p (k)| r 1/r<label>(8)</label></formula><p>where CDF p (k) is the cumulative distribution function as k i=1 p i , and N = 10 represents the number score bins. In our experiments we set r = 2. We remove the classification head used in MaxViT, and instead append a fully-connected layer with 10 neurons followed by softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 GAN Model</head><p>The above image recognition tasks can validate the power of our proposed MaxViT block used in downsampling (contracting) models. For this GAN experiment, we would like to demonstrate its effectiveness in upsampling (expanding) architectures. The MaxViT-GAN model for image generation is illustrated in <ref type="figure">Figure 7</ref>. For unconditional image generation, MaxViT-GAN first takes a latent code z ? N (0, I) as input, then progressively generates an image of target resolution through a hierarchically upsampling structure. We start by linearly projecting the input to a feature with spatial dimension 8 ? 8. During the generation, the feature will go through five stages consisting of identical GAN blocks with gradually increased spatial resolution, similar to the design of our main model. Similar to <ref type="bibr" target="#b102">[103]</ref>, we apply a cross-attention layer before the MaxViT block as a memory-efficient form of self-modulation in every stage, which has been shown to stabilize GAN training and also improve mode coverage <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b102">103]</ref>. We use pixel shuffle <ref type="bibr" target="#b71">[72]</ref> for upsampling in the end of each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 ImageNet Classification</head><p>We provide ImageNet-1K experimental settings of MaxViT models for both pretraining and fine-tuning in <ref type="table" target="#tab_3">Table 12</ref>. All the MaxViT variants used similar hyperparameters except that we mainly customize the stochastic depth rate to regularize each model separately.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Image Aesthetics Assessment</head><p>We trained and evaluated the MaxViT model on the AVA benchmark <ref type="bibr" target="#b60">[61]</ref>. This dataset consists of 255K images rated by armature photographers through photography contests. Each image is rated by an average of 200 human raters, assigning a score from 1 to 10 to images. The higher the score, the better the visual aesthetic quality of the image. Each image in the dataset has a histogram of scores associated with it, which we use as the ground truth label. Similar to <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b76">77]</ref>, we split the dataset into train and test sets, such that 20% of the data is used for testing. We train MaxViT for three different input resolutions: 224 ? 224, 384 ? 384 and 512 ? 512. We initialized the model with ImageNet-1K 224?224 pre-trained weights. The weight and bias momentums are set to 0.9, and a dropout rate of 0.75 is applied on the last layer of the baseline network. We use an initial learning rate of 1e-3, exponentially decayed with decay factor 0.9 every 10 epochs. We set the stochastic depth rate to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Image Generation</head><p>We use a ResNet-based discriminator following <ref type="bibr" target="#b41">[42]</ref>. To train the model, we also used the standard non-saturating logistic GAN loss with R1 gradient penalty <ref type="bibr" target="#b59">[60]</ref> applied to the discriminator with the gradient penalty weight set to 10. We employ the Adam <ref type="bibr" target="#b44">[45]</ref> optimizer with a learning rate of 1e-4 for both generator and discriminator. The model is trained on TPU for one million steps with batch size 256. Notably, we do not employ extra GAN training tricks such as pixel norm, noise injection, progressive growing, etc. on which recent state-of-the-art models are heavily relied to attain good results <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. The overall objectives of the GAN training are defined as:</p><formula xml:id="formula_12">L G = ?E z?Pz [log(D(G(z))],<label>(9)</label></formula><formula xml:id="formula_13">L D = ?E x?Px [log(D(x))] ? E z?Pz [log(1 ? D(G(z)))] + ?E x?Px [ ? x D(x) 2 2 ],<label>(10)</label></formula><p>where ? denotes the R 1 gradient penalty weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Complete Experimental Results</head><p>We provide complete experiment comparisons for ImageNet-1K, Image-21K, and JFT datasets in <ref type="table" target="#tab_4">Table 13</ref> and <ref type="table" target="#tab_5">Table 14</ref>, respectively. We also provide more visual results for unconditional image generation on ImageNet-1K in <ref type="figure" target="#fig_11">Figure 8</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Accuracy vs. FLOPs performance scaling curve under ImageNet-1K training setting at input resolution 224?224.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Accuracy vs. Parameters scaling curve under ImageNet-1K fine-tuning setting allowing for higher sizes (384/512).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Performance comparison of MaxViT with state-of-the-art vision Transformers on ImageNet-1K. Our model shows superior performance in terms of both accuracy vs. computation and accuracy vs. parameters tradeoff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Accuracy vs. Params scaling curve for JFT-300M pre-trained models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Performance comparison on large-scale pre-trained models. MaxViT shows superior scaling performance under both ImageNet-21K and JFT-300M pre-trained settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>-BA 30.9M 5.6G 83.54(-0.08) BA-C-GA 31.1M 5.3G 83.07(-0.55) BA-GA-C 31.1M 5.3G 83.02(-0.60) GA-C-BA 31.1M 5.3G 83.08(-0.54) GA-BA-C 31.1M 5.3G 83.03(-0.59) -GA 18.6M 31.40 21.49(-1.19)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>2G 83.24(-0.38) Ablate-S2 30.5M 5.4G 83.02(-0.60) Ablate-S3 27.6M 5.1G 82.65(-0.97) Ablate-S4 25.7M 5.4G 83.09(-0.53)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>Vertical layout ablation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison of Axial attention and our proposed Multi-Axis attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>#</head><label></label><figDesc>input: features (b, h, w, c). Assume h==w; x/output: features (b, h, w, c). # p/g: block/grid size. Use 7 by default. def RelSelfAttn(x): return x # A self-attn function applied on the -2 axis # Window/grid partition function from einops import rearrange def block(x,p): return rearrange(x,"b(hy)(wx)c-&gt;b(hw)(yx)c",h=x.shape[1]//p,w=x.shape[2]//p,y=p ,x=p) def unblock(x,g,p): return rearrange(x,"b(hw)(yx)c-&gt;b(hy)(wx)c",h=g,w=g,y=p,x=p) x = MBConv(input) # MBConv layer x = block(x,p) # window partition x = RelSelfAttn(x) # Apply window-attention x = unblock(x,x.shape[1]//p,p) # reverse x = block(x,x.shape[1]//g) # grid partition x = swapaxes(x,-2,-3) # move grid-axis to -2 x = RelSelfAttn(x) # Apply grid-attention x = swapaxes(x,-2,-3) # reverse swapaxes output = unblock(x,g,x.shape[1]//g) # reverse</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Pre-training Fine-tuning Pre-training Fine-tuning Pre-training Fine-tuning (MaxViT-T/S/B/L) (MaxViT-B/L/XL) (MaxViT-B/L/XL)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 :</head><label>8</label><figDesc>Unconditional generation results on ImageNet-1k 128 ? 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison under ImageNet-1K setting. Throughput is measured on a single V100 GPU with batch size 16, following<ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b79">80]</ref>.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Eval size</cell><cell cols="2">Params FLOPs</cell><cell>Throughput (image/s)</cell><cell>IN-1K top-1 acc.</cell></row><row><cell></cell><cell>?EffNet-B6 [79]</cell><cell>528</cell><cell>43M</cell><cell>19.0G</cell><cell>96.9</cell><cell>84.0</cell></row><row><cell></cell><cell>?EffNet-B7 [79]</cell><cell>600</cell><cell>66M</cell><cell>37.0G</cell><cell>55.1</cell><cell>84.3</cell></row><row><cell></cell><cell>?RegNetY-16 [62]</cell><cell>224</cell><cell>84M</cell><cell>16.0G</cell><cell>334.7</cell><cell>82.9</cell></row><row><cell></cell><cell>?NFNet-F0 [5]</cell><cell>256</cell><cell>72M</cell><cell>12.4G</cell><cell>533.3</cell><cell>83.6</cell></row><row><cell>ConvNets</cell><cell>?NFNet-F1 [5] ?EffNetV2-S [80]</cell><cell cols="2">320 132M 384 24M</cell><cell>35.5G 8.8G</cell><cell>228.5 666.6</cell><cell>84.7 83.9</cell></row><row><cell></cell><cell>?EffNetV2-M [80]</cell><cell>480</cell><cell>55M</cell><cell>24.0G</cell><cell>280.7</cell><cell>85.1</cell></row><row><cell></cell><cell cols="2">?ConvNeXt-S [57] 224</cell><cell>50M</cell><cell>8.7G</cell><cell>447.1</cell><cell>83.1</cell></row><row><cell></cell><cell cols="2">?ConvNeXt-B [57] 224</cell><cell>89M</cell><cell>15.4G</cell><cell>292.1</cell><cell>83.8</cell></row><row><cell></cell><cell cols="3">?ConvNeXt-L [57] 224 198M</cell><cell>34.4G</cell><cell>146.8</cell><cell>84.3</cell></row><row><cell></cell><cell>?ViT-B/32 [22]</cell><cell>384</cell><cell>86M</cell><cell>55.4G</cell><cell>85.9</cell><cell>77.9</cell></row><row><cell></cell><cell>?ViT-B/16 [22]</cell><cell cols="3">384 307M 190.7G</cell><cell>27.3</cell><cell>76.5</cell></row><row><cell></cell><cell>?DeiT-B [81]</cell><cell>384</cell><cell>86M</cell><cell>55.4G</cell><cell>85.9</cell><cell>83.1</cell></row><row><cell></cell><cell>?CaiT-M24 [82]</cell><cell cols="2">224 186M</cell><cell>36.0G</cell><cell>-</cell><cell>83.4</cell></row><row><cell></cell><cell>?CaiT-M24 [82]</cell><cell cols="3">384 186M 116.1G</cell><cell>-</cell><cell>84.5</cell></row><row><cell></cell><cell cols="2">?DeepViT-L [105] 224</cell><cell>55M</cell><cell>12.5G</cell><cell>-</cell><cell>83.1</cell></row><row><cell>ViTs</cell><cell cols="2">?T2T-ViT-24 [101] 224</cell><cell>64M</cell><cell>15.0G</cell><cell>-</cell><cell>82.6</cell></row><row><cell></cell><cell>?Swin-S [56]</cell><cell>224</cell><cell>50M</cell><cell>8.7G</cell><cell>436.9</cell><cell>83.0</cell></row><row><cell></cell><cell>?Swin-B [56]</cell><cell>384</cell><cell>88M</cell><cell>47.0G</cell><cell>84.7</cell><cell>84.5</cell></row><row><cell></cell><cell>?CSwin-B [21]</cell><cell>224</cell><cell>78M</cell><cell>15.0G</cell><cell>250</cell><cell>84.2</cell></row><row><cell></cell><cell>?CSwin-B [21]</cell><cell>384</cell><cell>78M</cell><cell>47.0G</cell><cell>-</cell><cell>85.4</cell></row><row><cell></cell><cell>?Focal-S [99]</cell><cell>224</cell><cell>51M</cell><cell>9.1G</cell><cell>-</cell><cell>83.5</cell></row><row><cell></cell><cell>?Focal-B [99]</cell><cell>224</cell><cell>90M</cell><cell>16.0G</cell><cell>-</cell><cell>83.8</cell></row><row><cell></cell><cell>CvT-21 [93]</cell><cell>384</cell><cell>32M</cell><cell>24.9G</cell><cell>-</cell><cell>83.3</cell></row><row><cell></cell><cell>CoAtNet-2 [19]</cell><cell>224</cell><cell>75M</cell><cell>15.7G</cell><cell>247.7</cell><cell>84.1</cell></row><row><cell></cell><cell>CoAtNet-3 [19]</cell><cell cols="2">224 168M</cell><cell>34.7G</cell><cell>163.3</cell><cell>84.5</cell></row><row><cell></cell><cell>CoAtNet-3 [19]</cell><cell cols="3">384 168M 107.4G</cell><cell>48.5</cell><cell>85.8</cell></row><row><cell></cell><cell>CoAtNet-3 [19]</cell><cell cols="3">512 168M 203.1G</cell><cell>22.4</cell><cell>86.0</cell></row><row><cell></cell><cell>MaxViT-T</cell><cell>224</cell><cell>31M</cell><cell>5.6G</cell><cell>349.6</cell><cell>83.62</cell></row><row><cell></cell><cell>MaxViT-S</cell><cell>224</cell><cell>69M</cell><cell>11.7G</cell><cell>242.5</cell><cell>84.45</cell></row><row><cell>Hybrid</cell><cell>MaxViT-B MaxViT-L</cell><cell cols="2">224 120M 224 212M</cell><cell>23.4G 43.9G</cell><cell>133.6 99.4</cell><cell>84.95 85.17</cell></row><row><cell></cell><cell>MaxViT-T</cell><cell>384</cell><cell>31M</cell><cell>17.7G</cell><cell>121.9</cell><cell>85.24</cell></row><row><cell></cell><cell>MaxViT-S</cell><cell>384</cell><cell>69M</cell><cell>36.1G</cell><cell>82.7</cell><cell>85.74</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell cols="2">384 120M</cell><cell>74.2G</cell><cell>45.8</cell><cell>86.34</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell cols="3">384 212M 133.1G</cell><cell>34.3</cell><cell>86.40</cell></row><row><cell></cell><cell>MaxViT-T</cell><cell>512</cell><cell>31M</cell><cell>33.7G</cell><cell>63.8</cell><cell>85.72</cell></row><row><cell></cell><cell>MaxViT-S</cell><cell>512</cell><cell>69M</cell><cell>67.6G</cell><cell>43.3</cell><cell>86.19</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell cols="3">512 120M 138.5G</cell><cell>24.0</cell><cell>86.66</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell cols="3">512 212M 245.4G</cell><cell>17.8</cell><cell>86.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison for large-scale data regimes: ImageNet-21K and JFT pretrained models. accuracy 86.7% , setting new SOTA performance on ImageNet-1K under the normal training setting. AsFigure 1shows, MaxViT scales much better than SOTA vision Transformers on the ImageNet-1K trained model scale. ImageNet-21K.Table 3shows the results of models pre-trained on ImageNet-21K. Remarkably, the MaxViT-B model achieves 88.38% accuracy, outperforming the previous best model CoAtNet-4 by 0.28% using only 43% of parameter count and 38% of FLOPs, demonstrating greater parameter and computing efficiency.Figure 4avisualizes the model size comparison -MaxViT scales significantly better than previous attention-based models of similar complexities, across the board. Additionally, the MaxViT-XL model achieves new SOTA performance, an accuracy of 88.70% when fine-tuned at resolution 512 ? 512.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Eval size</cell><cell cols="2">Params FLOPs</cell><cell cols="2">IN-1K top-1 acc. 21K?1K JFT?1K</cell></row><row><cell></cell><cell>?BiT-R-101x3 [46]</cell><cell>384</cell><cell cols="2">388M 204.6G</cell><cell>84.4</cell><cell>-</cell></row><row><cell></cell><cell>?BiT-R-152x4 [46]</cell><cell>480</cell><cell cols="2">937M 840.5G</cell><cell>85.4</cell><cell>-</cell></row><row><cell></cell><cell>?EffNetV2-L [80]</cell><cell>480</cell><cell>121M</cell><cell>53.0G</cell><cell>86.8</cell><cell>-</cell></row><row><cell>ConvNets</cell><cell>?EffNetV2-XL [80]</cell><cell>512</cell><cell>208M</cell><cell>94.0G</cell><cell>87.3</cell><cell>-</cell></row><row><cell></cell><cell>?ConvNeXt-L [57]</cell><cell>384</cell><cell cols="2">198M 101.0G</cell><cell>87.5</cell><cell>-</cell></row><row><cell></cell><cell cols="2">?ConvNeXt-XL [57] 384</cell><cell cols="2">350M 179.0G</cell><cell>87.8</cell><cell>-</cell></row><row><cell></cell><cell>?NFNet-F4+ [5]</cell><cell>512</cell><cell>527M</cell><cell>367G</cell><cell>-</cell><cell>89.20</cell></row><row><cell></cell><cell>?ViT-B/16 [22]</cell><cell>384</cell><cell>87M</cell><cell>55.5G</cell><cell>84.0</cell><cell>-</cell></row><row><cell></cell><cell>?ViT-L/16 [22]</cell><cell>384</cell><cell cols="2">305M 191.1G</cell><cell>85.2</cell><cell></cell></row><row><cell></cell><cell>?ViT-L/16 [22]</cell><cell>512</cell><cell>305M</cell><cell>364G</cell><cell>-</cell><cell>87.76</cell></row><row><cell>ViTs</cell><cell>?ViT-H/14 [22]</cell><cell>518</cell><cell>632M</cell><cell>1021G</cell><cell>-</cell><cell>88.55</cell></row><row><cell></cell><cell>?HaloNet-H4 [84]</cell><cell>512</cell><cell>85M</cell><cell>-</cell><cell>85.8</cell><cell>-</cell></row><row><cell></cell><cell>?SwinV2-B [56]</cell><cell>384</cell><cell>88M</cell><cell>-</cell><cell>87.1</cell><cell>-</cell></row><row><cell></cell><cell>?SwinV2-L [56]</cell><cell>384</cell><cell>197M</cell><cell>-</cell><cell>87.7</cell><cell>-</cell></row><row><cell></cell><cell>CvT-W24 [93]</cell><cell>384</cell><cell cols="2">277M 193.2G</cell><cell>87.7</cell><cell>-</cell></row><row><cell></cell><cell>R+ViT-L/16 [22]</cell><cell>384</cell><cell>330M</cell><cell>-</cell><cell>-</cell><cell>87.12</cell></row><row><cell></cell><cell>CoAtNet-3 [19]</cell><cell>384</cell><cell cols="2">168M 107.4G</cell><cell>87.6</cell><cell>88.52</cell></row><row><cell></cell><cell>CoAtNet-3 [19]</cell><cell>512</cell><cell>168M</cell><cell>214G</cell><cell>87.9</cell><cell>88.81</cell></row><row><cell></cell><cell>CoAtNet-4 [19]</cell><cell>512</cell><cell cols="2">275M 360.9G</cell><cell>88.1</cell><cell>89.11</cell></row><row><cell>Hybrid</cell><cell>CoAtNet-5 [19]</cell><cell>512</cell><cell>688M</cell><cell>812G</cell><cell>-</cell><cell>89.77</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell>384</cell><cell>119M</cell><cell>74.2G</cell><cell>88.24</cell><cell>88.69</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell>384</cell><cell cols="2">212M 128.7G</cell><cell>88.32</cell><cell>89.12</cell></row><row><cell></cell><cell>MaxViT-XL</cell><cell>384</cell><cell cols="2">475M 293.7G</cell><cell>88.51</cell><cell>89.36</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell>512</cell><cell cols="2">119M 138.3G</cell><cell>88.38</cell><cell>88.82</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell>512</cell><cell cols="2">212M 245.2G</cell><cell>88.46</cell><cell>89.41</cell></row><row><cell></cell><cell>MaxViT-XL</cell><cell>512</cell><cell cols="2">475M 535.2G</cell><cell>88.70</cell><cell>89.53</cell></row><row><cell cols="2">achieves top-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>JFT-300M. We also trained our model on a larger-scale proprietary dataset JFT-300M which contains ?300 million weakly labeled images. As shown in Table 3 and Figure 4b, our model is also scalable to massive scale training data -MaxViT-XL achieves a high accuracy of 89.53% with 475 million parameters, outperforming previous models under comparable model sizes. Due to resource</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 ,</head><label>4</label><figDesc>AP , AP 50 , and AP 75 are reported for comparison. The parameters and FLOPs are also reported as a reference for model complexity. The MaxViT backbone models, used in object detection and segmentation tasks, outperform all other backbones by large margins, including Swin, ConvNeXt, and UViT at various model sizes with respect to both accuracy and efficiency. Note that MaxViT-S outperforms other base-level models (e.g., Swin-B, UViT-B), with about 40% less computational cost.</figDesc><table /><note>4.3 Image Aesthetic Assessment.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of two-stage object detection and instance segmentation on COCO2017. All models are pretrained on ImageNet-1K. Resolution AP AP50 AP75 AP m AP m 50 AP m 75 FLOPs Pars. ?ResNet-50 [29] 1280?800 46.3 64.3 50.5 40.1 61.7 43.4 739G 82M ?X101-32 [95] 1280?800 48.1 66.5 52.4 41.6 63.9 45.2 819G 101M ?X101-64 [95] 1280?800 48.3 66.4 52.3 41.7 64.0 45.1 972G 140M ?ConvNeXt-T [57] 1280?800 50.4 69.1 54.8 43.7 66.5 47.3 741G -?ConvNeXt-S [57] 1280?800 51.9 70.8 56.5 45.0 68.4 49.1 827G -?ConvNeXt-B [57] 1280?800 52.7 71.3 57.2 45.6 68.9 49.5 964G -</figDesc><table><row><cell>Backbone</cell><cell></cell></row><row><cell>?Swin-T [56]</cell><cell>1280?800 50.4 69.2 54.7 43.7 66.6 47.3 745G 86M</cell></row><row><cell>?Swin-S [56]</cell><cell>1280?800 51.9 70.7 56.3 45.0 68.2 48.8 838G 107M</cell></row><row><cell>?Swin-B [56]</cell><cell>1280?800 51.9 70.5 56.4 45.0 68.1 48.9 982G 145M</cell></row><row><cell>?UViT-T [14]</cell><cell>896?896 51.1 70.4 56.2 43.6 67.7 47.2 613G 47M</cell></row><row><cell>?UViT-S [14]</cell><cell>896?896 51.4 70.8 56.2 44.1 68.2 48.0 744G 54M</cell></row><row><cell>?UViT-B [14]</cell><cell>896?896 52.5 72.0 57.6 44.3 68.7 48.3 975G 74M</cell></row><row><cell>?As-ViT-L [15]</cell><cell>1024?1024 52.7 72.3 57.9 45.2 69.7 49.8 1094G 139M</cell></row><row><cell>MaxViT-T</cell><cell>896?896 52.1 71.9 56.8 44.6 69.1 48.4 475G 69M</cell></row><row><cell>MaxViT-S</cell><cell>896?896 53.1 72.5 58.1 45.4 69.8 49.5 595G 107M</cell></row><row><cell>MaxViT-B</cell><cell></cell></row></table><note>896?896 53.4 72.9 58.1 45.7 70.3 50.0 856G 157M</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Image aesthetic assessment results on the AVA benchmark<ref type="bibr" target="#b60">[61]</ref>. PLCC and SRCC represent the Pearson's linear and Spearman's rank correlation coefficients.</figDesc><table><row><cell>Model</cell><cell>Res.</cell><cell>Pars. PLCC? SRCC?</cell></row><row><cell>?NIMA [77]</cell><cell>224</cell><cell>56M 0.636 0.612</cell></row><row><cell>?EffNet-B0 [79]</cell><cell>224</cell><cell>5.3M 0.642 0.620</cell></row><row><cell>?AFDC [10]</cell><cell>224</cell><cell>44.5M 0.671 0.649</cell></row><row><cell>?ViT-S/32 [43]</cell><cell>384</cell><cell>22M 0.665 0.656</cell></row><row><cell>?ViT-B/32 [43]</cell><cell>384</cell><cell>88M 0.664 0.664</cell></row><row><cell cols="3">?MUSIQ [43] 224 ? 512 27M 0.720 0.706</cell></row><row><cell>MaxViT-T</cell><cell>224</cell><cell>31M 0.707 0.685</cell></row><row><cell>MaxViT-T</cell><cell>384</cell><cell>31M 0.736 0.699</cell></row><row><cell>MaxViT-T</cell><cell>512</cell><cell>31M 0.745 0.708</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of image generation on Ima-geNet. ? used a pre-trained Im-ageNet classifier. R1 [103] 37.18 19.55 ?HiT [103] (32.9M) 30.83 21.64 MaxViT (18.6M) 30.77 22.58</figDesc><table><row><cell>Model</cell><cell cols="2">FID? IS?</cell></row><row><cell>?GAN [26]</cell><cell cols="2">54.17 14.01</cell></row><row><cell>?PacGAN2 [54]</cell><cell cols="2">57.51 13.50</cell></row><row><cell>?MGAN [34]</cell><cell cols="2">50.90 14.44</cell></row><row><cell>?LogoGAN [68] ?</cell><cell cols="2">38.41 18.86</cell></row><row><cell>?SS-GAN [12]</cell><cell>43.87</cell><cell>-</cell></row><row><cell>?SC GAN [55]</cell><cell cols="2">40.30 15.82</cell></row><row><cell>?ConvNet-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Block order study. C, BA, GA represent MBConv, block-, and grid-attention respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Sequential vs. parallel. We compared our model with modified parallel multi-axis scheme P aral-.</figDesc><table><row><cell>Model</cell><cell cols="2">Pars. FLOPs Top-1 acc.</cell></row><row><cell cols="2">MaxViT-T 30.9M 5.6G</cell><cell>83.62</cell></row><row><cell>P aral-T</cell><cell cols="2">34.5M 6.2G 82.64(-0.98)</cell></row><row><cell cols="2">MaxViT-S 68.9M 11.7G</cell><cell>84.45</cell></row><row><cell>P aral-S</cell><cell cols="2">76.9M 13.0G 83.45(-1.00)</cell></row><row><cell cols="2">MaxViT-B 119.4M 24.2G</cell><cell>84.95</cell></row><row><cell>P aral-B</cell><cell cols="2">133.4M 26.9G 83.70(-1.25)</cell></row><row><cell cols="2">MaxViT-L 211.8M 43.9G</cell><cell>85.17</cell></row><row><cell>P aral-L</cell><cell cols="2">236.6M 48.8G 83.54(-1.63)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Sec. A describes the detailed architectures of MaxViT for image classification (Sec. A.1), object detection and segmentation (Sec. A.2), image aesthetics assessment (Sec. A.3), and image generation (Sec. A.4).</figDesc><table><row><cell>-Sec. B presents complete training settings and hyperparameters for image</cell></row><row><cell>classification (Sec. B.1), object detection and segmentation (Sec. B.2), image</cell></row><row><cell>aesthetics assessment (Sec. B.3), and image generation (Sec. B.4).</cell></row><row><cell>-Sec. C demonstrates comprehensive experimental results, including image</cell></row><row><cell>classification on ImageNet-1K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Detailed architectural specifications for MaxViT families.</figDesc><table><row><cell></cell><cell>dsp. rate (out size)</cell><cell></cell><cell></cell><cell>MaxViT-T</cell><cell></cell><cell></cell><cell></cell><cell>MaxViT-S</cell></row><row><cell>stem</cell><cell>2? (112?112)</cell><cell></cell><cell></cell><cell>3?3, 64, stride 2 3?3, 64, stride 1</cell><cell></cell><cell></cell><cell></cell><cell>3?3, 64, stride 2 3?3, 64, stride 1</cell></row><row><cell>S1</cell><cell>4? (56 ? 56)</cell><cell cols="2">? ?</cell><cell>MBConv, 64, E 4, R 4 Rel-MSA, G 7?7, H 2 Rel-MSA, P 7?7, H 2</cell><cell cols="2">? ? ? 2</cell><cell cols="2">? ?</cell><cell>MBConv, 96, E 4, R 4 Rel-MSA, G 7?7, H 3 Rel-MSA, P 7?7, H 3</cell><cell>? ? ? 2</cell></row><row><cell>S2</cell><cell>8? (28 ? 28)</cell><cell>? ?</cell><cell cols="3">MBConv, 128, E 4, R 4 Rel-MSA, G 7?7, H 4 Rel-MSA, P 7?7, H 4</cell><cell>? ? ? 2</cell><cell cols="2">? ?</cell><cell>MBConv, 192, E 4, R 4 Rel-MSA, G 7?7, H 6 Rel-MSA, P 7?7, H 6</cell><cell>? ? ? 2</cell></row><row><cell>S3</cell><cell>16? (14 ? 14)</cell><cell>? ?</cell><cell cols="3">MBConv, 256, E 4, R 4 Rel-MSA, G 7?7, H 8 Rel-MSA, P 7?7, H 8</cell><cell>? ? ? 5</cell><cell cols="2">? ?</cell><cell>MBConv, 384, E 4, R 4 Rel-MSA, G 7?7, H 12 Rel-MSA, P 7?7, H 12</cell><cell>? ? ? 5</cell></row><row><cell>S4</cell><cell>32? (7 ? 7)</cell><cell>? ?</cell><cell cols="3">MBConv, 512, E 4, R 4 Rel-MSA, G 7?7, H 16 Rel-MSA, P 7?7, H 16</cell><cell>? ? ? 2</cell><cell cols="2">? ?</cell><cell>MBConv, 768, E 4, R 4 Rel-MSA, G 7?7, H 24 Rel-MSA, P 7?7, H 24</cell><cell>? ? ? 2</cell></row><row><cell></cell><cell>dsp. rate (out size)</cell><cell></cell><cell></cell><cell>MaxViT-B</cell><cell></cell><cell></cell><cell></cell><cell>MaxViT-L</cell></row><row><cell>stem</cell><cell>2? (112?112)</cell><cell></cell><cell></cell><cell>3?3, 64, stride 2 3?3, 64, stride 1</cell><cell></cell><cell></cell><cell></cell><cell>3?3, 128, stride 2 3?3, 128, stride 1</cell></row><row><cell>S1</cell><cell>4? (56 ? 56)</cell><cell cols="2">? ?</cell><cell>MBConv, 96, E 4, R 4 Rel-MSA, G 7?7, H 3 Rel-MSA, P 7?7, H 3</cell><cell cols="2">? ? ? 2</cell><cell cols="2">? ?</cell><cell>MBConv, 128, E 4, R 4 Rel-MSA, G 7?7, H 4 Rel-MSA, P 7?7, H 4</cell><cell>? ? ? 2</cell></row><row><cell>S2</cell><cell>8? (28 ? 28)</cell><cell>? ?</cell><cell cols="3">MBConv, 192, E 4, R 4 Rel-MSA, G 7?7, H 6 Rel-MSA, P 7?7, H 6</cell><cell>? ? ? 6</cell><cell cols="2">? ?</cell><cell>MBConv, 256, E 4, R 4 Rel-MSA, G 7?7, H 8 Rel-MSA, P 7?7, H 8</cell><cell>? ? ? 6</cell></row><row><cell>S3</cell><cell>16? (14 ? 14)</cell><cell>? ?</cell><cell cols="2">MBConv, 384, E 4, R 4 Rel-MSA, G 7?7, H 12 Rel-MSA, P 7?7, H 12</cell><cell></cell><cell>? ? ?14</cell><cell>? ?</cell><cell>MBConv, 512, E 4, R 4 Rel-MSA, G 7?7, H 16 Rel-MSA, P 7?7, H 16</cell><cell>? ? ? 14</cell></row><row><cell>S4</cell><cell>32? (7 ? 7)</cell><cell>? ?</cell><cell cols="3">MBConv, 768, E 4, R 4 Rel-MSA, G 7?7, H 24 Rel-MSA, P 7?7, H 24</cell><cell>? ? ? 2</cell><cell>? ?</cell><cell>MBConv, 1024, E 4, R 4 Rel-MSA, G 7?7, H 32 Rel-MSA, P 7?7, H 32</cell><cell>? ? ? 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Detailed hyperparameters used in ImageNet-1K experiments. Multiple values separated by '/' are for each model size respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Coco Detection and SegmentationWe evaluated MaxViT on the COCO2017<ref type="bibr" target="#b52">[53]</ref> object bounding box detection and instance segmentation tasks. The dataset contains 118K training and 5K validation samples. All the MaxViT backbones used are pretrained on ImageNet-1k at resolution 224 ? 224. These pretrained checkpoints are then used as the warm-up weights for fine-tuning the detection and segmentation tasks. For both tasks, the input images are resized to 896 ? 896. The training is conducted with a batch size of 256, using the AdamW<ref type="bibr" target="#b58">[59]</ref> optimizer with learning rate of 1e-3, 3e-3, 3e-3, and stochastic depth of 0.8, 0.3, 0.3 for MaxViT-T/S/B, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1/0.2/0.2</cell></row><row><cell>Center crop</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell></row><row><cell>RandAugment</cell><cell>2, 15</cell><cell>2, 15</cell><cell>2, 5</cell><cell>2, 15</cell><cell>2, 5</cell><cell>2, 15</cell></row><row><cell>Mixup alpha</cell><cell>0.8</cell><cell>0.8</cell><cell>None</cell><cell>None</cell><cell>None</cell><cell>None</cell></row><row><cell>Loss type</cell><cell>Softmax</cell><cell>Softmax</cell><cell>Sigmoid</cell><cell>Softmax</cell><cell>Sigmoid</cell><cell>Softmax</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0001</cell><cell>0.1</cell><cell>0</cell><cell>0.1</cell></row><row><cell>Train epochs</cell><cell>300</cell><cell>30</cell><cell>90</cell><cell>30</cell><cell>14</cell><cell>30</cell></row><row><cell>Train batch size</cell><cell>4096</cell><cell>512</cell><cell>4096</cell><cell>512</cell><cell>4096</cell><cell>512</cell></row><row><cell>Optimizer type</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>Peak learning rate</cell><cell>3e-3</cell><cell>5e-5</cell><cell>1e-3</cell><cell>5e-5</cell><cell>1e-3</cell><cell>5e-5</cell></row><row><cell>Min learning rate</cell><cell>1e-5</cell><cell>5e-5</cell><cell>1e-5</cell><cell>5e-5</cell><cell>1e-5</cell><cell>5e-5</cell></row><row><cell>Warm-up</cell><cell>10K steps</cell><cell>None</cell><cell>5 epochs</cell><cell>None</cell><cell>20K steps</cell><cell>None</cell></row><row><cell>LR decay schedule</cell><cell>Cosine</cell><cell>None</cell><cell>Linear</cell><cell>None</cell><cell>Linear</cell><cell>None</cell></row><row><cell>Weight decay rate</cell><cell>0.05</cell><cell>1e-8</cell><cell>0.01</cell><cell>1e-8</cell><cell>0.01</cell><cell>1e-8</cell></row><row><cell>Gradient clip</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>EMA decay rate</cell><cell>None</cell><cell>0.9999</cell><cell>None</cell><cell>0.9999</cell><cell>None</cell><cell>0.9999</cell></row><row><cell>B.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Complete performance comparison under ImageNet-1K only setting.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Eval size</cell><cell cols="2">Params FLOPs</cell><cell>throughput (img/s)</cell><cell>ImageNet top-1 acc.</cell></row><row><cell></cell><cell>?EffNet-B3 [79]</cell><cell>300</cell><cell>12M</cell><cell>1.8G</cell><cell>732.1</cell><cell>81.6</cell></row><row><cell></cell><cell>?EffNet-B4 [79]</cell><cell>380</cell><cell>19M</cell><cell>4.2G</cell><cell>349.4</cell><cell>82.9</cell></row><row><cell></cell><cell>?EffNet-B5 [79]</cell><cell>456</cell><cell>30M</cell><cell>9.9G</cell><cell>169.1</cell><cell>83.6</cell></row><row><cell></cell><cell>?EffNet-B6 [79]</cell><cell>528</cell><cell>43M</cell><cell>19.0G</cell><cell>96.9</cell><cell>84.0</cell></row><row><cell></cell><cell>?EffNet-B7 [79]</cell><cell>600</cell><cell>66M</cell><cell>37.0G</cell><cell>55.1</cell><cell>84.3</cell></row><row><cell></cell><cell>?RegNetY-8GF [62]</cell><cell>224</cell><cell>39M</cell><cell>8.0G</cell><cell>591.6</cell><cell>81.7</cell></row><row><cell></cell><cell cols="2">?RegNetY-16GF [62] 224</cell><cell>84M</cell><cell>16.0G</cell><cell>334.7</cell><cell>82.9</cell></row><row><cell></cell><cell>?NFNet-F0 [5]</cell><cell>256</cell><cell>72M</cell><cell>12.4G</cell><cell>533,.3</cell><cell>83.6</cell></row><row><cell></cell><cell>?NFNet-F1 [5]</cell><cell>320</cell><cell>132M</cell><cell>35.5G</cell><cell>228.5</cell><cell>84.7</cell></row><row><cell>ConvNets</cell><cell>?NFNet-F2 [5] ?NFNet-F3 [5]</cell><cell>352 416</cell><cell>194M 255M</cell><cell>62.6G 114.7G</cell><cell>129.0 78.8</cell><cell>85.1 85.7</cell></row><row><cell></cell><cell>?NFNet-F4 [5]</cell><cell>512</cell><cell>316M</cell><cell>215.2G</cell><cell>51.7</cell><cell>85.9</cell></row><row><cell></cell><cell>?NFNet-F5 [5]</cell><cell>544</cell><cell>377M</cell><cell>289.8G</cell><cell>-</cell><cell>86.0</cell></row><row><cell></cell><cell>?EffNetV2-S [80]</cell><cell>384</cell><cell>24M</cell><cell>8.8G</cell><cell>666.6</cell><cell>83.9</cell></row><row><cell></cell><cell>?EffNetV2-M [80]</cell><cell>380</cell><cell>55M</cell><cell>24.0G</cell><cell>280.7</cell><cell>85.1</cell></row><row><cell></cell><cell>?EffNetV2-L [80]</cell><cell>480</cell><cell>121M</cell><cell>53.0G</cell><cell>163.2</cell><cell>85.7</cell></row><row><cell></cell><cell>?ConvNeXt-T [57]</cell><cell>224</cell><cell>29M</cell><cell>4.5G</cell><cell>774.7</cell><cell>82.1</cell></row><row><cell></cell><cell>?ConvNeXt-S [57]</cell><cell>224</cell><cell>50M</cell><cell>8.7G</cell><cell>447.1</cell><cell>83.1</cell></row><row><cell></cell><cell>?ConvNeXt-B [57]</cell><cell>224</cell><cell>89M</cell><cell>15.4G</cell><cell>292.1</cell><cell>83.8</cell></row><row><cell></cell><cell>?ConvNeXt-L [57]</cell><cell>384</cell><cell>198M</cell><cell>101.0G</cell><cell>50.4</cell><cell>85.5</cell></row><row><cell></cell><cell>?ViT-B/32 [22]</cell><cell>384</cell><cell>86M</cell><cell>55.4G</cell><cell>85.9</cell><cell>77.9</cell></row><row><cell></cell><cell>?ViT-B/16 [22]</cell><cell>384</cell><cell>307M</cell><cell>190.7G</cell><cell>27.3</cell><cell>76.5</cell></row><row><cell></cell><cell>?DeiT-S [81]</cell><cell>224</cell><cell>22M</cell><cell>4.6G</cell><cell>940.4</cell><cell>79.8</cell></row><row><cell></cell><cell>?DeiT-B [81]</cell><cell>224</cell><cell>86M</cell><cell>17.5G</cell><cell>292.3</cell><cell>81.8</cell></row><row><cell></cell><cell>?DeiT-B [81]</cell><cell>384</cell><cell>86M</cell><cell>55.4G</cell><cell>85.9</cell><cell>83.1</cell></row><row><cell></cell><cell>?CaiT-S36 [82]</cell><cell>224</cell><cell>68M</cell><cell>13.9G</cell><cell>-</cell><cell>83.3</cell></row><row><cell></cell><cell>?CaiT-M24 [82]</cell><cell>224</cell><cell>186M</cell><cell>36.0G</cell><cell>-</cell><cell>83.4</cell></row><row><cell></cell><cell>?CaiT-M24 [82]</cell><cell>384</cell><cell>186M</cell><cell>116.1G</cell><cell>-</cell><cell>84.5</cell></row><row><cell></cell><cell>?DeepViT-S [105]</cell><cell>224</cell><cell>27M</cell><cell>6.2G</cell><cell>-</cell><cell>82.3</cell></row><row><cell>ViTs</cell><cell>?DeepViT-L [105] ?T2T-ViT-14 [101]</cell><cell>224 224</cell><cell>55M 22M</cell><cell>12.5G 6.1G</cell><cell>--</cell><cell>83.1 81.7</cell></row><row><cell></cell><cell>?T2T-ViT-19 [101]</cell><cell>224</cell><cell>39M</cell><cell>9.8G</cell><cell>-</cell><cell>82.2</cell></row><row><cell></cell><cell>?T2T-ViT-24 [101]</cell><cell>224</cell><cell>64M</cell><cell>15.0G</cell><cell>-</cell><cell>82.6</cell></row><row><cell></cell><cell>?Swin-T [56]</cell><cell>224</cell><cell>29M</cell><cell>4.5G</cell><cell>755.2</cell><cell>81.3</cell></row><row><cell></cell><cell>?Swin-S [56]</cell><cell>224</cell><cell>50M</cell><cell>8.7G</cell><cell>436.9</cell><cell>83.0</cell></row><row><cell></cell><cell>?Swin-B [56]</cell><cell>384</cell><cell>88M</cell><cell>47.0G</cell><cell>84.7</cell><cell>84.5</cell></row><row><cell></cell><cell>?CSwin-B [21]</cell><cell>224</cell><cell>78M</cell><cell>15.0G</cell><cell>250</cell><cell>84.2</cell></row><row><cell></cell><cell>?CSwin-B [21]</cell><cell>384</cell><cell>78M</cell><cell>47.0G</cell><cell>-</cell><cell>85.4</cell></row><row><cell></cell><cell>?Focal-S [99]</cell><cell>224</cell><cell>51M</cell><cell>9.1G</cell><cell>-</cell><cell>83.5</cell></row><row><cell></cell><cell>?Focal-B [99]</cell><cell>224</cell><cell>90M</cell><cell>16.0G</cell><cell>-</cell><cell>83.8</cell></row><row><cell></cell><cell>CvT-13 [93]</cell><cell>224</cell><cell>20M</cell><cell>4.5G</cell><cell>-</cell><cell>81.6</cell></row><row><cell></cell><cell>CvT-21 [93]</cell><cell>224</cell><cell>32M</cell><cell>7.1G</cell><cell>-</cell><cell>82.5</cell></row><row><cell></cell><cell>CvT-21 [93]</cell><cell>384</cell><cell>32M</cell><cell>24.9G</cell><cell>-</cell><cell>83.3</cell></row><row><cell></cell><cell>CoAtNet-0 [19]</cell><cell>224</cell><cell>25M</cell><cell>4.2G</cell><cell>534.5</cell><cell>81.6</cell></row><row><cell></cell><cell>CoAtNet-1 [19]</cell><cell>224</cell><cell>42M</cell><cell>8.4G</cell><cell>336.5</cell><cell>83.3</cell></row><row><cell></cell><cell>CoAtNet-2 [19]</cell><cell>224</cell><cell>75M</cell><cell>15.7G</cell><cell>247.6</cell><cell>84.1</cell></row><row><cell></cell><cell>CoAtNet-3 [19]</cell><cell>384</cell><cell>168M</cell><cell>107.4G</cell><cell>48.5</cell><cell>85.8</cell></row><row><cell></cell><cell>CoAtNet-3 [19]</cell><cell>512</cell><cell>168M</cell><cell>203.1G</cell><cell>22.4</cell><cell>86.0</cell></row><row><cell>Hybrid</cell><cell>MaxViT-T</cell><cell>224</cell><cell>31M</cell><cell>5.6G</cell><cell>349.6</cell><cell>83.62</cell></row><row><cell></cell><cell>MaxViT-S</cell><cell>224</cell><cell>69M</cell><cell>11.7G</cell><cell>242.5</cell><cell>84.45</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell>224</cell><cell>120M</cell><cell>23.4G</cell><cell>133.6</cell><cell>84.95</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell>224</cell><cell>212M</cell><cell>43.9G</cell><cell>99.4</cell><cell>85.17</cell></row><row><cell></cell><cell>MaxViT-T</cell><cell>384</cell><cell>31M</cell><cell>17.7G</cell><cell>121.9</cell><cell>85.24</cell></row><row><cell></cell><cell>MaxViT-S</cell><cell>384</cell><cell>69M</cell><cell>36.1G</cell><cell>82.7</cell><cell>85.74</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell>384</cell><cell>120M</cell><cell>74.2G</cell><cell>45.8</cell><cell>86.34</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell>384</cell><cell>212M</cell><cell>133.1G</cell><cell>34.3</cell><cell>84.40</cell></row><row><cell></cell><cell>MaxViT-T</cell><cell>512</cell><cell>31M</cell><cell>33.7G</cell><cell>63.8</cell><cell>85.72</cell></row><row><cell></cell><cell>MaxViT-S</cell><cell>512</cell><cell>69M</cell><cell>67.6G</cell><cell>43.3</cell><cell>86.19</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell>512</cell><cell>120M</cell><cell>138.5G</cell><cell>24.0</cell><cell>86.66</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell>512</cell><cell>212M</cell><cell>245.4G</cell><cell>17.8</cell><cell>86.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>Complete performance comparison for ImageNet-21K and JFT pretrained models.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Eval size</cell><cell cols="2">Params FLOPs</cell><cell cols="2">IN-1K top-1 acc. 21K?1K JFT?1K</cell></row><row><cell></cell><cell>?BiT-R-101x3 [46]</cell><cell>384</cell><cell>388M</cell><cell>204.6G</cell><cell>84.4</cell><cell></cell></row><row><cell></cell><cell>?BiT-R-152x4 [46]</cell><cell>480</cell><cell>937M</cell><cell>840.5G</cell><cell>85.4</cell><cell></cell></row><row><cell></cell><cell>?EffNetV2-S [80]</cell><cell>384</cell><cell>24M</cell><cell>8.8G</cell><cell>85.0</cell><cell></cell></row><row><cell></cell><cell>?EffNetV2-M [80]</cell><cell>480</cell><cell>55M</cell><cell>24.0G</cell><cell>86.1</cell><cell></cell></row><row><cell>ConvNets</cell><cell>?EffNetV2-L [80] ?EffNetV2-XL [80]</cell><cell>480 512</cell><cell>121M 208M</cell><cell>53.0G 94.0G</cell><cell>86.8 87.3</cell><cell></cell></row><row><cell></cell><cell>?NFNet-F4+ [5]</cell><cell>512</cell><cell>527M</cell><cell>367G</cell><cell>-</cell><cell>89.20</cell></row><row><cell></cell><cell>?ConvNeXt-B [57]</cell><cell>384</cell><cell>89M</cell><cell>45.1G</cell><cell>86.8</cell><cell></cell></row><row><cell></cell><cell>?ConvNeXt-L [57]</cell><cell>384</cell><cell>198M</cell><cell>101.0G</cell><cell>87.5</cell><cell></cell></row><row><cell></cell><cell>?ConvNeXt-XL [57]</cell><cell>384</cell><cell>350M</cell><cell>179.0G</cell><cell>87.8</cell><cell></cell></row><row><cell></cell><cell>?ViT-B/16 [22]</cell><cell>384</cell><cell>87M</cell><cell>55.5G</cell><cell>84.0</cell><cell></cell></row><row><cell></cell><cell>?ViT-L/16 [22]</cell><cell>384</cell><cell>305M</cell><cell>191.1G</cell><cell>85.2</cell><cell></cell></row><row><cell></cell><cell>?ViT-L/16 [22]</cell><cell>512</cell><cell>305M</cell><cell>364G</cell><cell>-</cell><cell>87.76</cell></row><row><cell></cell><cell>?ViT-H/14 [22]</cell><cell>518</cell><cell>632M</cell><cell>1021G</cell><cell>-</cell><cell>88.55</cell></row><row><cell></cell><cell>?HaloNet-H4 [84]</cell><cell>384</cell><cell>85M</cell><cell>-</cell><cell>85.6</cell><cell></cell></row><row><cell>ViTs</cell><cell>?HaloNet-H4 [84] ?Swin-B [56]</cell><cell>512 384</cell><cell>85M 88M</cell><cell>-47.0G</cell><cell>85.8 86.4</cell><cell></cell></row><row><cell></cell><cell>?Swin-L [56]</cell><cell>384</cell><cell>197M</cell><cell>103.9G</cell><cell>87.3</cell><cell></cell></row><row><cell></cell><cell>?SwinV2-B [56]</cell><cell>384</cell><cell>88M</cell><cell>-</cell><cell>87.1</cell><cell></cell></row><row><cell></cell><cell>?SwinV2-L [56]</cell><cell>384</cell><cell>197M</cell><cell>-</cell><cell>87.7</cell><cell></cell></row><row><cell></cell><cell>?CSwin-B [21]</cell><cell>384</cell><cell>78M</cell><cell>47.0G</cell><cell>87.0</cell><cell></cell></row><row><cell></cell><cell>?CSwin-L [21]</cell><cell>384</cell><cell>173M</cell><cell>96.8G</cell><cell>87.5</cell><cell></cell></row><row><cell></cell><cell>CvT-13 [93]</cell><cell>384</cell><cell>20M</cell><cell>16.0G</cell><cell>83.3</cell><cell></cell></row><row><cell></cell><cell>CvT-21 [93]</cell><cell>384</cell><cell>32M</cell><cell>25.0G</cell><cell>84.9</cell><cell></cell></row><row><cell></cell><cell>CvT-W24 [93]</cell><cell>384</cell><cell>277M</cell><cell>193.2G</cell><cell>87.7</cell><cell></cell></row><row><cell></cell><cell cols="2">ResNet+ViT-L/16 [22] 384</cell><cell>330M</cell><cell>-</cell><cell>-</cell><cell>87.12</cell></row><row><cell></cell><cell>CoAtNet-2 [19]</cell><cell>384</cell><cell>75M</cell><cell>49.8G</cell><cell>87.1</cell><cell></cell></row><row><cell></cell><cell>CoAtNet-3 [19]</cell><cell>384</cell><cell>168M</cell><cell>107.4G</cell><cell>87.6</cell><cell></cell></row><row><cell></cell><cell>CoAtNet-4 [19]</cell><cell>384</cell><cell>275M</cell><cell>189.5G</cell><cell>87.9</cell><cell></cell></row><row><cell></cell><cell>CoAtNet-2 [19]</cell><cell>512</cell><cell>75M</cell><cell>96.7G</cell><cell>87.3</cell><cell></cell></row><row><cell>Hybrid</cell><cell>CoAtNet-3 [19]</cell><cell>512</cell><cell>168M</cell><cell>203.1G</cell><cell>87.9</cell><cell>88.81</cell></row><row><cell></cell><cell>CoAtNet-4 [19]</cell><cell>512</cell><cell>275M</cell><cell>360.9G</cell><cell>88.1</cell><cell>89.11</cell></row><row><cell></cell><cell>CoAtNet-5 [19]</cell><cell>512</cell><cell>688M</cell><cell>812G</cell><cell>-</cell><cell>89.77</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell>384</cell><cell>119M</cell><cell>74.2G</cell><cell>88.24</cell><cell>88.69</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell>384</cell><cell>212M</cell><cell>128.7G</cell><cell>88.32</cell><cell>89.12</cell></row><row><cell></cell><cell>MaxViT-XL</cell><cell>384</cell><cell>475M</cell><cell>293.7G</cell><cell>88.51</cell><cell>89.36</cell></row><row><cell></cell><cell>MaxViT-B</cell><cell>512</cell><cell>119M</cell><cell>138.3G</cell><cell>88.38</cell><cell>88.82</cell></row><row><cell></cell><cell>MaxViT-L</cell><cell>512</cell><cell>212M</cell><cell>245.2G</cell><cell>88.46</cell><cell>89.41</cell></row><row><cell></cell><cell>MaxViT-XL</cell><cell>512</cell><cell>475M</cell><cell>535.2G</cell><cell>88.70</cell><cell>89.53</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">MaxViT: Multi-Axis Vision Transformer</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We thank Xianzhi Du and Wuyang Chen for extensive help on experiments. We also thank Hanxiao Liu, Zihang Dai, Anurag Arnab, Huiwen Chang, Junjie Ke, Mauricio Delbracio, Sungjoon Choi, and Irene Zhu for valuable discussions and help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revisiting resnets: Improved training and scaling strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proxiqa: A proxy approach to perceptual optimization of learned image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive fractional dilated convolution network for image aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01365</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward a unified model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A simple single-scale vision transformer for object localization and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/2112.09747</idno>
		<ptr target="https://arxiv.org/abs/2112.0974711" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Auto-scaling vision transformers without training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11921</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v15/coates11a.html4" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research</title>
		<editor>Gordon, G., Dunson, D., Dud?k, M.</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics. Machine Learning Research<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2011-04" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="11" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Instaboost: Boosting instance segmentation via probability map guided copy-pasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.322</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.322" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<title level="m">Axial attention in multidimensional transformers</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mgan: Training generative adversarial nets with multiple generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generative adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>PMLR (2021) 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4487" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transgan: Two pure transformers can make one strong gan, and that can scale up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Musiq: Multi-scale image quality transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transformers in vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Comisr: Compressioninformed video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pacgan: The power of two samples in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Diverse image generation via self-conditioned gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ava: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf10" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Einops: Clear and reliable tensor manipulations with einsteinlike notation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogozhnikov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=oapKSVM2bcj6" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Logo synthesis and manipulation with clustered generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<title level="m">Neural image assessment. IEEE transactions on image processing</title>
		<meeting><address><addrLine>Nima</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning to resize images for computer vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>7, 8, 9</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note>PMLR (2021) 2, 4, 8</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02973</idno>
		<title level="m">Maxim: Multi-axis mlp for image processing</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Rich features for perceptual quality assessment of ugc videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Deblurring via stochastic refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10430</idno>
		<title level="m">Pay less attention with lightweight and dynamic convolutions</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note>2021) 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Cobevt: Cooperative bird&apos;s eye view semantic segmentation with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02202</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10638</idno>
		<title level="m">V2x-vit: Vehicleto-everything cooperative perception with vision transformer</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Co-scale conv-attentional image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Focal selfattention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<title level="m">Scaling vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Improved transformer for high-resolution gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05518</idno>
		<title level="m">Tracking objects as pixel-wise distributions</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
