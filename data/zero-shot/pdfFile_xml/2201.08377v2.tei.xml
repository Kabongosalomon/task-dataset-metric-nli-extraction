<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OMNIVORE: A Single Model for Many Visual Modalities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OMNIVORE: A Single Model for Many Visual Modalities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Equal technical contribution.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image (RGB)   Depth map (D)   Single-view 3D (RGBD) Video (RGBT) <ref type="figure">Figure 1</ref>. OMNIVORE is a single vision model for many different visual modalities. It learns to construct representations that are aligned across visual modalities, without requiring training data that specifies correspondences between those modalities. Using OMNIVORE's shared visual representation, we successfully identify nearest neighbors of left: an image (ImageNet-1K validation set) in vision datasets that contain right: depth maps (ImageNet-1K training set), single-view 3D images (ImageNet-1K training set), and videos (Kinetics-400 validation set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our 'OMNIVORE' model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. OMNIVORE is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modalityspecific models of the same size. A single OMNIVORE model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. OMNIVORE's shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. OMNIVORE is a single vision model for many different visual modalities. It learns to construct representations that are aligned across visual modalities, without requiring training data that specifies correspondences between those modalities. Using OMNIVORE's shared visual representation, we successfully identify nearest neighbors of left: an image (ImageNet-1K validation set) in vision datasets that contain right: depth maps (ImageNet-1K training set), single-view 3D images (ImageNet-1K training set), and videos (Kinetics-400 validation set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our 'OMNIVORE' model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. OMNIVORE is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modalityspecific models of the same size. A single OMNIVORE model obtains <ref type="bibr" target="#b87">86</ref>.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. OMNIVORE's shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together. * Equal technical contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computer vision research spans multiple modalities related to our perception of the visual world, such as images, videos, and depth. In general, we study each of these modalities in isolation, and tailor our computer vision models to learn the best features from their specificities. While these modality-specific models achieve impressive performance, sometimes even surpassing humans on their specific tasks, they do not possess the flexibility that a human-like vision system does-the ability to work across modalities. We argue that the first step towards a truly all-purpose vision system is to build models that work seamlessly across modalities, instead of being over-optimized for each modality.</p><p>Beyond their flexibility, such modality-agnostic models have several advantages over their traditional, modalityspecific counterparts. First, a modality-agnostic model can perform cross-modal generalization: it can use what it has learned from one modality to perform recognition in other modalities. For example, it can recognize pumpkins in 3D images even if it has only seen labeled videos of pumpkins. In turn, this allows existing labeled datasets to be used more effectively: it becomes possible to train models on the union of vision datasets with different input modalities. Second, it saves the research and engineering effort spent on optimizing models for a specific modality. For example, image and video models have followed a similar trajectory of evolution, from hand-crafted descriptors <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b55">55]</ref> to convolutional networks <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b92">91]</ref> and, eventually, vision transformers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">21]</ref>; however, each had to be developed and tuned individually. A common architecture would make scientific progress readily available to users of any visual modality. Finally, a model that operates on many visual modalities is naturally multi-modal and can easily leverage new visual sensors as they becomes available. For instance, a modalityagnostic recognition model running on a robot can readily exploit a new depth sensor when it is installed on that robot. Despite such clear advantages, modality-agnostic models have rarely been studied and their performance compared to their modality-specific counterparts has been disappointing. There are many reasons that explain this situation, such as the need for a flexible architecture with enough capacity to learn modality-specific cues from the different modalities; and enough compute to train it on video, images, and single-view 3D simultaneously. This paper develops a modality-agnostic vision model that leverages recent advances in vision architectures <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b51">51]</ref>. The model we develop is "omnivorous" in that it works on three different visual modalities: images, videos, and single-view 3D. Our OMNIVORE model does not use a custom architecture for each visual modality. It performs recognition on all three modalities using the same, shared model parameters. It works by converting each input modality into embeddings of spatio-temporal patches, which are processed by exactly the same Transformer <ref type="bibr" target="#b93">[92]</ref> to produce a representation of the input. We train OMNI-VORE on a collection of standard, off-the-shelf classification datasets that have different input modalities. Unlike prior work <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b78">77]</ref>, our training does not use explicit correspondences between different input modalities.</p><p>Our experiments demonstrate the advantages of our OM-NIVORE models. Surprisingly, we find that OMNIVORE representations generalize well across visual modalities (see <ref type="figure">Figure 1</ref>) even though OMNIVORE was not explicitly trained to model cross-modal correspondences. These capabilities emerge without explicit cross-modal supervision simply due to the parameter sharing between models for different modalities. On standard image, video, and singleview 3D benchmarks, OMNIVORE performs at par with or better than modality-specific vision models with the same number of parameters. at par with recent large transformers on ImageNet-1K, sets a new state-of-the-art on action recognition benchmarks such as EPIC-Kitchens-100, Something Something-v2, and on single-view 3D classification and segmentation benchmarks. We believe our work presents a compelling argument for shifting towards the development of vision models that can operate on any visual modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We build on prior work in ConvNet architectures, Transformers, multi-modal learning, and multi-task learning. ConvNet architectures in vision. ConvNet architectures <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b48">48]</ref> have been popular for many computer vision tasks in images, video, and 3D recognition. 2D convolutions are the main building block in ConvNets for images <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b78">77,</ref><ref type="bibr" target="#b85">84]</ref>, whereas 3D convolutions are used on 3D data <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b32">32]</ref> or are combined with 2D convolutions for recognition of videos <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b91">90,</ref><ref type="bibr" target="#b92">91]</ref>. I3D <ref type="bibr" target="#b12">[13]</ref> introduced a way to "inflate" 2D image convolutions into 3D convolutions, which allows 3D ConvNets for videos and 3D data to leverage image data indirectly via initialization from pretrained image models. Since video and 3D datasets are relatively small, they benefit from inflated pretrained image networks. However, while the inflation technique is applicable only to model finetuning, OMNIVORE models are pretrained jointly on images, videos, and single-view 3D data.</p><p>Transformers in vision. The Transformer architecture <ref type="bibr" target="#b93">[92]</ref> originally proposed for NLP tasks has been successfully applied in computer vision on images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b89">88,</ref><ref type="bibr" target="#b94">93,</ref><ref type="bibr" target="#b95">94]</ref>, video <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b66">66]</ref>, and 3D data <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b104">103]</ref>. Models such as ViT <ref type="bibr" target="#b21">[21]</ref>, Swin <ref type="bibr" target="#b51">[51]</ref>, and MViT <ref type="bibr" target="#b24">[24]</ref> perform competitively on benchmark tasks such as image classification, detection, and video recognition. For example, Swin <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref> and MViT <ref type="bibr" target="#b24">[24]</ref> require minimal changes to be used in image or video recognition tasks. Similarly, the Perceiver <ref type="bibr" target="#b38">[38]</ref> can model image, point cloud, audio, and video inputs. However, all these studies train separate models for each visual modality. Instead, we train a single model on multiple input modalities simultaneously, which equips our model with cross-modal generalization capabilities. Multi-modal learning. Our work uses multiple visual modalities to train the model. Multi-modal learning architectures may involve training separate encoders for each type of input modality. For example, a range of tasks require training separate encoders for images and text <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b59">59]</ref>, for video and audio <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b71">71]</ref>, or for video and optical flow <ref type="bibr" target="#b78">[77]</ref>. Recently, Transformers have been used to fuse multiple modalities: Transformers have been used to fuse features in vision-and-language tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b87">86]</ref> and video-and-audio tasks <ref type="bibr" target="#b64">[64]</ref>, video-and-image tasks <ref type="bibr" target="#b6">[7]</ref>, and even tasks that involve video, audio, and text <ref type="bibr" target="#b0">[1]</ref>. Unlike our work, most prior work assumes that all input modalities are in correspondence and available simultaneously, which restricts them to using only multi-modal datasets. In our work, we train a single model on different visual modalities without assuming simultaneous access to all modalities. This allows us to leverage standard off-the-shelf single-modality vision datasets and we show that using a single shared encoder naturally leads to cross-modal generalization. Multi-task learning. Our work is also related to studies on multi-task learning <ref type="bibr" target="#b13">[14]</ref>, which develop models that output predictions for multiple tasks on the same input <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b103">102]</ref>. Such multi-task learners are known to work well when the target tasks exhibit strong similarities <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b100">99]</ref>. They differ from OMNIVORE in that they operate on a single input modality but are trained to perform multiple tasks. By contrast, our models are trained to perform a single task (i.e., classification) on a variety of input modalities. Other multi-task learners operate on multi-modal inputs <ref type="bibr" target="#b39">[39]</ref>, but they use hand-designed model components for each modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to learn a single model that can operate on three major visual modalities: images, videos, and singleview 3D. Because the model's input modalities have different sizes and layouts-videos have a temporal axis and single-view 3D has an extra depth channel-this poses a challenge in designing the model. To overcome this challenge, we adopt the Transformer <ref type="bibr" target="#b93">[92]</ref> architecture because the self-attention mechanism gracefully handles variablesized inputs. <ref type="figure">Figure 2</ref> presents an overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The OMNIVORE Model</head><p>We convert all visual modalities into a common format by representing them via embeddings. Our model then uses a series of spatio-temporal attention operations to construct a unified representation of the different visual modalities. Input patches. We represent the different types of visual input as a 4D tensor X ? R T ?H?W ?C , where T is the size of the temporal dimension, H and W of the spatial dimensions, and C of the channel dimension. Thus, RGB images I ? R 1?H?W ?3 have T = 1 frame with C = 3 channels, RGB videos V ? R T ?H?W ?3 have T &gt; 1 frames, and single-view 3D images D ? R 1?H?W ?4 have T = 1 frame with three RGB channels and one depth channel.</p><p>We follow <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref> and split the input into a collection of patches. We illustrate this process in <ref type="figure">Figure 2</ref>. Specifically, we convert the visual input X into a set of 4D sub-tensors x of size t ? h ? w ? c. Images I are split into a set of non-overlapping image patches of size 1?h?w ?3. Similarly, videos V are split into a set of nonoverlapping spatio-temporal patches of shape t ? h ? w ? 3. For single-view 3D images D, the image (RGB) and depth (D) channels are converted separately into patches of size 1 ? h ? w ? 3 and 1 ? h ? w ? 1, respectively. Model architecture. Our model f maps the resulting spatio-temporal visual patches into a shared representation ? for images, videos, and single-view 3D. We design the model to enable maximal parameter sharing across visual modalities. The input layer of the model processes each patch x independently, and projects the patches into an embedding e using a linear layer followed by a LayerNorm <ref type="bibr" target="#b5">[6]</ref> (linear+LN). Each patch x of shape t ? h ? w ? c is converted into an embedding of size d. We use the same layers to embed all the three-channel RGB patches, i.e., for image patches, video patches, and patches of the first three channels of a single-view 3D image. We zero-pad the singleframe patches on one side to ensure all patches have the same shape, t ? h ? w ? 3. We use a separate linear+LN layer to embed the depth-channel patches and add its output to the embedding of the corresponding RGB patch.</p><p>We use the same model (parameters) to process all the resulting embeddings. While OMNIVORE can use any vision transformer architecture <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b24">24]</ref> to process the patch embeddings, we use the Swin transformer architecture <ref type="bibr" target="#b51">[51]</ref> as our base model given its strong performance on image and video tasks. We rely on the self-attention <ref type="bibr" target="#b93">[92]</ref> operation for spatio-temporal modeling across the patch embeddings, e. Akin to <ref type="bibr" target="#b51">[51]</ref>, the self-attention involves patch embeddings from spatially and temporally nearby patches. We also use two sets of relative positional encodings: one for the spatial dimension and the other for the temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training the OMNIVORE Model</head><p>The OMNIVORE model f creates a single embedding f (X) = ? for multiple types of visual inputs. We train our model using a collection of classification tasks that provide inputs {(X i , y i )} with a visual input, X i , and a label, y i . For example, we train most OMNIVORE models jointly on the ImageNet-1K dataset for image classification, the Kinetics-400 dataset for action recognition, and the SUN RGB-D dataset for single-view 3D scene classification.</p><p>This approach is similar to multi-task learning <ref type="bibr" target="#b13">[14]</ref> and cross-modal alignment <ref type="bibr" target="#b14">[15]</ref>, but there important differences. In particular, we neither assume that the input observations are aligned (i.e., we do not assume access to correspondences between images, videos, and 3D data) nor do we assume that these datasets share the same label space. To achieve this, we employ dataset-specific linear classification layers on top of the final representation, ?, produced by the model. The training loss of a sample is computed based solely on the output of the classification layer that corresponds to that sample's source dataset. Loss and optimization. We train OMNIVORE to minimize the cross-entropy loss on the training datasets using minibatch SGD. We experiment with two different mini-batch construction strategies for SGD. In our first strategy, we construct mini-batches from each dataset (modality) separately. This strategy is easy to implement but alternating between datasets may potentially lead to training instabilities. Hence, we experiment with a second strategy that constructs mini-batches that mix samples from all datasets. We evaluate both mini-batch construction strategies in ? 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform a series of experiments to assess the effectiveness of OMNIVORE. Specifically, we compare OMNI-VORE models to their modality-specific counterparts and to state-of-the-art models on a variety of recognition tasks. We also ablate several design choices we made in OMNIVORE. Pre-training datasets. We train OMNIVORE on images from the ImageNet-1K dataset <ref type="bibr" target="#b75">[75]</ref>, videos from the Kinetics dataset <ref type="bibr" target="#b42">[42]</ref>, and single-view 3D images from the SUN RGB-D dataset <ref type="bibr" target="#b80">[79]</ref>. We measure the top-1 and top-5 classification accuracy of our models on the respective validation sets. We note that the three datasets have negligible overlap in their visual concepts: ImageNet-1K focuses on object-centric classes, Kinetics-400 on action classes, and SUN RGB-D on indoor scene classes. Images. The ImageNet-1K (IN1K) dataset has ?1.2M training and 50K validation images that comprise 1,000 classes. Videos. The Kinetics-400 (K400) dataset consists of ?240K training and 20K validation video clips that are 10 seconds</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Task #cls #train #val iNaturalist-2018 (iNat18) <ref type="bibr" target="#b36">[36]</ref> Fine-grained cls. 8142 437K 24K Oxford-IIIT Pets (Pets) <ref type="bibr" target="#b69">[69]</ref> Fine-grained cls. 37 3.6K 3.6K Places-365 (P365) <ref type="bibr" target="#b106">[105]</ref> Scene cls. 365 1.8M 36K Something Something-v2 (SSv2) <ref type="bibr" target="#b31">[31]</ref> Action cls. 174 169K 25K EPIC-Kitchens-100 (EK100) <ref type="bibr" target="#b20">[20]</ref> Action cls. 3806 67K 10K NYU-v2 (NYU) <ref type="bibr" target="#b65">[65]</ref> Scene cls. 10 794 653 NYU-v2-seg (NYU-seg) <ref type="bibr">[</ref> Transfer datasets and metrics. We evaluate OMNIVORE in transfer learning experiments on a diverse set of image, video, and single-view 3D tasks; see <ref type="table" target="#tab_1">Table 1</ref> for a summary. We present details on the experimental setup in Appendix B.</p><p>Images. We evaluate OMNIVORE on fine-grained object recognition on the iNaturalist-2018 dataset <ref type="bibr" target="#b36">[36]</ref>, finegrained classification on the Oxford-IIIT Pets dataset <ref type="bibr" target="#b69">[69]</ref>, and in scene classification on the Places-365 dataset <ref type="bibr" target="#b106">[105]</ref>.</p><p>Videos. We use the Something Something-v2 dataset, which has a special emphasis on temporal modeling for action recognition. We also use the EPIC-Kitchens-100 dataset, which has 100 hours of unscripted egocentric video. Each clip is labeled with a verb and a noun that together form an action. Our model is trained to recognize all 3,806 actions, i.e., verb-noun pairs in the dataset. We marginalize over verbs to obtain noun predictions and vice versa.</p><p>Single-view 3D. We use the NYU-v2 dataset for single-view 3D scene classification and segmentation. We follow the setup from <ref type="bibr" target="#b33">[33]</ref> for scene classification and <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">33]</ref> for segmentation. For segmentation, we follow <ref type="bibr" target="#b51">[51]</ref> and use the UPerNet <ref type="bibr" target="#b96">[95]</ref> head with the Swin trunk. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with Modality-Specific Models</head><p>We compare OMNIVORE to models trained on a specific visual modality. We train OMNIVORE from scratch jointly on the IN1K, K400, and SUN datasets. Our modality-specific baseline models use the same Swin transformer architecture as OMNIVORE; we refer to them as Im-ageSwin, VideoSwin, and DepthSwin. Excluding the patchembedding linear layers, these models have the same number of parameters as OMNIVORE. Following standard practice <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref>, the ImageSwin model is trained on IN1K, whereas VideoSwin and DepthSwin models are finetuned by inflating the ImageSwin model. We experiment with three model sizes: viz., Swin-T, Swin-S, and Swin-B. <ref type="bibr" target="#b0">1</ref> Pretraining performance. In <ref type="table" target="#tab_2">Table 2</ref>, we compare OM- <ref type="bibr" target="#b0">1</ref> We refer to <ref type="bibr" target="#b51">[51]</ref> for details on these model sizes.</p><p>NIVORE to modality-specific models on the pretraining datasets. The results in the table show that across model sizes, OMNIVORE models match or exceed the performance of their modality-specific counterparts. This observation supports our hypothesis that it is possible to learn a single visual representation that works across visual modalities. OMNIVORE learns representations that are as good as modality-specific representations using the same training data, same model parameters and same model capacity. This implies that OMNIVORE provides a viable alternative to the pretrain-then-finetune paradigm commonly used to deploy modality-specific models: it can deliver the same or better recognition accuracy with a third of the parameters.</p><p>From our results, we also observe that higher-capacity models benefit more from omnivorous training. OMNI-VORE models using the larger Swin-B architecture improve over their modality-specific counterparts on both IN1K and K400, whereas the smallest Swin-T model does not. <ref type="figure">Figure 3</ref> presents a detailed analysis of the improvements of OMNIVORE over the VideoSwin baseline (both using the Swin-B architecture) on the K400 dataset. Here VideoSwin is pre-trained on IN1K and finetuned on K400, whereas OMNIVORE is trained jointly on IN1K, K400, and SUN RGB-D. Both models use the the Swin-B architecture. OMNIVORE particularly improves the recognition of classes that require reasoning about parts of the human body such as the hands, arms, head, mouth, hair etc. We surmise this is because joint training on images helps OMNIVORE to learn a better model of the spatial configuration of parts.</p><p>Transfer learning performance. We compare OMNIVORE to modality-specific models by finetuning on various downstream tasks. <ref type="table">Table 3</ref> presents the results of these experiments. We observe that OMNIVORE transfers better than modality-specific models on nearly all downstream tasks. In particular, OMNIVORE provides significant gains on videorecognition tasks, even though it does not get any additional video supervision during pre-training compared to the baseline. We reiterate that OMNIVORE has the same model ca- pacity as the modality-specific baselines. This observation underscores one of the key benefits of multi-modal training: because OMNIVORE was pretrained jointly on more diverse training data, it generalizes better out-of-distribution. As before, <ref type="table">Table 3</ref> also shows that higher-capacity models benefit the most from omnivorous training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the state-of-the-art</head><p>Next, we perform experiments comparing OMNIVORE to existing state-of-the-art models. In these experiments, like many state-of-the-art modality-specific methods, we use the ImageNet-21K (IN21K) dataset during pretraining. The  <ref type="table" target="#tab_3">Table 4</ref> compares the performance of the OMNIVORE models to state-of-the-art models on each of the three benchmarks. OMNIVORE performs at par with or exceeds modalityspecific methods despite using a model architecture that is not tailored towards any specific modality. Even when compared to modality-specific models with a similar number of parameters, OMNIVORE models match the state-of-the-art on IN1K, and outperform the previous state-of-the-art on K400 by achieving 84.1% accuracy -a gain of 1% which was previously only possible by using additional large video datasets. This demonstrates the strong performance of using the same OMNIVORE model across image, video and single-view 3D benchmarks.  <ref type="bibr" target="#b90">[89]</ref>).</p><p>Transfer learning performance. We compare OMNIVORE models to modality-specific models by finetuning on downstream tasks. In <ref type="table">Table 5</ref>, we report results on image classification. OMNIVORE models outperform prior state-of-theart in scene classification on Places-365, and in fine-grained classification on iNaturalist-2018 and Oxford-IIIT Pets. We finetune OMNIVORE on video-classification and report the results in <ref type="table">Table 6</ref>. On the EPIC-Kitchens-100 dataset, the OMNIVORE Swin-B model achieves the absolute best performance across verb, noun, and verb-noun pair (action) classification. Similarly, on the SSv2 dataset, which requires temporal reasoning, OMNIVORE outperforms all prior work. This suggests that OMNIVORE representations transfer well to temporal-reasoning tasks -OM-NIVORE sets a new state-of-the-art while outperforming architectures specialized for these video tasks.</p><p>Finally, in <ref type="table" target="#tab_9">Table 7</ref>, we report finetuning results for RGBD scene classification and segmentation. While prior work relies on specialized 3D operators <ref type="bibr" target="#b9">[10]</ref>, fusion techniques <ref type="bibr" target="#b98">[97]</ref>, or depth encoding schemes <ref type="bibr" target="#b33">[33]</ref>, OMNIVORE uses a generic architecture and operates directly on disparity. OMNIVORE achieves state-of-the-art performance on both the scene classification and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We ablate some of OMNIVORE's key design choices in <ref type="table">Table 8</ref>  Hence, we use the 0.1:1:1:10 setting for our final model. Batching strategy. We evaluate the two different batching strategies described in ? 3, and observe that they perform similarly. We also find that the separate batching strategy (which alternates between datasets during training) does not lead to instabilities during training. Additionally, since it is easier to implement, we use it to train OMNIVORE. Patch embedding model for depth channel. OMNIVORE uses a separate linear+LN layer for the depth channel in RGBD images. We compare this to using a four-channel convolutional model to embed depth patches instead, and find that the separate layer leads to better performance on SUN. We also observed that using the separate layer helps OMNIVORE transfer better to downstream RGBD tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Cross-Modal Generalization</head><p>A key advantage of OMNIVORE over modality-specific models is that it can generalize across visual modalities. This generalization emerges naturally because we use the same model for all modalities. Our model is neither trained with corresponding data across modalities nor with any cross-modal consistency losses. Retrieval across images and depth. We use the OMNI-VORE representation to retrieve depth maps given an RGB image. To create a database of depth maps, we run a monocular depth-prediction model <ref type="bibr" target="#b74">[74]</ref> on the ImageNet-1K train set. We note that OMNIVORE was not trained on ImageNet-1K depth maps nor on predicted depth. We use the ImageNet-1K val set (RGB) images as queries. <ref type="figure">Figure 4</ref> shows five examples of retrieved maps. These results illustrate that OMNIVORE constructs good depth-map representations, even though it had not previously observed ImageNet-1K depth maps during training. We emphasize that this cross-modal generalization ability is not the result of explicitly learning correspondences between visual modalities <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b78">77]</ref>. Instead, it emerges due to the use of an almost entirely shared encoder for those modalities. To quantitatively measure OMNIVORE's generalization performance across different modalities, we perform k-nearest neighbor (k-NN, k = 20) classification experiments on the ImageNet-1K dataset using the predicted depth maps. We extract OMNIVORE representations from the RGB images on the val set and measure the model's ability to retrieve images, RGBD images, and depth-only images from the train set. We observe that OMNIVORE produces a representation that allows for successful k-NN classification, which demonstrates its strong generalization performance. Surprisingly, we observe a high accuracy is attained even when retrieving depth-images, which provide less information about the object class than RGB images. Retrieval across all modalities. We further probe the OMNIVORE visual representations in retrieval experiments across images, videos, and depth maps. We use the RGB images from the ImageNet-1K val set as queries and use them to retrieve similar depth maps from ImageNet-1K (predicted depth) and videos from Kinetics-400. <ref type="figure">Figure 1</ref> shows examples of the resulting retrievals. The results illustrate how OMNIVORE supports retrieval of visual concepts across images (RGB), single-view 3D (RGBD), and videos (RGBT) using its shared representation space. Bridging frame-based and clip-based video models. OMNIVORE's cross-modality generalization capabilities also make it more robust to changes in lengths of videos to be classified. We demonstrate this in in <ref type="figure">Figure 5</ref>, where we classify videos using different length clips at inference time. The model is trained with 32 frames at stride 2, and by default uses 4 clips of the same length and stride to cover the Clip length (number of frames) Top-1 accuracy VideoSwin-B OMNIVORE <ref type="figure">Figure 5</ref>. Accuracy as a function of clip length on the K400 dataset. Models are trained on 32-frame clips but evaluated on clips of different length (with the same fps used for frame sampling). The performance of OMNIVORE degrades more gracefully than that of the VideoSwin-B model, and is still effective when doing frame-level inference (i.e., when the clip length is 1). full 10 second video at inference time. In this experiment, we vary the clip length from 1 to 32, increasing the number of clips proportionally to still cover the full video in each case. The results show that OMNIVORE's performance degrades more gracefully as the video length decreases. Notably, OMNIVORE outperforms the baseline by 18.5% at a clip length of 1 frame (frame-level inference). This suggests that joint training on images and videos enables the model to use both temporal and spatial cues effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Limitations</head><p>Although OMNIVORE presents an advance over traditional modality-specific models, it has several limitations. Current implementation of OMNIVORE only works on single-view 3D images and does not generalize to other 3D representations such as voxels, point clouds, etc. A simple approach to deal with such inputs may be to render multiple single-view 3D images from such inputs and average our predictions over those images, but such an approach would not effectively leverage multi-view information. Another caveat is that depth inputs are not scale-invariant; we used normalizations to alleviate this issue <ref type="bibr" target="#b74">[74]</ref>. Also, OM-NIVORE focuses only on visual modalities, so co-occurring modalities such as audio are not used. OMNIVORE was pretrained using only classification; using structured prediction tasks such as segmentation might yield richer representations. We leave such extensions to future work. Ethical Considerations. Our study focuses on technical innovations in training models for visual recognition. These innovations themselves appear to be neutral from an ethics point-of-view. However, all ethical considerations that apply to other visual-recognition models apply equally to OM-NIVORE. Any real-world deployment of a model like OM-NIVORE is best preceded by a careful analysis of that model for ethical problems, including but not limited to: performance disparities between different user groups, associations that may be harmful to some users, and predictions that may propagate stereotypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details for Pretraining</head><p>We train using AdamW with a batch size of 4096 for each dataset, and use a cosine learning rate (LR) schedule with linear warm up and cool down phases for the first and last 10% of training, respectively. We train for 500 epochs with a peak LR of 2 ? 10 ?3 and a weight decay of 5 ? 10 ?2 . Swin-T, Swin-S and Swin-L use a window size of 8?7?7, whereas Swin-B uses a window size of 16?7?7. The models are trained with stochastic depth with a drop rate of 0.1 for Swin-T, 0.2 for Swin-S, and 0.3 for Swin-B, and Swin-L. We use exponential moving average (EMA) <ref type="bibr" target="#b73">[73]</ref> with a decay of 10 ?4 and report the best results during training since EMA results peak before the end of training.</p><p>For IN1K and IN21K we use RandAugment <ref type="bibr" target="#b19">[19]</ref>, mixup <ref type="bibr" target="#b102">[101]</ref>, CutMix <ref type="bibr" target="#b99">[98]</ref>, label smoothing <ref type="bibr" target="#b86">[85]</ref>, and Random Erasing <ref type="bibr" target="#b105">[104]</ref> with the same settings as used in <ref type="bibr" target="#b89">[88]</ref>, and color jittering of 0.4. For SUN RGB-D we clamp and normalize the disparity channel, drop the RGB channels with a probability of 0.5, and we also apply 0.5 Dropout <ref type="bibr" target="#b83">[82]</ref> before the linear head when pre-training with ImageNet-21K. For Kinetics-400 we use mixup, CutMix and label smoothing, and Dropout of 0.5 before the linear head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details on the Transfer Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Image Classification</head><p>We finetune all models on the downstream tasks for 100 epochs and optimize the models with mini-batch SGD. We use a half-wave cosine learning rate <ref type="bibr" target="#b54">[54]</ref> and set the weight decay to zero. For all models, including the modalityspecific models, we perform a grid search for the best learning rate in the range [5e-3, 1e-2, 2e-2, 4e-2, 8e-2, 1e-1, 2e-1, 3e-1, 4e-1, 5e-1, 6e-1] and drop path in [0.1, 0.3]. We use the strong augmentations from <ref type="bibr" target="#b89">[88]</ref> for finetuning. For the evaluations in <ref type="table">Tables 3 and 5</ref>, we follow <ref type="bibr" target="#b79">[78]</ref> and resize the images to shortest side of 224px and evaluate the models on the center crop of 224 ? 224. For higher resolution (384px) evaluations in <ref type="table">Table 5</ref>, we similarly resize the images to shortest side of 384px and evaluate the models on the center crop of 384 ? 384. We also increase the spatial window size for all the Swin models from 7 to 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Video Classification</head><p>In <ref type="table">Table 3</ref>, we finetune video models using hyperparameters as described in <ref type="bibr" target="#b52">[52]</ref>. For Something Something-v2, we finetune for 60 epochs with AdamW optimizer. We use halfwave cosine learning rate with warmup. We start the learning rate from 10 ?6 and linearly warmup to a peak learning rate of 6 ?10 ?3 over 5% of the training, and rest 95% we use half-wave cosine schedule to decay the learning rate back to 10 ?6 . We train the classification head with this learning rate, and the backbone with 0.1? the above learning rate.  . We plot the gain in per-class F1-score on the K400 dataset for all the action groups defined in <ref type="bibr" target="#b12">[13]</ref>. The baseline model is first pretrained on ImageNet-1K and then fine-tuned on K400 whereas OMNIVORE is trained jointly on ImageNet-1K, K400 and the single-view 3D SUN RGB-D dataset. OMNIVORE improves the performance for all the 38 groups.</p><p>Throughout we use a weight decay of 0.05. We use a batch size of 4? 64 distributed over 64 32GB GPUs. For EPIC-Kitchens-100, we use similar hyperparamters with only difference being that we use a peak learning rate of 2 ? 10 ?3 and we train for 150 epochs. These settings provided better performance for the modality-specific baseline, and we use it for finetuning both the baseline and OMNIVORE models. In terms of preprocessing, at train time we sample a 32 frame video clip at stride 2 from the full video using temporal segment sampling as in <ref type="bibr" target="#b52">[52]</ref>. We scale the short side of the video to 256px, take a 224px random resized crop, followed by RandAugment and Random Erasing. At test time, we again sample a 32 frame clip with stride 2, scale the short side to 224px and take 3 spatial crops along the longer axis to get 224 ? 224 crops. The final predictions are averaged over these crops.</p><p>For comparison to the state-of-the-art in <ref type="table">Table 6</ref>, when finetuning OMNIVORE models trained with IN21K, we found slightly different hyperparameters to perform better. For Something Something-v2, we used peak learning rate of 1.2 ? 10 ?3 over 150 epochs. For EPIC-Kitchens-100, we used weight decay of 0.004, over 100 epochs, peak learning rate of 4 ? 10 ?4 , with the same learning rate schedule for backbone and head. We also used cutmix augmentation and label smoothing. All other hyperparameters in both cases were as described earlier. We also use EMA with similar settings as used during pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Single-view 3D Tasks</head><p>NYU Scene classification. We follow the setup from <ref type="bibr" target="#b33">[33]</ref> for scene classification and use 10 classes derived from the original 19 classes. In  B and Swin L models were trained for 200 epochs with starting learning rate of 5 ? 10 ?3 , weight decay of 0 for Swin B and 1 ? 10 ?4 for Swin L. All other hyperparameters were as described earlier. NYU RGBD Segmentation. We follow the training and evaluation setup from <ref type="bibr" target="#b9">[10]</ref>. We follow the Swin segmentation architecture which uses an UperNet <ref type="bibr" target="#b96">[95]</ref> head with the Swin trunk. All models are finetuned with AdamW <ref type="bibr" target="#b53">[53]</ref> with a weight decay of 0.01. The learning rate follows a Polynomial Decay (power 1) schedule and starts at 0.00006. We warmup the learning rate for 1500 iterations and train the model with a batchsize of 32. All the depth maps in NYU are converted into disparity maps by using the camera baseline and focal length of the Kinect sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. k-NN experiments</head><p>Extracting depth on ImageNet-1K. We ran a monocular depth-prediction model <ref type="bibr" target="#b74">[74]</ref> on the IN1K train set. We used the pretrained dpt large model and followed the input image preprocessing steps as provided in <ref type="bibr" target="#b74">[74]</ref>.</p><p>Classifying ImageNet-1K using different modalities.</p><p>For the experiments involving classification using different modalities, we extract features from the IN1K train set using the RGB, RGBD or just Depth (D) modalities, and on IN1K validation set using the RGB modality. We follow the k-NN protocol from <ref type="bibr" target="#b11">[12]</ref> for evaluation and briefly describe it next. We extract the stage 3 <ref type="bibr" target="#b51">[51]</ref> features and L 2 normalize them. For each validation feature as the query, we retrieve the nearest neighbors from the train set using euclidean distance, and take the top-k closest matches. For VideoSwin-B OMNIVORE (Swin-B) 3-split accuracy 96.9 98.2 <ref type="table">Table 9</ref>. UCF-101. As in <ref type="table">Table 3</ref>, the VideoSwin model is inflated from IN1K and pre-trained on K400. OMNIVORE is pretrained with IN1K, K400 and SUN RGB-D. Both models are then finetuned and evaluated on UCF-101 for each split separately. Performance reported is averaged over the standard 3 splits. each match we create a one-hot vector using its ground truth label, and scale it by e s/? , where s is the dot product between the feature of the matched image the query image, and ? is a temperature hyperparameter (set to 0.07). We compute an effective prediction for the query by summing the top-k one-hot vectors. Similar processing is used for the visualizations in <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Other Results</head><p>Results on UCF-101. We also evaluate OMNIVORE on another popular (albeit smaller) video recognition benchmark, UCF-101 <ref type="bibr" target="#b82">[81]</ref>. As shown in <ref type="table">Table 9</ref>, OMNIVORE pretraining is effective for sports action recognition in UCF-101 as well. Note that the results shown are with RGB modality only; the state-of-the-art on these datasets often leverages additional features such as optical flow, dense trajectories (IDT) etc.</p><p>Low-data regime fine-tuning. We analyzed low-shot versions of the Places-365 benchmark (models from Table 3). As shown in <ref type="table" target="#tab_1">Table 10</ref>, OMNIVORE outperforms the modality-specific baseline in the low-shot regime too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Places-365 1% 2% 5% 10% OMNIVORE 46.2 49.0 51.5 53.9 Image-specific 44.8 47.9 50.9 53.4 <ref type="table" target="#tab_1">Table 10</ref>. Low-shot finetuning. Performance of finetuning OM-NIVORE on low-shot versions of the Places-365 dataset.</p><p>Per-class gains. We present the gain of OMNIVORE over the VideoSwin baseline ( ? 4.1 of the main paper) in Figs. 6 and 7.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>OMNIVORE Swin-B and Swin-L models are trained from scratch on IN21K, IN1K, K400, and SUN, where a single epoch consists of one epoch each of IN1K and K400, 10 epochs of SUN, and 0.1 epochs of ImageNet-21K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Gain of OMNIVORE over baseline on Action recognition (per group)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Gain of OMNIVORE over baseline on Action Recognition (per class). We plot the gain in per-class F1-score on the K400 dataset for the top twenty and bottom twenty classes. The baseline model is first pretrained on ImageNet-1K and then fine-tuned on K400 whereas OMNIVORE is trained jointly on ImageNet-1K, K400 and the single-view 3D SUN RGB-D dataset. OMNIVORE improves the F1 score on 308 out of the 400 total classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The same OMNIVORE model obtains 85.6% top-1 accuracy on ImageNet-1K, 83.4% top-1 on Kinetics-400, and 67.4% top-1 accuracy on SUN RGB-D. OMNIVORE's strong generalization capabilities also extend to transfer learning experiments. OMNIVORE performs</figDesc><table><row><cell>Input</cell><cell>Patches</cell><cell cols="2">Omnivore Model</cell></row><row><cell></cell><cell></cell><cell>Linear</cell></row><row><cell>Image</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Linear</cell><cell>Transformer</cell></row><row><cell>Video</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Linear</cell></row><row><cell></cell><cell></cell><cell>Linear</cell></row><row><cell>Single-view 3D</cell><cell></cell><cell>Embeddings</cell></row></table><note>+ Figure 2. Multiple visual modalities in the OMNIVORE model. We convert image, video, and single-view 3D modalities into em- beddings that are fed into a Transformer model. The images are converted into patches, videos into spatio-temporal tubes, and the single-view 3D images are converted into RGB patches and depth patches. The patches are projected into embeddings using linear layers. We use the same linear layer for (image or video) RGB patches and a separate one for depth patches.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Transfer datasets used to evaluate OMNIVORE on image, video and single-view 3D modalities. The table reports the task, number of classes (#cls), number of training samples (#train), and number of validation samples (#val) for each dataset.long, and are labeled into one of 400 action classes.Single-view 3D. The SUN RGB-D dataset has ?5K train and ?5K val RGBD images with 19 scene classes. Following<ref type="bibr" target="#b74">[74]</ref>, we convert the depth maps into disparity maps. Implementation details. We use the Swin transformer<ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref> architecture as the backbone for OMNIVORE, and attach linear heads for each target dataset. At training time, we use a resolution of 224?224 and train using standard image augmentations<ref type="bibr" target="#b89">[88]</ref> on ImageNet. For Kinetics, we sample 32 frames at stride 2. SUN RGB-D is processed similarly to ImageNet but we randomly drop the RGB channels with a probability of 0.5 in order to encourage the model to use the depth channel for recognition as well. We provide complete implementation details in Appendix A. Our models are optimized using AdamW<ref type="bibr" target="#b53">[53]</ref> for 500 epochs where a single epoch consists of one epoch each for ImageNet-1K and Kinetics, and 10 epochs for SUN RGB-D.</figDesc><table><row><cell>65]</cell><cell>Segmentation</cell><cell>40 794 653</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Specific 57.9 87.3 69.7 87.6 93.7 99.6 62.2 88.7 41.8 62.8 72.5 47.9 OMNIVORE 58.2 87.4 69.0 87.7 94.2 99.7 64.4 89.7 42.7 63.1 77.3 49.7 Swin-S Specific 58.7 88.1 72.9 90.2 94.4 99.6 66.8 91.1 42.5 63.4 76.7 51.3 OMNIVORE 58.8 88.0 73.6 90.8 95.2 99.7 68.2 91.8 44.9 64.8 76.9 52.7 Swin-B Specific 58.9 88.3 73.2 90.9 94.2 99.7 65.8 90.6 42.8 64.0 76.4 51.1 OMNIVORE 59.2 88.3 74.4 91.1 95.1 99.8 68.3 92.1 47.4 67.7 79.4 54.0Table 3. Comparing OMNIVORE with modality-specific models after finetuning the models on seven downstream tasks. Results are presented for three different model sizes: T, S, and B. Our image specific model is pretrained on IN1K. The video specific and single-view 3D specific models are both initialized using inflation from the pretrained image-specific model and finetuned on K400 and SUN RGB-D respectively. OMNIVORE models are at par with or outperform modality-specific models on nearly all downstream tasks. OMNIVORE vs. modality-specific models that have the same model architecture and number of parameters. OMNI-</figDesc><table><row><cell>F1 on Kinetics-400</cell><cell>1 2 3 4 5 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model Method Swin-T</cell><cell>P365 top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-1 iNat18 Pets SSv2 EK100 NYU NYU-seg mIoU</cell></row><row><cell></cell><cell>0</cell><cell>martial arts</cell><cell>dancing</cell><cell>racquet + bat sports</cell><cell>touching person</cell><cell>heights</cell><cell>gymnastics</cell><cell>eating + drinking</cell><cell>hands</cell><cell>playing games</cell><cell>mobility -water</cell><cell>auto maintenance</cell><cell>athletics</cell><cell>body motions</cell><cell>head + mouth</cell><cell>golf</cell></row><row><cell cols="17">Figure 3. Comparing OMNIVORE</cell></row><row><cell cols="17">with VideoSwin on K400. OMNIVORE</cell></row><row><cell cols="17">improves over VideoSwin on F1 score</cell></row><row><cell cols="17">on all 38 class groups defined in [42]</cell></row><row><cell cols="15">(top 15 shown here for brevity).</cell><cell></cell></row><row><cell></cell><cell cols="4">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">ImageNet-1K Kinetics-400 SUN top-1 top-5 top-1 top-5 top-1</cell></row><row><cell></cell><cell cols="8">ImageSwin-T [51]</cell><cell></cell><cell></cell><cell cols="3">81.2</cell><cell></cell><cell cols="2">95.5</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell cols="8">VideoSwin-T [52]</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>78.8</cell><cell>93.6</cell><cell>?</cell></row><row><cell></cell><cell cols="6">DepthSwin-T</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>63.1</cell></row><row><cell></cell><cell cols="13">OMNIVORE (Swin-T) 80.9</cell><cell></cell><cell cols="2">95.5</cell><cell>78.9</cell><cell>93.8</cell><cell>62.3</cell></row><row><cell></cell><cell cols="8">ImageSwin-S [51]</cell><cell></cell><cell></cell><cell cols="3">83.2</cell><cell></cell><cell cols="2">96.2</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell cols="8">VideoSwin-S [52]</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>80.6</cell><cell>94.5</cell><cell>?</cell></row><row><cell></cell><cell cols="6">DepthSwin-S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>64.9</cell></row><row><cell></cell><cell cols="10">OMNIVORE (Swin-S)</cell><cell cols="3">83.4</cell><cell></cell><cell cols="2">96.6</cell><cell>82.2</cell><cell>95.4</cell><cell>64.6</cell></row><row><cell></cell><cell cols="8">ImageSwin-B [51]</cell><cell></cell><cell></cell><cell cols="3">83.5</cell><cell></cell><cell cols="2">96.5</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell cols="8">VideoSwin-B [52]</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>80.6</cell><cell>94.6</cell><cell>?</cell></row><row><cell></cell><cell cols="6">DepthSwin-B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>64.8</cell></row><row><cell></cell><cell cols="13">OMNIVORE (Swin-B) 84.0</cell><cell></cell><cell cols="2">96.8</cell><cell>83.3</cell><cell>95.8</cell><cell>65.4</cell></row></table><note>VORE is a single model trained from scratch jointly on the IN1K, K400 and SUN datasets whereas the modality-specific models are trained specifically for each dataset (modality). The ImageSwin model is trained from scratch while the VideoSwin and Depth- Swin models are finetuned from the ImageSwin model. OMNI-VORE performs at-par or outperforms modality-specific models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="5">ImageNet-1K Kinetics-400 SUN top-1 top-5 top-1 top-5 top-1</cell></row><row><cell>MViT-B-24 [24]</cell><cell>83.1</cell><cell>-</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ViT-L/16 [21]</cell><cell>85.3</cell><cell>-</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ImageSwin-B [51]</cell><cell>85.2</cell><cell>97.5</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ImageSwin-L [51]</cell><cell>86.3</cell><cell>97.9</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ViT-B-VTN [66]</cell><cell>?</cell><cell>?</cell><cell>79.8</cell><cell>94.2</cell><cell>?</cell></row><row><cell>TimeSformer-L [8]</cell><cell>?</cell><cell>?</cell><cell>80.7</cell><cell>94.7</cell><cell>?</cell></row><row><cell>ViViT-L/16x2 320 [5]</cell><cell>?</cell><cell>?</cell><cell>81.3</cell><cell>94.7</cell><cell>?</cell></row><row><cell>MViT-B 64?3 [24]</cell><cell>?</cell><cell>?</cell><cell>81.2</cell><cell>95.1</cell><cell>?</cell></row><row><cell>VideoSwin-B [52]</cell><cell>?</cell><cell>?</cell><cell>82.7</cell><cell>95.5</cell><cell>?</cell></row><row><cell>VideoSwin-L [52]</cell><cell>?</cell><cell>?</cell><cell>83.1</cell><cell>95.9</cell><cell>?</cell></row><row><cell>DF 2 Net [50]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>54.6</cell></row><row><cell>G-L-SOOR [80]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>55.5</cell></row><row><cell>TRecgNet [22]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>56.7</cell></row><row><cell>CNN-RNN [9]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>60.7</cell></row><row><cell>Depth Swin-B</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>69.1</cell></row><row><cell>Depth Swin-L</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>68.7</cell></row><row><cell cols="2">OMNIVORE (Swin-B) 85.3</cell><cell>97.5</cell><cell>84.0</cell><cell>96.2</cell><cell>67.2</cell></row><row><cell cols="2">OMNIVORE (Swin-L) 86.0</cell><cell>97.7</cell><cell>84.1</cell><cell>96.3</cell><cell>67.1</cell></row></table><note>. Comparing OMNIVORE with state-of-the-art models on the image, video, and single-view 3D classification datasets used to pre-train OMNIVORE. OMNIVORE performs on par with or better than state-of-the-art models on all three pre-training tasks, including modality-specific models of similar size.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. Together, the results suggest OMNIVORE's performance is relatively stable under different design choices. For a faster turnaround time in the ablations, we train the model for 300 epochs. Training from scratch or finetuning. We compare training OMNIVORE models from scratch on different modalities (top row) with initializing the model via image classification followed by finetuning on all modalities (second row). For the finetuning result, we initialize OMNIVORE (Swin-B) using a pretrained ImageNet-21K model followed by joint finetuning on IN1K, K400, and SUN for 100 epochs. The</figDesc><table><row><cell></cell><cell></cell><cell>EK100</cell><cell></cell><cell cols="2">SSv2</cell></row><row><cell>Method</cell><cell cols="5">verb noun action top-1 top-5</cell></row><row><cell>RGB-only methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SlowFast [25]</cell><cell cols="2">65.6 50.0</cell><cell>38.5</cell><cell>63.0</cell><cell>88.5</cell></row><row><cell>TimeSformer [8]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.4</cell><cell>-</cell></row><row><cell>MViT-B-24 [24]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.7</cell><cell>91.5</cell></row><row><cell>TAR [76]</cell><cell cols="2">66.0 53.4</cell><cell>45.3</cell><cell>-</cell><cell>-</cell></row><row><cell>VIMPAC [87]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.1</cell><cell>-</cell></row><row><cell>ViViT-L [5]</cell><cell cols="2">66.4 56.8</cell><cell>44.0</cell><cell>65.9</cell><cell>89.9</cell></row><row><cell>MFormer-L [72]</cell><cell cols="2">67.1 57.6</cell><cell>44.1</cell><cell>68.1</cell><cell>91.2</cell></row><row><cell>ORViT [35]</cell><cell cols="2">68.4 58.7</cell><cell>45.7</cell><cell>69.5</cell><cell>91.5</cell></row><row><cell>COVER [100]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.9</cell><cell>-</cell></row><row><cell>VideoSwin-B [52]</cell><cell cols="2">67.8 57.0</cell><cell>46.1</cell><cell>69.6</cell><cell>92.7</cell></row><row><cell cols="3">OMNIVORE (Swin-B) 69.5 61.7</cell><cell>49.9</cell><cell>71.4</cell><cell>93.5</cell></row><row><cell>Multi-modal methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MML [45]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.1</cell><cell>92.1</cell></row><row><cell>MTCN [43]</cell><cell cols="2">70.7 62.1</cell><cell>49.6</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Comparing OMNIVORE with state-of-the-art models in video classification finetuning experiments on two datasets. We highlighted columns that show the two primary classification metrics used in prior work. OMNIVORE models obtain state-ofthe-art results on both datasets, even outperforming some multimodal methods. Comparing OMNIVORE with state-of-the-art models in RGBD finetuning experiments on the NYU-v2 dataset. The left column shows the scene classification accuracy while the right column shows the mean intersection-over-union of semantic segmentation. OMNIVORE outperforms prior art in RGBD classification and segmentation. model trained from scratch performs better in both image and video classification. Data ratio. Since the IN1K and K400 datasets are much larger than SUN, we replicate SUN when training OMNI-VORE. Although replication helps, a higher replication factor hurts the model performance on SUN (which hints at overfitting), whereas the performance on IN1K and K400 is unchanged. Based on the same logic, we undersample the IN21K dataset to have a similar size as IN1K. Increasing the proportion of IN21K has no effect on IN1K, decreases performance on K400, and improves performance on SUN.</figDesc><table><row><cell>Method</cell><cell cols="2">Classification Segmentation</cell></row><row><cell>DF 2 Net [50]</cell><cell>65.4</cell><cell>?</cell></row><row><cell>TRecgNet [22]</cell><cell>69.2</cell><cell>?</cell></row><row><cell>ShapeConv [10]</cell><cell>?</cell><cell>51.3</cell></row><row><cell>BCMFP + SA-Gate [16]</cell><cell>?</cell><cell>52.4</cell></row><row><cell>TCD [97]</cell><cell>?</cell><cell>53.1</cell></row><row><cell>OMNIVORE (Swin-B)</cell><cell>80.0</cell><cell>55.1</cell></row><row><cell>OMNIVORE (Swin-L)</cell><cell>80.3</cell><cell>56.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 (</head><label>7</label><figDesc>classification) the best Swin</figDesc><table><row><cell>busking</cell><cell>making sandwich</cell><cell>sweeping floor</cell><cell>yoga</cell><cell>playing clarinet</cell><cell>massaging legs</cell><cell>filling eyebrows</cell><cell>eating watermelon</cell><cell>eating carrots</cell><cell>reading newspaper</cell><cell>washing hair</cell><cell>playing harmonica</cell><cell>texting</cell><cell>making pizza</cell><cell>juggling soccer ball</cell><cell>playing basketball</cell><cell>auctioning</cell><cell>doing laundry</cell><cell>playing saxophone</cell><cell>waxing legs</cell><cell>breakdancing</cell><cell>eating hotdog</cell><cell>dancing charleston</cell><cell>tickling</cell><cell>folding paper</cell><cell>riding scooter</cell><cell>building shed</cell><cell>dribbling basketball</cell><cell>training dog</cell><cell>playing accordion</cell><cell>playing xylophone</cell><cell>skiing (not slalom or crosscountry)</cell><cell>bending back</cell><cell>catching fish</cell><cell>tap dancing</cell><cell>snowboarding</cell><cell>getting a haircut</cell><cell>whistling</cell><cell>playing cymbals</cell><cell>egg hunting</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linagzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11178</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ViViT: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">G?l Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">When cnns meet random rnns: Towards multi-level analysis for rgb-d object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevrez</forename><surname>Imamoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12349</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Ahmet Burak Can, and Ryosuke Nakamura</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shapeconv: Shape-aware convolutional layer for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<title level="m">Multitask learning. Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning aligned cross-modal representations from weakly aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-andaggregation gate for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Minkowski convolutional neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>4d spatio-temporal convnets</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rescaling egocentric vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Translate-to-recognize networks for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiko</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-task self-training for learning general representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Anticipative Video Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving image-sentence embeddings using large weakly annotated photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Ben-Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06915</idno>
		<title level="m">Trevor Darrell, and Amir Globerson. Object-region video transformers</title>
		<meeting><address><addrLine>Anna Rohrbach</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unit: Multimodal multitask learning with a unified transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<title level="m">One model to learn them all</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ishan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">With a little help from my temporal context: Multimodal egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">UberNet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Mutual modality learning for video action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Petiushko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02543</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Df 2 net: Discriminative feature learning and fusion network for RGB-D indoor scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
		<title level="m">Distinctive image features from scaleinvariant keypoints. IJCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attentive single-tasking of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilija Radosavovic, and Iasonas Kokkinos</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An End-to-End Transformer Model for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust audio-visual instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">3d object detection with pointformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuran</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Multimodal self-supervision from generalized data transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra Florian Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Henriques</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dibyadip</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03152</idno>
		<title level="m">Temporal aggregate representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll?r, and Laurens van der Maaten. Revisiting weakly supervised pre-training of visual perception models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Adcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Image representations with spatial object-to-object relations for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongwei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">VIM-PAC: Video pre-training via masked token prediction and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11250</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Twostage cascaded decoder for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wujie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsheng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07175</idno>
		<title level="m">Ruoming Pang, and Fei Sha. Co-training transformer with videos and images improves action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Philip Torr, and Vladlen Koltun. Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
