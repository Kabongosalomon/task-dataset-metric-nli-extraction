<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring High-quality Target Domain Information for Unsupervised Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>October 10-14, 2022. 2022. October 10-14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
							<email>zlwang@ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><forename type="middle">Hu</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">University of Science and Technology of China Hefei</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">University of Science and Technology of China Hefei</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">University of Science and Technology of China Hefei</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring High-quality Target Domain Information for Unsupervised Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 30th ACM International Con-ference on Multimedia (MM &apos;22)</title>
						<meeting>the 30th ACM International Con-ference on Multimedia (MM &apos;22) <address><addrLine>Lisboa, Portugal; Lisboa, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<date type="published">October 10-14, 2022. 2022. October 10-14, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503161.3548114</idno>
					<note>Code is available at https://github.com/ljjcoder/EHTDI. * Corresponding Author New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In unsupervised domain adaptive (UDA) semantic segmentation, the distillation based methods are currently dominant in performance. However, the distillation technique requires complicate multi-stage process and many training tricks. In this paper, we propose a simple yet effective method that can achieve competitive performance to the advanced distillation methods. Our core idea is to fully explore the target-domain information from the views of boundaries and features. First, we propose a novel mix-up strategy to generate high-quality target-domain boundaries with ground-truth labels. Different from the source-domain boundaries in previous works, we select the high-confidence target-domain areas and then paste them to the source-domain images. Such a strategy can generate the object boundaries in target domain (edge of target-domain object areas) with the correct labels. Consequently, the boundary information of target domain can be effectively captured by learning on the mixed-up samples. Second, we design a multi-level contrastive loss to improve the representation of target-domain data, including pixel-level and prototypelevel contrastive learning. By combining two proposed methods, more discriminative features can be extracted and hard object boundaries can be better addressed for the target domain. The experimental results on two commonly adopted benchmarks (i.e., GTA5 ? Cityscapes and SYNTHIA ? Cityscapes) show that our method achieves competitive performance to complicated distillation methods. Notably, for the SYNTHIA? Cityscapes scenario, our method achieves the state-of-the-art performance with 57.8% mIoU and 64.6% mIoU on 16 classes and 13 classes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The goal of semantic segmentation is to assign a semantic class label to each pixel, which is an important problem in computer vision. In recent years, deep neural networks have shown significant advantages in semantic segmentation tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref>. However, such methods often require a large amount of manually annotated training data, and such pixel-level annotations are expensive and time-consuming. Therefore, a natural idea to overcome this bottleneck is to use synthetic data to train the model and then directly apply it in real-world scenarios. However, due to the huge gap between synthetic data and real data, applying models trained on synthetic data (source domain) to real data (target domain) often leads to a sharp drop in performance. In the field of computer vision, domain adaptive segmentation have emerged to solve this problem. In particular, this paper focuses on unsupervised domain adaptive (UDA) semantic segmentation.</p><p>In UDA semantic segmentation, the methods using distillation techniques <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> generally outperform non-distilled methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. For example, the first distillation-based method, ProDA <ref type="bibr" target="#b38">[39]</ref>, is still a barrier that non-distillation methods cannot stride. Although the distillation technique has shown strong abilities to improve the generalization performance of the model, it interrupts the end-to-end training process and require some special training tricks. For example, ProDA requires three training stages and needs to be initialized with the ASOS <ref type="bibr" target="#b30">[31]</ref> model in the first stage. In addition, in the distillation stage, ProDA also needs to be initialized with the model of SimCLRV2 <ref type="bibr" target="#b7">[8]</ref>. However, contemporaneous non-distilled methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> only require one training stage and do not need special training strategies, <ref type="figure">Figure 1</ref>: ProDA <ref type="bibr" target="#b38">[39]</ref> v.s Ours on GTA5 <ref type="bibr" target="#b27">[28]</ref>. ProDA needs to be initialized with the model of ASOS <ref type="bibr" target="#b30">[31]</ref> and it requires an additional two-step distillation stage (red box). Compared to ProDA, our method only requires a source-only model for initialization and does not require the distillation stage.</p><p>although their performance is not as good as ProDA. Recently, few methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref> can outperform ProDA with the same backbone (DeepLab-V2 <ref type="bibr" target="#b2">[3]</ref>). However, they still need employ the distillation techniques and require some more sophisticated training techniques, further increasing the complexity of the methods. In pursuit of simplicity, we hope to design a non-distillation method that can achieve competitive performance to complicated distillation methods under the same condition with DeepLab-V2 as the backbone.</p><p>Besides the distillation-based methods, the methods employing pseudo-labeling techniques <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref> have attracted many attention due to their simplicity and efficiency. These methods use the prediction results of the target image as labels, so that the target image can enjoy the same supervised training as the source image, thereby improving the performance of the model in the target image. With the enhancement of pseudo-labeling technology, the current method has good enough classification performance in the interior pixels of the target objects, and the bottleneck is mainly concentrated on the wrong prediction of edge pixels. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b), wrong pixels are almost located in the boundary area, and the inner area of the object has few errors. Therefore, the key to improve segmentation performance is to improve the accuracy of the boundary pixels of the target images. However, since the pseudo-labels of the boundary pixels of the target domain are often wrong, it is inherently difficult for the pseudo-label techniques to improve the accuracy of boundary pixels in a supervised learning manner.</p><p>In particular, we noticed that the prediction results of the interior pixels of objects in the target image are usually accurate. If we convert these interior pixels into boundary pixels, we would get the target domain boundary pixels with accurate labels. Usually, the interior pixels have high confidence because there is no much ambiguity. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c), when a high threshold is adopted, most of the remaining pixels are interior pixels with correct labels. . White indicates wrong pixels. When the threshold is set to 0, most of the wrong pixels are concentrated near the boundary. When the threshold is set to 0.95, most of the wrong pixels are filtered out.</p><p>Therefore, we choose to utilize the high-confidence target domain pixels to construct boundaries. Here we particularly figure out the definition of the boundary. Generally speaking, the boundary refers to the area where the pixel-level semantics change (i.e., different categories are connected). Since there are multiple semantic categories around the pixels in these areas, the extracted features are usually mixed with other category information, leading to misjudgment. If the interior pixels of objects with correct labels (high-confidence pixels) in the target domain are placed in the regions with different semantics, we would obtain "new boundary" pixels with accurate labels. Then learning on these pixels can indirectly enhance the model to distinguish the boundary pixels of actual objects.</p><p>In this paper, we first propose a novel mix-up strategy to generate high-quality target domain boundaries with ground-truth label. Unlike DACS <ref type="bibr" target="#b29">[30]</ref>, which explicitly learns cross-domain invariant source domain boundary representations by copying the parts of the source image to the target image, we directly reinforce the learning of the target boundary pixels by constructing target boundary pixels with correct labels. Our core idea is to copy the high-confidence pixels in the target image to the source image so that some high-confidence target pixels are in the position of category semantic change, i.e., becoming boundary pixels. <ref type="figure">Figure 3</ref> depicts the process of generating high-quality target domain boundary pixels. Furthermore, since the target domain lacks ground-truth supervision signals, the learned features in target domain are not discriminative enough. To address this issue, we propose a multilevel contrastive learning to explore more discriminative target domain representations.Specifically, we use both pixel-to-pixel and prototype-to-prototype contrastive learning to obtain more discriminative feature representations. By combining the proposed two techniques, our method successfully circumvents complex distillation techniques while achieving the state-of-the-art performance. As shown in <ref type="figure">Figure 1</ref>, our method can beat the distillation-based ProDA and does not require any special training tricks, such as ASOS <ref type="bibr" target="#b30">[31]</ref> initialization.</p><p>Our main contributions are three-folds.</p><p>(1) We propose a novel high-confidence target domain pixel cross-domain mixing strategy (HTCM) to construct highquality target domain boundary pixels to enhance the learning of target domain boundaries. <ref type="figure">Figure 3</ref>: Illustration of the process of generating highquality target domain boundary pixels. The boundary pixels of the target image are usually mispredicted (yellow region), but the inner pixels have the correct pseudo-labels (red region). Copies interior pixels into the source image so that some of the interior pixels become boundary pixels (dotted areas in the mixed image).</p><p>(2) We propose multi-level contrastive learning to make target domain features more discriminative. Multi-level contrastive learning constrains the consistency from both pixel-level and prototype-level to improve the learning of target domain features. (3) We construct an efficient non-distillation method. To the best of our knowledge, this is the first work that does not use the distillation technique while outperforming the distillationbased methods on both GTA5 <ref type="bibr" target="#b27">[28]</ref> and SYNTHIA <ref type="bibr" target="#b28">[29]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Unsupervised Domain Adaptive Semantic Segmentation</head><p>Early unsupervised domain adaptive (UDA) semantic segmentation methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>, usually use adversarial training to align source and target domain images. However, these methods often suffer from instability in the training process <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. Different from adversarial-based methods, there are methods utilizing pseudo-labels <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref> and contrastive learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref> to improve the performance. These methods are more concise than methods based on adversarial training. In addition to the above technical routes, ProDA <ref type="bibr" target="#b38">[39]</ref> surpasses the above methods by using distillation technology. Subsequent works which outperform ProDA methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref>, generally employ distillation techniques and use much more sophisticated training techniques. For example, MFA <ref type="bibr" target="#b37">[38]</ref> proposes multiple fusion adaptation to fuse three kinds of information and combine distillation technology to further improve the performance of ProDA. CRA <ref type="bibr" target="#b32">[33]</ref>, based on ProDA, aligns the distribution of confident pixels and unconfident pixels in the target domain through an additional cross-region adaptation operation.</p><p>In this paper, we propose an efficient and non-distillation method that leverages well-designed pseudo-labeling techniques and contrastive learning techniques to reach or even surpass distillationbased methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pseudo Labels</head><p>The workflow of the pseudo-label methods can be roughly divided into the following two steps: first, generating pseudo-labels in the unlabeled data, and second, finetuning the model on these pseudolabels. Early methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> were usually iteratively performed in an offline manner. Therefore, these methods often require multiple training stages and are very cumbersome. Recently, some works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref> have greatly simplified the training process through online pseudo-label techniques. For example, Pixmatch <ref type="bibr" target="#b25">[26]</ref> uses random transformations to obtain a pair of strongly and weakly transformed images. Then the strongly transformed images can then enjoy the "supervised-form" of the training process with the pseudo-labels produced by the weaker transformations. DACS <ref type="bibr" target="#b29">[30]</ref> proposes to construct a mixed domain by copying image patches from the source domain to the target domain and learn the consistency between the student model and the teacher model under different contexts. In this way, it explicitly learns the crossdomain consistency of source boundaries. The DSP <ref type="bibr" target="#b13">[14]</ref> additionally introduces a mix up strategy between source domain images on the basis of DACS to further improve the performance. BAPA <ref type="bibr" target="#b23">[24]</ref> increases the loss weight of boundary pixels generated by DACS to improve classification accuracy of boundary pixels. The above methods use the mix up strategy of DACS to strengthen the learning of boundary samples, but DACS only explicitly generates source domain boundaries and does not explicitly construct target domain boundaries, resulting in insufficient learning of the boundary pixels of the target domain.</p><p>Different from these previous methods, we improve the performance of the model by explicitly constructing high-quality target domain boundary pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning</head><p>The goal of contrastive learning is to learn a model that can extract semantically compact features from raw images by encouraging positive pairs to be close and pulling negative pairs. Many works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref> have demonstrated the effectiveness of contrastive learning. In these works, an InfoNCE <ref type="bibr" target="#b26">[27]</ref> loss is usually adopted to implement contrastive learning.</p><p>Some recent works introduce contrastive learning into UDA semantic segmentation. RCCR <ref type="bibr" target="#b42">[43]</ref> builds different contexts for a target domain region through cutmix between the target domain and the source domain. Then they use pixel-to-pixel contrastive learning to maximize the difference of inter-region pixels and minimize the inconsistency of intra-region pixels. SPCL <ref type="bibr" target="#b34">[35]</ref> propose a novel semantic prototype-based contrastive learning framework to achieve pixel and prototype alignment.</p><p>Different from these methods, we design a multi-level contrastive learning loss, which both considers pixel-to-pixel and prototypeto-prototype contrastive learning losses, to explore more discriminative target domain feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>Unsupervised domain adaptive (UDA) semantic segmentation contains two important datasets: an annotated source dataset D = {( , )} =1 and an unlabeled target dataset D = { } =1 . D and To solve this problem, we can directly train a segmentation model with source data. Specifically, given a source image and its corresponding label , the segmentation loss on the source domain can be defined as:</p><formula xml:id="formula_0">L = ? ? ?? =1 L ( , , , ),<label>(1)</label></formula><p>where , represents the predicted probability of pixel , . L is the standard cross-entropy loss. Due to the domain shift in the source and target domains, it is difficult for the model trained in the above way to generalize well to the target domain data. To make the model better fit the target data, we can optimize the cross-entropy loss of the target image with the pseudo-label ? . In order to further improve the performance of the model on the target domain, we follow the mix-up strategy of DACS <ref type="bibr" target="#b29">[30]</ref> to construct a mixed image and a mixed label . Specifically, we randomly select half of the classes in the source image to generate a random mask and utilize to generate the mixed images from the source image and the target image.</p><formula xml:id="formula_1">= ? + ? (1 ? ).<label>(2)</label></formula><p>Similarly, the labels of the mix images can be obtained:</p><formula xml:id="formula_2">= ? + ? ? (1 ? ).<label>(3)</label></formula><p>Similar to the source domain segmentation loss, the segmentation loss of the mixed image is defined as follows:</p><formula xml:id="formula_3">L = ? ? ?? =1 L ( , , , ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>In this work, we aim to build an efficient unsupervised domain adaptive semantic segmentation method that does not require distillation techniques. To achieve our goal, we propose two novel self-supervised learning methods. <ref type="figure" target="#fig_1">Figure 4</ref> illustrates the overall architecture of our proposed method. In particular, we propose a novel high-confidence target domain pixel cross-domain mixing strategy (HTCM) to construct target domain boundary pixels with correct semantic labels. In principle, HTCM only copies the high-confidence pixels in the target domain to the source domain image so that some high-confidence target domain pixels become boundary pixels. In addition, we design a multi-level contrastive learning to improve the discriminativeness of features from both pixel-level and prototype-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generating High-quality Target Boundary Pixels</head><p>In general, only the inner pixels of the target image can obtain the correct pseudo-labels, while it is difficult to obtain accurate labels for the boundary pixels of the target image. As a result, when training target domain images directly with pseudo-labels, only the inner pixels can be effectively trained with supervision, while the boundary pixels of the target image are difficult to be effectively trained. To alleviate this problem, we need to obtain target boundaries with correct labels, strengthening the learning of target boundary pixels.</p><p>In this section, we propose a simple yet effective high-confidence target domain pixel cross-domain mixing strategy (HTCM) to generate high-quality target domain boundary pixels. Generating highquality target boundary pixels means constructing target pixels located in semantic mutation regions with correct pseudo-labels. In the pseudo-labels of the target domain pixels, the labels of highconfidence pixels tend to be correct, but these pixels are usually located inside objects rather than boundaries. We want to have these correctly labeled target domain pixels at boundary locations so that we can get the target domain boundary pixels with the correct labels. To do this, we paste these pixels into a source image so that some of the target domain pixels are at the border. Specifically, for a target domain image , we can get its corresponding predicted probability map . Then we get a high-confidence template ? , which is the selected indicator for the pixels whose predicted probability is greater than a threshold . Finally, we use the obtained templates ? to construct high-quality boundary samples as follows:</p><formula xml:id="formula_4">? = ? (1 ? ? ) + ? ? .<label>(5)</label></formula><p>Similarly, the labels of the mix images can be obtained:</p><formula xml:id="formula_5">? = ? (1 ? ? ) +?? ? .<label>(6)</label></formula><p>The segmentation loss of the mixed image ? is defined as follows:</p><formula xml:id="formula_6">L ? = ? ? ?? =1 L ( ? , , ? , ).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-level Contrastive Learning</head><p>Due to the lack of explicit supervision signals in the target domain data, the features of the target domain are not sufficiently discriminative. Fortunately, contrastive learning can improve the distinguishability of unlabeled data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>, so we employ contrastive learning to further improve our model. In particular, we design a multi-level contrastive learning loss to improve feature distinguishability from both prototype-level and pixel-level. Since we want the features to be invariant across domains, we choose to perform contrastive learning on the mixed images instead of directly on the target image. Because the prototype of the mixed image will contain the information of the source domain and the target domain, the features of the same category in the source domain and the target domain will be as consistent as possible during prototype-level contrastive learning. At the same time, because the target pixels in the mixed image ? are all high-confidence samples, they have the correct pseudo-labels. It means that all the pixels of the mixed image ? can be trained like the source domain image. At this time, using contrastive learning to improve the discriminativeness of features will not have much effect. Therefore, we choose to perform contrastive learning on mixed images . Specifically, for the mixed image , we first obtain the transformed mixed images 1 and 2 through two different random transformations. Then, we use the feature extractor to obtain the feature maps f 1 , f 2 corresponding to 1 and 2 , whose resolution ? ? ? is 1 8 of the native resolution ? . In order to alleviate the interference of wrong samples on the prototype, we only use the features with high confidence to calculate the prototype. Therefore, we can get the prototype of each category as follows.</p><formula xml:id="formula_7">1, = ? 1 ?? f 1 ? (1 ? ),<label>(8)</label></formula><formula xml:id="formula_8">2, = ? 1 ?? f 2 ? (1 ? ).<label>(9)</label></formula><p>is a binary mask, 1 indicates that the predicted category of this position is and the predicted probability is greater than , 0 indicates that the predicted category is other categories or predicted probability is less than . For a query sample p 1, , the prototype p 2, is the positive and all other prototype are the negative. In domain adaptation tasks, it is not enough to improve the discriminativeness of features, and we also need to narrow the distance between features and classifier weights <ref type="bibr" target="#b21">[22]</ref>. Therefore, following PCL <ref type="bibr" target="#b21">[22]</ref>, we compute the contrastive loss using the probabilities instead of features and then the prototype-level contrastive loss (ProCL) is defined as:</p><formula xml:id="formula_9">? p 1, = ?log exp( 1 (p 1, ,p 2, ) ? exp( 1 (p 1, , p 1, ) + exp( 1 (p 1, ,p 2, )</formula><p>.</p><p>In addition to prototype-level contrastive learning, we further consider pixel-level contrastive learning. For a query sample f 1, , the feature f 2, at the corresponding position of another transformed image is taken as a positive sample, and all the other features are taken as negative samples. The pixel-level contrastive learning loss (PiCL) is defined as follows:</p><formula xml:id="formula_11">? f 1, = ?log exp( 2 (f 1, , f 2, ) ? exp( 2 (f 1, , f 1, ) + exp( 2 (f 1, , f 2, )</formula><p>. <ref type="formula" target="#formula_0">(11)</ref> where 1 and 2 are the scaling factors, and we set 1 to 7 and set 2 to 20. ( , ) = ( ( )) ? ( ( )) calculates the inner product of the classification probabilities corresponding to the two vectors and represents the softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss Function</head><p>Our loss function is defined as: Here L = (? p 1, + ? p 2, ) and L = (? f 1, + ? f 2, ). 1 and 2 are the hyper-parameters to balance different losses.</p><formula xml:id="formula_12">= L + 1 (L + L ? ) + 2 (L + L ).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Setup</head><p>5.1.1 Dataset. We evaluate the performance of our methods on the two standard UDA semantic segmentation tasks: GTA5?Cityscapes and SYNTHIA?Cityscapes. GTA5 <ref type="bibr" target="#b27">[28]</ref> is a synthetic dataset consisting of 24, 966 annotated images with 1914?1052 resolution taken from the GTA5 game, which is used as a source domain dataset. SYNTHIA <ref type="bibr" target="#b28">[29]</ref> is also a synthetic dataset consisting of 9, 400 annotated images with 1280 ? 720 resolution, which is used as another source domain dataset. Cityscapes <ref type="bibr" target="#b9">[10]</ref> is a representative dataset in semantic segmentation and autonomous driving domain, which contains 5000 pixel-level annotated images with 2048 ? 1024 resolution taken from real urban street scenes and is use as the target domain dataset. In all experiments, we can obtain 2975 unlabeled target domain images for training and we use 500 validation images for test. Following existing works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>, we evaluate our method on 19 common categories for GTA5?Cityscapes and both 16 and 13 common categories for SYNTHIA?Cityscapes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Implementation Details.</head><p>Following common UDA protocols <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref>, we use the DeepLab-v2 segmentation model <ref type="bibr" target="#b2">[3]</ref> with a ResNet-101 <ref type="bibr" target="#b17">[18]</ref> backbone pre-trained on ImageNet <ref type="bibr" target="#b10">[11]</ref> as our model. Following <ref type="bibr" target="#b4">[5]</ref>, we first perform warm-up training on source domain to obtain an initialized model. Then we train the model with our method. Specifically, we use the "poly" learning weight decay and the power is set to 0.9. The SGD optimizer is implemented with weight decay 5 ? 10 ?4 and momentum 0.9. The learning rate is set at 2.5 ? 10 ?4 for backbone parameters and 2.5 ? 10 ?3 for others. The maximum iteration number is 250 and we use an early stopping strategy. For the hyper-parameters in our method, we set 1 = 0.1 in all experiments. We set 2 = 0.01 for GTA5 and set 2 = 0.008 for SYNTHIA. All experiments of our method are conducted on a single NVIDIA RTX 3090Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to State-of-the-Art Methods</head><p>We compare our method with the state-of-the-art UDA methods on two common UDA benchmarks. Depending on whether the distillation technique is used, we divide these methods into two broad categories. <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> give the results. From the results, we have the following observations. First, our method outperforms all non-distillation methods, especially under the SYNTHIA?Cityscapes setting, where our method outperforms the second-best method by 4.5% mIoU and 3.4% mIoU on 16 classes and 13 classes. Second, we found that ProDA, the first method using distillation technology, outperforms all previous nondistillation methods. However, our method not only outperforms ProDA, but also a series of subsequent methods based on distillation techniques. For example, our method is better than CRA <ref type="bibr" target="#b32">[33]</ref> by 0.2% in GTA5?Cityscapes and 0.9% in SYNTHIA?Cityscapes. Third, it should be noted that our method not only outperforms the distillation-based methods, but also simplifies the training process. In particular, our method does not require an additional two-step distillation process, nor does it require special training techniques.</p><p>In addition, we qualitatively compare our model with the source only model (L ) and baseline model (L +L ) by showing the segmentation results in <ref type="figure" target="#fig_3">Figure 6</ref>. It can be seen that baseline significantly improves the segmentation results compared to the source only model. But for the thin objects such as "bicycle" (yellow box) and "pole"(green box), almost all pixels are boundary samples, resulting in an incorrect prediction by the baseline model. After using our proposed HTCM and multi-level contrastive loss, many of these mispredicted pixels are corrected back. This demonstrates the effectiveness of our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>In this section, we conduct experiments to reveal the effectiveness of our proposed method. All experiments are particularly conducted on GTA5?Cityscapes benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effect of Components.</head><p>In this section, we validate the individual effects of our proposed HTCM, ProCL, and PiCL. As shown in <ref type="table" target="#tab_2">Table 3</ref>, our final method achieves an improvement of 6.7% over the model only using CMix, while removing each one of our components will cause a performance drop comparing to our final method.</p><p>The results validate that each of our proposed components plays an important role in UDA semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Comparing Different</head><p>Mix-up Strategies. In this section, we compare different mix-up strategies to demonstrate the superiority of HTCM. In particular, we compare DACS <ref type="bibr" target="#b29">[30]</ref> and DSP <ref type="bibr" target="#b13">[14]</ref>. <ref type="table" target="#tab_3">Table 4</ref> gives the results. First, compared to DACS, which explicitly constructs target-domain-style context for source boundaries by copying source pixels into a target image, our method HTCM   exceeds it by 0.7%. This demonstrates the importance of constructing target boundaries with correct labels. In addition, combining HTCM and DACS can further improve performance, proving that our method is orthogonal to DACS. Secondly, DSP additionally introduces a mix-up strategy of source-to-source based on DACS.</p><p>To be fair, we use DACS+HTCM to compare with DSP. In particular, DSP adopts a soft-paste strategy to improve performance. We use hard or soft to distinguish whether it adopts this strategy. It can be seen that the source-to-source strategy used by DSP is inferior to HTCM. This further demonstrates the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Confident Threshold .</head><p>In this section, we analyze the impact of different thresholds on performance. In <ref type="table" target="#tab_4">Table 5</ref>, we show the results for = 0, 0.35, 0.55, 0.75, 0.95, 0.97. We find that setting relatively high threshold ( = 0.95) can more effectively boost performance (compared to = 0). This well shows the importance of constructing the boundaries with high-confidence pixels. However, when we increase the threshold from 0.95 to 0.97, the mIoU decreases from 56.6% to 55.8%. This is because a too high threshold would filter too many pixels, resulting in insufficient learning. In this section, we compare the performance of and ? for contrastive learning. <ref type="table" target="#tab_5">Table 6</ref> gives the results. It can be seen that the contrastive learning on ? is 2 % lower than the contrastive learning on . As analyzed in Sec. 4.2, almost all pixels in ? have the correct label, the classification loss at this time provides a good enough supervision signal to learn the discriminative feature representation. As a result, the effect of using contrastive learning to enhance the discriminative of ? features would become very weak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Training Cost Discussion.</head><p>Here we particularly compare the training costs of DSP <ref type="bibr" target="#b13">[14]</ref> and ProDA <ref type="bibr" target="#b38">[39]</ref> with our method on GTA5?Cityscapes. <ref type="table" target="#tab_6">Table 7</ref> gives the results. Compared with ProDA and DSP, our method has not only lower training cost but also higher performance. It demonstrates the superiority of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we design a simple yet effective non-distillation framework for UDA semantic segmentation. We propose a novel mix-up strategy to generate high-quality target domain boundary pixels. Specifically, we copy the high-confidence target domain boundary pixels into the source image so that some high-confidence target domain pixels become new boundary pixels. In addition, we propose multi-level contrastive learning to obtain more discriminative feature representations, which contains pixel-to-pixel and prototype-to-prototype contrastive loss. We experimentally verified the effectiveness of our proposed methods on GTA5?Cityscapes and SYNTHIA?Cityscapes. We believe that our work provides a feasible technical route for designing efficient UDA semantic segmentation methods that bypass the distillation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the National Natural Science Foundation of China under Grant 62176246 and 61836008.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>False foreground pixels under different thresholds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Framework of our method. CMix represents the class mix used in DACS. 1 and 2 represent two random transformations. In addition to Cmix, our method generates high-quality target domain boundary pixels via HTCM and performs multi-level contrastive learning on the mixed image . D share the same classes. The goal of UDA semantic segmentation is to use D and D to train a semantic segmentation model such that can correctly predict the class of each pixel in the target domain image. Generally, the model = ? can be regarded as a composite of a feature extractor and a classifier .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Visual demonstration of the HTCM algorithm. The green parts represent high-confidence target domain pixels. The red part of the mixed image represents the border pixels with the correct label, while the yellow part represents the border pixels with the wrong label. It can be seen that most of the border pixels in the mixed image have the correct labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the segmentation results on GTA5?Cityscapes task. Baseline means that the model is only trained with L and L .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the GTA5?Cityscapes benchmark. D means using distillation technique.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>road</cell><cell>sdwk</cell><cell>bld</cell><cell>wall</cell><cell>fnc</cell><cell>pole</cell><cell>lght</cell><cell>sign</cell><cell>veg.</cell><cell>trrn.</cell><cell>sky</cell><cell>pers</cell><cell>rdr</cell><cell>car</cell><cell>trck</cell><cell>bus</cell><cell>trn</cell><cell>mtr</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>Source only</cell><cell></cell><cell cols="20">58.6 13.5 64.1 16.8 14.0 23.8 36.2 19.8 80.3 19.5 66.3 58.9 28.7 64.9 28.7 3.5 8.0 29.6 35.3 35.3</cell></row><row><cell>DACS [30]</cell><cell>WACV 2020</cell><cell cols="20">89.9 39.7 87.9 30.7 39.5 38.5 46.4 52.8 88.0 44.0 88.8 67.2 35.8 84.5 45.7 50.19 0.0 27.3 34.0 52.1</cell></row><row><cell>SPCL [35]</cell><cell>arXiv 2021</cell><cell cols="20">90.3 50.3 85.7 45.3 28.4 36.8 42.2 22.3 85.1 43.6 87.2 62.8 39.0 87.8 41.3 53.9 17.7 35.9 33.8 52.1</cell></row><row><cell>RCCR [43]</cell><cell>arXiv 2021</cell><cell cols="20">93.7 60.4 86.5 41.1 32.0 37.3 38.7 38.6 87.2 43.0 85.5 65.4 35.1 88.3 41.8 51.6 0.0 38.0 52.1 53.5</cell></row><row><cell>SAC [1]</cell><cell>CVPR 2021</cell><cell cols="20">90.4 53.9 86.6 42.4 27.3 45.1 48.5 42.7 87.4 40.1 86.1 67.5 29.7 88.5 49.1 54.6 9.8 26.6 45.3 53.8</cell></row><row><cell>Pixmatch [26]</cell><cell>CVPR 2021</cell><cell cols="20">91.6 51.2 84.7 37.3 29.1 24.6 31.3 37.2 86.5 44.3 85.3 62.8 22.6 87.6 38.9 52.3 0.7 37.2 50.0 50.3</cell></row><row><cell>DPL [9]</cell><cell>ICCV 2021</cell><cell cols="20">92.8 54.4 86.2 41.6 32.7 36.4 49.0 34.0 85.8 41.3 83.0 63.2 34.2 87.2 39.3 44.5 18.7 42.6 43.1 53.3</cell></row><row><cell>BAPA [24]</cell><cell>ICCV 2021</cell><cell cols="20">94.4 61.0 88.0 26.8 39.9 38.3 46.1 55.3 87.8 46.1 89.4 68.8 40.0 90.2 60.4 59.0 0.00 45.1 54.2 57.4</cell></row><row><cell>UPST [32]</cell><cell>ICCV 2021</cell><cell cols="20">90.5 38.7 86.5 41.1 32.9 40.5 48.2 42.1 86.5 36.8 84.2 64.5 38.1 87.2 34.8 50.4 0.2 41.8 54.6 52.6</cell></row><row><cell>DSP [14]</cell><cell cols="21">ACM MM 2021 92.4 48.0 87.4 33.4 35.1 36.4 41.6 46.0 87.7 43.2 89.8 66.6 32.1 89.9 57.0 56.1 0.0 44.1 57.8 55.0</cell></row><row><cell>MFA (D) [38]</cell><cell>BMVC 2021</cell><cell cols="20">93.5 61.6 87.0 49.1 41.3 46.1 53.5 53.9 88.2 42.1 85.8 71.5 37.9 88.8 40.1 54.7 0.0 48.2 62.8 58.2</cell></row><row><cell>ProDA (D) [39]</cell><cell>CVPR 2021</cell><cell cols="20">87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4 57.5</cell></row><row><cell cols="2">ProDA + CaCo (D) [20] CVPR 2022</cell><cell cols="20">93.8 64.1 85.7 43.7 42.2 46.1 50.1 54.0 88.7 47.0 86.5 68.1 2.9 88.0 43.4 60.1 31.5 46.1 60.9 58.0</cell></row><row><cell cols="2">ProDA + CRA (D) [33] arXiv 2021</cell><cell cols="20">89.4 60.0 81.0 49.2 44.8 45.5 53.6 55.0 89.4 51.9 85.6 72.3 40.8 88.5 44.3 53.4 0.0 51.7 57.9 58.6</cell></row><row><cell>Ours</cell><cell></cell><cell cols="20">95.4 68.8 88.1 37.1 41.4 42.5 45.7 60.4 87.3 42.6 86.8 67.4 38.6 90.5 66.7 61.4 0.3 39.4 56.1 58.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on the SYNTHIA?Cityscapes benchmark. mIoU-16 and mIoU-13 refer to mean intersection-over-union on the standard sets of 16 and 13 classes, respectively. Classes not evaluated are replaced by '-'. D means using distillation technique.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>road</cell><cell>sdwk</cell><cell>bld</cell><cell>wall  *</cell><cell>fnc  *</cell><cell>pole  *</cell><cell>light</cell><cell>sign</cell><cell>veg.</cell><cell>sky</cell><cell>pers</cell><cell>rdr</cell><cell>car</cell><cell>bus</cell><cell>mtr</cell><cell>bike</cell><cell cols="2">mIoU-16 mIoU-13</cell></row><row><cell>Source only</cell><cell></cell><cell cols="16">40.1 18.3 64.9 5.9 0.1 24.6 5.9 9.0 74.8 81.6 58.7 16.8 43.9 11.8 6.4 24.11</cell><cell>30.4</cell><cell>35.1</cell></row><row><cell>DACS [30]</cell><cell>WACV 2020</cell><cell cols="16">80.6 25.1 81.9 21.5 2.9 37.2 22.7 24.0 83.7 90.8 67.6 38.3 82.9 38.9 28.5 47.6</cell><cell>48.3</cell><cell>54.8</cell></row><row><cell>SPCL [35]</cell><cell>arXiv 2021</cell><cell cols="16">86.9 43.2 81.6 16.2 0.2 31.4 12.7 12.1 83.1 78.8 63.2 23.7 86.9 56.1 33.8 45.7</cell><cell>47.2</cell><cell>54.4</cell></row><row><cell>RCCR [43]</cell><cell>arXiv 2021</cell><cell cols="4">79.4 45.3 83.3 -</cell><cell>-</cell><cell cols="11">-24.7 29.6 68.9 87.5 63.1 33.8 87.0 51.0 32.1 52.1</cell><cell>-</cell><cell>56.8</cell></row><row><cell>SAC [1]</cell><cell>CVPR 2021</cell><cell cols="16">89.3 47.2 85.5 26.5 1.3 43.0 45.5 32.0 87.1 89.3 63.6 25.4 86.9 35.6 30.4 53.0</cell><cell>52.6</cell><cell>59.3</cell></row><row><cell>Pixmatch [26]</cell><cell>CVPR 2021</cell><cell cols="16">92.5 54.6 79.8 4.8 0.1 24.1 22.8 17.8 79.4 76.5 60.8 24.7 85.7 33.5 26.4 54.4</cell><cell>46.1</cell><cell>54.5</cell></row><row><cell>DPL [9]</cell><cell>ICCV 2021</cell><cell cols="16">87.5 45.7 82.8 13.3 0.6 33.2 22.0 20.1 83.1 86.0 56.6 21.9 83.1 40.3 29.8 45.7</cell><cell>47.0</cell><cell>54.2</cell></row><row><cell>BAPA [24]</cell><cell>ICCV 2021</cell><cell cols="16">91.7 53.8 83.9 22.4 0.8 34.9 30.5 42.8 86.6 88.2 66.0 34.1 86.6 51.3 29.4 50.5</cell><cell>53.3</cell><cell>61.2</cell></row><row><cell>UPST [32]</cell><cell>ICCV 2021</cell><cell cols="16">79.4 34.6 83.5 19.3 2.8 35.3 32.1 26.9 78.8 79.6 66.6 30.3 86.1 36.6 19.5 56.9</cell><cell>48.0</cell><cell>54.6</cell></row><row><cell>DSP [14]</cell><cell cols="17">ACM MM 2021 86.4 42.0 82.0 2.1 1.8 34.0 31.6 33.2 87.2 88.5 64.1 31.9 83.8 65.4 28.8 54.0</cell><cell>51.0</cell><cell>59.9</cell></row><row><cell>MFA (D) [38]</cell><cell>BMVC 2021</cell><cell cols="4">81.8 40.2 85.3 -</cell><cell>-</cell><cell cols="11">-38.0 33.9 82.3 82.0 73.7 41.1 87.8 56.6 46.3 63.8</cell><cell>-</cell><cell>62.5</cell></row><row><cell>ProDA (D) [39]</cell><cell>CVPR 2021</cell><cell cols="16">87.8 45.7 84.6 37.1 0.6 44.0 54.6 37.0 88.1 84.4 74.2 24.3 88.2 51.1 40.5 45.6</cell><cell>55.5</cell><cell>62.0</cell></row><row><cell cols="2">ProDA + CRA (D) [33] arXiv 2021</cell><cell cols="16">85.6 44.2 82.7 38.6 0.4 43.5 55.9 42.8 87.4 85.8 75.8 27.4 89.1 54.8 46.6 49.8</cell><cell>56.9</cell><cell>63.7</cell></row><row><cell>Ours</cell><cell></cell><cell cols="16">93.0 69.8 84.0 36.6 9.1 39.7 42.2 43.8 88.2 88.1 68.3 29.0 85.5 54.1 37.1 56.3</cell><cell>57.8</cell><cell>64.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on our proposed components.</figDesc><table><row><cell cols="5">CMix PiCL ProCL HTCM mIoU (%)</cell></row><row><cell>? ? ? ? ? ? ? ?</cell><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>52.1 55.2 55.6 56.6 56.2 57.5 57.7 58.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different mix-up strategies.</figDesc><table><row><cell>Methods</cell><cell>mIoU (%)</cell></row><row><cell>Source Only</cell><cell>35.3</cell></row><row><cell>DACS</cell><cell>52.1</cell></row><row><cell>HTCM</cell><cell>52.8</cell></row><row><cell>DSP (Hard)</cell><cell>53.6</cell></row><row><cell>DSP (Soft)</cell><cell>54.5</cell></row><row><cell>DACS+HTCM</cell><cell>56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of confident threshold . A threshold of ? [0, 1) corresponds to the confident score of pseudo labeled pixels which is used for GHTB.</figDesc><table><row><cell></cell><cell>0.00</cell><cell>0.35</cell><cell>0.55</cell><cell>0.75</cell><cell>0.95</cell><cell>0.97</cell></row><row><cell>mIoU</cell><cell>52.9</cell><cell>53.8</cell><cell>54.6</cell><cell>55.2</cell><cell>56.6</cell><cell>55.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The effect of contrastive learning on different mixed images. Contrastive Learning with Different Images.</figDesc><table><row><cell>Mixed Image</cell><cell>?</cell><cell></cell></row><row><cell>mIoU</cell><cell>56.8</cell><cell>58.8</cell></row><row><cell>5.3.4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Training cost for different methods.</figDesc><table><row><cell>Method</cell><cell>GPUs</cell><cell>Time (hours)</cell><cell>mIoU</cell></row><row><cell>ProDA</cell><cell>4*Tesla-V100-32GB</cell><cell>108</cell><cell>57.5</cell></row><row><cell>DSP</cell><cell>1*RTX-3090-24GB</cell><cell>96</cell><cell>55.0</cell></row><row><cell>Ours</cell><cell>1*RTX-3090-24GB</cell><cell>63</cell><cell>58.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15384" to="15394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2090" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial-learned loss for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3521" to="3528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dual Path Learning for Domain Adaptation of Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9082" to="9091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation via dynamic selftraining and classbalanced curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dmt: Dynamic mutual training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dsp: Dual soft-paste for unsupervised domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2825" to="2833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Category contrast for unsupervised domain adaptation in visual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoran</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1203" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextualrelation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="705" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06021</idno>
		<title level="m">Semantic-aware Representation Learning Via Probability Contrastive Loss</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">BAPA-Net: Boundary adaptation and prototype alignment for cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8801" to="8811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PixMatch: Unsupervised domain adaptation via pixelwise consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Kyriazi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun K</forename><surname>Manrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12435" to="12445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via cross-domain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junran</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9092" to="9101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cross-Region Domain Adaptation for Class-level Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06422</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12635" to="12644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binhui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejia</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12358</idno>
		<title level="m">SPCL: A New Framework for Domain Adaptive Semantic Segmentation via Semantic Prototype-based Contrastive Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-ensembling attention networks: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5581" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiple Fusion Adaptation: A Strong Framework for Unsupervised Semantic Segmentation Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Low-confidence Samples Matter for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02802</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint adversarial learning for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6877" to="6884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuyun</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05170</idno>
		<title level="m">Domain Adaptive Semantic Segmentation with Regional Contrastive Consistency Regularization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
