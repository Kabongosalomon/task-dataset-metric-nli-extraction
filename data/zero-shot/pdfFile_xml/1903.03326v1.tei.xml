<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge-Embedded Routing Network for Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
							<email>tianshuichen@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
							<email>weihaoyu6@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-Sen University DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Sun Yat-Sen University DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge-Embedded Routing Network for Scene Graph Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To understand a scene in depth not only involves locating/recognizing individual objects, but also requires to infer the relationships and interactions among them. However, since the distribution of real-world relationships is seriously unbalanced, existing methods perform quite poorly for the less frequent relationships. In this work, we find that the statistical correlations between object pairs and their relationships can effectively regularize semantic space and make prediction less ambiguous, and thus well address the unbalanced distribution issue. To achieve this, we incorporate these statistical correlations into deep neural networks to facilitate scene graph generation by developing a Knowledge-Embedded Routing Network. More specifically, we show that the statistical correlations between objects appearing in images and their relationships, can be explicitly represented by a structured knowledge graph, and a routing mechanism is learned to propagate messages through the graph to explore their interactions. Extensive experiments on the large-scale Visual Genome dataset demonstrate the superiority of the proposed method over current state-ofthe-art competitors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene graph <ref type="bibr" target="#b12">[13]</ref> is a structured representation of image content that not only encodes semantic and spatial information of individual objects in the scene but also represents the relationship between each pair of objects. In recent years, inferring such graph has drawn increasing attentions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b5">6]</ref> as it provides a deeper understanding for the image * Tianshui Chen and Weihao Yu share first-authorship. Corresponding author is Liang Lin.  <ref type="bibr" target="#b32">[33]</ref> on the scene graph classification task on the Visual Genome dataset <ref type="bibr" target="#b13">[14]</ref>. Both models are trained on the whole training set and evaluated on the two subsets, respectively. Note that SMN is the previous best-performing method. and thus facilitates various vision tasks ranging from fundamental recognition and detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b7">8]</ref> to high-level tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Existing methods for scene graph generation rely on the target object regions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6]</ref> or further introduce contextual cues <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> to aid recognition. Generally, these methods require large amounts of annotated samples for model optimization. However, the distribution of real-world relationships is seriously uneven, leading to relatively poor performance for the relationships with limited training samples. Take the Visual Genome dataset <ref type="bibr" target="#b13">[14]</ref> as an example, we evaluate the performance on samples of top 10 most frequent relationships (namely "top 10" subset) and that on samples of the rest less frequent relationships (namely "the rest" subset), respectively. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, current best-performing method (i.e, SMN <ref type="bibr" target="#b32">[33]</ref>) can achieve competitive performance if it has sufficient training samples, but its performance suffers from a severe drop otherwise.</p><p>Objects in visual scene commonly have strongly structured regularities <ref type="bibr" target="#b32">[33]</ref>. For example, people tend to wear clothes, while cars are inclined to have wheels. The statistical analysis <ref type="bibr" target="#b32">[33]</ref> on the Visual Genome dataset <ref type="bibr" target="#b13">[14]</ref> revealed that a baseline method, which directly predicts the most frequent relationship of object pairs with given labels, outperforms most existing state-of-the-art methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>. Therefore, modeling these statistical correlations between object pairs and relationships can effectively regularize the semantic prediction space, and thus address the uneven distribution issue. On the other hand, the interplay of relationships and objects in the scene also plays a significant role in scene graph generation <ref type="bibr" target="#b29">[30]</ref>.</p><p>We show that the statistical correlations between object pairs and their relationships can be explicitly represented by a structured knowledge graph, and the interplay between these two factors can be captured by propagating node messages through the graph. Similarly, contextual cues can also be represented and explored by another graph with proper message propagation. In this work, we introduce a novel Knowledge-Embedded Routing Network (KERN), which captures the interplay of target objects and their relationships under the explicit guidance of prior statistical knowledge and automatically mines contextual cues to facilitate scene graph generation. Although previous studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> have also taken notice of the statistical knowledge, they merely implicitly mine this information by iterative message propagation between relationships and objects <ref type="bibr" target="#b29">[30]</ref> or by encoding the global context of objects and relationships <ref type="bibr" target="#b32">[33]</ref>. Instead, our model formally represents this statistical knowledge in the form of a structured graph and incorporates the graph into deep propagation network as extra guidance. In this way, it can effectively regularize the distribution of possible relationships of object pairs and thus make prediction less ambiguous. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, compared with current best-performing method (i.e., SMN <ref type="bibr" target="#b32">[33]</ref>), our model achieves slight improvement for the relationships with sufficient samples, and the improvement is much more evident for the relationships with limited samples.</p><p>Our model builds on the Faster RCNN detector <ref type="bibr" target="#b24">[25]</ref> to generate a set of object regions. Then, a graph that correlates these regions according to the statistical object cooccurrences is first built, and a propagation network is employed to propagate node messages through the graph to learn contextualized feature representation to predict the class label regarding each region. For each object pair with predicted labels, we build a graph, in which nodes represent the objects and relationships, and edges represent the statistical co-occurrence probabilities between the given object pair and all relationships. Further, we adopt another propagation network to explore the interplay between the relationships and corresponding objects to predict their relationship. This process is performed for all object pairs, and the whole scene graph is generated.</p><p>On the other hand, existing works utilize the recall@K (short as R@K) <ref type="bibr" target="#b18">[19]</ref> as the evaluation metric. However, this metric is easily dominated by the performance of the relationships with a large proportion of samples. As the distribution of different relationships is severely uneven, if one method performs well on several most frequent relationships, it can achieve a high R@K score. Thus, it can not well measure the performance of all relationships. To address this issue, we further propose a mean recall@K (short as mR@K) as a complimentary evaluation metric. It first computes the R@K for samples of each relationship and then averages over all relationships to obtain mR@K. Compared with R@K, mR@K can give a more comprehensive performance evaluation for all relationships.</p><p>To the best of our knowledge, this work is the first to explicitly unify the statistical knowledge with the deep architecture to facilitate scene graph generation. Compared with existing methods, our model incorporates this knowledge to regularize the semantic space of relationship prediction and thus improves the performance of scene graph generation. We conduct experiments on the most widely used and challenging Visual Genome dataset <ref type="bibr" target="#b13">[14]</ref>, and demonstrate our model can achieve best R@K performance than existing leading competitors. Notably, by explicitly regularizing the semantic space of relationship prediction, our model can well address the issue of uneven distribution of realworld relationships and achieves much more obvious improvement on the mR@K metric. For example, our model improves the mR@50 and mR@100 from 15.4% and 20.6% to 19.8% and 26.2% on the scene graph classification task, with relative improvements of 28.6% and 27.2%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual relationship detection</head><p>Visual relationship detection involves detecting semantic objects that occur in the images and inferring the relationship between each object pair (i.e., a subject and an object). Over the past decade, a series of works were dedicated to recognizing spatial relationships <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5]</ref> like "above", "below", "inside", and "around", and to exploring using these relationships to improve various vision tasks such as object recognition <ref type="bibr" target="#b8">[9]</ref>, detection <ref type="bibr" target="#b7">[8]</ref>, and segmentation <ref type="bibr" target="#b10">[11]</ref>. Some other works also attempted to learn human-object interactions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1]</ref>, in which the subject was a person.</p><p>Latterly, lots of attentions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref> were drawn to the visual relationship detection task under a more general and practical setting, where the subject and object can be any objects in the scene and their relationships cover a wide range of relationship types including spatial (e.g., above, below), actions (e.g., ride, wear), affiliations (e.g., part of), etc. As a pioneer work, Lu et al. <ref type="bibr" target="#b18">[19]</ref> trained vi-sual models of subject, relationship, and object individually to tackle the problem of the long-tail distribution of relationship triplets and leveraged language prior from semantic word embedding to further improve the predicted performance. Xu et al. <ref type="bibr" target="#b29">[30]</ref> introduced an end-to-end model that learned to iteratively refine relationship and object prediction via message passing based on the RNNs <ref type="bibr" target="#b20">[21]</ref>. Li et al. <ref type="bibr" target="#b15">[16]</ref> formulated a multi-task framework to explore semantic associations over three tasks of object detection, scene graph generation, and image caption generation, and found that jointly learning the three tasks could bring about mutual improvements. More recently, Dai et al. <ref type="bibr" target="#b5">[6]</ref> designed a deep relational network that exploited both spatial configuration and statistical dependency to resolve the ambiguities during relationship recognition. Zeller et al. <ref type="bibr" target="#b32">[33]</ref> presented an analysis of statistical co-occurrences between relationships and object pairs on the Visual Genome dataset <ref type="bibr" target="#b13">[14]</ref> and came to a conclusion that these statistical co-occurrences provided strong regularization for relationship prediction. They encoded the global context of objects and relationships by LSTM sequential architectures <ref type="bibr" target="#b11">[12]</ref> to facilitate scene graph parsing.</p><p>The works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> also took notice of the statistical cooccurrences between object pair and their relationship, but they devised deep models to implicitly mine this information via message passing. Different from these works, our model formally represents this information and explicitly incorporates them into graph propagation network to help scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge representation</head><p>It has been extensively studied to incorporate prior knowledge to aid numerous vision tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref>. For example, Marino et al. <ref type="bibr" target="#b19">[20]</ref> constructed a knowledge graph based on the WordNet <ref type="bibr" target="#b21">[22]</ref> and the Visual Genome dataset <ref type="bibr" target="#b13">[14]</ref>, and learned the representation of this graph to enhance image feature representation to promote multilabel recognition. Lee et al. <ref type="bibr" target="#b14">[15]</ref> further extended this method to multi-label zero-shot learning. Some works also utilized the knowledge graph as extra constraints for model training. Fang et al. <ref type="bibr" target="#b7">[8]</ref> incorporated semantic consistency into object detection systems with the constraint that more semantically consistent concepts were more likely to occur in an image. Deng et al. <ref type="bibr" target="#b6">[7]</ref> introduced semantic relations including mutual exclusion, overlap, and subsumption, as constraints in the loss function to train the classifiers. These methods learned graph representation for feature enhancement or use graph as extra constraints on the loss functions. Differently, our model introduces the graph that correlates target object pair and their possible relationships to explicitly regularize the semantic space of relationship prediction, and thus addresses the uneven distribution issue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model</head><p>A scene graph is a structured representation of content in an image. It consists of the class labels and locations of individual objects and the relationship between each object pair, which can be defined as a 3-tuple set G = {B, O, R}:</p><formula xml:id="formula_0">? B = {b 1 , b 2 , . . . , b n } is the region candidate set, with element b i ? R 4 denoting the bounding box of the i-th region. ? O = {o 1 , o 2 , . . . , o n } is the object set, with element o i ? N denoting the corresponding class label regard- ing region b i . ? R = {r 1?2 , r 1?3 , . . . , r n?n?1 } is the corresponding relationship triplet set, where r i?j is a triplet of a sub- ject (b i , o i ) ? B ? O, an object (b j , o j ) ? B ? O, and a relationship label x i?j ? R.</formula><p>R is the set of all relationships including no-relationship that indicates no relationship between the given object pair.</p><p>Given an image I, we decompose the probability distribution of the scene graph p(G|I) into three components similar to <ref type="bibr" target="#b32">[33]</ref>: p(G|I) = p(B|I)p(O|B, I)p(R|O, B, I).</p><p>(1)</p><p>In this equation, the bounding box component p(B|I) generates a set of candidate regions that cover most of the key objects directly from the input image. Similar to previous scene graph works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>, this component is implemented by the widely used Faster RCNN detector <ref type="bibr" target="#b24">[25]</ref>. The object component p(O|B, I) then predicts the class label regarding each detected region. Here, we construct a graph that correlates the detected regions based on the statistical object co-occurrence information (see <ref type="figure" target="#fig_1">Figure 2</ref>(a)). Then, our model adopts a graph neural network <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref> to propagate messages through the graph to learn contextualized representation for each region and achieves better label prediction under the constraint of statistical information of object co-occurrences. Conditioned on the predicted labels, the relationship component p(R|O, B, I) infers the relationship  <ref type="figure">Figure 3</ref>. An overall pipeline of the knowledge-embedded routing network. Given an image, we first adopt the Faster RCNN to detect a set of regions. Then, a graph is built to correlate the regions, and a graph neural network is employed to learn contextualized representation to predict the class label for each region. For each object pair with predicted labels, we build another graph to correlate the given object pair with all the possible relationships and employ a graph neural network to infer their relationship. The process is repeated for all object pairs and the scene graph is generated.</p><formula xml:id="formula_1">? ? ? ? ? ? ? ? ? ??? ??? " , $ GNN ??? GNN GNN % , &amp; GNN ??? GNN GNN ' , ( GNN ??? GNN GNN %?&amp; "?$ '?(</formula><p>of each object pair and finally generates the whole scene graph. For each object pair with predicted labels, we construct a graph, in which nodes refer to the objects and relationships, and edges represent the statistical co-occurrences between the corresponding object pair and all the relationships (see <ref type="figure" target="#fig_1">Figure 2</ref>(b)). Similarly, another graph neural network is learned to explore the interplay between relationships and objects, and finally, the features from all nodes are aggregated to predict the relationship. Our model performs this process for all object pairs and generates the whole scene graph. <ref type="figure">Figure 3</ref> illustrates an overall pipeline of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bounding box localization</head><p>Given an image, the model first obtains a set of candidate regions. In this work, we utilize the Faster RCNN <ref type="bibr" target="#b24">[25]</ref> to automatically generate the region set B = {b 1 , b 2 , . . . , b n } directly from input image I. For each region, besides a bounding box b i ? R 4 denoting its position, our model also extracts a feature vector f i using the ROI pooling layer <ref type="bibr" target="#b9">[10]</ref>. These feature vectors are then fed into the propagation networks for subsequent inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Knowledge-embedded routing network</head><p>Object. Statistical information of object co-occurrence is a crucial cue to correlate objects in an image and regularizes object label prediction. In this work, we build a graph to as-sociate the regions detected in the image according to these statistical correlations and employ a graph neural network to propagate messages through the graph that can learn contextualized representation to predict the class label regarding each region.</p><p>To this end, we first count the statistical co-occurrence probabilities of objects from different categories on the training set of the target dataset (e.g., Visual Genome <ref type="bibr" target="#b13">[14]</ref>). More specifically, for two categories of c and c , we count the probability m cc of the existence of object belonging to category c in the presence of object belonging to category c . We count these co-occurrence probabilities for all category pair and obtain a matrix M c ? R C?C , where C is the number of object categories. We then correlate the regions from B based on the matrix M c . Given two regions of b i and b j , we duplicate b i C times to obtain C nodes {b i1 , b i2 , . . . , b iC }, with node b ic denoting the correlation of region b i with category c. The same process is performed for b j . Intuitively, m cc can be used to correlate node b jc to b ic , and thus M c can be used to correlate nodes of region b i and nodes of b j . In this way, we can correlate all regions and construct the graph.</p><p>Inspired by the Graph Gated Neural Networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29]</ref>, we adopt a gated recurrent update mechanism to iterative propagate node messages through the graph. Specifically, at timestep t, each node b ic has a hidden state h t ic . As each node corresponds to a specific region, we use the feature vector of this region to initialize the hidden state at t = 0, which can be expressed as</p><formula xml:id="formula_2">h 0 ic = ? o (f i ),<label>(2)</label></formula><p>where ? o is a transformation that maps f i to a feature vector of low dimension, and it is implemented by a fully connected layer. At each timestep t, each node aggregates messages from its neighbors according to the graph structure, formulated as</p><formula xml:id="formula_3">a t ic = ? ? n j=1,j =i C c =1 m c c h t?1 jc , n j=1,j =i C c =1 m cc h t?1 jc ? ? .</formula><p>(3) Then, the model take a t ic and its previous hidden state as input to update its hidden state by a gated mechanism similar to the Gated Recurrent Unit <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> </p><formula xml:id="formula_4">z t ic =?(W z o a t ic + U z o h t?1 ic ) r t ic =?(W r o a t ic + U r o h t?1 ic ) h t ic = tanh W o a t ic + U o (r t ic h t?1 ic ) h t ic =(1 ? z t ic ) h t?1 ic + z t ic h t ic<label>(4)</label></formula><p>In this way, each node can aggregate messages from its neighbors and meanwhile transfer its message to its neighbors, enabling interactions among all nodes in the graph. After T o steps, the node messages have been propagated through the graph and we obtain the final hidden state for each region i, i.e., {h To i1 , h To i2 , . . . , h To iC }. We use an output network that takes the initial hidden state and final hidden state as input to compute the output feature for each node</p><formula xml:id="formula_5">f o ic = o o (h 0 ic , h T ic ),<label>(5)</label></formula><p>where o o (?) is implemented by a fully connected layer. Finally, for each region, we aggregate all correlated output feature vectors to predict its class label</p><formula xml:id="formula_6">o i = ? o (f o i1 , f o i2 , . . . , h o iC )<label>(6)</label></formula><p>The predicted class label o i = argmax(o i ) are then used for relationship inference. Relationship. Given the categories of object pair, the probability distribution of their relationships is highly skewed. For example, given a subject "man" and an object "horse", their relationship is likely to be "riding". Here, we represent the correlations of object pair and their relationships in the form of a structured graph and adopt another graph neural network to explore the interplay of these two factors to infer the relationship.</p><p>To this, we also count the statistical co-occurrence probability on the training part of the target dataset to obtain these correlations. Concretely, we count the probabilities of all possible relationships given a subject of the category c and an object of the category c , which are denoted as {m cc 1 , m cc 2 , . . . , m cc K }. Here, K is the relationship number. For a subject o i and an object o j taken from the object set O, we construct a graph with a subject node, an object node, and K relationship nodes. We use m oioj k to denote the correlations between o i and relationship node k as well as between o j and relationship node k. In this way, a graph with statistic co-occurrences embedded is built.</p><p>Our model learns to explore the node interaction using the identical graph gated recurrent update mechanism <ref type="bibr" target="#b16">[17]</ref>. Similarly, each node v ? V = {o i , o j , 1, 2, . . . , K} has a hidden state h t v at timestep t. At timestep t = 0, we initialize the object nodes with the feature vectors of corresponding regions and the relationship nodes with the feature vector from the union region of the two objects together with their spatial information</p><formula xml:id="formula_7">h 0 v = ? o (f i ) if v is the object node o i ? r (f ij ) if v is a relationship node ,<label>(7)</label></formula><p>where ? o and ? r are two transformations, and both are implemented by a fully-connected layer, respectively. f ij is a feature vector that encodes the visual feature of the union region of b i and b j as well as the spatial information following <ref type="bibr" target="#b32">[33]</ref>. At each timestep t, the relationship nodes aggregate messages from the object nodes while object nodes aggregate messages from the relationship nodes</p><formula xml:id="formula_8">a t v = K k=1 m oioj k h t?1 k if v is a object node m oioj k (h t?1 oi + h t?1 oj ) if v is the relationship node k .<label>(8)</label></formula><p>Then, the model incorporates these aggregated features with the previous hidden states to update the hidden state for each node using the gated mechanism as Eq. 4. The model repeats the iterations T r times and generates the final hidden state of each node, i.e., {h Tr oi , h Tr oj , h Tr 1 , . . . , h Tr K , }. Similar to <ref type="bibr" target="#b16">[17]</ref>, our model use an output sub-network implemented by a fully-connected layer to compute node-level features and aggregates these features to infer the relationship</p><formula xml:id="formula_9">f o v = o r ([h Tr v , h 0 v ]) x i?j = ? r ([f o oi , f o oj , f o 1 , . . . , f o K ]).<label>(9)</label></formula><p>? r is the relationship classifier implemented by a fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setting</head><p>Implementation details. Similar to prior works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> for scene graph generation, we adopt the Faster RCNN detector <ref type="bibr" target="#b24">[25]</ref>   <ref type="figure" target="#fig_0">1</ref>. Comparison of the mR@50 and mR@100 in % with and without constraint on the three tasks of the VG dataset. We compute Mean mR by averaging mR@50 and mR@100 over the three tasks. As existing works do not present the mR@K metric, we utilize the released models (IMP, FREQ, SMN, AE) or train the model using the released code (IMP+) to generate the results to compute the metric. <ref type="bibr" target="#b25">[26]</ref> as its backbone network as in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>. We follow <ref type="bibr" target="#b32">[33]</ref> to set the input image size as 592 ? 592, and use anchor scales and aspect ratios similar to YOLO-9000 <ref type="bibr" target="#b23">[24]</ref>. Then, we train the detector on the target dataset using the SGD algorithm with a batch size of 18, momentum of 0.9, and weight decay of 0.0001. The learning rate is initialized as 0.001 and is divided by 10 when the mAP of the validation set plateaus. After that, we freeze the weights of all the convolution layers and train the fully-connected layers as well as the stacked graph neural networks using the Adam algorithm with a batch size of 2, and momentums of 0.9 and 0.999. In this process, we initialize the learning rate as 0.00001 and divide it by 10 when the recall of the validation set plateaus. Datasets. We evaluate the proposed method and existing state-of-the-art competitors on the Visual Genome (VG) <ref type="bibr" target="#b13">[14]</ref> benchmark. VG contains 108,077 images with average annotations of 38 objects and 22 relationships per image. It is a challenging and most widely used benchmark for scene graph generation. In the experiments, we follow previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b29">30]</ref> to use the most frequent 150 object categories and 50 relationships and use the training/test split in <ref type="bibr" target="#b29">[30]</ref> for evaluation. Tasks. Scene graph generation aims to predict a set of subject-relationship-object triplets. Following <ref type="bibr" target="#b29">[30]</ref>, we evaluate the proposed model with three task setups as below:</p><p>? Predicate classification (PredCls) predicts the relationship label of given object pair from a set of objects with ground truth annotations of class labels and bounding boxes.</p><p>? Scene graph classification (SGCls) predicts the class labels for the set of objects with ground truth bounding boxes and predicts the relationship label of each object pair.</p><p>? Scene graph generation (SGGen) simultaneously de-tects objects appearing in the image and predicts the relationship label of each object pair.</p><p>Evaluation metrics. All the methods are evaluated using the recall@K (short as R@K) metric that measures the fraction of the ground truth relationship triplets that appear among the top K most confident triplet predictions in an image. However, as shown in <ref type="figure" target="#fig_2">Figure 4(a)</ref>, the distribution of different relationships is seriously uneven, and this metric is easily dominated by the performance of the most frequent relationships. To evaluate the performance of each relationship more comprehensively, we further propose a new metric, i.e., mean recall@K (short as mR@K). This metric computes the R@K for the samples of each relationship, respectively, and then averages R@K over all relationships to obtain mR@K. Some previous works <ref type="bibr" target="#b29">[30]</ref> compute R@K with the constraint that merely one relationship is obtained for a given object pair. Some other works <ref type="bibr" target="#b22">[23]</ref> omit this constraint so that multiple relationships can be obtained, leading to higher values. In this work, we report both the R@K and mR@K with and without constraint respectively for comprehensive comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-art methods</head><p>VG <ref type="bibr" target="#b13">[14]</ref> is the largest and most widely used benchmark for evaluating the scene graph generation task. In this part, we compare our proposed method with the existing state-of-the-art methods, including Visual Relationship Detection (VRD) <ref type="bibr" target="#b13">[14]</ref>, Iterative Message Passing (IMP) <ref type="bibr" target="#b29">[30]</ref> and its improved version by using a better detector (IMP+) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>, Associative Embedding (AE) <ref type="bibr" target="#b22">[23]</ref>, FREQuency baseline (FREQ) <ref type="bibr" target="#b32">[33]</ref>, and Stacked Motif Networks (SMN) <ref type="bibr" target="#b32">[33]</ref>.</p><p>We first present the mR@50 and mR@100 on three tasks on the VG dataset in <ref type="table">Table 1</ref>. As shown, the FREQ baseline method, which directly predicts the most frequent relationship of object pairs with given labels, performs better  <ref type="table">Table 3</ref>. Comparison of the mR@50, mR@100 (above) and the R@50, R@100 (below) with constraint in % of our full model, our model without relationship correlation (w/o rc), and our model without relationship correlation and object correlation (w/o rc &amp; oc). We compute Mean mR by averaging mR@50 and mR@100 over the three tasks and mean R in the same way. than most existing works. This comparison suggests that the statistical correlations between object pairs and their relationships play an equally or even more important role than other information like contextual cues <ref type="bibr" target="#b29">[30]</ref>. SMN is the best-performing method among existing works, which implicitly captures these statistical correlations by encoding global context. It achieves the mean mR of 9.0% and 20.6% under the evaluation settings with and without constraint. By explicitly incorporating the statistical correlations, our method can make better use of them, leading to notable performance improvement. Specifically, it consistently outperforms existing methods on all three tasks under the two settings. For example, it obtains the mean mR of 11.7% and 26.5%, with a relative improvement of 30.0% and 28.6% compared with the previous best-performing method (i.e., SMN). Note that we use prior statistical correlations to aid scene graph generation. But these correlations are obtained merely based on the annotations of samples from the training set, and no additional supervision is introduced. Thus, the preceding comparisons are fair.</p><p>For more comprehensive comparison with existing methods, we also present the R@50 and R@100 on the three tasks on the VG dataset in <ref type="table" target="#tab_2">Table 2</ref>. Still, our method achieves best results on these metrics. Concretely, the mean R is 44.1% and 55.4% under the settings with and without constraint, with an improvement of 0.4% and 0.7% compared with SMN.</p><p>As shown in the above discussion and comparison, our method exhibits an improvement compared with existing state-of-the-art methods, both on the mR@K and R@K metrics. However, we find that the improvement on the mR@K metric is much more obvious than that on the R@K metric. Here, we give a deeper and more comprehensive analysis for this phenomenon. We first present the distribution of different relationships on the VG dataset in <ref type="figure" target="#fig_2">Figure 4(a)</ref>, and the corresponding distributions on the training and test splits are basically the same to this distribution. As shown, the distribution is extremely uneven. The samples of the top 10 most frequent relationships account for almost 90% samples, while those of the rest 40 relationships merely account for about 10%. Thus, the R@K metric is dominated by the performance of these most frequent relationships. As shown in <ref type="figure" target="#fig_2">Figure 4(b)</ref>, current state-of-theart method (i.e., SMN) performs quite well for these relationships such as "on", "has"; thus it can achieve a good R@K. However, SMN performs quite poorly for the relationships that have fewer samples (e.g., "make of", "to"). The mR@K metric measures the overall performance over all relationships; thus these poor results lead to an obvious drop on this metric. Different from existing methods,  The R@50 without contraint of our method and the SMN on the predicate classification task on the VG dataset.</p><p>our model integrates prior knowledge to explicitly regularize the semantic space; thus it also performs well for these less frequent relationships. In this way, our model can well address the issue of uneven distribution of relationships.</p><p>To present a more direct comparison of the relation between the performance improvement and sample number, we further present the R@50 improvement for each relationship and sample proportion in <ref type="figure" target="#fig_3">Figure 5</ref>(a) and 5(b). As shown, our model achieves evident improvement in almost all relationships (47/50). Besides, the improvement is more obvious for the relationships with fewer samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablative study</head><p>The core of our method is the explicit incorporation of statistical correlation of object pair and their relationship. To better verify its effectiveness, we replace the statistical probabilities with uniform distribution, i.e., assigning each m cc k to 1 K , leaving other components unchanged. The experiment is conducted on the VG dataset and the results are presented in <ref type="table">Table 3</ref>. We find that the mean mR decreases from 11.7% to 7.9% and the mean R decreases from 44.1% to 40.6%. This obvious performance drop clearly indicates incorporating statistical correlations significantly helps scene graph generation.</p><p>It is another important module that our method propa-  gates messages through regions appearing in the image to learn contextualized representation. Similarly, we analyze its significance by replacing the statistical probabilities with a uniform distribution, and retrain the model on the VG dataset. As shown in <ref type="table">Table 3</ref>, both the mean mR and mean R suffer from 0.3% drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The prior knowledge of statistical correlations between object pair and their relationship can help regularize the semantic space of relationship prediction given target object pair, and thus effectively address the issue of the uneven distribution over different relationships. In this work, we show these correlations can be explicitly represented by a knowledge graph, in which a routing mechanism is learned to propagate node messages through the graph under the explicit guidance of the structured knowledge. We conduct experiments on the most widely used Visual Genome benchmark and demonstrate the superiority of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Recall@50 and (b) Recall@100 of our proposed method and the SMN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) A graph correlating the detected regions appearing in an image; (b) A graph correlating given object pair oi and oj with all the relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>(a) The distribution of different relationships on the VG dataset. The training and test splits share similar distribution. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>(a) The R@50 improvement of different relationships of our methods to the SMN and (b) the relation between the R@50 improvement and sample proportion on the predicate classification task on the VG dataset. The R@50 are computed without contraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the R@50 and R@100 in % with and without constraint on the three tasks of the VG dataset. We compute Mean R by averaging R@50 and R@100 over the three tasks.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell cols="2">SGGen R@50 R@100</cell><cell cols="2">SGCls R@50 R@100</cell><cell cols="2">PredCls R@50 R@100</cell><cell>Mean</cell></row><row><cell></cell><cell cols="2">VRD [19]</cell><cell>0.3</cell><cell>0.5</cell><cell>11.8</cell><cell>14.1</cell><cell>27.9</cell><cell>35.0</cell><cell>14.9</cell></row><row><cell></cell><cell cols="2">IMP [30]</cell><cell>3.4</cell><cell>4.2</cell><cell>21.7</cell><cell>24.4</cell><cell>44.8</cell><cell>53.0</cell><cell>25.3</cell></row><row><cell>Constraint</cell><cell cols="2">IMP+ [30, 33] FREQ [33]</cell><cell>20.7 23.5</cell><cell>24.5 27.6</cell><cell>34.6 32.4</cell><cell>35.4 34.0</cell><cell>59.3 59.9</cell><cell>61.3 64.1</cell><cell>39.3 40.3</cell></row><row><cell></cell><cell cols="2">SMN [33]</cell><cell>27.2</cell><cell>30.3</cell><cell>35.8</cell><cell>36.5</cell><cell>65.2</cell><cell>67.1</cell><cell>43.7</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>27.1</cell><cell>29.8</cell><cell>36.7</cell><cell>37.4</cell><cell>65.8</cell><cell>67.6</cell><cell>44.1</cell></row><row><cell></cell><cell>AE [23]</cell><cell></cell><cell>9.7</cell><cell>11.3</cell><cell>26.5</cell><cell>30.0</cell><cell>68.0</cell><cell>75.2</cell><cell>36.8</cell></row><row><cell></cell><cell cols="2">IMP+ [30, 33]</cell><cell>22.0</cell><cell>27.4</cell><cell>43.4</cell><cell>47.2</cell><cell>75.2</cell><cell>83.6</cell><cell>49.8</cell></row><row><cell>No constraint</cell><cell cols="2">FREQ [33]</cell><cell>25.3</cell><cell>30.9</cell><cell>40.5</cell><cell>43.7</cell><cell>71.3</cell><cell>81.2</cell><cell>48.8</cell></row><row><cell></cell><cell cols="2">SMN [33]</cell><cell>30.5</cell><cell>35.8</cell><cell>44.5</cell><cell>47.7</cell><cell>81.1</cell><cell>88.3</cell><cell>54.7</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>30.9</cell><cell>35.8</cell><cell>45.9</cell><cell>49.0</cell><cell>81.9</cell><cell>88.9</cell><cell>55.4</cell></row><row><cell cols="2">Methods</cell><cell cols="8">SGGen mR@50 mR@100 mR@50 mR@100 mR@50 mR@100 Mean SGCls PredCls</cell></row><row><cell cols="2">Ours w/o rk &amp; w/o ok</cell><cell>5.1</cell><cell>5.8</cell><cell>6.1</cell><cell>6.5</cell><cell></cell><cell>10.5</cell><cell>11.5</cell><cell>7.6</cell></row><row><cell cols="2">Ours w/o rk</cell><cell>5.2</cell><cell>5.9</cell><cell>6.5</cell><cell>6.9</cell><cell></cell><cell>11.1</cell><cell>12.0</cell><cell>7.9</cell></row><row><cell>Ours</cell><cell></cell><cell>6.4</cell><cell>7.3</cell><cell>9.4</cell><cell>10.0</cell><cell></cell><cell>17.7</cell><cell>19.2</cell><cell>11.7</cell></row><row><cell></cell><cell></cell><cell>R@50</cell><cell>R@100</cell><cell>R@50</cell><cell cols="2">R@100</cell><cell>R@50</cell><cell>R@100</cell><cell>Mean</cell></row><row><cell cols="2">Ours w/o rk &amp; w/o ok</cell><cell>25.2</cell><cell>27.9</cell><cell>33.9</cell><cell>34.8</cell><cell></cell><cell>58.7</cell><cell>61.0</cell><cell>40.3</cell></row><row><cell cols="2">Ours w/o rk</cell><cell>25.5</cell><cell>28.0</cell><cell>34.3</cell><cell>35.2</cell><cell></cell><cell>59.2</cell><cell>61.5</cell><cell>40.6</cell></row><row><cell>Ours</cell><cell></cell><cell>27.1</cell><cell>29.8</cell><cell>36.7</cell><cell>37.4</cell><cell></cell><cell>65.8</cell><cell>67.6</cell><cell>44.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05448</idno>
		<title level="m">Learning to detect human-object interactions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Neural task planning with and-or graph representations. TMM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledgeembedded representation learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding indoor scenes using 3d geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3298" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection meets knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1661" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-class segmentation with relative location prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="300" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multilabel zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06526</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1261" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge-guided recurrent neural network learning for task-oriented action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="625" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2673" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?ernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2168" to="2177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: better, faster, stronger</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TNN</publisher>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep reasoning with knowledge graph for social relationship understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06640</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1681" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
