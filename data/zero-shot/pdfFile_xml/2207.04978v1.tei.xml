<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
							<email>cwngo@smu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@jd.com</email>
							<affiliation key="aff0">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vision Transformer</term>
					<term>Wavelet Transform</term>
					<term>Self-attention Learn- ing</term>
					<term>Image Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (Wave-ViT) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at https://github.com/YehLi/ ImageNetModel.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, leveraging Transformer architecture <ref type="bibr" target="#b49">[50]</ref> for visual representation learning has achieved widespread dominance in computer vision field. Transformer architecture has brought forward milestone improvement for a series of downstream vision tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b67">68]</ref>, including both image recognition and dense prediction tasks (e.g., object detection and semantic segmentation). At its heart is a basic self-attention block that triggers long-range interaction among visual tokens. The Vision Transformer (ViT) <ref type="bibr" target="#b12">[13]</ref> is one of the early attempts that directly employs a pure Transformer over image patches, and manages to attain competitive image recognition performance against CNN counterparts. However, applying the primary ViT architecture using its outputs of single-scale and low-resolution feature map for the pixel-level dense prediction tasks (e.g., instance/semantic segmentation) is not trivial. Therefore, considering that visual patterns commonly occur at multiple scales in natural scenery, there has been research efforts pushing the limits of ViT backbones by aggregating contexts from multiple scales (e.g., "pyramid" strategy). For example, Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b53">54]</ref> integrates pyramid structure into Transformer framework, yielding multi-scale feature maps for dense prediction tasks. Multiscale Vision Transformers (MViT) <ref type="bibr" target="#b13">[14]</ref> learns multi-scale feature hierarchies in Transformer architecture by hierarchically expanding the channel capacity while reducing the spatial resolution.</p><p>One primary challenge of applying self-attention over multi-scale feature maps is the quadratical computational cost that scales w.r.t the number of input patches (i.e., spatial resolution). Thus, typical multi-scale ViT approaches usually perform down-sampling operations (e.g., average pooling in <ref type="bibr" target="#b53">[54]</ref> or pooling kernels in <ref type="bibr" target="#b13">[14]</ref>) over keys/values to reduce computational cost. Nevertheless, these pooling based operations inevitably result in information dropping (e.g., the high-frequency components of object texture details), and thus adversely affect the performances especially for dense prediction tasks. Furthermore, the recent studies (e.g., <ref type="bibr" target="#b65">[66]</ref>) also have shown that applying pooling operations in CNNs would hurt the shift-equivariance of deep networks.</p><p>In this paper, we propose Wavelets block to perform invertible downsampling through wavelet transforms, aiming to preserve the original image details for self-attention learning while reducing computational cost. Wavelet transform is a fundamental time-frequency analysis method that decomposes input signals into different frequency subbands to address the aliasing problem. In particular, Discrete Wavelet Transform (DWT) <ref type="bibr" target="#b39">[40]</ref> enables invertible downsampling by transforming 2D data into four discrete wavelet subbands <ref type="figure" target="#fig_0">(Figure 1</ref> (a)): low-frequency component (I LL ) and high-frequency components (I LH , I HL , I HH ). Here the low-frequency component reflects the basic object structure at coarse-grained level, while the high-frequency components retain the object texture details at fine-grained level. In this way, various levels of image details are preserved in different subbands of lower resolution without information dropping. Furthermore, inverse DWT (IDWT) can be applied to reconstruct the original image. The information preserving transformation motivates the design of an efficient Transformer block with lossless and invertible down-sampling for self-attention learning over multi-scale feature maps.</p><p>Technically, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b), Wavelets block first employs DWT to transform each input key/value to four subbands of lower resolution. After stacking the four subbands into a down-sampled feature map, a 3?3 convolution is performed to further impose spatial locality over the frequency subbands. This leads to locally contextualized down-sampled keys/values. The multi-head self-attention learning is conducted on the down-sampled keys/values and input query. Meanwhile, IDWT can be applied over the down-sampled keys/values to reconstruct high-resolution feature map that preserves image details. Compared to the single 3?3 convolution <ref type="figure" target="#fig_0">(Figure 1 (c)</ref>), the process of DWT-Convolution-IDWT ( <ref type="figure" target="#fig_0">Figure 1 (d)</ref>) enables a stronger local contextualization via enlarged receptive field, with negligible increase in computation and memory. Finally, we combine the attended feature map via self-attention learning and the reconstructed feature map with local contextualization as the outputs of Wavelets block.</p><p>By operating Wavelets block over multi-scale features in the multi-stage Transformer framework, we present a new Wavelet Vision Transformer (Wave-ViT) for visual representation learning. The proposed Wave-ViT has been properly analyzed and verified through extensive experiments over different vision tasks, which demonstrate its superiority against state-of-the-art ViT backbones. More remarkably, under a comparable number of parameters, Wave-ViT achieves 85.5% top-1 accuracy on ImageNet for image recognition, which absolutely improves PVT (83.8%) with 1.7%. For object detection and instance segmentation on COCO, Wave-ViT absolutely surpasses PVT with 1.3% and 0.5% mAP, with 25.9% less parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Representation Learning</head><p>Early studies in the last decade predominantly focused on exploring Convolutional Neural Networks (CNN) for visual representation learning, leading to a series of CNN backbones, e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>. Most of them stack low-tohigh convolutions by going deeper, targeting for producing low-resolution and high-level representations tailored for image recognition. However, dense prediction tasks like instance/semantic segmentation require high-resolution and even pixel-level representations. To tackle this, several multi-scale CNN backbones are established. For example, Res2Net <ref type="bibr" target="#b15">[16]</ref> presents a multi-scale building block that contains hierarchical residual-like connections. HRNet <ref type="bibr" target="#b51">[52]</ref> connects highto-low resolution convolution streams in parallel and meanwhile exchanges the information across resolutions repeatedly, thereby maintaining high-resolution features throughout the process.</p><p>Recently, due to the powerful long-range interaction modeling in Transformer <ref type="bibr" target="#b49">[50]</ref>, Transformer has advanced natural language understanding. Inspired by this, numerous Transformer-based architectures for vision understanding have started. A few attempts augment convolutional operators with the global selfattention <ref type="bibr" target="#b1">[2]</ref> or local self-attention <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b68">69]</ref>, yielding a hybrid backbone of CNN and Transformer. On a parallel note, Vision Transformer (ViT) <ref type="bibr" target="#b12">[13]</ref> first employs a pure Transformer over the sequence of image patches for image recognition. DETR <ref type="bibr" target="#b3">[4]</ref> also leverages a pure Transformer to construct an end-to-end detector for object detection. Different from ViT that solely divides input image into patches, TNT <ref type="bibr" target="#b18">[19]</ref> first decomposes the inputs into several patches as "visual sentences", and then divides them into sub-patches as "visual words". A sub-transformer is additionally integrated into Transformer to perform self-attention over smaller "visual words". Subsequently, to facilitate dense prediction tasks, multi-scale paradigm is introduced into Transformer structure, leading to multi-scale Vision Transformer backbones <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>. In particular, Swin Transformer <ref type="bibr" target="#b34">[35]</ref> upgrades ViT by constructing hierarchical feature maps via merging image patches in deeper layers. Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b54">[55]</ref> designs a pyramid structure Transformer that produces multi-scale feature maps in a four-stage architecture. PVTv2 <ref type="bibr" target="#b53">[54]</ref> further improves PVT by using average pooling to reduce spatial dimension of keys/values, rather than convolutions in PVT. Multiscale Vision Transformers (MViT) <ref type="bibr" target="#b13">[14]</ref> integrates Transformer framework with multi-scale feature hierarchies, and pooling kernels is employed over query/keys/values for spatial reduction.</p><p>Our Wave-ViT is also a type of multi-scale ViT. Existing multi-scale ViTs (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>) commonly adopt irreversible down-sampling operations like average pooling or pooling kernels for spatial reduction. In contrast, Wave-ViT capitalizes on wavelet transforms to reduce spatial dimension of keys/values via invertible down-sampling for self-attention learning over multi-scale features, leading to a better trade-off between computation cost and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Wavelet Transform in Computer Vision</head><p>Wavelet Transform is effective for time-frequency analysis. Considering that Wavelet Transform is invertible and capable of preserving all information, Wavelet Transform has been exploited in CNN architectures for performance boosting in various vision tasks. For example, in <ref type="bibr" target="#b0">[1]</ref>, Bae et al. validate that learning CNN representations over wavelet subbands can benefit the task of image restoration. DWSR <ref type="bibr" target="#b17">[18]</ref> takes low-resolution wavelet subbands as inputs to recover the missing details for image super-resolution task. Multi-level wavelet transform <ref type="bibr" target="#b33">[34]</ref> is utilized to enlarge receptive field without information dropping for image restoration. Williams et al. <ref type="bibr" target="#b55">[56]</ref> utilize Wavelet Transform to decompose input features into a second level decomposition, and discard first-level subbands to reduce feature dimensions for image recognition. Haar wavelet CNNs is integrated with multi-resolution analysis in <ref type="bibr" target="#b14">[15]</ref> for texture classification and image annotation. In <ref type="bibr" target="#b40">[41]</ref>, ResNet is remoulded by combining the first layer with a wavelet scattering network, which achieves comparable performances on image recognition with less parameters. Although wavelet transform has been exploited as down-sampling/up-sampling operations in CNNs, it is never explored for Transformer architecture. In this work, our Wave-ViT goes beyond existing CNNs that operate wavelet transform over feature maps across different stages, and leverages wavelet transform to down-sample keys/values within Transformer block, making the impact more thorough for feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach: Wavelet Vision Transformer</head><p>This section starts by briefly reviewing the most typical multi-head self-attention block in ViTs, particularly on how the self-attention block is scaled down for reducing computational cost in the existing multi-scale ViTs. After that, a novel principled Transformer building block, named Wavelets block, is designed to integrate self-attention learning with wavelet transforms in a unified fashion. Such design upgrades typical self-attention block by exploiting wavelet transforms to perform invertible down-sampling, which elegantly reduces spatial dimension of keys/values without information dropping. Furthermore, this block applies inverse wavelet transforms over down-sampled keys/values to enhance outputs with enlarged receptive field. Finally, after applying Wavelets block over multiscale features in the multi-stage Transformer architecture, we elaborate a new multi-scale ViT backbone, i.e., Wavelet Vision Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Multi-head Self-attention in ViT Backbones. Mainstream Transformer architectures, especially Vision Transformer backbones <ref type="bibr" target="#b12">[13]</ref>, often rely on the typical multi-head self-attention that captures long-range dependencies among inputs in a scalable fashion. Here we present a general formulation of multihead self-attention as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (a). Technically, let X ? R H?W ?D be the input 2D feature map, where H/W /D denote the height/width/channel number, respectively. Here X can be reshaped as a patch sequence, consisting of n = H ? W image patches and the dimension of each patch is D. We linearly transform the input patch sequence X into three groups in parallel: queries Q ? R n?D , keys K ? R n?D , and values V ? R n?D . After that, the multihead self-attention (MultiHead) module <ref type="bibr" target="#b49">[50]</ref> decomposes each query/key/value into N h parts along channel dimension, leading to queries Q j ? R n?D h , keys K j ? R n?D h , and values V j ? R n?D h for the j-th head. Note that N h is head number and D h denotes the dimension of each head. Then, we perform selfattention learning (Attention) over queries, keys and values for each head, and the outputs of each head are concatenated, followed by a linear transformation to compose the final outputs:</p><formula xml:id="formula_0">MultiHead(Q, K, V ) = Concat(head0, head1, ..., headN h )W O , headj = Attention(Qj, Kj, Vj), Attention(Qj, Kj, Vj) = Softmax( QjKj T ? D h )Vj,<label>(1)</label></formula><p>where Concat(?) is the concatenation operation and W O is the transformation matrix. According to the general formulation in Eq.(1), the computational cost of multi-head self-attention for the input feature map</p><formula xml:id="formula_1">X ? R H?W ?D is O(H 2 W 2 D)</formula><p>, which scales quadratically w.r.t. the input patch number. In this way, such design inevitably leads to a sharp rise in computational cost especially for high-resolution inputs.</p><p>Self-attention with Down-sampling in Multi-scale ViT Backbones. To alleviate the heavy self-attention computation overhead for high-resolution inputs, the existing multi-scale ViT backbones commonly adopt the down-sampling operations (e.g., average pooling in <ref type="bibr" target="#b53">[54]</ref> or pooling kernels in <ref type="bibr" target="#b13">[14]</ref>) over keys/values for spatial reduction. Taking the self-attention block with 2? down-sampling in <ref type="figure" target="#fig_1">Figure 2</ref> (b) as an example, the input 2D feature map X is first down-sampled by a factor r (r = 2 in this case). Here the down-sampling operator is denoted as DS <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref>, that reduces the spatial scale of both height and width by half. Next, the down-sampled feature map is linearly transformed into keys K d ? R n r 2 ?D and values V d ? R n r 2 ?D to trigger multi-head self-attention learning. As such, the overall computational cost of multi-head self-attention is dramatically reduced by a factor of r 2 (i.e., O( H 2 W 2 D r 2 )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Wavelets Block</head><p>Although the aforementioned multi-scale ViT backbones reduce self-attention computation via down-sampling, the commonly adopted down-sampling operations like average pooling are irreversible, and inevitably result in information dropping. To mitigate this issue, we design a principled self-attention block, named Wavelets block, that novelly capitalizes on wavelet transforms to enable invertible down-sampling for self-attention learning. Such invertible downsampling is seamlessly incorporated into the typical self-attention block, pursuing efficient multi-head self-attention learning with lossless down-sampling. Formally, given the input 2D feature map X ? R H?W ?D , we first linearly transform it into X = XW d with reduced channel dimension via embedding</p><formula xml:id="formula_2">matrix W d ? R D? D 4 . Next, we employ Discrete Wavelet Transform (DWT) to down-sample the input X ? R H?W ? D 4</formula><p>by decomposing it into four wavelet subbands. Note that here we choose the classical Haar wavelet for DWT as in <ref type="bibr" target="#b32">[33]</ref> for simplicity. Concretely, DWT applies the low-pass filter</p><formula xml:id="formula_3">f L = (1/ ? 2, 1/ ? 2) and high-pass filter f H = (1/ ? 2, ?1/ ? 2)</formula><p>along the rows to encode X into two subbands X L and X H . Next, the same low-pass filter f L and high-pass filter f H are employed along the columns of the learnt subbands X L and X H , leading to all the four wavelet subbands:</p><formula xml:id="formula_4">X LL ? R H 2 ? W 2 ? D 4 , X LH ? R H 2 ? W 2 ? D 4 , X HL ? R H 2 ? W 2 ? D 4 , and X HH ? R H 2 ? W 2 ? D 4</formula><p>. X LL refers to the low-frequency component that reflects the basic object structure at coarse-grained level. X LH , X HL , and X HH represent the high-frequency components that retain the object texture details at fine-grained level. In this way, each wavelet subband can be regarded as the down-sampled version of X, and all of them cover every detail of inputs without any information dropping.</p><p>We concatenate the four wavelet subbands along the channel dimension to formX = [X LL , X LH , X HL , X HH ] ? R H 2 ? W 2 ?D . A 3?3 convolution is further applied to impose spatial locality overX, yielding the locally contextualized down-sampled feature map X c . Next, this down-sampled feature map X c is linearly transformed into down-sampled keys K w ? m?D and values V w ? m?D , where m = H 2 ? W 2 is the number of patches. Similarly, the wavelet-based multihead self-attention learning Attention w is thus performed over the queries and the corresponding down-sampled keys/values for each head:</p><formula xml:id="formula_5">headj = Attention w (Qj, K w j , V w j ) = Softmax( QjK w j T ? D h )V w j ,<label>(2)</label></formula><p>where K w j /V w j denotes the down-sampled keys/values for the j-th head, respectively. Here the aggregated output of self-attention learning for each head (head j ) can be interpreted as the long-range contextualized information of inputs.</p><p>As a beneficial by-product, we additionally apply inverse DWT (IDWT) over the locally contextualized down-sampled feature X c . According to the wavelet theory, the reconstructed feature map X r is able to retain every detail of primary input X. It is worthy to note that compared to a single 3?3 convolution, such process of DWT-Convolution-IDWT in Wavelets block triggers a stronger local contextualization with enlarged receptive field, with negligible increase in computational cost/memory.</p><p>Finally, we concatenate all the long-range contextualized information of each head plus the reconstructed locally contextualized information X r , followed by a linear transformation to compose the outputs of our Wavelets block:</p><formula xml:id="formula_6">WaveletsBlock(X) = MultiHead w (XW q , X c W k , X c W v , X r ), MultiHead w (Q, K, V, X r ) = Concat(head0, head1, ..., headN h , X r ) W O ,<label>(3)</label></formula><p>where W O is the transformation matrix. <ref type="table">Table 1</ref>. Detailed architecture specifications for three variants of our Wave-ViT with different model sizes, i.e., Wave-ViT-S (small size), Wave-ViT-B (base size), and Wave-ViT-L (large size). Ei, Headi, and Ci represents the expansion ratio of feed-forward layer, the head number, and the channel dimension in each stage i, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Size</head><p>Wave </p><formula xml:id="formula_7">-ViT-S Wave-ViT-B Wave-ViT-L Stage 1 H 4 ? W 4 ? ? E1 = 8 Head1 = 2 C1 = 64 ? ? ?3 ? ? E1 = 8 Head1 = 2 C1 = 64 ? ? ?3 ? ? E1 = 8 Head1 = 3 C1 = 96 ? ? ?3 Stage 2 H 8 ? W 8 ? ? E2 = 8 Head2 = 4 C2 = 128 ? ? ?4 ? ? E2 = 8 Head2 = 4 C2 = 128 ? ? ?4 ? ? E2 = 8 Head2 = 6 C2 = 192 ? ? ?6</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Wavelet Vision Transformer</head><p>Recall that our Wavelets block is a principled unified self-attention block, it is feasible to construct multi-scale ViT backbones with Wavelets blocks. Following the basic configuration of existing multi-scale ViTs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b54">55]</ref>, we present three variants of our Wavelet Vision Transformer (Wave-ViT) with different model sizes, i.e., Wave-ViT-S (small size), Wave-ViT-B (base size), and Wave-ViT-L (large size). Note that Wave-ViT-S/B/L shares similar model size and computational complexity with Swin-T/S/B <ref type="bibr" target="#b34">[35]</ref>. Specifically, given the input image (size: 224 ? 224), the entire architecture of Wave-ViT consists of four stages, and each stage is comprised of a patch embedding layer, and a stack of Wavelets blocks followed by feed-forward layers. We follow the design principle of ResNet <ref type="bibr" target="#b20">[21]</ref> by progressively increasing the channel dimensions of all the four stages and meanwhile shrinking the spatial resolutions. <ref type="table">Table 1</ref> details the architectures of all the three variants of Wave-ViT, where E i , Head i , and C i is the expansion ratio of feed-forward layer, head number, and the channel dimension in stage i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the effectiveness of our proposed multi-scale ViT backbone, denoted as Wave-ViT, through various empirical evidence on a series of mainstream CV tasks, including image recognition, object detection, instance segmentation, and semantic segmentation. Concretely, we consider the following evaluations to compare the quality of learnt feature representations obtained from various vision backbones: (a) Training from scratch for image recognition task on Ima-geNet1K <ref type="bibr" target="#b11">[12]</ref>; (b) Fine-tuning the backbones (pre-trained on ImageNet1K) for downstream tasks, i.e., object detection and instance segmentation on COCO <ref type="bibr" target="#b31">[32]</ref>, and semantic segmentation on ADE20K <ref type="bibr" target="#b70">[71]</ref>; (c) Ablation studies that support each design in our Wavelets block; (d) Visualization of learnt visual representation by Wave-ViT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Recognition on ImageNet1K</head><p>Dataset and Optimization Setups. In the task of image recognition, we adopt the ImageNet1K benchmark, which comprises 1.28 million training images and 50K validation images from 1,000 classes. All vision backbones are trained from scratch on the training set, and both top-1 and top-5 accuracies metrics are used to evaluate the trained backbones on the validation set. During training, we follow the setups in <ref type="bibr" target="#b62">[63]</ref> by applying RandAug <ref type="bibr" target="#b9">[10]</ref>, CutOut <ref type="bibr" target="#b69">[70]</ref>, and Token Labeling objective with MixToken <ref type="bibr" target="#b23">[24]</ref> for data augmentation. We adopt the AdamW optimizer <ref type="bibr" target="#b38">[39]</ref> with a momentum of 0.9. In particular, the optimization process includes 10 epochs of linear warm-up and 300 epochs with cosine decay learning rate scheduler <ref type="bibr" target="#b37">[38]</ref>. The batch size is set as 1,024, which is distributed on 8 V100 GPUs. We fix the learning rate and weight decay as 0.001 and 0.05.</p><p>Performance Comparison. <ref type="table" target="#tab_1">Table 2</ref> summarizes the performance comparisons between the state-of-the-art vision backbones and our Wave-ViT variants. Note that the most competitive ViT backbones VOLO variants (i.e., VOLO-D1 ? , VOLO-D2 ? , and VOLO-D3 ? ) are trained with additional Token Labeling objective with MixToken <ref type="bibr" target="#b23">[24]</ref> and convolutional stem (conv-stem) <ref type="bibr" target="#b52">[53]</ref> for better patch encoding. We also adopt the same upgraded strategies to train our Wave-ViT, yielding the variants in each size (i.e., Wave-ViT-S ? , Wave-ViT-B ? , Wave-ViT-L ? ). Moreover, for fair comparison with other vision backbones without these strategies, we also implement a degraded version of Wave-ViT in Small size without Token Labeling objective and conv-stem (i.e., Wave-ViT-S). As shown in this that aggregates multi-scale contexts for image recognition. Furthermore, instead of using irreversible down-sampling for self-attention learning in PVTv2-B5, our Wave-ViT-L ? enables invertible down-sampling with wavelet transforms, and thus achieves better efficiency-vs-accuracy trade-off. It is worthy to note that VOLO-D3 ? does not employ down-sampling operations to reduce computational cost for high-resolution inputs, but instead directly reduces the input resolution (28?28) at initial stage. In contrast, Wave-ViT-L ? keeps the high-resolution inputs (56?56), and exploits wavelet transforms to trigger lossless down-sampling for multi-scale self-attention learning, leading to performance boosts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Detection and Instance Segmentation on COCO</head><p>Dataset and Optimization Setups. In this section, we examine the pretrained Wave-ViT's behavior on COCO dataset for two downstream tasks that localize objects ranging from bounding-box level to pixel level, i.e., object detection and instance segmentation. Two mainstream detectors, i.e., RetinaNet <ref type="bibr" target="#b30">[31]</ref> and Mask R-CNN <ref type="bibr" target="#b19">[20]</ref>, are employed for each downstream task, and we replace the CNN backbones in each detector with our Wave-ViT for evaluation. Specifically, each vision backbone is first pre-trained over ImageNet1K, and the newly added layers are initialized with Xavier <ref type="bibr" target="#b16">[17]</ref>. Next, we follow the standard setups in <ref type="bibr" target="#b34">[35]</ref> to train all models on the COCO train2017 (?118K images). Here the batch size is set as 16, and AdamW <ref type="bibr" target="#b38">[39]</ref> is utilized for optimization (weight decay: 0.05, initial learning rate: 0.0001). All models are finally evaluated on the COCO val2017 (5K images). For the downstream task of object detection, we report the Average Precision(AP ) at different IoU thresholds and for three different object sizes (i.e., small, medium, large (S/M/L)). For the downstream task of instance segmentation, both bounding box and mask Average Precision (i.e., AP b , AP m ) are reported. During training, we resize each input training <ref type="table">Table 3</ref>. The performances of various vision backbones on COCO val2017 dataset for the downstream tasks of object detection and instance segmentation. For object detection task, we employ RetinaNet as the object detector, and the Average Precision(AP ) at different IoU thresholds or three different object sizes (i.e., small, medium, large (S/M/L)) are reported for evaluation. For instance segmentation task, we adopt Mask R-CNN as the base model, and the bounding box and mask Average Precision (i.e., AP b and AP m ) are reported for evaluation. We group all vision backbones into two categories: Small size and Base size.  <ref type="table">Table 4</ref>. The performances of various vision backbones on COCO val2017 dataset for the downstream task of object detection. Four kinds of object detectors, i.e., GFL <ref type="bibr" target="#b27">[28]</ref>, Sparse RCNN <ref type="bibr" target="#b45">[46]</ref>, Cascade Mask R-CNN <ref type="bibr" target="#b2">[3]</ref>, and ATSS <ref type="bibr" target="#b66">[67]</ref> in mmdetection <ref type="bibr" target="#b6">[7]</ref>, are adopted for evaluation. We report the bounding box Average Precision (AP b ) in different IoU thresholds.</p><formula xml:id="formula_8">Backbone RetinaNet 1x [31] Mask R-CNN 1x [20] AP AP50 AP75 AP S AP M AP L AP b AP b 50 AP b</formula><formula xml:id="formula_9">Backbone Method AP b AP b 50 AP b 75 Backbone Method AP b AP b 50 AP b 75</formula><p>ResNet50 <ref type="bibr" target="#b20">[21]</ref> GFL <ref type="bibr" target="#b27">[28]</ref> 44 image by fixing the shorter side as 800 pixels and meanwhile making the longer side not exceeding 1,333 pixels. Note that for RetinaNet and Mask R-CNN, 1 ? training schedule (i.e., 12 epochs) is adopted to train the two mainstream detectors. In addition to RetinaNet, we also include four state-of-the-arts detectors (GFL <ref type="bibr" target="#b27">[28]</ref>, Sparse RCNN <ref type="bibr" target="#b45">[46]</ref>, Cascade Mask R-CNN <ref type="bibr" target="#b2">[3]</ref>, and ATSS [67]) for object detection task. Following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">54]</ref>, we utilize 3 ? schedule (i.e., 36 epochs) with multi-scale strategy for training, and the shorter side of each input image is randomly resized within the range of [480, 800] while the longer side is forced to be less than 1,333 pixels.</p><p>Performance Comparison. <ref type="table">Table 3</ref> lists the performance comparisons across different pre-trained vision backbones under the base detector of RetinaNet and Mask R-CNN for object detection and instance segmentation task, respectively. Note that we follow the evaluation for image recognition task by grouping all the pre-trained backbones into two categories (i.e., Small size and Base size). As shown in this table, the performance trends in each downstream task <ref type="table">Table 5</ref>. The performances of various vision backbones on ADE20K validation dataset for the downstream task of semantic segmentation. We employ the commonly adopted base model (UPerNet) for semantic segmentation and report the mean IoU (mIoU) averaged over all classes for evaluation. We group all vision backbones into two categories: Small size and Base size. are similar to those in image recognition task. Concretely, under the similar model size for each group, the multi-scale ViT backbones (e.g., Swin-T/S and PVTv2-B2/B3) consistently exhibit better performances than CNN backbones (e.g., ResNet50/101) across all evaluation metrics. Furthermore, by capitalizing on wavelet transforms to enable lossless down-sampling in multi-scale selfattention learning, Wave-ViT variants outperform PVTv2-B2/B3 that explore sub-optimal down-sampling with pooling kernels. The results confirm that unifying self-attention learning and lossless down-sampling with wavelet transforms can improve the transfer capability of pre-trained multi-scale representations on dense prediction tasks.</p><p>To further verify the generalizability of the pre-trained multi-scale features via Wave-ViT for object detection, we evaluate various pre-trained vision backbones on four state-of-the-arts detectors (GFL, Sparse RCNN, Cascade Mask R-CNN, and ATSS). <ref type="table">Table 4</ref> shows the detailed performances of four object detectors with different pre-trained vision backbones under Small size. Similar to the observations in the base detector of RetinaNet, our Wave-ViT-S achieves consistent performance gains against both CNN backbone (ResNet50) and multiscale ViT backbones (Swin-T and PVTv2-B2) across all the four state-of-thearts detectors. This again validates the advantage of integrating multi-scale selfattention with invertible down-sampling in our Wave-ViT for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Segmentation on ADE20K</head><p>Dataset and Optimization Setups. We next evaluate our pre-trained Wave-ViT in the downstream task of semantic segmentation on ADE20K dataset. This dataset is the most typical benchmark for evaluating semantic segmentation techniques, which consists of 25K images (20K training images, 2K validation images, and 3K testing images) derived from 150 semantic categories. Here we choose the commonly adopted UPerNet <ref type="bibr" target="#b57">[58]</ref> as the base model for this task and the CNN backbone in primary UPerNet is replaced with our Wave-ViT. During training, we train the models on 8 GPUs for 160K iterations via AdamW <ref type="bibr" target="#b38">[39]</ref>   the random horizontal flipping, random photometric distortion, and random rescaling within the ratio range [0.5, 2.0] as data augmentations. We report the metric of mean IoU (mIoU) averaged over all classes for evaluation. For fair comparison with other vision backbones for semantic segmentation downstream task, we set all the hyperparameters and detection heads as in Swin <ref type="bibr" target="#b34">[35]</ref>.</p><p>Performance Comparison. <ref type="table">Table 5</ref> shows the mIoU scores of different pretrained vision backbones under the base models (e.g., UPerNet, DeeplabV3, Semantic FPN) for semantic segmentation. As in the evaluation for image recognition, object detection, and instance segmentation tasks, we group all the pretrained backbones into two categories (i.e., Small size and Base size). Similarly, by upgrading multi-scale self-attention learning with Wavelet based invertible down-sampling, our Wave-ViT variants yield consistent gains against both CNN backbones (e.g., ResNet-50/101 and ResNeSt-50/101) and existing multi-scale ViT backbones (e.g., Swin-T/S and Twins-SVT-S/B). Concretely, under a comparable model size within each group, Wave-ViT-S/B achieves 49.6%/51.5% mIoU on ADE20K validation dataset, which absolutely improves the best competitor Twins-SVT-S/Swin-S (47.1%/49.5%) with 2.5%/2.0%. The results basically demonstrates the superiority of Wave-ViT for semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We investigate how each design in our Wavelets block influences the overall performance of multi-scale ViTs on ImageNet1K dataset for image recognition task, as summarized in <ref type="figure" target="#fig_3">Figure 3</ref>. Note that all the variants of self-attention blocks here are constructed under similar model size (Small) for fair comparison.</p><p>Block (a) is a typical self-attention block with irreversible down-sampling. By directly operating average pooling over the input keys/values, (a) significantly reduces the computational cost for self-attention learning and the Top-1 score achieves 82.0%. Block (b) is another typical self-attention block with irreversible down-sampling via pooling kernels (convolution), rather than average pooling in (a). (b) reduces the spatial dimension of keys/values through pooling kernels, leading to the same performances as in (a). However, the number of parameters is inevitably increased. Block (c) can be regarded as a degraded  <ref type="figure">Fig. 4</ref>. Visualization of Score-CAM <ref type="bibr" target="#b50">[51]</ref> for PVTv2-B2 <ref type="bibr" target="#b53">[54]</ref> and our Wave-ViT-S on six images in ImageNet1K dataset.</p><p>version of our Wavelets block, that solely equips self-attention block with invertible down-sampling based on wavelet transforms (DWT). Compared to the most efficient (a) with irreversible down-sampling, the Top-1 score of (c) increases from 82.0% to 82.5%. This validates the effectiveness of unifying self-attention block and invertible down-sampling without information dropping. Block (d) (i.e., the full version of Wavelets block) further upgrades (c) by additionally exploiting inverse wavelet transforms (IDWT) to strengthen outputs with enlarged receptive field. Such design leads to performance boosts in Top-1 and Top-5 scores, with negligible increase in computational cost/memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization of Learnt Visual Representation</head><p>In order to further explain the visual representations learnt by our Wave-ViT, we produce the saliency map through Score-CAM <ref type="bibr" target="#b50">[51]</ref> to identify the importance of each pixel in presenting the class discrimination of the input image. <ref type="figure">Figure  4</ref> visualizes the saliency map derived from the visual representations learnt by two backbones with similar model size (PVTv2-B2 and our Wave-ViT-S). As illustrated in the figure, Wave-ViT-S consistently shows higher concentration at the semantically relevant object than PVTv2-B2, which validates that the representations learnt by Wave-ViT-S are more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we delve into the idea of unifying typical Transformer module and invertible down-sampling, thereby pursuing efficient multi-scale selfattention learning with lossless down-sampling. To verify our claim, we present a new principled Transformer module, i.e., Wavelets block, that capitalizes on Discrete Wavelet Transform (DWT) to perform invertible down-sampling over keys/values in self-attention learning. In addition, we adopt inverse DWT (IDWT) to reconstruct the down-sampled DWT outputs, which are utilized to strengthen the outputs of Wavelets block by aggregating local contexts with enlarged receptive field. Our Wavelets block is appealing in view that it is feasible to construct multi-scale ViT backbone with Wavelets blocks, with light computational cost/memory budget. In particular, by operating stacked Wavelets blocks over multi-scale features in four-stage architecture, a series of Wavelet Vision Transformer (Wave-ViT) are designed with different model sizes. We empirically validate the superiority of Wave-ViT over the state-of-the-art multi-scale ViT backbones for image recognition, under comparable numbers of parameters. Furthermore, our Wave-ViT also generalizes well to downstream tasks of object detection, instance segmentation, and semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An illustration of (a) Discrete Wavelet Transform (DWT) and Inverse DWT (IDWT) over an image, (b) our Wavelets block, and the comparison between (c) a single 3?3 convolution and (d) DWT-Convolution-IDWT process in our Wavelets block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The detailed architectures of (a) basic self-attention block in ViT Backbones, (b) self-attention block with down-sampling operation (i.e., DS(2, 2)) that reduces the spatial scale of both height and width by half, and (c) our Wavelets block that capitalizes on wavelet transforms to enable lossless down-sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>c) details the architecture of our Wavelets block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Performance comparisons across different ways on designing self-attention blocks with down-sampling in multi-scale ViT backbones (under Small size): (a) selfattention block with irreversible down-sampling operation of average pooling, (b) selfattention block with irreversible down-sampling operation of pooling kernels, (c) a degraded version of Wavelets block that solely equips self-attention block with invertible down-sampling via wavelet transforms (DWT), and (d) the full version of our Wavelets block with inverse wavelet transforms (IDWT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The performances of various vision backbones on ImageNet1K dataset for image recognition task. ? indicates that the vision backbone is additionally trained with Token Labeling objective with MixToken<ref type="bibr" target="#b23">[24]</ref> and convolutional stem (conv-stem)<ref type="bibr" target="#b52">[53]</ref> for patch encoding. We group the vision backbones into three categories, and all</figDesc><table><row><cell cols="10">backbones within each category shares similar GFLOPs: Small (GFLOPs&lt;6), Base</cell></row><row><cell cols="5">(6?GFLOPs&lt;10), Large (10?GFLOPs&lt;22).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">ParamsGFLOPsTop-1Top-5</cell><cell>Method</cell><cell cols="4">ParamsGFLOPsTop-1Top-5</cell></row><row><cell></cell><cell>Small</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Large</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 [21]</cell><cell>25.5M</cell><cell>4.1</cell><cell cols="2">78.3 94.3</cell><cell>ResNet-152 [21]</cell><cell>60.2M</cell><cell>11.6</cell><cell cols="2">81.3 95.5</cell></row><row><cell cols="2">BoTNet-S1-50 [45] 20.8M</cell><cell>4.3</cell><cell cols="2">80.4 95.0</cell><cell>ResNeXt101 [59]</cell><cell>83.5M</cell><cell>15.6</cell><cell>81.5</cell><cell>-</cell></row><row><cell>Swin-T [35]</cell><cell>29.0M</cell><cell>4.5</cell><cell cols="2">81.2 95.5</cell><cell>DeiT-B [48]</cell><cell>86.6M</cell><cell>17.6</cell><cell cols="2">81.8 95.6</cell></row><row><cell>ConViT-S [11]</cell><cell>27.8M</cell><cell>5.4</cell><cell cols="4">81.3 95.7 SE-ResNet-152 [23] 66.8M</cell><cell>11.6</cell><cell cols="2">82.2 95.9</cell></row><row><cell>T2T-ViT-14 [62]</cell><cell>21.5M</cell><cell>4.8</cell><cell cols="2">81.5 95.7</cell><cell>ResNeSt-101 [64]</cell><cell>48.3M</cell><cell>10.2</cell><cell>82.3</cell><cell>-</cell></row><row><cell cols="2">RegionViT-Ti+ [6] 14.3M</cell><cell>2.7</cell><cell>81.5</cell><cell>-</cell><cell>ConViT-B [11]</cell><cell>86.5M</cell><cell>16.8</cell><cell cols="2">82.4 95.9</cell></row><row><cell cols="2">SE-CoTNetD-50 [30] 23.1M</cell><cell>4.1</cell><cell cols="2">81.6 95.8</cell><cell>T2T-ViTt-24 [62]</cell><cell>64.1M</cell><cell>15.0</cell><cell cols="2">82.6 95.9</cell></row><row><cell>Twins-SVT-S [9]</cell><cell>24.1M</cell><cell>2.9</cell><cell cols="2">81.7 95.6</cell><cell>TNT-B [19]</cell><cell>65.6M</cell><cell>14.1</cell><cell cols="2">82.9 96.3</cell></row><row><cell cols="2">CoaT-Lite Small [60] 20.0M</cell><cell>4.0</cell><cell cols="2">81.9 95.5</cell><cell>DeepViT-L [72]</cell><cell>58.9M</cell><cell>12.8</cell><cell>83.1</cell><cell>-</cell></row><row><cell>PVTv2-B2 [54]</cell><cell>25.4M</cell><cell>4.0</cell><cell cols="2">82.0 96.0</cell><cell>RegionViT-B [6]</cell><cell>72.7M</cell><cell>13.0</cell><cell cols="2">83.2 96.1</cell></row><row><cell>Wave-ViT-S</cell><cell>19.8M</cell><cell>4.3</cell><cell cols="2">82.7 96.2</cell><cell>CaiT-S36 [49]</cell><cell>68.4M</cell><cell>13.9</cell><cell>83.3</cell><cell>-</cell></row><row><cell>Wave-ViT-S ?</cell><cell>22.7M</cell><cell>4.7</cell><cell cols="4">83.9 96.6 CrossViT-15-384 [5] 28.5M</cell><cell>21.4</cell><cell>83.5</cell><cell>-</cell></row><row><cell></cell><cell>Base</cell><cell></cell><cell></cell><cell cols="3">BoTNet-S1-128 [45] 75.1M</cell><cell>19.3</cell><cell cols="2">83.5 96.5</cell></row><row><cell>ResNet-101 [21]</cell><cell>44.6M</cell><cell>7.9</cell><cell cols="2">80.0 95.0</cell><cell>Swin-B [35]</cell><cell>88.0M</cell><cell>15.4</cell><cell cols="2">83.5 96.5</cell></row><row><cell cols="2">BoTNet-S1-59 [45] 33.5M</cell><cell>7.3</cell><cell cols="2">81.7 95.8</cell><cell>PVTv2-B4 [54]</cell><cell>62.6M</cell><cell>10.1</cell><cell cols="2">83.6 96.7</cell></row><row><cell>T2T-ViT-19 [62]</cell><cell>39.2M</cell><cell>8.5</cell><cell cols="2">81.9 95.7</cell><cell>Twins-SVT-L [9]</cell><cell>99.3M</cell><cell>15.1</cell><cell cols="2">83.7 96.5</cell></row><row><cell>CvT-21 [57]</cell><cell>32.0M</cell><cell>7.1</cell><cell>82.5</cell><cell>-</cell><cell cols="2">RegionViT-B+ [6] 73.8M</cell><cell>13.6</cell><cell>83.8</cell><cell>-</cell></row><row><cell>Swin-S [35]</cell><cell>50.0M</cell><cell>8.7</cell><cell cols="2">83.2 96.2</cell><cell>Focal-Base [61]</cell><cell>89.8M</cell><cell>16.0</cell><cell cols="2">83.8 96.5</cell></row><row><cell>Twins-SVT-B [9]</cell><cell>56.1M</cell><cell>8.6</cell><cell cols="2">83.2 96.3</cell><cell>PVTv2-B5 [54]</cell><cell>82.0M</cell><cell>11.8</cell><cell cols="2">83.8 96.6</cell></row><row><cell cols="2">SE-CoTNetD-101 [30] 40.9M</cell><cell>8.5</cell><cell cols="4">83.2 96.5 SE-CoTNetD-152 [30] 55.8M</cell><cell>17.0</cell><cell cols="2">84.0 97.0</cell></row><row><cell>PVTv2-B3 [54]</cell><cell>45.2M</cell><cell>6.9</cell><cell cols="2">83.2 96.5</cell><cell>LV-ViT-M ? [24]</cell><cell>55.8M</cell><cell>16.0</cell><cell cols="2">84.1 96.7</cell></row><row><cell cols="2">RegionViT-M+ [6] 42.0M</cell><cell>7.9</cell><cell>83.4</cell><cell>-</cell><cell>VOLO-D2 ? [63]</cell><cell>58.7M</cell><cell>14.1</cell><cell>85.2</cell><cell>-</cell></row><row><cell>VOLO-D1 ? [63]</cell><cell>26.6M</cell><cell>6.8</cell><cell>84.2</cell><cell>-</cell><cell>VOLO-D3 ? [63]</cell><cell>86.3M</cell><cell>20.6</cell><cell>85.4</cell><cell>-</cell></row><row><cell>Wave-ViT-B ?</cell><cell>33.5M</cell><cell>7.2</cell><cell cols="2">84.8 97.1</cell><cell>Wave-ViT-L ?</cell><cell>57.5M</cell><cell>14.8</cell><cell cols="2">85.5 97.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>table, under the similar GFLOPs for each group, our Wave-ViT variants consistently achieve better performances against the existing vision backbones, including both CNN backbones (e.g., ResNet and SE-ResNet), single-scale ViTs (e.g., TNT, CaiT, and CrossViT), and multi-scale ViTs (e.g., Swin, Twins-SVT, PVTv2, VOLO). In particular, under the Base size, the Top-1 accuracy score of Wave-ViT-B ? can reach 84.8%, which leads to the absolute improvement of 0.6% against the best competitive VOLO-D1</figDesc><table><row><cell>? (Top-1 accuracy:</cell></row><row><cell>84.2%). Moreover, when removing the upgraded strategies as in VOLO for train-</cell></row><row><cell>ing, our Wave-ViT-S still manages to outperform the best multi-scale ViT in</cell></row><row><cell>Small size (PVTv2-B2). These results generally demonstrate the key advantage</cell></row><row><cell>of unifying self-attention learning and invertible down-sampling with wavelet</cell></row><row><cell>transforms to facilitate visual representation learning. Most specifically, under</cell></row></table><note>the same Large size, compared to ResNet-152 and SE-ResNet-152 that solely capitalize on CNN architectures, the single-scale ViTs (e.g., TNT-B, CaiT-S36, and CrossViT-15-384) outperform them by capturing long-range dependency via Transformer structure. However, the performances of CaiT-S36 and CrossViT- 15-384 are still lower than most multi-scale ViTs (PVTv2-B5 and VOLO-D3 ? )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>optimizer (batch size: 16, initial learning rate: 0.00006, weight decay: 0.01). Both linear learning rate decay scheduler and a linear warmup of 1,500 iterations are utilized for optimization. The scale of input images is fixed as 512 ? 512. We perform</figDesc><table><row><cell>Average Multi-Head Attention Q K V Pooling</cell><cell>Conv Multi-Head Attention Q K V</cell><cell>Multi-Head Attention Conv Q K V DWT</cell><cell cols="2">Multi-Head Attention Q K V Conv DWT</cell><cell>IDWT</cell><cell cols="3">Params GFLOPs Top-1 Top-5 a 18.7M 3.9 82.0 96.0 b 20.4M 4.0 82.0 96.0 c 19.8M 4.3 82.5 96.1</cell></row><row><cell>(a) x</cell><cell>(b) x</cell><cell>(c) x</cell><cell>x</cell><cell>(d)</cell><cell></cell><cell>d 19.8M</cell><cell>4.3</cell><cell>82.7 96.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond deep residual learning for image restoration: Persistent homology-guided manifold simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chul Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Regionvit: Regional-to-local attention for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujieda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hachisuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08620</idno>
		<title level="m">Wavelet convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPMAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep wavelet prediction for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seyed Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">All tokens matter: Token labeling for training better vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Comprehending and ordering semantics for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contextual transformer networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Waveletbased dual-branch network for image demoir?ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Stand-alone inter-frame attention in video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dynamic temporal filtering in video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: the wavelet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Scaling the scattering transform: Deep hybrid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">X-linear attention networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10971" to="10980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Score-cam: Score-weighted visual explanations for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mardziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03810</idno>
		<title level="m">Scaled relu matters for training vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Wavelet pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Co-scale conv-attentional image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Token shift transformer for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Bridging the gap between anchorbased and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Exploring structureaware transformer over interaction proposals for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Semantic understanding of scenes through the ade20k dataset. IJCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
