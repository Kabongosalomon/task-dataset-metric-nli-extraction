<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Document Image Classification Using Region-Based Graph Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaya</forename><forename type="middle">Krishna</forename><surname>Mandivarapu</surname></persName>
							<email>jmandivarapu1@student.gsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">American Family Insurance</orgName>
								<orgName type="laboratory">Machine Learning Research Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bunch</surname></persName>
							<email>ebunch@amfam.com</email>
							<affiliation key="aff0">
								<orgName type="department">American Family Insurance</orgName>
								<orgName type="laboratory">Machine Learning Research Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>You</surname></persName>
							<email>qyou@amfam.com</email>
							<affiliation key="aff0">
								<orgName type="department">American Family Insurance</orgName>
								<orgName type="laboratory">Machine Learning Research Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
							<email>gfung@amfam.com</email>
							<affiliation key="aff0">
								<orgName type="department">American Family Insurance</orgName>
								<orgName type="laboratory">Machine Learning Research Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Document Image Classification Using Region-Based Graph Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document image classification remains a popular research area because it can be commercialized in many enterprise applications across different industries. Recent advancements in large pre-trained computer vision and language models and graph neural networks has lent document image classification many tools. However using large pre-trained models usually requires substantial computing resources which could defeat the costsaving advantages of automatic document image classification. In the paper we propose an efficient document image classification framework that uses graph convolution neural networks and incorporates textual, visual and layout information of the document. Empirical results on both publicly available and real-world data show that our methods achieve near SOTA performance yet require much less computing resources and time for model training and inference. This results in solutions than offer better cost advantages, especially in scalable deployment for enterprise applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gartner has estimated 80% of enterprises data is unstructured (emails, PDF and other documents). These documents contain rich information and knowledge about internal and external business communication and transactions. And they have ubiquitous applications in numerous industrial sectors such as finance, health care, and law etc. Therefore, being able to automatically and efficiently sort, analyze, and extract structure and content from document images can improve efficiency and reduce cost for many business workflows. Document image classification is an import task in these automation solutions, and has been a popular research area for decades. Early works usually build classifiers that rely on Optical Character Recognition (OCR) to extract text information, and employ heuristics to model layout structural features. In light of the advancement of computer vision and deep learning, VGG-16 (Simonyan and Zisserman 2014) pre-trained on ImageNet Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="bibr" target="#b1">(Deng et al. 2009</ref>) reported good classification performance on data sets mixed of business letters, print advertisement, emails and magazine articles <ref type="bibr" target="#b5">(Kumar, Ye, and Doermann 2014)</ref>. Both <ref type="bibr" target="#b2">(Denk and Reisswig 2019)</ref> and <ref type="bibr" target="#b7">(Xu et al. 2019</ref>) created document representations by encoding layout coordinates into positional embeddings as inputs to pre-trained BERT <ref type="bibr" target="#b3">(Devlin et al. 2018)</ref> or transformer architectures. The latest PubLayNet (Zhong, <ref type="bibr" target="#b10">Tang, and Jimeno-Yepes 2019)</ref> addresses the limited public available document image data sets by training a Mask R-CNN <ref type="bibr" target="#b5">(He et al. 2017</ref>) model on 360k images of scientific articles, and enables transfer learning to other document domains. Motivated by the development of graph neural network algorithms <ref type="bibr" target="#b7">(Wu et al. 2019;</ref><ref type="bibr" target="#b9">Zhang et al. 2018;</ref><ref type="bibr">?)</ref>, researchers <ref type="bibr" target="#b6">(Liu et al. 2019)</ref> attempted to use graph convolutions to model the interactions among structural components of a document and between the visual and textual features, as an alternative to pixel level or token level document modeling. In contrast to fast moving research progress in document analysis and classification, few have systematically studied the time and hardware resources when using different methods and the financial implications of the model design. However, as document image classifications have been primarily motivated by its potential in commercialization, it is imperative to study its model performance with computing resources requirements and financial implications. In this paper we propose an efficient document image classification framework as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Semantics regions of a document is extracted by pre-trained PubLayNet, textual features are extracted by text embedding models and the image features are extracted by a pre-trained VGG-16 model. Graphs formed for the document, with the document class labels are used to train a sort pooling graph convolution network <ref type="bibr" target="#b9">(Zhang et al. 2018</ref>) which normalizes and classify arbitrary graphs therefore documents. The major contributions of our papers are as follows:</p><p>? We propose a novel document image classification framework which applies a graph convolution neural network to a document image graph formed by semantic regions extracted from a pre-trained document segmentation model. Moreover both image and text features of the regions are extracted and assigned to the nodes so that information from both modalities are captured and propagated in the graph convolutions. To our best knowledge, our framework is the the first in effectively and economically integrating image, text, and layout information for document image classification using a graph convolution neural network. ? We have rigorously bench marked our proposed method against state-of-the-art pre-trained vision models and transformer language models on document image data sets. These include an insurance related document image data set consisted of 11 classes and an open source data set of 10 classes. The results showed the classification results of our method are comparable to those of baseline models, if not better. ? We extensively bench marked the computing resources required by all methods. The results showed our framework needs substantially less computing resources and less time, further indicating the cost advantages of training, deployment and hosting at scale. Efficient model also helps accelerate model iterations and update. We also discussed a few potential document image classification applications and the infrastructure to deploy our framework. The potentially large scale adoption of document image classification further reinforced the need for an efficient document image classification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early document image classification algorithms relied on OCR to extract content information and exploited the visual structure and layout of a document image, e.g. using tree-related data structures to model a document <ref type="bibr">(Dengel 1993;</ref><ref type="bibr" target="#b7">Shin, Doermann, and Rosenfeld 2001;</ref><ref type="bibr" target="#b4">Diligenti, Frasconi, and Gori 2003</ref>  <ref type="bibr" target="#b6">(Lewis et al. 2006</ref>). Document understanding and analysis community have also been leveraging word embedding techniques <ref type="bibr" target="#b6">(Mikolov et al. 2013)</ref> in NLP and large language models <ref type="bibr" target="#b3">(Devlin et al. 2018)</ref> to create contextualized embedding for textual content in an document image. BERTGrid <ref type="bibr" target="#b2">(Denk and Reisswig 2019)</ref> uses both the contextualized word embeddings and its 2D layout coordinates to extract information by predicting segmentation masks and bounding boxes. Assuming syntactic features matter less than content categories in document classification. DocBERT achieved an economical solution for document classification task by distilling BERT (?). LayoutLM (Xu et al. 2019) jointly models interactions between text and layout information by inputting both text embeddings along with its 2D layout positional embeddings extracted using OCR and Region of Interest (ROI) Regressions. Considering both the image and textual modalities in the document images, multi-modalities methods <ref type="bibr" target="#b0">(Audebert et al. 2019;</ref><ref type="bibr" target="#b8">Yang et al. 2017</ref>) are adapted to document classification tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we briefly review the advantages and limitations when using either CNNs or large language models in document classification. We also discuss document segmentation and the intuitions of using region based representations. We then describe our proposed efficient graph neural network, Eff-GNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Convolution Neural Network Learning Approaches</head><p>When using deep convolutional neural networks for document image classification, the document is treated as an image and is ingested as tensor representing the pixel values of the image. VGG-16 pre-trained on Ima-geNet (?) can achieve good results on general business documents (Das, Roy, and Bhattacharya 2018). Even for the insurance data set, VGG-16 pre-trained on Ima-geNet can be a powerful visual feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Model based Approaches</head><p>In general, BERT-like pre-trained language models achieve superior performance on natural language processing tasks by adding self attention mechanism and positional information to the encoder-decoder architecture. When using BERT-like models to classify document images, we classify based on the contextualized embedding of text extracted from image. DocBert (Adhikari et al. 2019) assumes syntax features matter less if only the categories of the document need to be decided. And DocBert successfully distills trained BERT into a much smaller LSTM model. This gives us some insight that token level modeling for document classification may not be necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Segmentation</head><p>A business document contains visually salient structural components such as header, footer, paragraph, table etc. Intuitively one can classify a document image by its layout and structural components without accessing much of its content. The regions of structural components are usually pixels or tokens with similar appearances or groupings. Hence regions are a higher level abstraction and representation which we can leverage to classify this document. Research in the computer vision community has provided plenty of tools of segmenting document images. The latest advancement is Pub-LayNet which trained a Mask R-CNN model for 360 thousand document images from scientific articles. The segmented region results from PubLayNet can be found at <ref type="figure" target="#fig_2">Figure 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient GNN for Document Image Classification</head><p>Ideally an effective document classification method need to leverage both textual, image and layout information. However, training or fine tuning CNNs or large language models do not only run into resource constraints (e.g. GPUs, memory ), but also prevent fast model iterations. We attempted to address this dilemma by graph representations using graph representations to represent document. We then assign image and text features to the nodes of the graph and apply a graph convolution neural network. Finally we classify the document as classifying a graph.</p><p>Details of training our proposed Eff-GNN can be found in Algorithm 1. For each image with class label, we extract its text using OCR PyTesseract and we extract its semantic regions using PubLayNet. Each image is converted into a graph where each node is a region. To generate the text feature of the nodes, we use Word2Vec to create the embeddings for words in the region; to generate image feature of the nodes, we extract visual features from that region using VGG-16 pre-trained on ImageNet. Text features and image features of the node can be concatenated and assigned to the nodes. A graph convolution neural network classifier with a SortPooling <ref type="bibr" target="#b9">(Zhang et al. 2018</ref>) layer is then trained on this data. We adapted to this specific graph convolution neural network because it preserve features of individual nodes and also enforces learning from graph global topology.</p><p>Till this end we have integrated textual, image and layout information into a document classification task using a graph convolution neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup Datasets</head><p>We use the following two datasets to evaluate our proposed model: Insurance dataset <ref type="figure" target="#fig_2">(Fig.2)</ref>, Tobacco-3482 dataset(Kumar, Ye, and Doermann 2014). Insurance dataset contains 5772 document images which spans across 11 categories and Tobacco-3482 dataset consists of 3482 images which spans across 10 categories. Categories from the Tobacco dataset include Advertisements, Emails, Memos, and Scientific Reports. Categories from the Insurance dataset include Medical Bills, Medical Authorizations, Medial Records, Police Reports, and Subrogation Letters. See <ref type="figure" target="#fig_2">Fig. 2</ref> for examples of documents from the Insurance dataset.</p><p>For the Insurance dataset, we use the splits of 4544 images for train set and 1280 images for test set; for Tobacco-3482 dataset, we use the standard splits of 2482 images for training, 800 images for testing, and rest 200 images for validation set.</p><p>We also summarized the statistics of graphs that those documents formed in <ref type="table" target="#tab_1">Table 1</ref>. As evidenced in the tables, the graph size of each document is significantly smaller relative to the number of pixels or the number of words in a document, which significantly simplifies the subsequent modeling and computation. Training GNN on graph data Z = {(G 1 , y 1 ) , (G 2 , y 2 ) , . . .} where y i ? Y is the label corresponding to graph G i ? G, with goal of learning a mapping f : G ? Y that maps graphs to the set of labels end Document Pre-processing</p><p>To construct a graph for each document image we need to utilize the different layout regions' information such as paragraphs, title, list, table present in each document. We process the scanned document images using pre-trained PubLayNet to obtain the necessary bound-class label 0 1 2 3 4 5 6 7 8 9 10</p><p>Insurance 13 5 6 18 5 12 3 11 8 13 11</p><p>Tobacco 5 9 3 11 10 5 2 9 10 2 -  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper parameters and infrastructure</head><p>We use Hedwig 1 , an open-source deep learning toolkit with a number of implemented of document classification models. We use a Tesla K80 GPU for all models requiring GPU for train, and use amazon EC2-t2.micro and EC2-c type machine when only CPU is needed . We use PyTorch 1.5 as the backend framework, and gensim (?eh??ek and Sojka 2010) package for computing the node feature vectors using word2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing classification accuracy</head><p>We compare our proposed approach with the stateof-the-art deep learning models, VGG-16 pre-trained on ImageNet and BERT base pre-trained on Wikipedia. When using BERT base for classification, we extracted tokens from document images as input and fine tune BERT base with class labels. When using pre-trained VGG-16, we used it directly to extract document image features for classification. No further fine-tuning is used. We also compared our model with DocBERT which is specially designed for document level classification by simplifying BERT models with knowledge distillation. <ref type="table" target="#tab_3">Table 2</ref> shows the model comparison results of classification AUC on the two data sets. In the insurance data set, our proposed model achieves 90.7% to 91.0 %, very competitive as compared to models in BERT families (91.95 %) and . In the Tobacco-3482 data set, our models achieved 73.5 % to 77.5 % , comparable to models in BERT families (82.3 %) and . The fact our proposed model shows more advantages when classifying the insurance data set could be due to the high intra-class variance and low interclass variance in the Tobacco-3482 data set .</p><p>We also experimented with combining text and image embedding features as node features in the graph neural network. The combination does provides ample AUC improvements in our proposed method on Tobacco-3482 dataset i.e. 73.5 % for Eff-GNN + Word2Vec and for 77.5 % for Eff-GNN + Word2Vec + Image Embedding. In the insurance data set, Eff-GNN + Word2Vec + Image Embedding shows little improvement over using Word2Vec text features alone. That could be due to the fact that both the textual and image content in the insurance data set provides enough information for classification. This assumption can be further justified by the similar classification results achieved by models in BERT family and VGG-16.</p><p>In addition to classification performance, we compare the number of trainable parameters of each model. In both the insurance data set and Tobacco-3482, our model size is drastically smaller than models in BERT families and VGG-16. We calculate the parameters of our model as the sum of parameters in the graph neural network and the parameters in trained word2vec. The small sizes of graph neural network model <ref type="table" target="#tab_1">(Table 1)</ref> results in only 160,000 parameters. The Word2Vec model is also relatively light weight because each of the data set contains a very limited vocabulary. Note we did not include the 44.2 million parameters of PubLayNet, because in our framework we do not train or fine-tune any parameters of the PubLayNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing computing resources</head><p>We also bench marked the time and memory required for training our proposed Eff-GNN against other models. <ref type="table" target="#tab_4">Table 3</ref> reports the statistics on the insurance data set, 4544 images for training and 1280 images for inference. Eff-GNN models take less than 5 minutes to train 50 epochs whereas VGG-16 or models in BERT Family take hours to train less number of epochs (15 epochs and 28 epochs respectively). In particular Eff-GNN can run on CPU alone and its model training time is comparable to its GPU counterpart. This is consistent with the small size of our model (See <ref type="table" target="#tab_3">Table 2</ref>, "Parameters" col-umn). Eff-GNN can achieve these advantages because it models the documents using a graph formed by regions extracted by PubLayNet. The time of using PubLayNet to extract regions for training images are negligible. The size of the resulting graph leads to a small model compared to deep models trained on pixel level information or transformers trained on token level information. Therefore Eff-GNN only uses 470MB in GPU memory with additional 3.5 GB for using PubLayNet. Consequently Eff-GNN requires drastically less time for the inference of 1280 images (0.79 seconds) as compared VGG-16 (103 seconds) and BERT (40 seconds). Note the time of document pre-porcessing steps such as OCR and training Word2Vec model are not included in the table. Although these two steps are extra for our proposed framework, we contend that their addition does not nullify the efficiencies gained through graph neural nets. Even BERT based models require OCR extraction preprocessing step. Just one Word2Vec needs to be trained for the entire data sets and OCR can be optimized by e.g. parallel processing.</p><p>Compared with the SOTA pre-trained large models, our proposed Eff-GNN framework achieved competitive classification results on our insurance document image data sets, and achieved comparable results on the the open source Tobacco-3482 data set. We also showed that combining text and image information as the node features in our graph neural network can be advantageous when OCR fails to extract text information or when the two modalities are complimentary. Our proposed method models document representations using extracted semantic regions, instead of using token level or pixel level information. Therefore our model size is dramatically less than other methods, and can be run on CPU machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applications and Deployment</head><p>Document image classification can find many enterprise level applications in insurance companies to reduce manual review and stream line claims processes. For example, casualty injuries claims usually have multiple correspondences with hospitals and clinics, each involving documents to be processed by the insurance company. Personal automobile claims processing can use automatic document classification of letters of guarantee, purchase receipts, and others in order to autoapprove a certain percentage of auto claim reimbursements. Each year, millions of claim related document images sent to insurance companies belonging to hundreds of categorizes related to financial institutions, medical providers, or legal organizations. A scalable document classifier can assign the correct categories as meta data to a document, which can be used to route to appropriate downstream tasks. Given the scale document image classification can be deployed in an enterprise, an efficient framework for model training and iterations, together with an economical model hosting solutions has clear cost advantages. To support all those applications and achieve potential scalability , we are in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>Millions of business document images, such as medical bills, attorney letters, contracts, bank statements and personal checks are processed in insurance companies to support a wide range of business workflows and applications. A scalable and efficient automation that is more intelligent than the brittle OCR-template based method is desired. In this paper we proposed a novel document image classification that uses graph convolution neural network to integrate text, image, and layout information of a document. We rigorously bench marked our method against the SOTA computer vision and language models on both the insurance dataset and Tobacco dataset. We also compared computing time and hardware resources required for training those models. The results showed our method is not only competitive on classification performance but also is much smaller in size therefore requires much less time and resource. This could translate to big cost advantages of hosting and deployment in real world applications. We are also working on enabling general document classification that can handle hundreds of document classes. A few options include training larger models for domain specific transfer learning, enabling few shot learning and continual learning when dynamically adding new document classes. In addition, we would like to further explore more effective document representations including more sophisticated graph representations or jointly trained layout <ref type="bibr" target="#b7">(Xu et al. 2019)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Eff-GNN Framework overview: textual embedding, segmented regions and image embeddings of an image are integrated when the graph of document is formed. Created graph is fed into the Graph Convolution Neural Network for graph classification as document image classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c) &amp; (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Examples of documents from the Insurance data set. The document classes (a) Medical Record, (b) Subrogation Letter, (c) Medical Authorization, and (d) Attorney Correspondence. The two images (c) and (d) include a visualization of the output of the PubLayNet model. Algorithm 1: Efficent Graph Neural Network Training Input: N Training documents {(D 1 , y 1 ), . . . , (D N , y N )}. Preprocessing for m = 1, . . . , N do Extract textual information t m , detect layout regions s m and visual features I m present in D m end Train word2vec W=word2vec(C) C = {t i | 1 ? i ? N } for m = 1, . . . , N do Convert each document (D m ) into graph (G m ) using t m ,s m , I m and W which would be D m ?G m end end Training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ing boxes for each region. To extract the textual information present in the bounding box of each region in the document images we apply PyTesseract, an opensource python OCR package. All the results of the Pub-LayNet and PyTesseract were then serialized and stored in tensor format using PyTorch.(Paszke et al. 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Median number of nodes per class for the Insurance and Tobacco data sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy on the Insurance, Tobacco-3482 dataset.</figDesc><table><row><cell>Insurance</cell><cell cols="4">Batch Size Epochs Training Time GPU Memory (Training)</cell><cell>Inference Time</cell></row><row><cell>VGG</cell><cell>32</cell><cell>28</cell><cell>2.30 hours</cell><cell>7.08GB</cell><cell>103 seconds</cell></row><row><cell>Eff-GNN (GPU)</cell><cell>32</cell><cell>50</cell><cell>3.5 mins.</cell><cell>470MB + 3.5 GB</cell><cell>0.79 seconds</cell></row><row><cell>Eff-GNN (CPU)</cell><cell>32</cell><cell>50</cell><cell>4.1 mins.</cell><cell>NA</cell><cell>0.79 seconds</cell></row><row><cell>BERT</cell><cell>16</cell><cell>15</cell><cell>6.2 hours</cell><cell>10.5GB</cell><cell>40 seconds</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"40 times faster</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>than BERT"</cell></row><row><cell>DocBERT</cell><cell>16</cell><cell>15</cell><cell>6.3 hours</cell><cell>8.1GB</cell><cell>(Adhikari et al. 2019)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Memory, hardware and time required by different models on the Insurance dataset. The numbers are reported for training 4544 images and inference for 1280 Images.the progress of deploying this trained model as an API under Container as a Service (CaaS) on AWS using Fargate managed Elastic Container Service (ECS). The efficient model size facilitates faster deployment and ease of maintenance. A standard ECS task with v4CPUs and 30G RAM can be used and the weights of the trained model can be conveniently packaged in a dockerized image and deployed at scale.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/castorini/hedwig</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cutting the error by half: Investigation of very deep CNN and advanced training strategies for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adhikari</surname></persName>
		</author>
		<idno>abs/1704.03557</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases,International Workshops of ECML PKDD 2019</title>
		<editor>Cellier, P., and Driessens, K.</editor>
		<meeting><address><addrLine>Wurzburg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="427" to="443" />
		</imprint>
	</monogr>
	<note>Document image classification with intra. domain transfer learning and stacked generalization of deep convolutional neural networks. CoRR abs/1801.09321</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference Document Analysis and Recognition, ICDAR &apos;93</title>
		<meeting><address><addrLine>Miami, Florida, USA; Tsukuba City, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
	<note>Initial learning of document structure</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bertgrid: Contextualized embedding for 2d document representation and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<idno>abs/1909.04948</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>and Reisswig</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hidden tree markov models for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frasconi</forename><surname>Diligenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diligenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="519" to="523" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time document image classification using deep CNN and extreme learning machines</title>
		<idno>abs/1703.06870</idno>
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on Document Analysis and Recognition</title>
		<meeting><address><addrLine>Stockholm, Sweden; Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-08-24" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
	<note type="report_type">Mask R-CNN. CoRR</note>
	<note>Structural similarity for document image classification and retrieval</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph convolution for multimodal information extraction from visually rich documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Wallach, H.</editor>
		<editor>Larochelle, H.</editor>
		<editor>Beygelzimer, A.</editor>
		<editor>d&apos;Alch?-Buc, F.</editor>
		<editor>Fox, E.</editor>
		<editor>and Garnett, R.</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Seattle, Washington, USA; Minneapolis, MN, USA; Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2006-08-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classification of document pages using structure-based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doermann</forename><surname>Rosenfeld ; Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<idno>Xu et al. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="232" to="247" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A comprehensive survey on graph neural networks. Layoutlm: Pre-training of text and layout for document image understanding. CoRR abs/1912.13318</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural network</title>
		<ptr target="CoRRabs/1706.02337" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18)</title>
		<editor>McIlraith, S. A., and Weinberger, K. Q.</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Publaynet: Largest dataset ever for document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition, ICDAR 2019</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-09-20" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
