<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Gou</surname></persName>
							<email>gouyanjie@stu.scu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
							<email>yinjie@scu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
							<email>lingqiao.liu@adelaide.edu.audaiyongya@yahoo.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Dai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxu</forename><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Incorporating knowledge bases (KB) into endto-end task-oriented dialogue systems is challenging, since it requires to properly represent the entity of KB, which is associated with its KB context and dialogue context. The existing works represent the entity with only perceiving a part of its KB context, which can lead to the less effective representation due to the information loss, and adversely favor KB reasoning and response generation. To tackle this issue, we explore to fully contextualize the entity representation by dynamically perceiving all the relevant entities and dialogue history. To achieve this, we propose a COntextaware Memory Enhanced Transformer framework (COMET), which treats the KB as a sequence and leverages a novel Memory Mask to enforce the entity to only focus on its relevant entities and dialogue history, while avoiding the distraction from the irrelevant entities. Through extensive experiments, we show that our COMET framework can achieve superior performance over the state of the arts. * Corresponding author</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Poi</head><p>Poi type Traffic Address Distance</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Task-oriented dialogue systems aim to achieve specific goals such as hotel booking and restaurant reservation. The traditional pipelines <ref type="bibr" target="#b34">(Young et al., 2013;</ref><ref type="bibr" target="#b30">Wen et al., 2017)</ref> consist of natural language understanding, dialogue management, and natural language generation modules. However, designing these modules often requires additional annotations such as dialogue states. To simplify this procedure, the end-to-end dialogue systems  are proposed to incorporate the KB (normally relational databases) into the learning framework, where the KB and dialogue history can be directly modeled for response generation, without the explicit dialogue state or dialogue action.   . The top is the entities in KB and the bottom is a twoturn dialogue between the user and system.</p><p>An example of the end-to-end dialogue systems is shown in Tab. 1. When generating the second response about the "traffic info": (1) the targeted entity "no traffic" is associated with its same-row entities (KB context) like "Tome's house", "friend's house" and "6 miles". These entities can help with enriching the information of its representation and modeling the structure of KB. (2) Also, the entity is related to the dialogue history (dialogue context), which provides clues about the goal-related row (like "Tom's house" and "580 Van Ness Ave" in the first response). These clues can be leveraged to further enhance the corresponding representations and activate the targeted row, which benefits the retrieval of "no traffic". Therefore, how to fully contextualize the entity with its KB and dialogue contexts, is the key point of end-to-end dialogue systems <ref type="bibr" target="#b21">(Madotto et al., 2018;</ref><ref type="bibr" target="#b24">Qin et al., 2020)</ref>, where the full-context enhanced entity representation can make the reasoning over KB and the response generation much easier.</p><p>However, the existing works can only contextualize the entity with perceiving parts of its KB context and ignoring the dialogue context: (1) <ref type="bibr" target="#b21">(Madotto et al., 2018;</ref><ref type="bibr" target="#b24">Qin et al., 2020)</ref>  Figure 1: Four ways to represent the KB, where e i,j means the entity representation for the j-th entity of the i-th row; R i means the row representation of the ith row; e ?,j means the entities shared between different row, like "no traffic" in Tab. 1; D means the dialogue context. Note that the existing three representations (a-c) only consider parts of the KB context and ignore the dialogue context, whereas our method (d) can fully contextualize the entity with both of them.</p><p>resent an entity as a triplet (cf. <ref type="figure">Fig. 1(a)</ref>), i.e., <ref type="bibr">(Subject, Relation, Object)</ref>. However, breaking one row into several triplets can only model the relation between two entities, whereas the information from other same-row entities and dialogue history are ignored.</p><p>(2) <ref type="bibr" target="#b12">(Gangi Reddy et al., 2019;</ref><ref type="bibr" target="#b23">Qin et al., 2019)</ref> represent KB in a hierarchical way, i.e., the row and entity-level representation (cf. <ref type="figure">Fig. 1(b)</ref>). This representation can only partially eliminate this issue at the row level. However, at the entity level, the entity can only perceive the information of itself, which is isolated with other KB and dialogue contexts.</p><p>(3) <ref type="bibr" target="#b33">(Yang et al., 2020)</ref> converts KB to a graph (cf. <ref type="figure">Fig. 1(c)</ref>). However, they fails to answer what is the optimal graph structure for KB. That indicates their graph structure may need manual design 1 . Also, the dialogue context is not encoded into the entity representation, which can also lead to the suboptimal entity representation. To sum up, these existing methods can not fully contextualize the entity, which leads to vulnerable KB reasoning and response generation. In this work, we propose COntext-aware Memory Enhanced Transformer (COMET), which provides a unified solution to fully contextualize the entity with the awareness of both the KB and dialogue contexts (shown in <ref type="figure">Fig. 1(d)</ref>). The key idea of COMET is that: a Memory-Masked En-coder is used to encode the entity sequence of KB, along with the information of dialogue history. The designed Memory Mask is utilized to ensure the entity can only interact with its same-row entities and the information in dialogue history, whereas the distractions from other rows are prohibited.</p><p>More specifically, (1) for the KB context, we represent the entities in the same row as a sequence. Then, a Transformer Encoder <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> is leveraged to encode them, where the same-row entities can interact with each other. Furthermore, to retain the structure of KB and avoid the distractions from the entities in different rows, we design a Memory Mask (shown in <ref type="figure">Fig. 3</ref>) and incorporate it into the encoder, which only allows the interactions between the same-row entities.</p><p>(2) For the dialogue context, we create a Summary Representation (Sum. Rep) to summarize the dialogue history, which is input into the encoder to interact with the entity representations (gray block in <ref type="figure" target="#fig_1">Fig. 2</ref>). We also utilize the Memory Mask to make the Sum. Rep overlook all of the entities for better entity representations, which will serve as the context-aware memory for further response generation.</p><p>By doing so, we essentially extend the entity of KB to (N + 1)-tuple representation, where N is the number of entities in one row and "1" is for the Sum. Rep of the dialogue history. By leveraging the KB and dialogue contexts, our method can effectively model the information existing in KB and activate the goal-related entities, which benefits the entity retrieval and response generation. Please note that the function of fully contextualizing entity is unified by the designed Memory Mask scheme, which is the key of our work.</p><p>We conduct extensive experiments on two public benchmarks, i.e., SMD <ref type="bibr" target="#b21">Madotto et al., 2018)</ref> and Multi-WOZ 2.1 <ref type="bibr">(Budzianowski et al., 2018;</ref><ref type="bibr" target="#b33">Yang et al., 2020)</ref>. The experimental results demonstrate significant performance gains over the state of the arts. It validates that contextualizing KB with Transformer benefits entity retrieval and response generation.</p><p>In summary, our contributions are as follows:</p><p>? To the best of our knowledge, we are the first to fully contextualize the entity representation with both the KB and dialogue contexts, for end-to-end task-oriented dialogue systems.  awareness of both the relevant entities and dialogue history.</p><p>? Extensive experiments demonstrate that our method gives a state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we first introduce the general workflow for this task. Then, we elaborate on each part of COMET, i.e., the Dialogue History Encoder, Context-aware Memory Generation, and Response Generation Decoder (as depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>). Finally, the objective function will be introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Workflow</head><p>Given a dialogue history with k turns, which is denoted as H = {u 1 , s 1 , u 2 , s 2 , ..., u k } (u i and s i denote the i-th turn utterances between the user and the system), the goal of dialogue systems is to generate the k-th system response s k with an</p><formula xml:id="formula_0">external KB B = {[b 11 , ..., b 1c ], ..., [b r1 , ..., b rc ]},</formula><p>which has r rows and c columns. Formally, the procedure mentioned above is defined as:</p><formula xml:id="formula_1">p(s k |H, B) = n i=1 p(s k,t |s k,1 , ..., s k,t?1 , H, B),</formula><p>where we first derive the dialogue history representation (Section 2.2) and generate the Context-aware Memory, a.k.a., contextualized entity representation (Section 2.3), where these two parts will be used to generate the response s k (Section 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dialogue History Encoder</head><p>We first transform H into the word-by-word form with a special token [SUM]:</p><formula xml:id="formula_2">? = {x 1 , x 2 , ..., x n }, x 1 = [SUM]</formula><p>, which is used to globally aggregate information from H. Then, the sequence? is encoded by a standard Transformer Encoder and generate the dialogue history representation H enc N , where H enc N,1 is denoted as the Summary Representation (Sum. Rep) of the dialogue history. 2 It will be used to make the memory aware of the dialogue context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Context-aware Memory Generation</head><p>In this subsection, we describe how to "fully contextualize KB". That is, the Memory Mask is leveraged to ensure the entities of KB with the awareness of all of its related entities and dialogue history, which is the key contribution of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Memory Generation</head><p>Different from existing works which fail to contextualize all the useful context information for the entity representation, we treat KB as a sequence, along with Sum. Rep. Then, a Transformer Encoder with the Memory Mask is utilized to model it, which can dynamically generate the entity representation with the awareness of its all favorable contexts, i.e., the same-row entities and dialogue history, while blocking the distraction from the irrelevant entities. The procedure of memory generation is as follows.</p><p>Firstly, the entities in the KB B is flatten as a memory sequence, i.e., M =</p><formula xml:id="formula_3">[b 11 , ..., b 1c , ..., b r1 , ..., b rc ] = [m 1 , m 2 , ..., m |M| ],</formula><p>where the memory entity m i means an entity of KB in the k-th row. By doing so, the Memory-Masked Transformer Encoder can interact the same-row entities with each other while retaining the structure information of KB. <ref type="bibr">3</ref> Then, M will be transformed into the entity embeddings, i.e., E = [e m 1 , ..., e m |M| ], where e m i corresponds to m i in M and it is the sum of the word embedding u i and the type embedding t i , i.e., e m i = u i + t i . Note that, the entity types are the corresponding column names, e.g., "poi_type" in <ref type="table" target="#tab_1">Table 1</ref>. For the entities which have more than one token, we simply treat them as one word, e.g., "Stanford Exp" ? "Stanford_Exp".</p><p>Next, the entity embeddings are concatenated with the Sum. Rep from the Dialogue History En-</p><formula xml:id="formula_4">coder, i.e. E 0 = [H enc N,1 ; E].</formula><p>The purpose of introducing H enc N,1 is that it passes the information from the dialogue history and further enhances the entity representation with the dialogue context.</p><p>Finally, E 0 and the Memory Mask M mem are used as the input of the Transformer Encoder (tf _enc(?)) to generate the context-aware memory (a.k.a, contextualized entity representation):</p><formula xml:id="formula_5">E l = tf _enc(E l?1 , M mem ), l ? [1, K],</formula><p>where K is the total number of Transformer Encoder layers. E K ? R (|M|+1)?dm is the generated memory, which is queried when generating the response for entity retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Memory Mask Construction</head><p>To highlight, we design a special Memory Mask scheme to take ALL the contexts grounded by the entity into account, where the Memory Mask ensures that the entity can only attend to its context part, which is the key contribution of this work. This is in contrast to the standard Transformer Encoder, where each entity can attend to all of the other entities. The rationale of our design is that by doing so, we can avoid the noisy distraction of the non-context part.</p><formula xml:id="formula_6">Formally, M mem ? R (|M|+1)?(|M|+1) is de- fined as: M mem i,j = ? ? ? ? ? 1, if M i?1 , M j?1 ? b k , 1, if i or j = 1, ? ?, else.</formula><p>A detailed illustration of the Memory Mask construction is shown in <ref type="figure">Fig. 3</ref>. With this designed Memory Mask, a masked attention mechanism is leveraged to make the entity only attend the entities within the same row and the Sum. Rep.  <ref type="figure">Figure 3</ref>: The Construction of Memory Mask. C i means the column name (e.g., "Poi"). e ij means the j-th entity of i-th row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C1</head><p>[SUM] means the Sum. Rep. Only two rows of KB are shown for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Response Generation Decoder</head><p>Given the dialogue history representation H enc N and generated memory E K , the decoder will use them to generate the response for a specific query. In COMET, we use a modified Transformer Decoder, which has two cross attention modules to model the information in H enc N and E K , respectively. Then, a gate mechanism is leveraged to adaptively fuse H enc N and E K for the decoder, where the response generation is tightly anchored by them.</p><p>Following <ref type="bibr" target="#b24">Qin et al., 2020;</ref><ref type="bibr" target="#b33">Yang et al., 2020)</ref>, we first generate a sketch response that replaces the exact slot values with sketch tags. 4 Then, the decoder links the entities in the memory to their corresponding slots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Sketch Response Generation</head><p>For the k-th turn generating sketch response Y = [y 1 , ...y t?1 ], it is converted to the word representation H dec</p><formula xml:id="formula_7">0 = [w d 1 , ..., w d t?1 ]. w d i = v i + p i ,</formula><p>where v i and p i means the word embedding and absolute position embedding of i-th token in Y.</p><p>Afterward, N -stacked decoder layers are applied to decode the next token with the inputs of H dec 0 , E K and H enc N . The process in one decoder layer can be expressed as:</p><formula xml:id="formula_8">H d?d l = M HA(H dec l?1 , H dec l?1 , H dec l?1 , M dec ), H d?e l = M HA(H d?d l , H enc N , H enc N ), H d?m l = M HA(H d?d l , E K , E K ), g = sigmoid(F C(H d?m l )), H agg l = g H d?e l + (1 ? g) H d?m l , H dec l = F F N (H agg l ), l ? [1, N ],</formula><p>where the input {Q, K, V, M } of the Multi-Head Attention M HA(Q, K, V, M ) means the query, key, value, and optional attention mask. F F N (?) means the Feed-Forward Networks. M dec is the decoder mask, so as to make the decoded word can only attend to the previous words. F C(?) is a fully-connected layer to generate the gating signals, which maps a d m -dimension feature to a scalar. N is the number of the total decoder layers.</p><p>After obtaining the final H dec N , the posterior distribution for the t-th token, p v t ? R |V | (|V | denotes the vocabulary size), is calculated by:</p><formula xml:id="formula_9">p v t = sof tmax(H dec N,t?1 W v + b v ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Entity Linking</head><p>After the sketch response generation, we replace the sketch tags with the entities in the contextaware memory. We denote the representation from the decoder at the t-th time step, i.e., the t-th token, as H dec N,t , and represent the time steps that need to replace sketch tags with entities as T . The probability distribution over all possible linked entities can then be calculated by</p><formula xml:id="formula_10">p s t = sof tmax(H dec N,t E T K ), ?t ? T</formula><p>where E K means the final generated memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Objective Function</head><p>For the training process of COMET, we use the the cross-entropy loss to supervise the response generation and entity linking 5 . Moreover, we propose an additional regularization term to further regularize p s t . The regularization is based on the prior knowledge that for a given response, only a small subset of entities should be linked. Formally, we construct the following entity linking probability matrix P s = [p s t 1 , p s t 2 , ..., p s t |T | ] and minimize its L 2,1 -norm <ref type="bibr" target="#b22">(Nie et al., 2010)</ref>:</p><formula xml:id="formula_11">L r = |M| i=1 t?T (p s t,i ) 2 ,</formula><p>where p s t,i denotes the i-th dimension of p s t . This regularization term can encourage the network to select a small subset of entities to generate the response. The same idea has been investigated in <ref type="bibr" target="#b22">(Nie et al., 2010)</ref> for multi-class feature selection.</p><p>Finally, COMET is trained by jointly minimizing the combination of the above three losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Two public multi-turn task-oriented dialogue datasets are used to evaluate our model, i.e., SMD 6  and Multi-WOZ 2.1 7 <ref type="bibr">(Budzianowski et al., 2018)</ref>. Note that, for Multi-WOZ 2.1, to accommodate end-to-end settings, we use the revised version released by <ref type="bibr" target="#b33">(Yang et al., 2020)</ref>, which equips the corresponding KB to every dialogue. We follow the same partition as <ref type="bibr" target="#b21">(Madotto et al., 2018)</ref> on SMD and <ref type="bibr" target="#b33">(Yang et al., 2020)</ref> on Multi-WOZ 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>The dimension of embeddings and hidden vectors are all set to 512. The number of layers (N ) in Dialogue History Encoder and Response Generation Decoder is set to 6. The number of layers for Context-aware Memory Generation (K) is set to 3. The number of heads in each part of COMET is set to 8. A greedy strategy is used without beam-search during decoding. The Adam optimizer (Kingma 5 The label construction procedure of the entity linking module can be found in Appendix A.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare COMET with the following methods:</p><p>? Mem2Seq (Triplet) <ref type="bibr" target="#b21">(Madotto et al., 2018)</ref>: Mem2Seq incorporates the multi-hop attention mechanism in memory networks into the pointer networks. KB-retriever improves the entity-consistency by first selecting the target row and then picking the relevant column in this row. ? GLMP (Triplet) : GLMP uses a global memory encoder and a local memory decoder to incorporate the external knowledge into the learning framework. ? DF-Net (Triplet) <ref type="bibr" target="#b24">(Qin et al., 2020)</ref>: DF-Net applies a dynamic fusion mechanism to transfer knowledge in different domains. ? GraphDialog (Graph) <ref type="bibr" target="#b33">(Yang et al., 2020)</ref>:</p><p>GraphDialog exploits the graph structural information in KB and in the dependency parsing tree of the dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Following the existing works <ref type="bibr" target="#b24">(Qin et al., 2020;</ref><ref type="bibr" target="#b33">Yang et al., 2020)</ref>, we use the BLEU and Entity F1 metrics to evaluate model performance. The results are shown in Tab. 2. It is observed that: COMET achieves the best performance over both datasets, which indicates that our COMET framework can better leverage the information in the dialogue history and external KB, to generate more fluent responses with more accurate linked entities. Specifically, for the BLEU score, it outperforms the previous methods by 2.9% on the SMD dataset and 2.1% on the Multi-WOZ 2.1 dataset, at least. Also, COMET achieves the highest Entity F1 score on both datasets. That is, the improvements of 0.9% and 7.3% are attained on the SMD and Multi-WOZ 2.1 datasets, respectively. In each domain of the two datasets, improvement or competitive performance can be clearly observed.</p><p>The results indicate the superior of our COMET framework.</p><p>To highlight, KB-Transformer (E. et al., 2019) also leverages Transformer, but our COMET outperforms it by a large margin. On the SMD dataset, the BLEU score of COMET is higher than that of KB-Transformer by 3.4%. The improvement introduced by COMET on Entity F1 score is as significant as 26.5%. This shows naively introducing Transformer to the end-to-end dialogue system will not necessarily lead to higher performance. A careful design of the whole dialogue system, such as our proposed one, plays a vital role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>In this subsection, we first investigate the effects of the different components, i.e., the Memory Mask, Sum. Rep, gate mechanism, and L 2,1 -norm regularization (Tab. 3). Then, we design careful experiments to further demonstrate the effect of the Memory Mask, which is the key contribution of this work: (1) we replace the context-aware memory of COMET with the existing three representations of KB, (i.e., triplet, row-entity, and graph) to show the superior of the fully contextualized entity (Tab. 4).</p><p>(2) We also replace our Memory Mask with the full attention layer by layer, which further shows the importance of our Memory Mask (Tab. 5). Our ablation studies are based on the SMD dataset.  The effects of the key components in the COMET framework are reported in Tab. 3. As observed, removing any key component of the COMET, both the BLEU and Entity F1 metrics degrade to some extend. More specifically: (1) If the Memory Mask is removed, the Entity F1 score drops to 49.6. This significant discrepancy demonstrates the importance of restricting self-attention as our designed Memory Mask did.</p><p>(2) For the variant without the Sum. Rep, the Entity F1 score drops to 61.4. That indicates the effectiveness of contextualizing the KB with the dialogue history, which can further boost the performance. (3) We also remove the gate and only use the information from the dialogue history (H enc N ) or memory (E K ). We can see that the former case can only achieve 61.1 while the latter case achieves 61.4 of the Entity F1 score. It is obvious that using the gate mechanism to fuse both information sources is helpful for the entity linking. (4) When removing the L 2,1norm, the performance also drops to 62.3, which means regularizing the entity-linking distribution can further benefit the performance.  We also replace our context-aware memory with other ways of representing KB, while other parts of our framework keep unchanged 8 . The result is reported in Tab. 4. It is observed that, After replac- <ref type="bibr">8</ref> The implementation details are in Appendix A.3. ing our context-aware memory with the existing three representations of KB, the performance drops a lot in all the metrics, where the BLEU score drops 2.4% and the Entity F1 score drops 3.8% at least. Besides, the result of the variant which only considers the KB context part (i.e., w/o Sum. Rep), is also reported, so as to further fairly compare with the aforementioned KB representations. The result shows that only considering the KB context, our method can still outperform other KB representations by 1.6% of Entity F1 at least. That further indicates the fully contextualizing entity with its relevant entity and the dialogue history, can better represent the KB for dialogue systems.  We also conduct the experiment which replaces the Memory Mask with the full attention, layer by layer. That is, the first (n-k) layers use the proposed Memory Mask (M) and the last k layers use the full attention (F). As shown in Tab. 5, the more full attention is added, the more performance of COMET drops in all of the metrics since the full attention introduces too much distraction from other rows. The result further indicates that the Memory Mask is indeed a better choice which takes the inductive bias of KB into account.</p><p>Note that we also explore other Memory Mask schemes, but these schemes can not further boost the performance, where the results are omitted due to the page limitation. For further improvement, more advanced techniques like Pre-trained Model <ref type="bibr" target="#b8">(Devlin et al., 2018;</ref><ref type="bibr" target="#b25">Radford et al., 2019)</ref> may be needed to deeply understand the dialogue and KB context, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Case Study</head><p>To demonstrate the superiority of our method, several examples on the SMD test set, which are generated by our COMET and the existing state of the arts GLMP  and DF-Net <ref type="bibr" target="#b24">(Qin et al., 2020)</ref>, are given in Tab. 6. As reported, compared with GLMP and DF-Net, COMET can generate <ref type="table">Table 6</ref>: Responses generated by our COMET, GLMP  and DF-Net <ref type="bibr" target="#b24">(Qin et al., 2020)</ref> from the SMD dataset. Goal means the row that the user is queried. and ? mean the right or wrong entity linked. more fluent, informative, and accurate responses.</p><p>Specifically, in the first example, GLMP and DF-NET are lack of the necessary information "11am" or provide the wrong entity "5pm". But COMET can obtain all the correct entities, which is more informative. In the second example, our method can generated the response with the right "distance" information but GLMP and DF-Net can not. In the third example, GLMP and DF-Net can not even generate a fluent response, let alone the correct temperature information. But COMET can still perform well. The fourth example is more interesting: the user queries the information about "starbucks" which does not exist in the current KB. GLMP and DF-Net both fail to faithfully respond, whereas COMET can better reason KB to generate the right response and even provide an alternative option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Task-oriented dialogue systems can be mainly categorized into two parts: modularized (Williams and <ref type="bibr" target="#b31">Young, 2007;</ref><ref type="bibr" target="#b30">Wen et al., 2017)</ref> and end-toend . For the end-to-end task-oriented dialogue systems,  first explores the end-to-end method for the task-oriented dialogue systems. However, it can only link to the entities in the dialogue context and no KB is incorporated. To effectively incorporate the external KB,  proposes a keyvalue retrieval mechanism to sustain the grounded multi-domain discourse. <ref type="bibr" target="#b21">(Madotto et al., 2018)</ref> augments the dialogue systems with end-to-end memory networks <ref type="bibr" target="#b26">(Sukhbaatar et al., 2015)</ref>. ) models a dialogue state as a fixed-size distributed representation and uses this representation to query KB. <ref type="bibr" target="#b16">(Lei et al., 2018)</ref> designs belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a sequence-tosequence way. <ref type="bibr" target="#b12">(Gangi Reddy et al., 2019)</ref> proposes a multi-level memory to better leverage the external KB.  proposes a global-to-local memory pointer network to reduce the noise caused by KB. <ref type="bibr" target="#b17">(Lin et al., 2019)</ref> proposes Heterogeneous Memory Networks to handle the heterogeneous information from different sources. <ref type="bibr" target="#b24">(Qin et al., 2020)</ref> proposes a dynamic fusion mechanism to transfer the knowledge among different domains. <ref type="bibr" target="#b33">(Yang et al., 2020)</ref> exploits the graph structural informa-tion in KB and the dialogue. Other works also explore how to combine the Pre-trained Model <ref type="bibr" target="#b8">(Devlin et al., 2018;</ref><ref type="bibr" target="#b25">Radford et al., 2019)</ref> with the endto-end task-oriented dialogue systems. <ref type="bibr" target="#b19">(Madotto et al., 2020a)</ref> directly embeds the KB into the parameters of GPT-2 <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> via finetuning. <ref type="bibr">(Madotto et al., 2020b</ref>) proposes a dialogue model that is built with a fixed pre-trained conversational model and multiple trainable light-weight adapters.</p><p>We also notice that some existing works also combine Transformer with the memory component, e.g., <ref type="bibr" target="#b18">(Ma et al., 2021)</ref>. However, our method is distinguishable from them, since the existing works like <ref type="bibr" target="#b18">(Ma et al., 2021)</ref> simply inject the memory component into Transformer. In contrast, inspired by the dynamic generation mechanism <ref type="bibr">(Gou et al., 2020)</ref>, the memory in COMET (i.e., the entity representation) is dynamically generated by fully contextualizing the KB and dialogue context via the Memory-masked Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a novel COntext-aware Memory Enhanced Transformer (COMET) for the end-to-end task-oriented dialogue systems. By the designed Memory Mask scheme, COMET can fully contextualize the entity with all its KB and dialogue contexts, and generate the (N + 1)-tuple representations of the entities. The generated entity representations can further augment the framework and lead to better capabilities of response generation and entity linking. The extensive experiments demonstrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Label Construction of Entity Linking</head><p>In practice, the datasets do not provide the golden linked entity. However, We could obtain a pseudo annotation by following <ref type="bibr" target="#b23">(Qin et al., 2019)</ref> to use a distant supervision method. Specifically, we match the entities in the golden response against the entities in the memory M and use the matching result as the golden entity. For entities like "no_traffic", one may find matches in multiple rows. We resolve this ambiguity by choosing the entity from the row which has the most matches for all entities in the utterances.  We follow  to randomly mask a small number of entities into an unknown token to improve the generalization of our model. Besides, in the sketch generation and entity linking stages, we also use the label smoothing to regularize the model. The hyper-parameters such as dropout rate are tuned over the development set by grid search (Entity F1 for both datasets). The model is implemented in PyTorch. The hyper-parameters used in two datasets are shown in Tab. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyper-parameter Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details of Other KB Representations with Transformer</head><p>To further compare the different methods of representing KB with our method, we also adopt the triplet, row-entity, and graph representation to replace our contextualized entity representation, where we keep the other parts of COMET unchanged. Specifically, for the triplet representation, we follow <ref type="bibr" target="#b21">(Madotto et al., 2018;</ref><ref type="bibr" target="#b24">Qin et al., 2020)</ref> to implement Transformer+Triplet, where the entity representation is the sum of the subject, relation, and object. Besides, the multihop reasoning <ref type="bibr" target="#b26">(Sukhbaatar et al., 2015)</ref> is leveraged to further boost the performance. For the row-ent representation, we refer to <ref type="bibr" target="#b12">(Gangi Reddy et al., 2019;</ref><ref type="bibr" target="#b23">Qin et al., 2019)</ref> to implement Trans-former+Row&amp;Ent, where Bag-of-word embedding and entity-type embedding are used for the row-level representation and entity-level representation. Besides, the row-level representation and entity-level representation are hierarchically queried, where the distribution of the entity-level embedding is used for the response generation. For the graph representation, we adopt the memory part of GraphDialog <ref type="bibr" target="#b33">(Yang et al., 2020)</ref> to implement Transformer+Graph, where the entity embedding is further augmented by Graph Neural Networks <ref type="bibr" target="#b28">(Veli?kovi? et al., 2018)</ref>. Besides, the last hop of the triplet and graph representation, and the entitylevel representation of Row&amp;Entity representation will be also used to adaptively fuse the information of KB in the Decoder of COMET. More details can be found in the aforementioned papers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of COMET. The gray block in the top left means Sum. Rep of dialogue history, which is used as the input for the Memory Generation. ? means concatenation. The detailed construction of the Memory Mask can be found inFig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>? KB-Transformer (Triplet) (E. et al., 2019): KB-Transformer combines a Multi-Head Key-Value memory network with Transformer. ? KB-Retriever (Row-entity) (Qin et al., 2019):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>An example in SMD dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>@distance Sketch Response Generation (Subsection 2.4.1)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Entity Linking</cell></row><row><cell></cell><cell>SoftMax</cell><cell cols="2">(Subsection 2.4.2)</cell><cell>6 miles</cell></row><row><cell>Sum. Rep.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Linear</cell><cell></cell><cell></cell></row><row><cell>N</cell><cell>N</cell><cell>K</cell><cell></cell></row><row><cell>Transformer</cell><cell>Transformer</cell><cell cols="2">Transformer</cell></row><row><cell>Encoder</cell><cell>Decoder</cell><cell>Encoder</cell><cell></cell></row><row><cell>Positional</cell><cell></cell><cell></cell><cell></cell><cell>Memory Mask</cell></row><row><cell>Encoding</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>Output</cell><cell>Entity</cell><cell>Type</cell></row><row><cell>Embedding</cell><cell>Embedding</cell><cell>Embedding</cell><cell cols="2">Embedding</cell></row><row><cell>[SUM] where does my friend live ?</cell><cell>[sos] @poi is</cell><cell>? Tom's_house ?</cell><cell cols="2">? poi ?</cell></row><row><cell>Dialogue History Encoding</cell><cell></cell><cell cols="3">Context-aware Memory Generation</cell></row><row><cell>(Section 2.2)</cell><cell></cell><cell cols="2">(Section 2.3)</cell></row><row><cell></cell><cell></cell><cell cols="3">? We propose Context-aware Memory En-</cell></row><row><cell></cell><cell></cell><cell cols="3">hanced Transformer, which incorporates a de-</cell></row><row><cell></cell><cell></cell><cell cols="3">signed Memory Mask to represent entity with</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>BLEU and Entity F1 comparison of COMET with other counterparts. The best results are in bold font and the second-best results are underlined. The results on the SMD and Multi-WOZ 2.1 datasets are adopted from<ref type="bibr" target="#b24">(Qin et al., 2020)</ref> and<ref type="bibr" target="#b33">(Yang et al., 2020)</ref>, respectively.</figDesc><table><row><cell>and Ba, 2014) is used to train our model from</cell></row><row><cell>scratch with a learning rate of 1e ?4 . More details</cell></row><row><cell>about the hyper-parameter settings can be found in</cell></row><row><cell>Appendix A.2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The effects of different components.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>The performance of replacing the contextaware memory with Triplet, Row-Ent and Graph representations in COMET. Note that in the second row, we also report the result of a variant which only considers the KB context and ignores the dialogue context.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>The performance of replacing the Memory Mask with the full attention. The meanings of the scheme names are that the Memory Mask (M) is replaced with the Full attention (F).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters used in the two datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For instance, on the SMD dataset, they only activate the edges between the primary key ("poi") and other keys(e.g., "address") in the Navigation domain, but assign a fullyconnected graph to the Schedule domain.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This module is as same as the standard Transformer Encoder, please refer to<ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">When the memory sequence is long, some existing methods like the linear attention<ref type="bibr" target="#b15">(Kitaev et al., 2020)</ref> can be used to tackle the issue of O(N 2 ) complexity of Self Attention.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For instance, "Tom's house is 6 miles away at 580 Van Ness Ave ." ? "@poi is @distance away at @address.".</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Query: what time do i go to yoga and who is going with me ? Response: yoga is with</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Example</forename><surname>Goal</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>yoga_activity, 11am, thursday, alex. alex] at [11am</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">GLMP: your yoga is on</title>
		<imprint/>
	</monogr>
	<note>thursday] with [alex</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Goal: [stanford_express_care, 6_miles, heavy_traffic, hospital, 214_el_camino_real] Query: car i need a hospital , please send me a quick route to the one near me in the direction we are driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Df-Net</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>your yoga is on [thursday] at [5pm] ? . COMET: your yoga is on [thursday] at [11am] with [alex] . (More informative response. Response: stanford_express_care is on the way. GLMP: stanford_express_care is [2_miles] ? away. (Wrong distance info</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Df-Net</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>stanford_express_care is [4_miles] ? away. (Wrong distance info.) COMET: stanford_express_care is [6_miles] away. Goal: [mountain_view, friday, clear_skies, 30f, 50f] Query: what will be the highest_temperature on friday in mountain_view ? Response: on friday , the highest_temperature in mountain_view is predicted to be [50f</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">GLMP: it will be friday on mountain_view in highest_temperature. ? (Fail to retrieve the temperature info</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Df-Net</surname></persName>
		</author>
		<title level="m">it will be highest_temperature on friday in mountain_view. ? (Fail to retrieve the temperature info.) COMET: on friday the temperature in mountain_view will be low of [30f</title>
		<imprint/>
	</monogr>
	<note>and high of [50f</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Goal: [teavana, 5_miles, moderate_traffic, coffee_or_tea_place, 145_amherst_st] Query: get me directions to the nearest starbucks</title>
		<imprint/>
	</monogr>
	<note>Response: there is NO starbucks locally. GLMP: there is a starbucks [1_miles] ? away. (Unfaithful response</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MultiWOZ -a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ga?i?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1547</idno>
	</analytic>
	<monogr>
		<title level="m">DF-Net: the nearest starbucks is teavana , it s [1_miles] ? away. (Not fluent and wrong entities retrieved.) COMET: there is NO starbucks nearby</title>
		<meeting><address><addrLine>Brussels; Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kbtransformer: Incorporating knowledge into end-toend task-oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Conference on Semantics, Knowledge and Grids</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="44" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5506</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbr?cken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="468" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-level memory for task oriented dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Revanth Gangi Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachindra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1375</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3754" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pingping Zhang, and Xi Peng. 2020. A dynamic parameter enhanced network for distant supervised relation extraction. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2020.105912</idno>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page">105912</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Task-oriented conversation generation using heterogeneous memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1463</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Streaming simultaneous speech translation with augmented memory transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Javad</forename><surname>Dousti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7523" to="7527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning knowledge bases with parameters for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13656</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12579</idno>
		<title level="m">Yejin Bang, and Pascale Fung. 2020b. The adapter-bot: All-in-one controllable conversational model</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mem2Seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1468" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint l 2, 1 -norms minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1813" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Entity-consistent end-to-end task-oriented dialogue system with KB retriever</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic fusion network for multidomain end-to-end task-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6344" to="6354" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems</title>
		<meeting>the International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems</title>
		<meeting>the International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning for task-oriented dialogue with dialogue state representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3781" to="3792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrk?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Ga?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Partially observable markov decision processes for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Global-to-local memory pointer networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GraphDialog: Integrating graph knowledge into endto-end task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiquan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1878" to="1888" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ga?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
