<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
							<email>yifuzhang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename><surname>Xinggang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>FairMOT ? Multi-Object Tracking ? One-Shot ? Anchor-Free ? Real-Time Inference</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and re-ID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat re-ID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the re-ID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchorfree object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-ofthe-art methods by a large margin on several public datasets. have contributed equally.</p><p>The source code and pre-trained models are released at https: //github.com/ifzhang/FairMOT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-Object Tracking (MOT) has been a longstanding goal in computer vision <ref type="bibr" target="#b5">(Bewley et al., 2016;</ref><ref type="bibr" target="#b82">Wojke et al., 2017;</ref><ref type="bibr" target="#b12">Chen et al., 2018a;</ref><ref type="bibr" target="#b88">Yu et al., 2016)</ref>. The goal is to estimate trajectories for objects of interest presented in videos. The successful resolution of the problem can immediately benefit many applications such as intelligent video analysis, human computer interaction, human activity recognition <ref type="bibr" target="#b78">(Wang et al., 2013;</ref><ref type="bibr" target="#b50">Luo et al., 2017)</ref>, and even social computing.</p><p>Most of the existing methods such as <ref type="bibr" target="#b52">(Mahmoudi et al., 2019;</ref><ref type="bibr" target="#b101">Zhou et al., 2018;</ref><ref type="bibr" target="#b22">Fang et al., 2018;</ref><ref type="bibr" target="#b5">Bewley et al., 2016;</ref><ref type="bibr" target="#b82">Wojke et al., 2017;</ref><ref type="bibr" target="#b12">Chen et al., 2018a;</ref><ref type="bibr" target="#b88">Yu et al., 2016)</ref> attempt to address the problem by two separate models: the detection model firstly detects objects of interest by bounding boxes in each frame, then the association model extracts re-identification (re-ID) features from the image regions corresponding to each bounding box, links the detection to one of the existing tracks or creates a new track according to certain metrics defined on features.</p><p>There has been remarkable progress on object detection <ref type="bibr" target="#b61">(Ren et al., 2015;</ref><ref type="bibr" target="#b30">He et al., 2017;</ref><ref type="bibr" target="#b98">Zhou et al., 2019a;</ref><ref type="bibr" target="#b60">Redmon and Farhadi, 2018;</ref><ref type="bibr" target="#b25">Fu et al., 2020;</ref><ref type="bibr">Sun et al., 2021b,a)</ref> and re-ID <ref type="bibr" target="#b96">(Zheng et al., 2017a;</ref><ref type="bibr" target="#b12">Chen et al., 2018a)</ref> respectively recently which in turn boosts the overall tracking accuracy. However, these two-step methods suffer from scalability issues. They cannot achieve real-time inference speed when there are a large number of objects in the environment because the two models do not share features and they need arXiv:2004.01888v6 [cs.CV] 19 Oct 2021 to apply the re-ID models for every bounding box independently in the video.</p><p>With the maturity of multi-task learning <ref type="bibr" target="#b40">(Kokkinos, 2017;</ref><ref type="bibr" target="#b13">Chen et al., 2018b)</ref>, one-shot trackers which estimate objects and learn re-ID features using a single network have attracted more attention <ref type="bibr" target="#b80">(Wang et al., 2020b;</ref><ref type="bibr">Voigtlaender et al., 2019)</ref>. For example, Voigtlaender et al. <ref type="bibr">(Voigtlaender et al., 2019)</ref> add a re-ID branch to Mask R-CNN to extract a re-ID feature for each proposal <ref type="bibr" target="#b30">(He et al., 2017)</ref>. It reduces inference time by re-using backbone features for the re-ID network. But the performance drops remarkably compared to the two-step models. In fact, the detection accuracy is still good but the tracking performance drops a lot. For example, the number of ID switches increases by a large margin. The result suggests that combining the two tasks is a non-trivial task and should be treated carefully.</p><p>In this paper, we investigate the reasons behind the failure, and present a simple yet effective solution. Three factors are identified to account for the failure. The first issue is caused by anchors. Anchors are originally designed for object detection <ref type="bibr" target="#b61">(Ren et al., 2015)</ref>. However, we show that anchors are not suitable for extracting re-ID features for two reasons. First, anchor-based one-shot trackers such as Track R- <ref type="bibr">CNN (Voigtlaender et al., 2019)</ref> overlook the re-ID task because they need anchors to first detect objects (i.e. , using RPN <ref type="bibr" target="#b61">(Ren et al., 2015)</ref>) and then extract the re-ID features based on the detection results (re-ID features are useless when detection results are incorrect). So when competition occurs between the two tasks, it will favor the detection task. Anchors also introduce a lot of ambiguity during training the re-ID features because one anchor may correspond to multiple identities and multiple anchors may correspond to one identity, especially in crowded scenes.</p><p>The second issue is caused by feature sharing between the two tasks. Detection task and re-ID task are two totally different tasks and they need different features. In general, re-ID features need more low-level features to discriminate different instances of the same class while detection features need to be similar for different instances. The shared features in one-shot trackers will lead to feature conflict and thus reduce the performance of each task.</p><p>The third issue is caused by feature dimension. The dimension of re-ID features is usually as high as 512 <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref> or 1024 <ref type="bibr" target="#b96">(Zheng et al., 2017a)</ref> which is much higher than that of object detection. We find that the huge difference between dimensions will harm the performance of the two tasks. More importantly, our experiments suggest that it is a generic rule that learning low-dimensional re-ID features for "joint detection and re-ID" networks achieves both higher tracking accuracy and efficiency. This also reveals the difference between the MOT task and the re-ID task, which is overlooked in the field of MOT.</p><p>In this work, we present a simple approach termed as FairMOT which elegantly address the three issues as illustrated in <ref type="figure">Figure 1</ref>. FairMOT is built on top of CenterNet <ref type="bibr" target="#b98">(Zhou et al., 2019a)</ref>. In particular, the detection and re-ID tasks are treated equally in FairMOT which essentially differs from the previous "detection first, re-ID secondary" framework. It is worth noting that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. <ref type="figure">Figure 1</ref> shows an overview of FairMOT. It has a simple network structure which consists of two homogeneous branches for detecting objects and extracting re-ID features, respectively. Inspired by <ref type="bibr" target="#b98">(Zhou et al., 2019a;</ref><ref type="bibr" target="#b42">Law and Deng, 2018;</ref><ref type="bibr" target="#b99">Zhou et al., 2019b;</ref><ref type="bibr" target="#b20">Duan et al., 2019)</ref>, the detection branch is implemented in an anchor-free style which estimates object centers and sizes represented as position-aware measurement maps. Similarly, the re-ID branch estimates a re-ID feature for each pixel to characterize the object centered at the pixel. Note that the two branches are completely homogeneous which essentially differs from the previous methods which perform detection and re-ID in a two-stage cascaded style. So FairMOT eliminates the unfair disadvantage of the detection branch as reflected in <ref type="table">Table 1</ref>, effectively learns high-quality re-ID features and obtains a good trade-off between detection and re-ID.</p><p>We evaluate FairMOT on the MOT Challenge benchmark via the evaluation server. It ranks first among all trackers on the 2DMOT15 <ref type="bibr" target="#b43">(Leal-Taix? et al., 2015)</ref>, MOT16 <ref type="bibr" target="#b54">(Milan et al., 2016)</ref>, MOT17 <ref type="bibr" target="#b54">(Milan et al., 2016)</ref> and MOT20 <ref type="bibr" target="#b17">(Dendorfer et al., 2020)</ref> datasets. When we further pre-train our model using our proposed single image training method, it achieves additional gains on all datasets. In spite of the strong results, the approach is very simple and runs at 30 FPS on a single RTX 2080Ti GPU. It sheds light on the relationship between detection and re-ID in MOT and provides guidance for designing one-shot video tracking networks.</p><p>Our contributions are as follows:</p><p>-We empirically demonstrate that the prevalent anchorbased one-shot MOT architectures have limitations in terms of learning effective re-ID features which has been overlooked.  <ref type="figure">Fig. 1</ref> Overview of our one-shot tracker FairMOT. The input image is first fed to an encoder-decoder network to extract high resolution feature maps (stride=4). Then we add two homogeneous branches for detecting objects and extracting re-ID features, respectively. The features at the predicted object centers are used for tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The best-performing MOT methods <ref type="bibr" target="#b3">(Bergmann et al., 2019;</ref><ref type="bibr" target="#b8">Bras? and Leal-Taix?, 2020;</ref><ref type="bibr" target="#b34">Hornakova et al., 2020;</ref><ref type="bibr" target="#b88">Yu et al., 2016;</ref><ref type="bibr" target="#b52">Mahmoudi et al., 2019;</ref><ref type="bibr" target="#b101">Zhou et al., 2018;</ref><ref type="bibr" target="#b82">Wojke et al., 2017;</ref><ref type="bibr" target="#b12">Chen et al., 2018a;</ref><ref type="bibr" target="#b80">Wang et al., 2020b;</ref><ref type="bibr">Voigtlaender et al., 2019;</ref><ref type="bibr" target="#b94">Zhang et al., 2021a)</ref> usually follow the trackingby-detection paradigm, which first detect objects in each frame and then associate them over time. We classify the existing works into two categories based on whether they use a single model or separate models to detect objects and extract association features. We discuss the pros and cons of the methods and compare them to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Detection and Tracking by Separate Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Detection Methods</head><p>Most benchmark datasets such as MOT17 <ref type="bibr" target="#b54">(Milan et al., 2016)</ref> provide detection results obtained by popular methods such as DPM <ref type="bibr" target="#b24">(Felzenszwalb et al., 2008)</ref>, Faster R-CNN <ref type="bibr" target="#b61">(Ren et al., 2015)</ref> and SDP <ref type="bibr" target="#b86">(Yang et al., 2016)</ref> such that the works that focus on the tracking part can be fairly compared on the same object detections. Some works such as <ref type="bibr" target="#b88">(Yu et al., 2016;</ref><ref type="bibr" target="#b82">Wojke et al., 2017;</ref><ref type="bibr" target="#b101">Zhou et al., 2018;</ref><ref type="bibr" target="#b52">Mahmoudi et al., 2019)</ref> use a large private pedestrian detection dataset to train the Faster R-CNN detector with VGG-16 (Simonyan and Zisserman, 2014) as backbone, which obtain better detection performance. A small number of works such as <ref type="bibr" target="#b27">(Han et al., 2020)</ref> use more powerful detectors which are developed recently such as Cascade R-CNN <ref type="bibr" target="#b9">(Cai and Vasconcelos, 2018)</ref> to boost the detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Tracking Methods</head><p>Most of the existing works focus on the tracking part of the problem. We classify them into two classes according to the type of cues used for association.</p><p>Location and Motion Cues based Methods SORT <ref type="bibr" target="#b5">(Bewley et al., 2016)</ref> first uses Kalman Filter <ref type="bibr" target="#b35">(Kalman, 1960)</ref> to predict future locations of the tracklets, computes their overlap with the detections, and uses Hungarian algorithm <ref type="bibr" target="#b41">(Kuhn, 1955)</ref> to assign detections to tracklets. IOU-Tracker <ref type="bibr" target="#b6">(Bochinski et al., 2017)</ref> directly computes the overlap between the tracklets (of the previous frame) and the detections without using using Kalman filter to predict future locations. The approach achieves 100K fps inference speed (detection time not counted) and works well when object motion is small. Both SORT and IOU-Tracker are widely used in practice due to their simplicity. However, they may fail in challenging cases of crowded scenes and fast motion. Some works such as <ref type="bibr" target="#b83">(Xiang et al., 2015;</ref><ref type="bibr" target="#b102">Zhu et al., 2018;</ref> leverage sophisticated single object tracking methods to get accurate object locations and reduce false negatives. However, these methods are extremely slow especially when there are a large number of people in the scene. To solve the problem of trajectory fragments, <ref type="bibr" target="#b93">Zhang et al. (Zhang et al., 2020)</ref> propose a motion evaluation network to learn longrange features of tracklets for association. MAT <ref type="bibr" target="#b27">(Han et al., 2020)</ref> is an enhanced SORT, which additionally models the camera motion and uses dynamic windows for long-range re-association.</p><p>Appearance Cues based Methods Some recent works <ref type="bibr" target="#b88">(Yu et al., 2016;</ref><ref type="bibr" target="#b52">Mahmoudi et al., 2019;</ref><ref type="bibr" target="#b101">Zhou et al., 2018;</ref><ref type="bibr" target="#b82">Wojke et al., 2017)</ref> propose to crop the image regions of the detections and feed them to re-ID networks <ref type="bibr" target="#b97">(Zheng et al., 2017b;</ref><ref type="bibr" target="#b33">Hermans et al., 2017;</ref><ref type="bibr" target="#b50">Luo et al., 2019a)</ref> to extract image features. Then they compute the similarity between tracklets and detections based on re-ID features and use Hungarian algorithm <ref type="bibr" target="#b41">(Kuhn, 1955)</ref> to accomplish assignment. The method is robust to fast motion and occlusion. In particular, it can re-initialize lost tracks because appearance features are relatively stable over time.</p><p>There are also some works <ref type="bibr" target="#b0">(Bae and Yoon, 2014;</ref><ref type="bibr" target="#b74">Tang et al., 2017;</ref><ref type="bibr" target="#b64">Sadeghian et al., 2017;</ref><ref type="bibr" target="#b12">Chen et al., 2018a;</ref><ref type="bibr" target="#b85">Xu et al., 2019)</ref> focusing on enhancing appearance features. For example, Bae et al. <ref type="bibr" target="#b0">(Bae and Yoon, 2014)</ref> propose an online appearance learning method to handle appearance variations. <ref type="bibr" target="#b74">Tang et al. (Tang et al., 2017)</ref> leverage body pose features to enhance the appearance features. Some methods <ref type="bibr" target="#b64">(Sadeghian et al., 2017;</ref><ref type="bibr" target="#b85">Xu et al., 2019;</ref><ref type="bibr" target="#b67">Shan et al., 2020)</ref> propose to fuse multiple cues (i.e. motion, appearance and location) to get more reliable similarity. MOTDT <ref type="bibr" target="#b12">(Chen et al., 2018a)</ref> proposes a hierarchical data association strategy which uses IoU to associate objects when appearance features are not reliable. A small number of works such as <ref type="bibr" target="#b52">(Mahmoudi et al., 2019;</ref><ref type="bibr" target="#b101">Zhou et al., 2018;</ref><ref type="bibr" target="#b22">Fang et al., 2018)</ref> also propose to use more complicated association strategies such as group models and RNNs.</p><p>Offline Methods The offline methods (or batch methods) <ref type="bibr" target="#b91">(Zhang et al., 2008;</ref><ref type="bibr" target="#b81">Wen et al., 2014;</ref><ref type="bibr" target="#b2">Berclaz et al., 2011;</ref><ref type="bibr" target="#b90">Zamir et al., 2012;</ref><ref type="bibr" target="#b53">Milan et al., 2013;</ref><ref type="bibr">Choi, 2015;</ref><ref type="bibr" target="#b8">Bras? and Leal-Taix?, 2020;</ref><ref type="bibr" target="#b34">Hornakova et al., 2020</ref>) often achieve better results by performing global optimization in the whole sequence. For example, Zhang et al. <ref type="bibr" target="#b91">(Zhang et al., 2008)</ref> build a graphical model with nodes representing detections in all frames. The optimal assignment is searched using a min-cost flow algorithm, which exploits the specific structure of the graph to reach the optimum faster than Linear Programming. <ref type="bibr" target="#b2">Berclaz et al. (Berclaz et al., 2011)</ref> also treat data association as a flow optimization task and use the Kshortest paths algorithm to solve it, which significantly speeds up computation and reduces parameters that need to be tuned. <ref type="bibr" target="#b53">Milan et al. (Milan et al., 2013)</ref> formulate multi-object tracking as minimization of a continuous energy and focus on designing the energy function. The energy depends on locations and motion of all targets in all frames as well as physical constraints. MPNTrack <ref type="bibr" target="#b8">(Bras? and Leal-Taix?, 2020)</ref> proposes trainable graph neural networks to perform a global association of the entire set of detections and make MOT fully differentiable. Lif T <ref type="bibr" target="#b34">(Hornakova et al., 2020)</ref> formulates MOT as a lifted disjoint path problem and introduces lifted edges for long range temporal interactions, which significantly reduces id switches and re-identify lost.</p><p>Advantages and Limitations For the methods which perform detection and tracking by separate models, the main advantage is that they can develop the most suitable model for each task separately without making compromise. In addition, they can crop the image patches according to the detected bounding boxes and resize them to the same size before estimating re-ID features. This helps to handle the scale variations of objects. As a result, these approaches <ref type="bibr" target="#b88">(Yu et al., 2016;</ref><ref type="bibr" target="#b32">Henschel et al., 2019)</ref> have achieved the best performance on the public datasets. However, they are usually very slow because the two tasks need to be done separately without sharing. So it is hard to achieve video rate inference which is required in many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detection and Tracking by a Single Model</head><p>With the quick maturity of multi-task learning <ref type="bibr" target="#b40">(Kokkinos, 2017;</ref><ref type="bibr" target="#b59">Ranjan et al., 2017;</ref><ref type="bibr" target="#b66">Sener and Koltun, 2018)</ref> in deep learning, joint detection and tracking using a single network has begun to attract more research attention. We classify them into two classes as discussed in the following.</p><p>Joint Detection and Re-ID The first class of methods <ref type="bibr">(Voigtlaender et al., 2019;</ref><ref type="bibr" target="#b80">Wang et al., 2020b;</ref><ref type="bibr" target="#b44">Liang et al., 2020;</ref><ref type="bibr" target="#b56">Pang et al., 2021;</ref><ref type="bibr" target="#b49">Lu et al., 2020)</ref> perform object detection and re-ID feature extraction in a single network in order to reduce inference time. For example, Track-RCNN (Voigtlaender et al., 2019) adds a re-ID head on top of Mask R-CNN <ref type="bibr" target="#b30">(He et al., 2017)</ref> and regresses a bounding box and a re-ID feature for each proposal. Similarly, JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref> is built on top of YOLOv3 <ref type="bibr" target="#b60">(Redmon and Farhadi, 2018)</ref> which achieves near video rate inference. However, the accuracy of these one-shot trackers is usually lower than that of the two-step ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Detection and Motion Prediction</head><p>The second class of methods <ref type="bibr" target="#b23">(Feichtenhofer et al., 2017;</ref><ref type="bibr" target="#b100">Zhou et al., 2020;</ref><ref type="bibr" target="#b55">Pang et al., 2020;</ref><ref type="bibr" target="#b57">Peng et al., 2020;</ref><ref type="bibr" target="#b70">Sun et al., 2020)</ref> learn detection and motion features in a single network. D&amp;T <ref type="bibr" target="#b23">(Feichtenhofer et al., 2017)</ref> propose a Siamese network which takes input of adjacent frames and predicts inter-frame displacements between bounding boxes. Tracktor <ref type="bibr" target="#b3">(Bergmann et al., 2019)</ref> directly exploits the bounding box regression head to propagate identities of region proposals and thus removes box association. Chained-Tracker <ref type="bibr" target="#b57">(Peng et al., 2020)</ref> proposes an end-to-end model using adjacent frame pair as input and generating the box pair representing the same target. These box-based methods assume that bounding boxes have a large overlap between frames, which is not true in lowframe rate videos. Different from these methods, Center-Track  predicts the object center displacements with pair-wise inputs and associate by these point distances. It also provides the tracklets as an additional point-based heatmap input to the network and is then able to match objects anywhere even if the boxes have no overlap at all. However, these methods only associate objects in adjacent frames without re-initializing lost tracks and thus have difficulty handling occlusion cases.</p><p>Our work belongs to the first class. We investigate the reasons why one-shot trackers get degraded association performance and propose a simple approach to address the problems. We show that the tracking accuracy is improved significantly without heavy engineering efforts. A concurrent work CSTrack  also aims to alleviate the conflicts between the two tasks from the perspective of features, and propose a cross-correlation network module to enable the model to learn task-dependent representations. Different from CSTrack, our method tries to address the problem from three perspectives in a systematic way and obtains notably better performances than CSTrack. Center-Track  is also related to our work since it also uses center-based object detection framework. But CenterTrack does not extract appearance features and only links objects in adjacent frames. In contrast, FairMOT can perform long-range association with the appearance features and handle occlusion cases.</p><p>Multi-task Learning There is a large body of literature <ref type="bibr" target="#b38">Kendall et al., 2018;</ref><ref type="bibr" target="#b13">Chen et al., 2018b;</ref><ref type="bibr" target="#b26">Guo et al., 2018;</ref><ref type="bibr" target="#b66">Sener and Koltun, 2018)</ref> on multi-task learning which may be used to balance the object detection and re-ID feature extraction tasks. Uncertainty <ref type="bibr" target="#b38">(Kendall et al., 2018)</ref> uses task-dependent uncertainty to automatically balance the single-task losses. MGDA is proposed in <ref type="bibr" target="#b66">(Sener and Koltun, 2018)</ref> to update the shared network weights by finding a common direction among the task-specific gradients. GradNorm <ref type="bibr" target="#b13">(Chen et al., 2018b)</ref> controls the training of multi-task networks by simulating the task-specific gradients to be of similar magnitude. We evaluate these methods in the experimental sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Video Object Detection</head><p>Video Object Detection (VOD) <ref type="bibr" target="#b23">(Feichtenhofer et al., 2017;</ref><ref type="bibr" target="#b51">Luo et al., 2019b</ref>) is related to MOT in the sense that it leverages tracking to improve object detection performances in challenging frames. Although these methods were not evaluated on MOT datasets, some of the ideas may be valuable for the field. So we briefly review them in this section. <ref type="bibr">Tang et al. (Tang et al., 2019)</ref> detect object tubes in videos which aims to enhance classification scores in challenging frames based on their neighboring frames. The detection rate for small objects increases by a large margin on the benchmark dataset. Similar ideas have also been explored in <ref type="bibr" target="#b28">(Han et al., 2016;</ref><ref type="bibr" target="#b36">Kang et al., 2016</ref><ref type="bibr" target="#b37">Kang et al., , 2017</ref><ref type="bibr">Tang et al., 2019;</ref><ref type="bibr" target="#b55">Pang et al., 2020)</ref>. One main limitation of these tube-based methods is that they are extremely slow especially when there are a large number of objects in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unfairness Issues in One-shot Trackers</head><p>In this section, we discuss three unfairness issues that arise in the existing one-shot trackers which usually lead to degraded tracking performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unfairness Caused by Anchors</head><p>The existing one-shot trackers such as Track R-CNN <ref type="bibr">(Voigtlaender et al., 2019)</ref> and JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref> are mostly anchor-based since they are directly modified from anchorbased object detectors such as YOLO <ref type="bibr" target="#b60">(Redmon and Farhadi, 2018)</ref> and Mask R-CNN <ref type="bibr" target="#b30">(He et al., 2017)</ref>. However, we find that the anchor-based design is not suitable for learning re-ID features which result in a large number of ID switches in spite of the good detection results. We explain the problem from three perspectives in the following.</p><p>Overlooked re-ID task Track R-CNN (Voigtlaender et al., 2019) operates in a cascaded style which first estimates object proposals (boxes) and then pools features from them to estimate the corresponding re-ID features. The quality of re-ID features heavily depends on the quality of proposals during training (re-ID features are useless if proposals are not accurate). As a result, in the training stage, the model is seriously biased to estimate accurate object proposals rather than high quality re-ID features. So the standard "detection first, re-ID secondary" design of the existing one-shot trackers makes the re-ID network not fairly learned.</p><p>One anchor corresponds to multiple identities The anchorbased methods usually use ROI-Align to extract features from proposals. Most sampling locations in ROI-Align may belong to other disturbing instances or background as shown in <ref type="figure">Figure 2</ref>. As a result, the extracted features are not optimal in terms of accurately and discriminatively representing the target objects. Instead, we find in this work that it is significantly better to only extract features at a single point, i.e. , the estimated object centers.</p><p>Multiple anchors correspond to one identity In both <ref type="bibr">(Voigtlaender et al., 2019)</ref> and <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>, multiple adjacent anchors, which correspond to different image patches, may be forced to estimate the same identity as long as their IOU is sufficiently large. This introduces severe ambiguity for training. See <ref type="figure">Figure 2</ref> for illustration. On the other hand, when an image undergoes small perturbation, e.g., due to data augmentation, it is possible that the same anchor is forced to estimate different identities. In addition, feature maps in object detection are usually down-sampled by 8/16/32 times to balance accuracy and speed. This is acceptable for object detection but it is too coarse for learning re-ID features because features extracted at coarse anchors may not be aligned with object centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unfairness Caused by Features</head><p>For one-shot trackers, most features are shared between the object detection and re-ID tasks. But it is well known that they actually require features from different layers to achieve the best results. In particular, object detection requires deep features to estimate object classes and positions but re-ID requires low-level appearance features to distinguish different instances of the same class. From the perspective of the multi-task loss optimization, the optimization objectives of detection and re-ID have conflicts. Thus, it is important to balance the loss optimization strategy of the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unfairness Caused by Feature Dimension</head><p>The previous re-ID works usually learn very high dimensional features and have achieved promising results on the benchmarks of their field. However, we find that learning lower-dimensional features is actually better for one-shot MOT for three reasons: (1) high-dimensional re-ID features notably harms the object detection accuracy due to the competition of the two tasks which in turn also has negative impact to the final tracking accuracy. So considering that the feature dimension in object detection is usually very low (class numbers + box locations), we propose to learn lowdimensional re-ID features to balance the two tasks;</p><p>(2) the MOT task is different from the re-ID task. The MOT task only performs a small number of one-to-one matchings between two consecutive frames. The re-ID task needs to match the query to a large number of candidates and thus requires more discriminative and high-dimensional re-ID features. So in MOT we do not need that high-dimensional features;</p><p>(3) learning low dimensional re-ID features improves the inference speed as will be shown in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FairMOT</head><p>In this section, we present the technical details of FairMOT including the backbone network, the object detection branch, the re-ID branch as well as training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Backbone Network</head><p>We adopt ResNet-34 as backbone in order to strike a good balance between accuracy and speed. An enhanced version of Deep Layer Aggregation (DLA) <ref type="bibr" target="#b98">(Zhou et al., 2019a</ref>) is applied to the backbone to fuse multi-layer features as shown in <ref type="figure">Figure 1</ref>. Different from original DLA , it has more skip connections between low-level and high-level features which is similar to the Feature Pyramid Network (FPN) <ref type="bibr" target="#b46">(Lin et al., 2017a)</ref>. In addition, convolution layers in all up-sampling modules are replaced by deformable convolution such that they can dynamically adjust the receptive field according to object scales and poses. These modifications are also helpful to alleviate the alignment issue.  , can be used in our framework to provide fair features for both detection and re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detection Branch</head><p>Our detection branch is built on top of CenterNet <ref type="bibr" target="#b98">(Zhou et al., 2019a)</ref> but other anchor-free methods such as <ref type="bibr" target="#b20">(Duan et al., 2019;</ref><ref type="bibr" target="#b42">Law and Deng, 2018;</ref><ref type="bibr" target="#b19">Dong et al., 2020;</ref><ref type="bibr" target="#b87">Yang et al., 2019)</ref> can also be used. We briefly describe the approach to make this work self-contained. In particular, three parallel heads are appended to DLA-34 to estimate heatmaps, object center offsets and bounding box sizes, respectively. Each head is implemented by applying a 3 ? 3 convolution (with 256 channels) to the output features of DLA-34, followed by a 1 ? 1 convolutional layer which generates the final targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Heatmap Head</head><p>This head is responsible for estimating the locations of the object centers. The heatmap based representation, which is the de facto standard for the landmark point estimation task, is adopted here. In particular, the dimension of the heatmap is 1 ? H ? W . The response at a location in the heatmap is expected to be one if it collapses with the ground-truth object center. The response decays exponentially as the distance between the heatmap location and the object center. ). Then the heatmap response at the location (x, y) is com-</p><formula xml:id="formula_0">For each GT box b i = (x i 1 , y i 1 , x i 2 , y i 2 ) in the image,</formula><formula xml:id="formula_1">puted as M xy = N i=1 exp ? (x? c i x ) 2 +(y? c i y ) 2 2? 2 c</formula><p>where N represents the number of objects in the image and ? c represents the standard deviation. The loss function is defined as pixelwise logistic regression with focal loss <ref type="bibr" target="#b47">(Lin et al., 2017b)</ref>:</p><formula xml:id="formula_2">L heat = ? 1 N xy (1 ?M xy ) ? log(M xy ), M xy = 1; (1 ? M xy ) ? (M xy ) ? log(1 ?M xy ) otherwise,<label>(1)</label></formula><p>whereM is the estimated heatmap, and ?, ? are the predetermined parameters in focal loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Box Offset and Size Heads</head><p>The box offset head aims to localize objects more precisely.</p><p>Since the stride of the final feature map is four, it will introduce quantization errors up to four pixels. This branch estimates a continuous offset relative to the object center for each pixel in order to mitigate the impact of down-sampling. The box size head is responsible for estimating height and width of the target box at each location.</p><p>Denote the output of the size and offset heads as?</p><formula xml:id="formula_3">? R 2?H?W and? ? R 2?H?W , respectively. For each GT box b i = (x i 1 , y i 1 , x i 2 , y i 2 ) in the image, we compute its size as s i = (x i 2 ? x i 1 , y i 2 ? y i 1 ). Similarly, the GT offset is com- puted as o i = ( c i x 4 , c i y 4 )?( c i x 4 , c i y 4 )</formula><p>. Denote the estimated size and offset at the corresponding location as? i and? i , respectively. Then we enforce l 1 losses for the two heads:</p><formula xml:id="formula_4">L box = N i=1 o i ?? i 1 + ? s s i ?? i 1 .<label>(2)</label></formula><p>where ? s is a weighting parameter and is set 0.1 as the original CenterNet <ref type="bibr" target="#b98">(Zhou et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Re-ID Branch</head><p>Re-ID branch aims to generate features that can distinguish objects. Ideally, affinity among different objects should be smaller than that between same objects. To achieve this goal, we apply a convolution layer with 128 kernels on top of backbone features to extract re-ID features for each location.</p><p>Denote the resulting feature map as E ? R 128?H?W . The re-ID feature E x,y ? R 128 of an object centered at (x, y) can be extracted from the feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Re-ID Loss</head><p>We learn re-ID features through a classification task. All object instances of the same identity in the training set are treated as the same class. For each GT box b i = (x i 1 , y i 1 , x i 2 , y i 2 ) in the image, we obtain the object center on the heatmap ( c i x , c i y ). We extract the re-ID feature vector E c i x , c i y and use a fully connected layer and a softmax operation to map it to a class distribution vector P = {p(k), k ? [1, K]}. Denote the one-hot representation of the GT class label as L i (k). Then we compute the re-ID loss as:</p><formula xml:id="formula_5">L identity = ? N i=1 K k=1 L i (k)log(p(k)),<label>(3)</label></formula><p>where K is the number of all the identities in the training data. During the training process of our network, only the identity embedding vectors located at object centers are used for training, since we can obtain object centers from the objectness heatmap in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training FairMOT</head><p>We jointly train the detection and re-ID branches by adding the losses (i.e., Eq.</p><p>(1), Eq.</p><p>(2) and Eq. <ref type="formula" target="#formula_5">(3)</ref>) together. In particular, we use the uncertainty loss proposed in <ref type="bibr" target="#b38">(Kendall et al., 2018)</ref> to automatically balance the detection and re-ID tasks:</p><formula xml:id="formula_6">L detection = L heat + L box ,<label>(4)</label></formula><formula xml:id="formula_7">L total = 1 2 ( 1 e w1 L detection + 1 e w2 L identity + w 1 + w 2 ),<label>(5)</label></formula><p>where w 1 and w 2 are learnable parameters that balance the two tasks. Specifically, given an image with a few objects and their corresponding IDs, we generate heatmaps, box offset and size maps as well as one-hot class representation of the objects. These are compared to the estimated measures to obtain losses to train the whole network. In addition to the standard training strategy presented above, we propose a single image training method to train FairMOT on image-level object detection datasets such as COCO <ref type="bibr" target="#b45">(Lin et al., 2014)</ref> and CrowdHuman <ref type="bibr" target="#b68">(Shao et al., 2018)</ref>. Different from CenterTrack ) that takes two simulated consecutive frames as input, we only take a single image as input. We assign each bounding box a unique identity and thus regard each object instance in the dataset as a separate class. We apply different transformations to the whole image including HSV augmentation, rotation, scaling, translation and shearing. The single image training method has significant empirical values. First, the pre-trained model on the CrowdHuman dataset can be directly used as a tracker and get acceptable results on MOT datasets such as MOT17 <ref type="bibr" target="#b54">(Milan et al., 2016)</ref>. This is because the CrowdHuman dataset can boost the human detection performance and also has strong domain generalization ability. Our training of the re-ID features further enhances the association ability of the tracker. Second, we can finetune it on other MOT datasets and further improve the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Online Inference</head><p>In this section, we present how we perform online inference, and in particular, how we perform association with the detections and re-ID features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Network Inference</head><p>The network takes a frame of size 1088?608 as input which is the same as the previous work JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>. On top of the predicted heatmap, we perform non-maximum suppression (NMS) based on the heatmap scores to extract the peak keypoints. The NMS is implemented by a simple 3 ? 3 max pooling operation as in <ref type="bibr" target="#b98">(Zhou et al., 2019a)</ref>. We keep the locations of the keypoints whose heatmap scores are larger than a threshold. Then, we compute the corresponding bounding boxes based on the estimated offsets and box sizes. We also extract the identity embeddings at the estimated object centers. In the next section, we discuss how we associate the detected boxes over time using the re-ID features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Online Association</head><p>We follow MOTDT <ref type="bibr" target="#b12">(Chen et al., 2018a)</ref> and use a hierarchical online data association method. We first initialize a number of tracklets based on the detected boxes in the first frame. Then in the subsequent frame, we link the detected boxes to the existing tracklets using a two-stage matching strategy. In the first stage, we use Kalman Filter <ref type="bibr" target="#b35">(Kalman, 1960</ref>) and re-ID features to obtain initial tracking results. In particular, we use Kalman Filter to predict tracklet locations in the following frame and compute the Mahalanobis distance D m between the predicted and detected boxes following Deep-SORT <ref type="bibr" target="#b82">(Wojke et al., 2017)</ref>. We fuse the Mahalanobis distance with the cosine distance computed on re-ID features: D = ?D r + (1 ? ?)D m where ? is a weighting parameter and is set to be 0.98 in our experiments. Following JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>, we set Mahalanobis distance to infinity if it is larger than a threshold to avoid getting trajectories with large motion. We use Hungarian algorithm <ref type="bibr" target="#b41">(Kuhn, 1955)</ref> with a matching threshold ? 1 = 0.4 to complete the first stage matching.</p><p>In the second stage, for unmatched detections and tracklets, we try to match them according to the overlap between their boxes. In particular, we set the matching threshold ? 2 = 0.5. We update the appearance features of the tracklets in each time step to handle appearance variations as in <ref type="bibr" target="#b7">(Bolme et al., 2010;</ref><ref type="bibr" target="#b31">Henriques et al., 2014)</ref>. Finally, we initialize the unmatched detections as new tracks and save the unmatched tracklets for 30 frames in case they reappear in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Metrics</head><p>There are six training datasets briefly introduced as follows: the ETH <ref type="bibr" target="#b21">(Ess et al., 2008)</ref> and CityPerson <ref type="bibr" target="#b92">(Zhang et al., 2017)</ref> datasets only provide box annotations so we only train the detection branch on them. The CalTech <ref type="bibr" target="#b18">(Doll?r et al., 2009</ref>), MOT17 <ref type="bibr" target="#b54">(Milan et al., 2016)</ref>, CUHK-SYSU  and PRW <ref type="bibr" target="#b96">(Zheng et al., 2017a)</ref> datasets provide both box and identity annotations which allows us to train both branches. Some videos in ETH also appear in the testing set of the MOT17 which are removed from the training dataset for fair comparison. The overall training strategy is described in Section 4.4, which is the same as <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>. For the self-supervised training of our method, we use the CrowdHuman dataset <ref type="bibr" target="#b68">(Shao et al., 2018)</ref> which only contains object bounding box annotations.</p><p>We evaluate our approach on the testing sets of four benchmarks: 2DMOT15, MOT16, MOT17 and MOT20. We use Average Precision (AP) to evaluate detection results. Following <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>, we use True Positive Rate (TPR) at a false accept rate of 0.1 for evaluating re-ID features. In particular, we extract re-ID features which correspond to ground truth boxes and use each feature to retrieve N most similar candidates. We report the true positive rate at false accept rate 0.1 (TPR@FAR=0.1). Note that TPR is not affected by detection results and faithfully reflects the quality of re-ID features. We use the CLEAR metric (Bernardin and Stiefelhagen, 2008) (i.e. MOTA, IDs) and IDF1 <ref type="bibr" target="#b62">(Ristani et al., 2016)</ref> to evaluate overall tracking accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We use a variant of DLA-34 proposed in <ref type="bibr" target="#b98">(Zhou et al., 2019a)</ref> as our default backbone. The model parameters pre-trained on the COCO dataset <ref type="bibr" target="#b45">(Lin et al., 2014)</ref> are used to initialize our model. We train our model with the Adam optimizer <ref type="bibr" target="#b39">(Kingma and Ba, 2014)</ref> for 30 epochs with a starting learning rate of 10 ?4 . The learning rate decays to 10 ?5 at 20 epochs. The batch size is set to be 12. We use standard data augmentation techniques including rotation, scaling and color jittering. The input image is resized to 1088 ? 608 and the feature map resolution is 272?152. The training step takes about 30 hours on two RTX 2080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablative Studies</head><p>In this section, we present rigorous studies of the three critical factors in FairMOT including anchor-less re-ID feature extraction, feature fusion and feature dimensions by carefully designing a number of baseline methods. <ref type="table">Table 1</ref> Comparison of different re-ID feature extraction (sampling) strategies on the validation set of MOT17. The rest of the models are kept the same for fair comparison. ? means the larger the better and ? means the smaller the better. The best results are shown in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>Anchor MOTA? IDF1? IDs? TPR? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Anchors</head><p>We evaluate four strategies for sampling re-ID features from the detected boxes which are frequently used by previous works <ref type="bibr" target="#b80">(Wang et al., 2020b</ref><ref type="bibr">) (Voigtlaender et al., 2019</ref>. The first strategy is ROI-Align used in Track R-CNN <ref type="bibr">(Voigtlaender et al., 2019)</ref>. It samples features from the detected proposals using ROI-Align. As discussed previously, many sampling locations deviate from object centers. The second strategy is POS-Anchor used in JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>. It samples features from positive anchors which may also deviate from object centers. The third strategy is "Center" used in FairMOT. It only samples features at object centers. Recall that, in our approach, re-ID features are extracted from discretized low-resolution maps. In order to sample features at accurate object locations, we also try to apply Bi-linear Interpolation (Center-BI) to extract more accurate features. We also evaluate a two-stage approach to first detect object bounding boxes and then extract re-ID features. In the first stage, the detection part is the same as our FairMOT. In the second stage, we use ROI-Align <ref type="bibr" target="#b30">(He et al., 2017)</ref> to extract the backbone features based on the detected bounding boxes and then use a re-ID head (a fully connected layer) to get re-ID features. The main difference between the twostage approach and the one-stage "ROI-Align" approach is that the re-ID features of the two-stage approach rely on the detection results while those of the one-stage approach do not during training.</p><p>The results are shown in <ref type="table">Table 1</ref>. Note that the five approaches are all built on top of FairMOT. The only difference lies in how they sample re-ID features from detected boxes. First, we can see that our approach (Center) obtains notably higher IDF1 score and True Positive Rate (TPR) than ROI-Align, POS-Anchor and the two-stage approach. This metric is independent of object detection results and faithfully reflects the quality of re-ID features. In addition, the number of ID switches (IDs) of our approach is also significantly smaller than the two baselines. The results validate that sampling features at object centers is more effective than the strategies used in the previous works. Bi-linear Interpolation (Center-BI) achieves even higher TPR than Cen-ter because it samples features at more accurate locations. The two-stage approach harms the quality of the re-ID features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Balancing Multi-task Losses</head><p>We evaluate different methods for balancing the losses of different tasks including Uncertainty <ref type="bibr" target="#b38">(Kendall et al., 2018)</ref>, GradNorm <ref type="bibr" target="#b13">(Chen et al., 2018b)</ref> and MGDA-UB <ref type="bibr" target="#b66">(Sener and Koltun, 2018)</ref>. We also evaluate a baseline with fixed weights obtained by grid search. We implement two versions for the uncertainty-based method. The first is "Uncertainty-task" which learns two parameters for the detection loss and re-ID loss, respectively. The second is "Uncertainty-branch" which learns four parameters for the heatmap loss, box size loss, offset loss and re-ID losses, respectively. The results are shown in <ref type="table">Table 2</ref>. We can see that the "Fixed" method gets the best MOTA and AP but the worst IDs and TPR. It means that the model is biased to the detection task. MGDA-UB has the highest TPR but the lowest MOTA and AP, which indicates that the model is biased to the re-ID task. Similar results can be found in <ref type="bibr" target="#b80">(Wang et al., 2020b;</ref><ref type="bibr" target="#b76">Vandenhende et al., 2021)</ref>. GradNorm gets the best overall tracking accuracy (highest IDF1 and second highest MOTA), meaning that ensuring different tasks to have similar gradient magnitude is helpful to handle feature conflicts. However, GradNorm takes longer training time. So we use the simpler Uncertainty method which is slightly worse than GradNorm in the rest of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Multi-layer Feature Fusion</head><p>We compare a number of backbones such as vanilla ResNet <ref type="bibr" target="#b29">(He et al., 2016)</ref>, Feature Pyramid Network (FPN) <ref type="bibr" target="#b46">(Lin et al., 2017a)</ref>, High-Resolution Network (HRNet) <ref type="bibr" target="#b79">(Wang et al., 2020a)</ref>, DLA <ref type="bibr" target="#b98">(Zhou et al., 2019a)</ref>, HarDNet <ref type="bibr" target="#b10">(Chao et al., 2019)</ref> and RegNet <ref type="bibr" target="#b58">(Radosavovic et al., 2020)</ref>. Note that the rest of the factors of these approaches such as training datasets are all controlled to be the same for fair comparison. In particular, the stride of the final feature map is four for all methods. We add three up-sampling operations for vanilla ResNet and RegNet to obtain feature maps of stride four. We divide these backbones into two categories, one without multi-layer fusion (i.e. ResNet and RegNet) and one with (i.e. FPN, HR-Net, DLA and HarDNet).</p><p>The results are shown in <ref type="table">Table 3</ref>. We also list the Ima-geNet <ref type="bibr" target="#b63">(Russakovsky et al., 2015)</ref> classification accuracy Acc in order to demonstrate that a strong backbone in one task does not mean it will also get good results in MOT. So detailed studies for MOT are necessary and useful.</p><p>By comparing the results of ResNet-34 and ResNet-50, we find that blindly using a larger network does not notably improve the overall tracking result measured by MOTA. In particular, the quality of re-ID features barely benefits from the larger network. For example, IDF1 only improves from 67.2% to 67.7% and TPR improves from 90.9% to 91.9%, respectively. In addition, the number of ID switches even increases from 435 to 501. By comparing ResNet-50 and RegNetY-4.0GF, we can find that using a even more powerful backbone also achieves very limited gain. The re-ID metric TPR of RegNetY-4.0GF is the same as ResNet-50 (91.9) while the ImageNet classification accuracy improves a lot (79.4 vs 77.8). All these results suggest that directly using a larger or a more powerful network cannot always improve the final tracking accuracy.</p><p>In contrast, ResNet-34-FPN, which actually has fewer parameters than ResNet-50, achieves a larger MOTA score than ResNet-50. More importantly, TPR improves significantly from 90.9% to 94.2%. By comparing RegNetY-4.0GF-FPN with RegNetY-4.0GF, we can see that adding multilayer feature fusion structure <ref type="bibr" target="#b46">(Lin et al., 2017a)</ref> to RegNet brings considerable gains (+1.9 MOTA, +1.3 IDF1, -36.9% IDs, +2.2 AP, +2.3 TPR), which suggests that multi-layer feature fusion has clear advantages over simply using larger or more powerful networks.</p><p>In addition, DLA-34, which is also built on top of ResNet-34 but has more levels of feature fusion, achieves an even <ref type="table">Table 4</ref> Demonstration of feature conflict between the detection and re-ID tasks on the validation set of the MOT17 dataset. "-det" means only the detection branch is trained and the re-ID branch is randomly initialized. The best results are shown in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>MOTA? <ref type="formula" target="#formula_2">IDF1?</ref>  larger MOTA score. In particular, TPR increases significantly from 90.9% to 94.4% which in turn decreases the number of ID switches (IDs) from 435 to 299. Similar conclusions can be obtained from the results of HRNet-W18. The results validate that feature fusion (FPN, DLA and HRNet) effectively improves the discriminative ability of re-ID features. On the other hand, although ResNet-34-FPN obtains equally good re-ID features (TPR) as DLA-34, its detection results (AP) are significantly worse than DLA-34. We think the use of deformable convolution in DLA-34 is the main reason because it enables more flexible receptive fields for objects of different sizes -it is very important for our method since FairMOT only extracts features from object centers without using any region features. We can only get 65.0 MOTA and 78.1 AP when replacing all the deformable convolutions with normal convolutions in DLA-34. As shown in <ref type="table">Table 5</ref>, we can see that DLA-34 mainly outperforms HRNet-W18 on middle and large size objects. When we further use a more powerful backbone HarDNet-85 with more multi-layer feature fusion structures, we achieve even better results than DLA-34 (+2.1 MOTA, +1.7 IDF1, -33.8% IDs, +1.4 AP, +1.4 TPR). Although HRNet-W18, DLA-34 and HarDNet-85 get lower ImageNet classification accuracy than ResNet-50 and RegNetY-4.0GF, they achieve much higher tracking accuracy. Based on the experimental results above, we believe that multi-layer feature fusion is the key to solve the "feature" issue.</p><p>To validate the existence of feature conflict between the detection and re-ID tasks, we introduce a baseline ResNet-34-det which only trains the detection branch (re-ID branch is randomly initialized). We can see from <ref type="table">Table 4</ref> that the detection result measured by AP improves by 1 point if we do not train the re-ID branch which shows the conflict between the two tasks. In particular, ResNet-34-det even gets higher MOTA score than ResNet-34 because the metric favors better detection than tracking results. In contrast, DLA-34, which adds multi-layer feature fusion over ResNet-34, achieves better detection as well as tracking results. It means multi-layer feature fusion helps alleviate the feature conflict problem by allowing each task to extract whatever it needs for its own task from the fused features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Feature Dimension</head><p>The previous one-shot trackers such as JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref> usually learns 512 dimensional re-ID features following the two-step methods without ablation study. However, we find in our experiments that the feature dimension actually plays an important role in balancing detection and tracking accuracy. Learning lower dimensional re-ID features causes less harm to the detection accuracy and improves the inference speed. We conduct experiments on different one-shot trackers and find it is a generic rule that lowdimensional (i.e. 64) re-ID features achieves better performance than high-dimensional (i.e. 512) re-ID features. We evaluate multiple choices for re-ID feature dimension of JDE and FairMOT in <ref type="table" target="#tab_5">Table 6</ref>. For JDE, we can see that 64 achieves better performance than 512 on all the metrics. For FairMOT, we can see that 512 achieves higher IDF1 and TPR scores which indicates that higher dimensional re-ID features lead to stronger discriminative ability. However, the MOTA score improves when we decrease the dimension from 512 to 64. This is mainly caused by the conflict between the detection and re-ID tasks. In particular, we can see that the detection result (AP) improves when we decrease the dimension of re-ID features. Different from the re-ID task, low-dimensional re-ID features achieves better performance and efficiency on the MOT task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Data Association Methods</head><p>This section evaluates the three ingredients in the data association step including bounding box IoU, re-ID features and  <ref type="figure">Fig. 3</ref> Visualization of the discriminative ability of the re-ID features. Query instances are marked as red boxes and target instances are marked as green boxes. The similarity maps are computed using re-ID features extracted based on different strategies (e.g., Center, Center-BI, ROI-Align and POS-Anchor as described in Section 5.3.1) and different backbones (e.g., . The query frames and target frames are randomly chosen from the MOT17-09 and the MOT17-02 sequence.</p><p>Kalman Filter <ref type="bibr" target="#b35">(Kalman, 1960)</ref>. These are used to compute the similarity between each pair of detected boxes. With that we use Hungarian algorithm <ref type="bibr" target="#b41">(Kuhn, 1955)</ref> to solve the assignment problem. <ref type="table" target="#tab_7">Table 7</ref> shows the results. We can see that only using box IoU causes a lot of ID switches. This is particularly true for crowded scenes and fast camera motion. Using re-ID features alone notably increases IDF1 and decreases the number of ID switches. In addition, adding Kalman filter helps obtain smooth (reasonable) tracklets which further decreases the number of ID switches. When an object is partly occluded, its re-ID features become unreliable. In this case, it is important to leverage box IoU, re-ID features and Kalman filter to obtain good tracking performance.</p><p>We also present a detailed runtime breakdown of different components including detection, re-ID matching, Kalman Filter and IoU matching. We test runtime on sequences with different density (average number of pedestrians per frame). The results are shown in <ref type="figure">Fig 4.</ref> The time spent on joint detection and re-ID is minimally affected by density. The time spent on Kalman Filter and IoU matching are around 1ms or 2ms and can be ignored. The time spent on re-ID matching increases linearly with the increase of density. This is because a large amount of time is cost on updating the re-ID feature of each tracklet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.6">Visualization of Re-ID Similarity</head><p>We use re-ID similarity maps to demonstrate the discriminative ability of re-ID features in <ref type="figure">Figure 3</ref>. We randomly choose two frames from our validation set. The first frame contains the query instance and the second frame contains the target instance that has the same ID. We obtain the re-ID similarity maps by computing the cosine similarity between the re-ID feature of the query instance and the whole re-ID feature map of the target frame, as described in Section 5.3.1 and Section 5.3.3 respectively. By comparing the similarity maps of ResNet-34 and ResNet-34-det, we can see that training the re-ID branch is important. By comparing DLA-34 and ResNet-34, we can see that multi-layer fea- <ref type="figure">Fig. 4</ref> Time spent on different parts of our whole MOT system. We run tracking on sequences with different density from the MOT17 dataset and the MOT20 dataset.</p><p>ture aggregation can get more discriminative re-ID features. Among all the sampling strategies, the proposed Center and Center-BI can better discriminate the target object from surrounding objects in crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Single Image Training</head><p>We first pre-train FairMOT on the CrowdHuman dataset <ref type="bibr" target="#b68">(Shao et al., 2018)</ref>. In particular, we assign a unique identity label for each bounding box and train FairMOT using the method described in section 4.4. Then we finetune the pre-trained model on the target dataset MOT17. <ref type="table" target="#tab_8">Table 8</ref> shows the results. First, the pre-trained model can be directly used as a tracker and get acceptable results on MOT datasets such as MOT17. This is because the Crowd-Human dataset can boost the human detection performance and also has strong domain generalization ability. Our training of the re-ID features further enhances the association ability of the tracker. Second, pre-training on CrowdHuman outperforms directly training on the MOT17 dataset by a large margin. Third, the single image training model even outperforms the model trained on the "MIX" and MOT17 datasets with identity annotations. The results validate the effectiveness of the proposed single image pre-training, which saves lots of annotation efforts and makes FairMOT more attractive in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results on MOTChallenge</head><p>We compare our approach to the state-of-the-art (SOTA) methods including both the one-shot methods and the twostep methods. There are two published works of JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref> and <ref type="bibr">TrackRCNN (Voigtlaender et al., 2019</ref>) that jointly perform object detection and identity feature embedding. We compare our approach to both of them. Following the previous work <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>, the testing dataset contains 6 videos from 2DMOT15. FairMOT uses the same training data as the two methods as described in their papers.</p><p>In particular, when we compare to JDE, both FairMOT and JDE use the large scale composed dataset described in Section 5.1. Since Track R-CNN requires segmentation labels to train the network, it only uses 4 videos of the MOT17 dataset which has segmentation labels as training data. In this case, we also use the 4 videos to train our model. The CLEAR metric <ref type="bibr" target="#b4">(Bernardin and Stiefelhagen, 2008)</ref> and IDF1 (Ristani et al., 2016) are used to measure their performance.</p><p>The results are shown in <ref type="table">Table 9</ref>. We can see that our approach remarkably outperforms JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>. In particular, the number of ID switches reduces from 218 to 80 which is big improvement in terms of user experience. The results validate the effectiveness of the anchor-free approach over the previous anchor-based one. The inference speed is near video rate for the both methods with ours being faster. Compared with Track R-CNN <ref type="bibr">(Voigtlaender et al., 2019)</ref>, their detection results are slightly better than ours (with lower FN). However, FairMOT achieves much higher IDF1 score (64.0 vs. 49.4) and fewer ID switches (96 vs. 294). This is mainly because Track R-CNN follows the "de- tection first, re-ID secondary" framework and use anchors which also introduce ambiguity to the re-ID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Comparing with Other SOTA MOT Methods</head><p>We compare our approach to the state-of-the-art trackers including the two-step methods in <ref type="table" target="#tab_9">Table 10</ref>. Since we do not use the public detection results, the "private detector" protocol is adopted. We report results on the testing sets of the 2DMOT15, MOT16, MOT17 and MOT20 datasets, respectively. Note that all of the results are directly obtained from the official MOT challenge evaluation server.</p><p>Our approach ranks first among all online and offline trackers on the four datasets. In particular, it outperforms other methods by a large margin. This is a very strong result especially considering that our approach is very simple. In addition, our approach achieves video rate inference. In contrast, most high-performance trackers such as <ref type="bibr" target="#b22">(Fang et al., 2018;</ref><ref type="bibr" target="#b88">Yu et al., 2016)</ref> are usually slower than ours. Our approach also ranks second under a very recent local MOT metric ALTA <ref type="bibr" target="#b75">(Valmadre et al., 2021)</ref>, which further indicates that our approach achieves very high tracking performance <ref type="table" target="#tab_9">(Table 10)</ref>. <ref type="table">Table 11</ref> Results of the MOT17 test set when using different datasets for training. "MIX" represents the large scale dataset mentioned in part 4.1 and "CH" is short for the CrowdHuman dataset. All the results are obtained from the MOT challenge server. The best results are shown in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data</head><p>Images </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Training Data Ablation Study</head><p>We also evaluate the performance of FairMOT using different amount of training data in <ref type="table">Table 11</ref>. We can achieve 69.8 MOTA when only using the MOT17 dataset for training, which already outperforms other methods using more training data. When we use the same training data as JDE <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref>, we can achieve 72.9 MOTA, which remarkably outperforms JDE. In addition, when we perform single image training on the CrowdHuman dataset, the MOTA score improves to 73.7. The results suggest that our approach is not data hungry which is a big advantage in practical applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOT17</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Qualitative Results</head><p>Figure 5 visualizes several tracking results of FairMOT on the test set of MOT17 <ref type="bibr" target="#b54">(Milan et al., 2016)</ref>. From the results of MOT17-01, we can see that our method can assign correct identities with the help of high-quality re-ID features when two pedestrians cross over each other. Trackers using bounding box IoUs <ref type="bibr" target="#b5">(Bewley et al., 2016;</ref><ref type="bibr" target="#b6">Bochinski et al., 2017)</ref> usually cause identity switches under these circumstances. From the results of MOT17-03, we can see that our method perform well under crowded scenes. From the results of MOT17-08, we can see that our method can keep both correct identities and correct bounding boxes when the pedestrians are heavily occluded. The results of MOT17-06 and MOT17-12 show that our method can deal with large scale variations. This mainly attributes to the using of multilayer feature aggregation. Our method can detect small objects accurately as shown in the results of MOT17-07 and MOT17-14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Future Work</head><p>Start from studying why the previous one-shot methods <ref type="bibr" target="#b80">(Wang et al., 2020b)</ref> fail to achieve comparable results as the twostep methods, we find that the use of anchors in object detection and identity embedding is the main reason for the degraded results. In particular, multiple nearby anchors, which correspond to different parts of an object, may be responsible for estimating the same identity which causes ambiguities for network training. Further, we find the feature unfairness issue and feature dimension issue between the detection and re-ID tasks in previous MOT frameworks. By addressing these problems in an anchor-free single-shot deep network, we propose FairMOT. It outperforms the previous state-of-the-art methods on several benchmark datasets by a large margin in terms of both tracking accuracy and inference speed. Besides, FairMOT is inherently training dataefficient and we propose single image training of multi-object trackers only using bounding box annotated images, which both make our method more appealing in real applications .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig</head><label></label><figDesc>(b) One anchor contains multiple identities (c) Multiple anchors response for one identity (d) One point for one identity Comparison of the existing one-shot trackers and FairMOT . 2 (a) Track R-CNN treats detection as the primary task and re-ID as the secondary one. Both Track R-CNN and JDE are anchor-based. The red boxes represent positive anchors and the green boxes represent the target objects. The three methods extract re-ID features differently. Track R-CNN extracts re-ID features for all positive anchors using ROI-Align. JDE extracts re-ID features at the centers of all positive anchors. FairMOT extracts re-ID features at the object center. (b) The red anchor contains two different instances. So it will be forced to predict two conflicting classes. (c) Three different anchors with different image patches are response for predicting the same identity. (d) FairMOT extracts re-ID features only at the object center and can mitigate the problems in (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. Then its location on the feature map is obtained by dividing the stride ( c i x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Example tracking results of our method on the test set of MOT17. Each row shows the results of sampled frames in chronological order of a video sequence. Bounding boxes and identities are marked in the images. Bounding boxes with different colors represent different identities. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The issues severely limit the tracking performance of those methods. -We present FairMOT to address the fairness issue. Fair-MOT is built on top of CenterNet. Although the adopted techniques are mostly not novel by themselves, we have new discoveries which are important to MOT. These are both novel and valuable. -We show that the achieved fairness allows our FairMOT</figDesc><table><row><cell></cell><cell></cell><cell>Detection</cell></row><row><cell></cell><cell></cell><cell>Detection</cell><cell>heatmap</cell></row><row><cell></cell><cell></cell><cell>Re-ID</cell><cell>box size</cell></row><row><cell>Image</cell><cell>encoder-decoder</cell></row><row><cell cols="2">Encoder-decoder network</cell><cell>center offset</cell></row><row><cell>1/4</cell><cell></cell></row><row><cell>1/8</cell><cell></cell><cell>Re-ID</cell></row><row><cell></cell><cell>1/16</cell><cell>Re-ID Embeddings</cell></row><row><cell></cell><cell>1/32</cell><cell>extract features</cell></row><row><cell cols="2">down sample up sample keep resolution</cell><cell>sum</cell></row></table><note>to obtain high levels of detection and tracking accuracy and outperform the previous state-of-the-art methods by a large margin on multiple datasets such as 2DMOT15, MOT16, MOT17 and MOT20.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The resulting model is named DLA-34. Denote the size of input image as H image ? W image , then the output feature map has the shape of C ? H ? W where H = H image /4 and W = W image /4. Besides DLA, other deep networks that provide multi-scale convolutional features, such as Higher HRNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 Table 3</head><label>23</label><figDesc>Comparison of different loss weighting strategies on the validation set of the MOT17 dataset. The best results are shown in bold. Comparison of different backbones on the validation set of MOT17 dataset. "MLFF" is short for multi-layer feature fusion. "Acc" is short for ImageNet classification accuracy. The results of the Ima-geNet classification accuracy are from the original papers of the backbone networks. The best results are shown in bold.</figDesc><table><row><cell>Loss Weighting</cell><cell></cell><cell cols="4">MOTA? IDF1? IDs? AP? TPR?</cell></row><row><cell>Fixed</cell><cell></cell><cell>69.6</cell><cell cols="2">71.6</cell><cell>387 81.9</cell><cell>93.8</cell></row><row><cell>Uncertainty-task</cell><cell></cell><cell>69.1</cell><cell cols="2">72.8</cell><cell>299 81.2</cell><cell>94.4</cell></row><row><cell cols="2">Uncertainty-branch</cell><cell>68.5</cell><cell cols="2">73.3</cell><cell>319 81.0</cell><cell>96.0</cell></row><row><cell>MGDA-UB</cell><cell></cell><cell>63.6</cell><cell cols="2">67.9</cell><cell>355 78.5</cell><cell>97.0</cell></row><row><cell>GradNorm</cell><cell></cell><cell>69.5</cell><cell cols="2">73.8</cell><cell>311 81.3</cell><cell>95.1</cell></row><row><cell>Backbone</cell><cell cols="5">w/ MLFF MOTA? IDF1? IDs? AP? TPR? Acc?</cell></row><row><cell>ResNet-34</cell><cell></cell><cell></cell><cell>63.6</cell><cell cols="2">67.2 435 75.1 90.9 75.2</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>63.7</cell><cell cols="2">67.7 501 75.5 91.9 77.8</cell></row><row><cell>RegNetY-4.0GF</cell><cell></cell><cell></cell><cell>63.9</cell><cell cols="2">68.0 407 75.8 91.9 79.4</cell></row><row><cell>ResNet-34-FPN</cell><cell></cell><cell></cell><cell>64.4</cell><cell cols="2">69.6 369 77.7 94.2 75.2</cell></row><row><cell>RegNetY-4.0GF-FPN</cell><cell></cell><cell></cell><cell>65.8</cell><cell cols="2">69.3 257 78.0 94.3 79.4</cell></row><row><cell>HRNet-W18</cell><cell></cell><cell></cell><cell>67.4</cell><cell cols="2">74.3 315 80.5 94.6 76.8</cell></row><row><cell>DLA-34</cell><cell></cell><cell></cell><cell>69.1</cell><cell cols="2">72.8 299 81.2 94.4 76.9</cell></row><row><cell>HarDNet-85</cell><cell></cell><cell></cell><cell>71.2</cell><cell cols="2">74.5 198 82.6 95.8 77.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>AP M AP L TPR S TPR M TPR L IDs S IDs M IDs L</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IDs? AP? TPR?</cell></row><row><cell>ResNet-34</cell><cell></cell><cell>63.6</cell><cell>67.2</cell><cell>435 75.1</cell><cell>90.9</cell></row><row><cell cols="2">ResNet-34-det</cell><cell>63.7</cell><cell>60.4</cell><cell>597 76.1</cell><cell>36.7</cell></row><row><cell>DLA-34</cell><cell></cell><cell>69.1</cell><cell>72.8</cell><cell>299 81.2</cell><cell>94.4</cell></row><row><cell cols="6">Table 5 The impact of different backbones on objects of different</cell></row><row><cell cols="6">scales. Small: area smaller than 7000 pixels; Medium: area from 7000</cell></row><row><cell cols="6">to 15000 pixels; Large: area larger than 15000 pixels. The best results</cell></row><row><cell cols="2">are shown in bold.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Backbone AP S ResNet-34 40.6 57.8 85.2 91.7 85.7 88.8 190 87 118</cell></row><row><cell>ResNet-50</cell><cell cols="5">39.7 59.4 86.0 91.3 85.3 89.0 248 91 124</cell></row><row><cell cols="6">ResNet-34-FPN 45.9 61.0 85.4 90.7 91.5 93.3 166 71 90</cell></row><row><cell>HRNet-W18</cell><cell cols="5">51.1 63.7 85.7 94.2 92.5 93.1 168 55 56</cell></row><row><cell>DLA-34</cell><cell cols="5">46.8 65.1 88.8 92.7 91.2 91.8 134 64 70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Evaluation of re-ID feature dimensions of JDE and FairMOT on the validation set of MOT17. The best results of the same method are shown in bold.</figDesc><table><row><cell>Method</cell><cell cols="5">Dim MOTA ? IDF1 ? IDs ? AP? TPR ? FPS?</cell></row><row><cell>JDE</cell><cell>512</cell><cell>59.9</cell><cell>64.1</cell><cell>536 73.3 76.8</cell><cell>22.2</cell></row><row><cell>JDE</cell><cell>64</cell><cell>60.3</cell><cell>65.0</cell><cell>474 73.6 82.0</cell><cell>24.4</cell></row><row><cell cols="2">FairMOT 512</cell><cell>68.5</cell><cell>73.7</cell><cell>312 80.9 94.6</cell><cell>24.1</cell></row><row><cell cols="2">FairMOT 64</cell><cell>69.2</cell><cell>73.3</cell><cell>283 81.3 94.3</cell><cell>26.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Evaluation of the three ingredients in the data association model. The backbone is DLA-34. The best results are shown in bold.</figDesc><table><row><cell cols="3">Box IoU Re-ID Features Kalman Filter MOTA ? IDF1 ? IDs ?</cell></row><row><cell>67.8</cell><cell>67.2</cell><cell>648</cell></row><row><cell>68.1</cell><cell>70.3</cell><cell>435</cell></row><row><cell>68.9</cell><cell>71.8</cell><cell>342</cell></row><row><cell>69.1</cell><cell>72.8</cell><cell>299</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>Effects of single image training on the validation set of MOT17. "CH" and "MIX" stand for CrowdHuman and the composed five datasets introduced in Section 5.1, respectively. * means no identity annotations are used. The best results are shown in bold.Training Data MOTA ? IDF1 ? IDs ? AP? TPR ?</figDesc><table><row><cell>CH *</cell><cell></cell><cell>64.1</cell><cell>64.9</cell><cell>476</cell><cell>80.5</cell><cell>79.9</cell></row><row><cell>MOT17</cell><cell></cell><cell>67.5</cell><cell>69.9</cell><cell>408</cell><cell>79.6</cell><cell>93.4</cell></row><row><cell cols="2">CH * +MOT17</cell><cell>71.1</cell><cell>75.6</cell><cell>327</cell><cell>83.0</cell><cell>93.6</cell></row><row><cell cols="2">MIX+MOT17</cell><cell>69.1</cell><cell>72.8</cell><cell>299</cell><cell>81.2</cell><cell>94.4</cell></row><row><cell cols="7">Table 9 Comparison of the state-of-the-art one-shot trackers on the</cell></row><row><cell cols="7">2DMOT15 dataset. "MIX" represents the large scale training dataset</cell></row><row><cell cols="7">and "MOT17 Seg" stands for the 4 videos with segmentation labels</cell></row><row><cell cols="7">in the MOT17 dataset. The best results of the same training data are</cell></row><row><cell>shown in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Training Data Method</cell><cell></cell><cell cols="4">MOTA? IDF1? IDs? FP? FN? FPS?</cell></row><row><cell>MIX</cell><cell>JDE</cell><cell></cell><cell>67.5</cell><cell cols="3">66.7 218 1881 2083 26.0</cell></row><row><cell></cell><cell cols="3">FairMOT(ours) 77.2</cell><cell cols="3">79.8 80 757 2094 30.9</cell></row><row><cell cols="3">MOT17 Seg Track R-CNN</cell><cell>69.2</cell><cell cols="3">49.4 294 1328 2349 2.0</cell></row><row><cell></cell><cell cols="3">FairMOT(ours) 70.2</cell><cell cols="3">64.0 96 1209 2537 30.9</cell></row><row><cell cols="7">5.5.1 Comparing with One-Shot SOTA MOT Methods</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc>Comparison of the state-of-the-art methods under the "private detector" protocol. It is noteworthy that FPS considers both detection and association time. The one-shot trackers are labeled by "*". The best results of each dataset are shown in bold.</figDesc><table><row><cell>Dataset</cell><cell>Tracker</cell><cell>MOTA?</cell><cell>IDF1?</cell><cell>MT?</cell><cell>ML?</cell><cell>IDs?</cell><cell>FPS?</cell></row><row><cell>MOT15</cell><cell>MDP SubCNN(Xiang et al., 2015)</cell><cell>47.5</cell><cell>55.7</cell><cell>30.0%</cell><cell>18.6%</cell><cell>628</cell><cell>&lt;1.7</cell></row><row><cell></cell><cell>CDA DDAL(Bae and Yoon, 2017)</cell><cell>51.3</cell><cell>54.1</cell><cell>36.3%</cell><cell>22.2%</cell><cell>544</cell><cell>&lt;1.2</cell></row><row><cell></cell><cell>EAMTT(Sanchez-Matilla et al., 2016)</cell><cell>53.0</cell><cell>54.0</cell><cell>35.9%</cell><cell>19.6%</cell><cell>7538</cell><cell>&lt;4.0</cell></row><row><cell></cell><cell>AP HWDPL(Chen et al., 2017)</cell><cell>53.0</cell><cell>52.2</cell><cell>29.1%</cell><cell>20.2%</cell><cell>708</cell><cell>6.7</cell></row><row><cell></cell><cell>RAR15(Fang et al., 2018)</cell><cell>56.5</cell><cell>61.3</cell><cell>45.1%</cell><cell>14.6%</cell><cell>428</cell><cell>&lt;3.4</cell></row><row><cell></cell><cell>TubeTK * (Pang et al., 2020)</cell><cell>58.4</cell><cell>53.1</cell><cell>39.3%</cell><cell>18.0%</cell><cell>854</cell><cell>5.8</cell></row><row><cell></cell><cell>FairMOT (Ours) *</cell><cell>60.6</cell><cell>64.7</cell><cell>47.6%</cell><cell>11.0%</cell><cell>591</cell><cell>30.5</cell></row><row><cell>MOT16</cell><cell>EAMTT(Sanchez-Matilla et al., 2016)</cell><cell>52.5</cell><cell>53.3</cell><cell>19.9%</cell><cell>34.9%</cell><cell>910</cell><cell>&lt;5.5</cell></row><row><cell></cell><cell>SORTwHPD16(Bewley et al., 2016)</cell><cell>59.8</cell><cell>53.8</cell><cell>25.4%</cell><cell>22.7%</cell><cell>1423</cell><cell>&lt;8.6</cell></row><row><cell></cell><cell>DeepSORT 2(Wojke et al., 2017)</cell><cell>61.4</cell><cell>62.2</cell><cell>32.8%</cell><cell>18.2%</cell><cell>781</cell><cell>&lt;6.4</cell></row><row><cell></cell><cell>RAR16wVGG(Fang et al., 2018)</cell><cell>63.0</cell><cell>63.8</cell><cell>39.9%</cell><cell>22.1%</cell><cell>482</cell><cell>&lt;1.4</cell></row><row><cell></cell><cell>VMaxx(Wan et al., 2018)</cell><cell>62.6</cell><cell>49.2</cell><cell>32.7%</cell><cell>21.1%</cell><cell>1389</cell><cell>&lt;3.9</cell></row><row><cell></cell><cell>TubeTK * (Pang et al., 2020)</cell><cell>64.0</cell><cell>59.4</cell><cell>33.5%</cell><cell>19.4%</cell><cell>1117</cell><cell>1.0</cell></row><row><cell></cell><cell>JDE * (Wang et al., 2020b)</cell><cell>64.4</cell><cell>55.8</cell><cell>35.4%</cell><cell>20.0%</cell><cell>1544</cell><cell>18.5</cell></row><row><cell></cell><cell>TAP(Zhou et al., 2018)</cell><cell>64.8</cell><cell>73.5</cell><cell>38.5%</cell><cell>21.6%</cell><cell>571</cell><cell>&lt;8.0</cell></row><row><cell></cell><cell>CNNMTT(Mahmoudi et al., 2019)</cell><cell>65.2</cell><cell>62.2</cell><cell>32.4%</cell><cell>21.3%</cell><cell>946</cell><cell>&lt;5.3</cell></row><row><cell></cell><cell>POI(Yu et al., 2016)</cell><cell>66.1</cell><cell>65.1</cell><cell>34.0%</cell><cell>20.8%</cell><cell>805</cell><cell>&lt;5.0</cell></row><row><cell></cell><cell>CTrackerV1 * (Peng et al., 2020)</cell><cell>67.6</cell><cell>57.2</cell><cell>32.9%</cell><cell>23.1%</cell><cell>1897</cell><cell>6.8</cell></row><row><cell></cell><cell>FairMOT (Ours) *</cell><cell>74.9</cell><cell>72.8</cell><cell>44.7%</cell><cell>15.9%</cell><cell>1074</cell><cell>25.9</cell></row><row><cell>MOT17</cell><cell>SST(Sun et al., 2019)</cell><cell>52.4</cell><cell>49.5</cell><cell>21.4%</cell><cell>30.7%</cell><cell>8431</cell><cell>&lt;3.9</cell></row><row><cell></cell><cell>TubeTK * (Pang et al., 2020)</cell><cell>63.0</cell><cell>58.6</cell><cell>31.2%</cell><cell>19.9%</cell><cell>4137</cell><cell>3.0</cell></row><row><cell></cell><cell>CTrackerV1 * (Peng et al., 2020)</cell><cell>66.6</cell><cell>57.4</cell><cell>32.2%</cell><cell>24.2%</cell><cell>5529</cell><cell>6.8</cell></row><row><cell></cell><cell>CenterTrack * (Zhou et al., 2020)</cell><cell>67.8</cell><cell>64.7</cell><cell>34.6%</cell><cell>24.6%</cell><cell>2583</cell><cell>17.5</cell></row><row><cell></cell><cell>FairMOT (Ours) *</cell><cell>73.7</cell><cell>72.3</cell><cell>43.2%</cell><cell>17.3%</cell><cell>3303</cell><cell>25.9</cell></row><row><cell>MOT20</cell><cell>FairMOT (Ours) *</cell><cell>61.8</cell><cell>67.3</cell><cell>68.8%</cell><cell>7.6%</cell><cell>5243</cell><cell>13.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was in part supported by NSFC (No. 61733007  and No. 61876212) and MSRA Collaborative Research Fund. We thank all the anonymous reviewers for their valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1218" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Confidence-based data association and discriminative deep appearance learning for robust online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="595" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="941" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP, IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3464" to="3468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-speed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6247" to="6257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3552" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="645" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, PMLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="794" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Choi W (2015) Near-online multi-target tracking with aggregated local flow descriptor</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3029" to="3037" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE international conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="6172" to="6181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with instance-aware tracker and dynamic model refreshment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<idno>arXiv:200309003</idno>
		<title level="m">Mot20: A benchmark for multi object tracking in crowded scenes</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Centripetalnet: Pursuing high-quality keypoint pairs for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="10519" to="10528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recurrent autoregressive networks for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV, IEEE</publisher>
			<biblScope unit="page" from="466" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Model adaption object detection system for robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 39th Chinese Control Conference (CCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3659" to="3664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic task prioritization for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="270" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">E</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>arXiv:200904794</idno>
		<title level="m">Mat: Motion-aware multi-object tracking</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>arXiv:160208465</idno>
		<title level="m">Seq-nms for video object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multiple people tracking using body and joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>arXiv:170307737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lifted disjoint paths with application in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4364" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Fluids Eng</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="727" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv:14126980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno>arXiv:150401942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multitarget tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rethinking the competition between detection and reid in multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno>arXiv:201012138</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1871" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Retinatrack: Online single stage joint detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Votel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14668" to="14678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y ; Aaai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
	<note>Learning discriminative activated simplices for action recognition</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Detect or track: Towards cost-effective video object detection/tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8803" to="8810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ahadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
		<title level="m">Multi-target tracking using cnn-based features: Cnnmtt. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="7077" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="58" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><forename type="middle">I</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno>arXiv:160300831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tubetk: Adopting tubes to track multi-object in a one-step training model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6308" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Quasi-dense similarity learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="164" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multipleobject detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>arXiv:180402767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="300" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Online multi-target tracking with strong and weak detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanchez-Matilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poiesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="84" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-task learning as multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="527" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<idno>arXiv:201009015</idno>
		<title level="m">Fgagt: Flow-guided adaptive graph tracking</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>arXiv:180500123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv:14091556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno>arXiv:201215460</idno>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">What makes for end-to-end object detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, PMLR, Proceedings of Machine Learning Research</title>
		<meeting>the 38th International Conference on Machine Learning, PMLR, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9934" to="9944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Deep affinity network for multiple object tracking. IEEE transactions on pattern analysis and machine intelligence Tang P</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M ;</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1272" to="1278" />
		</imprint>
	</monogr>
	<note>Object detection in videos by high quality object linking</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>arXiv:210402631</idno>
		<title level="m">Local metrics for multi-object tracking</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L ;</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bbg</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence Voigtlaender P</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7942" to="7951" />
		</imprint>
	</monogr>
	<note>Mots: Multi-object tracking and segmentation</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multiobject tracking using online metric learning with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="788" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Deep highresolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Towards real-time multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="107" to="122" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Multiple target tracking based on undirected hierarchical relation hypergraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1282" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4705" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Spatial-temporal relation networks for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3988" to="3998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2129" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Poi: Multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Gmcp-tracker: Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="343" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Long-term tracking with deep tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="6694" to="6706" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv:211006864</idno>
		<title level="m">Bytetrack: Multi-object tracking by associating every detection box</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Voxeltrack: Multi-person 3d human pose estimation and tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno>arXiv:210802452</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno>arXiv:190407850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="474" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Online multi-target tracking with tensor-based high-order graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1809" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="382" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
