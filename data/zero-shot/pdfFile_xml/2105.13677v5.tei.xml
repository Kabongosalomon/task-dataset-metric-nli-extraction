<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ResT: An Efficient Transformer for Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing-Long</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>21023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
							<email>yangyubin@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>21023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ResT: An Efficient Transformer for Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an efficient multi-scale vision Transformer, called ResT, that capably served as a general-purpose backbone for image recognition. Unlike existing Transformer methods, which employ standard Transformer blocks to tackle raw images with a fixed resolution, our ResT have several advantages:</p><p>(1) A memory-efficient multi-head self-attention is built, which compresses the memory by a simple depth-wise convolution, and projects the interaction across the attention-heads dimension while keeping the diversity ability of multi-heads; (2) Positional encoding is constructed as spatial attention, which is more flexible and can tackle with input images of arbitrary size without interpolation or fine-tune;</p><p>(3) Instead of the straightforward tokenization at the beginning of each stage, we design the patch embedding as a stack of overlapping convolution operation with stride on the token map. We comprehensively validate ResT on image classification and downstream tasks. Experimental results show that the proposed ResT can outperform the recently state-of-the-art backbones by a large margin, demonstrating the potential of ResT as strong backbones. The code and models will be made</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning backbone architectures have been evolved for years and boost the performance of computer vision tasks such as classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref>, object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>, and instance segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>, etc.</p><p>There are mainly two types of backbone architectures most commonly applied in computer vision: convolutional network (CNN) architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> and Transformer ones <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>. Both of them capture feature information by stacking multiple blocks. The CNN block is generally a bottleneck structure <ref type="bibr" target="#b9">[10]</ref>, which can be defined as a stack of 1 ? 1, 3 ? 3, and 1 ? 1 convolution layers with residual learning (shown in <ref type="figure" target="#fig_0">Figure 1a</ref>). The 1 ? 1 layers are responsible for reducing and then increasing channel dimensions, leaving the 3 ? 3 layer a bottleneck with smaller input/output channel dimensions. The CNN backbones are generally faster and require less inference time thanks to parameter sharing, local information aggregation, and dimension reduction. However, due to the limited and fixed receptive field, CNN blocks may be less effective in scenarios that require modeling long-range dependencies. For example, in instance segmentation, being able to collect and associate scene information from a large neighborhood can be useful in learning relationships across objects <ref type="bibr" target="#b21">[22]</ref>.</p><p>To overcome these limitations, Transformer backbones are recently explored for their ability to capture long-distance information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18]</ref>. Unlike CNN backbones, the Transformer ones first split an image into a sequence of patches (i.e., tokens), then sum these tokens with positional encoding to represent coarse spatial information, and finally adopt a stack of Transformer blocks to capture feature information. A standard Transformer block <ref type="bibr" target="#b26">[27]</ref> comprises a multi-head self-attention (MSA) that employs a query-key-value decomposition to model global relationships between sequence tokens, and a feed-forward network (FFN) to learn wider representations (shown in <ref type="figure" target="#fig_0">Figure 1b</ref>). As a result, Transformer blocks can dynamically adapt the receptive field according to the image content.</p><p>Despite showing great potential than CNNs, the Transformer backbones still have four major shortcomings: <ref type="bibr" target="#b0">(1)</ref> It is difficult to extract the low-level features which form some fundamental structures in images (e.g., corners and edges) since existing Transformer backbones direct perform tokenization of patches from raw input images. <ref type="bibr" target="#b1">(2)</ref> The memory and computation for MSA in Transformer blocks scale quadratically with spatial or embedding dimensions (i.e., the number of channels), causing vast overheads for training and inference. (3) Each head in MSA is responsible for only a subset of embedding dimensions, which may impair the performance of the network, particularly when the tokens embedding dimension (for each head) is short, making the dot product of query and key unable to constitute an informative function. (4) The input tokens and positional encoding in existing Transformer backbones are all of a fixed scale, which are unsuitable for vision tasks that require dense prediction. In this paper, we proposed an efficient general-purpose backbone ResT (named after ResNet <ref type="bibr" target="#b9">[10]</ref>) for computer vision, which can remedy the above issues. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, ResT shares exactly the same pipeline of ResNet, i.e., a stem module applied for extracting low-level information and strengthening locality, followed by four stages to construct hierarchical feature maps, and finally a head module for classification. Each stage consists of a patch embedding, a positional encoding module, and multiple Transformer blocks with specific spatial resolution and channel dimension. The patch embedding module creates a multi-scale pyramid of features by hierarchically expanding the channel capacity while reducing the spatial resolution with overlapping convolution operations. Unlike the conventional methods which can only tackle images with a fixed scale, our positional encoding module is constructed as spatial attention which is conditioned on the local neighborhood of the input token. By doing this, the proposed method is more flexible and can process input images of arbitrary size without interpolation or fine-tune. Besides, to improve the efficiency of the MSA, we build an efficient multi-head self-attention (EMSA), which significantly reduce the computation cost by a simple overlapping Depth-wise Conv2d. In addition, we compensate short-length limitations of the input token for each head by projecting the interaction across the attention-heads dimension while keeping the diversity ability of multi-heads.</p><p>We comprehensively validate the effectiveness of the proposed ResT on the commonly used benchmarks, including image classification on ImageNet-1k and downstream tasks, such as object detection, and instance segmentation on MS COCO2017. Experimental results demonstrate the effectiveness and generalization ability of the proposed ResT compared with the recently state-of-the-art Vision Transformers and CNNs. For example, with a similar model size as ResNet-18 (69.7%) and PVT-Tiny (75.1%), our ResT-Small obtains a Top-1 accuracy of 79.6% on ImageNet-1k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ResT</head><p>As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, ResT shares exactly the same pipeline as ResNet <ref type="bibr" target="#b9">[10]</ref>, i.e., a stem module applied to extract low-level information, followed by four stages to capture multi-scale feature maps. Each stage consists of three components, one patch embedding module (or stem module), one positional encoding module, and a set of L efficient Transformer blocks. Specifically, at the beginning of each stage, the patch embedding module is adopted to reduce the resolution of the input token and expanding the channel dimension. The positional encoding module is fused to restrain position information and strengthen the feature extracting ability of patch embedding. After that, the input token is fed to the efficient Transformer blocks (illustrated in <ref type="figure" target="#fig_0">Figure 1c</ref>). In the following sections, we will introduce the intuition behind ResT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Rethinking of Transformer Block</head><p>The standard Transformer block consists of two sub-layers of MSA and FFN. A residual connection is employed around each sub-layer. Before MSA and FFN, layer normalization (LN <ref type="bibr" target="#b0">[1]</ref>) is applied. For a token input x ? R n?dm , where n, d m indicates the spatial dimension, channel dimension, respectively. The output for each Transformer block is: y = x + FFN(LN(x )), and x = x + MSA(LN(x)) (1)</p><p>MSA. MSA first obtains query Q, key K, and value V by applying three sets of projections to the input, each consisting of k linear layers (i.e., heads) that map the d m dimensional input into a d k dimensional space, where d k = d m /k is the head dimension. For the convenience of description, we assume k = 1, then MSA can be simplified to single-head self-attention (SA). The global relationship between the token sequence can be defined as</p><formula xml:id="formula_0">SA(Q, K, V) = Softmax( QK T ? d k )V<label>(2)</label></formula><p>The output values of each head are then concatenated and linearly projected to form the final output. The computation costs of MSA are O(2d m n 2 + 4d 2 m n), which scale quadratically with spatial dimension or embedding dimensions according to the input token.</p><p>FFN. The FFN is applied for feature transformation and non-linearity. It consists of two linear layers with a non-linearity activation. The first layer expands the embedding dimensions of the input from d m to d f and the second layer reduce the dimensions from d f to d m .</p><p>FFN</p><formula xml:id="formula_1">(x) = ?(xW 1 + b 1 )W 2 + b 2<label>(3)</label></formula><p>where W 1 ? R dm?d f and W 2 ? R d f ?dm are weights of the two Linear layers respectively, b 1 ? R d f and b 2 ? R dm are the bias terms, and ?(?) is the activation function GELU <ref type="bibr" target="#b10">[11]</ref>. In standard Transformer block, the channel dimensions are expanded by a factor of 4, i.e., d f = 4d m . The computation costs of FFN are 8nd 2 m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Transformer Block</head><p>As analyzed above, MSA has two shortcomings: (1) The computation scales quadratically with d m or n according to the input token, causing vast overheads for training and inference; (2) Each head in MSA only responsible for a subset of embedding dimensions, which may impair the performance of the network, particularly when the tokens embedding dimension (for each head) is short. <ref type="figure">Figure 3</ref>: Efficient Multi-Head Self-Attention.</p><p>To remedy these issues, we propose an efficient multi-head self-attention module (illustrated in <ref type="figure">Figure 3</ref>). Here, we make some explanations.</p><p>(1) Similar to MSA, EMSA first adopt a set of projections to obtain query Q.</p><p>(2) To compress memory, the 2D input token x ? R n?dm is reshaped to 3D one along the spatial dimension (i.e.,x ? R dm?h?w ) and then feed to a depth-wise convolution operation to reduce the height and width dimension by a factor s. To make simple, s is adaptive set by the feature map size or the stage number. The kernel size, stride and padding are s + 1, s, and s/2 respectively.</p><p>(3) The new token map after spatial reductionx ? R dm?h/s?w/s is then reshaped to 2D one, i.e., x ? R n ?dm , n = h/s ? w/s. Thenx is feed to two sets of projection to get key K and value V.</p><p>(4) After that, we adopt Eq. 4 to compute the attention function on query Q, K and value V.</p><formula xml:id="formula_2">EMSA(Q, K, V) = IN(Softmax(Conv( QK T ? d k )))V<label>(4)</label></formula><p>Here, Conv(?) is a standard 1 ? 1 convolutional operation, which model the interactions among different heads. As a result, attention function of each head can depend on all of the keys and queries. However, this will impair the ability of MSA to jointly attend to information from different representation subsets at different positions. To restore this diversity ability, we add an Instance Normalization <ref type="bibr" target="#b25">[26]</ref> (i.e, IN(?)) for the dot product matrix (after Softmax).</p><p>(5) Finally, the output values of each head are then concatenated and linearly projected to form the final output.</p><p>The computation costs of EMSA are O( 2dmn 2</p><formula xml:id="formula_3">s 2 + 2d 2 m n(1 + 1 s 2 ) + d m n (s+1) 2 s 2 + k 2 n 2 s 2 )</formula><p>, much lower than the original MSA (assume s &gt; 1), particularly in lower stages, where s is tend to higher. Also, we add FFN after EMSA for feature transformation and non-linearity. The output for each efficient Transformer block is: y = x + FFN(LN(x )), and x = x + EMSA(LN(x)) (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Patch Embedding</head><p>The standard Transformer receives a sequence of token embeddings as input. Take ViT <ref type="bibr" target="#b4">[5]</ref> as an example, the input image x ? R 3?h?w is split with a patch size of p ? p. These patches are flattened into 2D ones and then mapped to latent embeddings with a size of c, i.e, x ? R n?c , where n = hw/p 2 . However, this straightforward tokenization is failed to capture low-level feature information (such as edges and corners) <ref type="bibr" target="#b31">[32]</ref>. In addition, the length of tokens in ViT are all of a fixed size in different blocks, making it unsuitable for downstream vision tasks such as object detection and instance segmentation that require multi-scale feature map representations.</p><p>Here, we build an efficient multi-scale backbone, calling ResT, for dense prediction. As introduced above, the efficient Transformer block in each stage operates on the same scale with identical resolution across the channel and spatial dimensions. Therefore, the patch embedding modules are required to progressively expand the channel dimension, while simultaneously reducing the spatial resolution throughout the network.</p><p>Similar to ResNet, the stem module (can be seen as the first patch embedding module) are adopted to shrunk both the height and width dimension with a reduction factor of 4. To effectively capture the low-feature information with few parameters, here we introduce a simple but effective way, i.e, stacking three 3 ? 3 standard convolution layers (all with padding 1) with stride 2, stride 1, and stride 2, respectively. Batch Normalization <ref type="bibr" target="#b12">[13]</ref> and ReLU activation <ref type="bibr" target="#b5">[6]</ref> are applied for the first two layers. In stage 2, stage 3, and stage 4, the patch embedding module is adopted to down-sample the spatial dimension by 4? and increase the channel dimension by 2?. This can be done by a standard 3 ? 3 convolution with stride 2 and padding 1. For example, patch embedding module in stage 2 changes resolution from <ref type="figure" target="#fig_1">Figure 2</ref>).</p><formula xml:id="formula_4">h/4 ? w/4 ? c to h/8 ? w/8 ? 2c (shown in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Positional Encoding</head><p>Positional encodings are crucial to exploiting the order of sequence. In ViT <ref type="bibr" target="#b4">[5]</ref>, a set of learnable parameters are added into the input tokens to encode positions. Let x ? R n?c be the input, ? ? R n?c be position parameters, then the encoded input can be represent a?</p><formula xml:id="formula_5">x = x + ?<label>(6)</label></formula><p>However, the length of positions is exactly the same as the input tokens length, which limits the application scenarios. To remedy this issue, the new positional encodings are required to have variable lengths according to input tokens. Let us look closer to Eq. 6, the summation operation is much like assigning pixel-wise weights to the input. Assume ? is related with x, i.e., ? = GL(x), where GL(?) is the group linear operation with the group number of c. Then Eq. 6 can be modified t?</p><formula xml:id="formula_6">x = x + GL(x)<label>(7)</label></formula><p>Besides Eq. 7, ? can also be obtained by more flexible spatial attention mechanisms. Here, we propose a simple yet effective spatial attention module calling PA(pixel-attention) to encode positions. Specifically, PA applies a 3 ? 3 depth-wise convolution (with padding 1) operation to get the pixel-wise weight and then scaled by a sigmoid function ?(?). The positional encoding with PA module can then be represented a?</p><formula xml:id="formula_7">x = PA(x) = x * ?(DWConv(x))<label>(8)</label></formula><p>Since the input token in each stage is also obtained by a convolution operation, we can embed the positional encoding into the patch embedding module. The whole structure of stage i can be illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. Note that PA can be replaced by any spatial attention modules, making the positional encoding flexible in ResT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Classification Head</head><p>The classification head is performed by a global average pooling layer on the output feature map of the last stage, followed by a linear classifier. The detailed ResT architecture for ImageNet-1k is shown in <ref type="table">Table 1</ref>, which contains four models, i.e., ResT-Lite, ResT-Small and ResT-Base and ResT-Large, which are bench-marked to ResNet-18, ResNet-18, ResNet-50, and ResNet-101, respectively. <ref type="table">Table 1</ref>: Architectures for ImageNet-1k. Here, we make some definitions. "Conv ? k_c_s" means convolution layers with kernel size k, output channel c and stride s. "MLP_c" is the FFN structure with hidden channel 4c and output channel c. And "EMSA_n_r" is the EMSA operation with the number of heads n and reduction r. "C" is 64 for ResT-Lite and ResT-Small, and 96 for ResT-Base and ResT-Large."PA" is short for pixel-wise attention, which are introduced in Section 2.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Output</head><formula xml:id="formula_8">Lite Small Base Large stem 56?56 patch_embed: Conv-3_C/2_2, Conv-3_C/2_1, Conv-3_C_2,PA stage1 56?56 EMSA_1_8 MLP_64 ?2 EMSA_1_8 MLP_64 ?2 EMSA_1_8 MLP_96 ?2 EMSA_1_8 MLP_96 ?2 stage2 28?28 patch_embed: Conv-3_2C_2, PA EMSA_2_4 MLP_128 ?2 EMSA_2_4 MLP_128 ?2 EMSA_2_4 MLP_192 ?2 EMSA_2_4 MLP_192 ?2 stage3 14?14 patch_embed: Conv-3_4C_2, PA EMSA_4_2 MLP_256 ?2 EMSA_4_2 MLP_256 ?6 EMSA_4_2 MLP_384 ?6 EMSA_4_2 MLP_384 ?18 stage4 7?7 patch_embed: Conv-3_8C_2, PA EMSA_8_1 MLP_512 ?2 EMSA_8_1 MLP_512 ?2 EMSA_8_1 MLP_768 ?2 EMSA_8_1 MLP_768 ?2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we conduct experiments on common-used benchmarks, including ImageNet-1k for classification, MS COCO2017 for object detection, and instance segmentation. In the following subsections, we first compared the proposed ResT with the previous state-of-the-arts on the three tasks. Then we adopt ablation studies to validate the important design elements of ResT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Classification on ImageNet-1k</head><p>Settings. For image classification, we benchmark the proposed ResT on ImageNet-1k, which contains 1.28M training images and 50k validation images from 1,000 classes. The setting mostly follows <ref type="bibr" target="#b24">[25]</ref>. Specifically, we employ the AdamW <ref type="bibr" target="#b18">[19]</ref> optimizer for 300 epochs using a cosine decay learning rate scheduler and 5 epochs of linear warm-up. A batch size of 2048 (using 8 GPUs with 256 images per GPU), an initial learning rate of 5e-4, a weight decay of 0.05, and gradient clipping with a max norm of 5 are used. We include most of the augmentation and regularization strategies of <ref type="bibr" target="#b24">[25]</ref> in training, including RandAugment <ref type="bibr" target="#b3">[4]</ref>, Mixup <ref type="bibr" target="#b33">[34]</ref>, Cutmix <ref type="bibr" target="#b32">[33]</ref>, Random erasing <ref type="bibr" target="#b37">[38]</ref>, and stochastic depth <ref type="bibr" target="#b11">[12]</ref>. An increasing degree of stochastic depth augmentation is employed for larger models, i.e., 0.1, 0.1, 0.2, 0.3 for ResT-Lite, Rest-Small, ResT-Base, and ResT-Large, respectively. For the testing on the validation set, the shorter side of an input image is first resized to 256, and a center crop of 224 ? 224 is used for evaluation.</p><p>Results. <ref type="table" target="#tab_1">Table 2</ref> presents comparisons to other backbones, including both Transformer-based ones and ConvNet-based ones. We can see, compared to the previous state-of-the-art Transformer-based architectures with similar model complexity, the proposed ResT achieves significant improvement Compared with the state-of-the-art ConvNets, i.e., RegNet, the ResT with similar model complexity also achieves better performance: an average improvement of 1.7% in terms of Top-1 Accuracy. Note that RegNet is trained via thorough architecture search, the proposed ResT is adapted from the standard Transformer and has strong potential for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object Detection and Instance Segmentation on COCO</head><p>Settings. Object detection and instance segmentation experiments are conducted on COCO 2017, which contains 118k training, 5k validation, and 20k test-dev images. We evaluate the performance of ResT using two representative frameworks: RetinaNet <ref type="bibr" target="#b16">[17]</ref> and Mask RCNN <ref type="bibr" target="#b8">[9]</ref>. For these two frameworks, we utilize the same settings: multi-scale training (resizing the input such that the shorter side is between 480 and 800 while the longer side is at most 1333), AdamW <ref type="bibr" target="#b18">[19]</ref> optimizer (initial learning rate of 1e-4, weight decay of 0.05, and batch size of 16), and 1? schedule (12 epochs). Unlike CNN backbones, which adopt post normalization and can directly apply to downstream tasks.</p><p>ResT employs the pre-normalization strategy to accelerate network convergence, which means the output of each stage is not normalized before feeding to FPN <ref type="bibr" target="#b15">[16]</ref>. Here, we add a layer normalization (LN <ref type="bibr" target="#b0">[1]</ref>) for the output of each stage (before FPN <ref type="bibr" target="#b15">[16]</ref>), similar to Swin <ref type="bibr" target="#b17">[18]</ref>. Results are reported on the validation split.</p><p>Object Detection Results. <ref type="table" target="#tab_2">Table 3</ref> lists the results of RetinaNet with different backbones. From these results, it can be seen that for smaller models, ResT-Small is +3.6 box AP higher (40.3 vs. 36.7) than PVT-T with a similar computation cost. For larger models, our ResT-Base surpassing the PVT-S by +1.6 box AP. Instance Segmentation Results. <ref type="table" target="#tab_3">Table 4</ref> compares the results of ResT with those of previous stateof-the-art models on the Mask RCNN framework. Rest-Small exceeds PVT-T by +2.9 box AP and +2.1 mask AP on the COCO val2017 split. As for larger models, ResT-Base brings consistent +1.2 and +0.9 gains over PVT-S in terms of box AP and mask AP, with slightly larger model size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>In this section, we report the ablation studies of the proposed ResT, using ImageNet-1k image classification. To thoroughly investigate the important design elements, we only adopt the simplest data augmentation and hyper-parameters settings in <ref type="bibr" target="#b9">[10]</ref>. Specifically, the input images are randomly cropped to 224 ? 224 with random horizontal flipping. All the architectures of ResT-Lite are trained with SGD optimizer (with weight decay 1e-4 and momentum 0.9) for 100 epochs, starting from the initial learning rate of 0.1 ? batch_size/512 (with a linear warm-up of 5 epochs) and decreasing it by a factor of 10 every 30 epochs. Also, a batch size of 2048 (using 8 GPUs with 256 images per GPU) is used.</p><p>Different types of stem module. Here, we test three type of stem modules: (1) the first patch embedding module in PVT <ref type="bibr" target="#b27">[28]</ref>, i.e., 4 ? 4 convolution operation with stride 4 and no padding;</p><p>(2) the stem module in ResNet <ref type="bibr" target="#b9">[10]</ref>, i.e., one 7 ? 7 convolution layer with stride 2 and padding 3, followed by one 3 ? 3 max-pooling layer; (3) the stem module in the proposed ResT, i.e., three 3 ? 3 convolutional layers (all with padding 1) with stride 2, stride 1, and stride 2, respectively. We report the results in <ref type="table" target="#tab_4">Table 5</ref>. The stem module in the proposed ResT is more effective than that in PVT and ResNet: +0.92% and +0.64% improvements in terms of Top-1 accuracy, respectively.  Ablation study on EMSA. As shown in <ref type="figure">Figure!3</ref>, we adopt a Depth-wise Conv2d to reduce the computation of MSA. Here, we provide the comparison of more strategies with the same reduction stride s. Results are shown in <ref type="table" target="#tab_5">Table 6</ref>. As can be seen, average pooling achieves slightly worse results (-0.24%) compared with the original Depth-wise Conv2d, while the results of the Max Pooling strategy are the worst. Since the pooling operation introduces no extra parameters, therefore, average pooling can be an alternative to Depth-wise Conv2d in practice.  In addition, EMSA also adding two important elements to the standard MSA, i.e., one 1 ? 1 convolution operation to model the interaction among different heads, and the Instance Normalization(IN) to restore diversity of different heads. Here, we validate the effectiveness of these two settings. Results are shown in <ref type="table" target="#tab_6">Table 7</ref>. We can see, without IN, the Top-1 accuracy is degraded by 0.9%, we attribute it to the destroying of diversity among different heads because the 1 ? 1 convolution operation makes all heads focus on all the tokens. In addition, the performance drops 1.16% without the convolution operation and IN. This can demonstrate that the combination of long sequence and diversity are both important for attention function.</p><p>Different types of positional encoding. In section 2.4, we introduced 3 types of positional encoding types, i.e., the original learnable parameters with fixed lengths <ref type="bibr" target="#b4">[5]</ref> (LE), the proposed group linear mode(GL), and PA mode. These encodings are added/multiplied to the input patch token at the beginning of each stage. Here, we compared the proposed GL and PA with LE, results are shown in <ref type="table" target="#tab_7">Table 8</ref>. We can see, the Top-1 accuracy degrades from 72.88% to 71.54% when the PA encoding is removed, this means that positional encoding is crucial for ResT. The LE and GL, achieve similar performance, which means it is possible to construct variable length of positional encoding. Moreover, the PA mode significantly surpasses the GL, achieving 0.84% Top-1 accuracy improvement, which indicates that spatial attention can also be modeled as positional encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed ResT, a new version of multi-scale Transformer which produces hierarchical feature representations for dense prediction. We compressed the memory of standard MSA and model the interaction between multi-heads while keeping the diversity ability. To tackle input images with arbitrary, we further redesign the positional encoding as spatial attention. Experimental results demonstrate that the potential of ResT as strong backbones for dense prediction. We hope that our approach will foster further research in visual recognition.</p><p>The base structure of ResNet is the residual bottleneck, which can be defined as a stack of one 1 ? 1, one 3 ? 3, and one 1 ? 1 Convolution layer with residual learning. Recent works explore replacing the 3 ? 3 Convolution layer with more complex modules <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15]</ref> or combining with attention modules <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b28">29]</ref>. Similar to vision Transformers, CNNs can also capture long-range dependencies if correctly incorporated with self-attention such as Non-Local or MSA. These studies show that the advantage of CNN lies in parameter sharing and focuses on the aggregation of local information, while the advantage of self-attention lies in the global receptive field and focuses on the aggregation of global information. Intuitively speaking, global and local information aggregation are both useful for vision tasks. Effectively combining global information aggregation and local information aggregation may be the right direction for designing the best network architecture.</p><p>Vision Transformers. Transformer is a type of neural network that mainly relies on self-attention to draw global dependencies between input and output. Recently, Transformer-based models are explored to solve various computer vision tasks such as image processing <ref type="bibr" target="#b2">[3]</ref>, classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref>, and object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>, etc. Here, we focus on investigating the classification vision Transformers. These Transformers usually view an image as a sequence of patches and perform classification with a Transformer encoder. The encoder consists of several Transformer blocks, each including an MSA and an FFN. Layer-norm (LN) is applied before each layer and residual connections are employed in both the self-attention and FFN module.</p><p>Among them, ViT <ref type="bibr" target="#b4">[5]</ref> is the first fully Transformer classification model. In particular, ViT split each image into 14 ? 14 or 16 ? 16 with a fixed length, then several Transformer layers are adopted to model global relation among these tokens for input classification. DeiT <ref type="bibr" target="#b24">[25]</ref> explores the dataefficient training and distillation of ViT. Tokens-to-Tokens (T2T-ViT) <ref type="bibr" target="#b31">[32]</ref> point out that the simple tokenization of input images in ViT fails to model the important local structure (e.g., edges, lines) among neighboring pixels, leading to its low training sample efficiency. Transformer-in-Transformer (TNT) <ref type="bibr" target="#b6">[7]</ref> split each image into a sequence of patches and each patch is reshaped to pixel sequence. After embedding, two Transformer layers are applied for representation learning where the outer Transformer layer models the global relationships among the patch embedding and the inner one extracts local structure information of pixel embedding. Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b27">[28]</ref> follows the ResNet paradigm to construct Transformer backbones, making it more suitable for downstream tasks. MViT <ref type="bibr" target="#b7">[8]</ref> further apply pooling function to reduce computation costs.</p><p>Positional Encoding. Different from CNNs, which can implicitly encode spatial position information by zero-padding <ref type="bibr" target="#b13">[14]</ref>, the self-attention in Transformers has no ability to distinguish token order in different positions. Therefore, positional encoding is essential for the patch embeddings to retain positional information. There are mainly two types of positional encoding most commonly applied in vision Transformers, i.e., absolute positional encoding and relative positional encoding. The absolute positional encoding is used in ViT <ref type="bibr" target="#b4">[5]</ref> and its extended methods, where a standard learnable 1D position embedding is added to the sequence of embedded patches. The relative method is used in BoTNet <ref type="bibr" target="#b21">[22]</ref> and Swin Transformer <ref type="bibr" target="#b17">[18]</ref>, where the split 2D relative position embeddings are added. Generally speaking, the relative positional encodings are better suited for vision tasks, this can be attributed to attention not only taking into account the content information but also relative distances between features at different locations <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Visualization and Interpretation</head><p>Analysis on EMSA. In this part, we measure the diversity of EMSA. To simplify, we apply A ? R k?n?n to denote the attention map, with k be the number of heads in EMSA. Assume A i ? R 1?m is the i-th attention map (after reshape), where m = nn . Then we can compute the cross-layer similarity to measure the diversity of different heads.</p><formula xml:id="formula_9">M ij = A i A T j A i A j<label>(9)</label></formula><p>To thoroughly measure the diversity, we calculate extract three types of A: the attention map before Conv-1 (i.e., QK T / ? d k ), the one after Conv-1 (i.e., Conv(QK T / ? d k )), and the one after Instance Normalization <ref type="bibr" target="#b25">[26]</ref> (i.e., IN(Softmax(Conv(QK T / ? d k )))). We randomly sample 1,000 images from the ImageNet-1k validation set and visualize the mean diversity in <ref type="figure" target="#fig_3">Figure 5</ref>. As shown in <ref type="figure" target="#fig_3">Figure 5b</ref>, although the 1 ? 1 convolution model the interactions among different heads, it impairs the ability of MSA to jointly attend to information from different representation subsets at different positions. After instance normalization (in <ref type="figure" target="#fig_3">Figure 5c</ref>), the diversity ability is restored.</p><p>Interpretabilty. In order to validate the effectiveness of ResT more intuitively, we sample 6 images from the ImageNet-1k validation split. We use Group-CAM <ref type="bibr" target="#b35">[36]</ref> to visualize the heatmaps at the last convolutional layer of ResT-Lite. For comparison, we also draw the heatmaps of its counterpart ResNet-50. As shown in <ref type="figure">Figure 6</ref>, the proposed ResT-Lite can adaptively produce heatmaps according to the image content. <ref type="figure">Figure 6</ref>: Sample visualization on ImageNet-1k val split generated by Group-CAM <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More Experiments</head><p>Comparisons of EMSA and MSA. In this part, we make a quantitative comparison of the performance and efficiency of the two modules on ResT-Lite (keeping other components intact). Experimental settings are the same as Ablation Study (Section 3.37), except for the batch size of MSA, which is 512 since each V100 GPU can only tackle 64 images at the same time. We report the results in <ref type="table" target="#tab_8">Table 9</ref>. Both versions of ResT-Lite share almost the same parameters. However, the EMSA version achieves better Top-1 accuracy (+0.2%) with fewer computations(-0.2G). The actual inference throughput indicates EMSA (1246) is 2.4x faster than the original MSA(512). Therefore, EMSA can capably serve as an effective replacement for MSA. Object Detection. In Section 3.2, we replace the backbone in RetinaNet <ref type="bibr" target="#b16">[17]</ref> with ResT and add a layer normalization <ref type="figure" target="#fig_0">(LN [1]</ref>) for the output of each stage (before FPN <ref type="bibr" target="#b15">[16]</ref>), just like Swin. Here, we further validate the effectiveness of LN. We follow the same setting as Section 3.2. Results are reported in <ref type="table" target="#tab_9">Table 10</ref>. From <ref type="table" target="#tab_9">Table 10</ref>, we can see, LN is indeed matters in downstream tasks. An average +0.8 box AP improvement is achieved with LN for RetinaNet <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Discussions</head><p>Mathematical Definition of GL. Given x with size R n?dm , where n is spatial dimension and d m is the channel dimension, GL first splits x into g non-overlapping groups, x = Concat(x 1 , ? ? ? , x g )</p><p>where the size of x i is n ? dm g . All x i 's are then simultaneously transformed by g linear operations to produce g outputs</p><formula xml:id="formula_11">y i = x i W i<label>(11)</label></formula><p>where W i is the linear operation weight.</p><p>y i 's are then concatenated to produce the final n ? d m output y = Concat(y1, ? ? ? , y g ). In ResT, we set g = d m , i.e., the channel dimension of the input.</p><p>Ablation Study Settings. Note that the settings between ablation study and the main results are different. Here, we give the explanations. We adopt the simplest data augmentation and hyperparameters settings in ResNet <ref type="bibr" target="#b9">[10]</ref> to thoroughly investigate the important components of ResT, i.e., eliminating the influence of strong data augmentation and training tricks. Under this setting, we demonstrate that Vision Transformers can still achieve better results without training tricks. Specifically, the Top-1 accuracy of ResT-Lite is 72.88, which outperforms the ResNet-18 (69.76) by +3.1 improvement. We believe the same setting as ResNet in the ablation study can eliminate the doubt of Vision Transformer to some extent, i.e., the improvements of Vision Transformer over CNN mainly come with strong data augmentation and training tricks. This can promote the ongoing and future research of Vision Transformer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of backbone blocks. Left: A standard ResNet Bottleneck Block [10]. Middle: A Standard Transformer Block. Right: The proposed Efficient Transformer Block. The only difference compared with standard Transformer block is the replacement of the Multi-Head Self-Attention (MSA) with Efficient Multi-head Self-Attention (EMSA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The pipeline of the proposed ResT. Similar to ResNet<ref type="bibr" target="#b9">[10]</ref>, ResT build stages with stacked blocks, making it flexible to serve as the backbone of downstream tasks, such as Object detection, Person ReID, and Instance Segmentation, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Patch and PE in ResT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Attention map visualization of the last blocks of stage 4 of the ResT-Lite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art backbones on ImageNet-1k benchmark. Throughput (images / s) is measured on a single V100 GPU, following<ref type="bibr" target="#b24">[25]</ref>. All models are trained and evaluated on 224?224 resolution. The best records and the improvements over bench-marked ResNets are marked in bold and blue, respectively.</figDesc><table><row><cell>Model</cell><cell cols="3">#Params (M) FLOPs (G) Throughput</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell></cell><cell></cell><cell>ConvNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-18 [10]</cell><cell>11.7</cell><cell>1.8</cell><cell>1852</cell><cell>69.7</cell><cell>89.1</cell></row><row><cell>ResNet-50 [10]</cell><cell>25.6</cell><cell>4.1</cell><cell>871</cell><cell>79.0</cell><cell>94.4</cell></row><row><cell>ResNet-101 [10]</cell><cell>44.7</cell><cell>7.9</cell><cell>635</cell><cell>80.3</cell><cell>95.2</cell></row><row><cell>RegNetY-4G [21]</cell><cell>20.6</cell><cell>4.0</cell><cell>1156</cell><cell>79.4</cell><cell>94.7</cell></row><row><cell>RegNetY-8G [21]</cell><cell>39.2</cell><cell>8.0</cell><cell>591</cell><cell>79.9</cell><cell>94.9</cell></row><row><cell>RegNetY-16G [21]</cell><cell>83.6</cell><cell>15.9</cell><cell>334</cell><cell>80.4</cell><cell>95.1</cell></row><row><cell></cell><cell></cell><cell>Transformer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-S [25]</cell><cell>22.1</cell><cell>4.6</cell><cell>940</cell><cell>79.8</cell><cell>94.9</cell></row><row><cell>DeiT-B [25]</cell><cell>86.6</cell><cell>17.6</cell><cell>292</cell><cell>81.8</cell><cell>95.6</cell></row><row><cell>PVT-T [28]</cell><cell>13.2</cell><cell>1.9</cell><cell>1038</cell><cell>75.1</cell><cell>92.4</cell></row><row><cell>PVT-S [28]</cell><cell>24.5</cell><cell>3.7</cell><cell>820</cell><cell>79.8</cell><cell>94.9</cell></row><row><cell>PVT-M [28]</cell><cell>44.2</cell><cell>6.4</cell><cell>526</cell><cell>81.2</cell><cell>95.6</cell></row><row><cell>PVT-L [28]</cell><cell>61.4</cell><cell>9.5</cell><cell>367</cell><cell>81.7</cell><cell>95.9</cell></row><row><cell>Swin-T [18]</cell><cell>28.29</cell><cell>4.5</cell><cell>755</cell><cell>81.3</cell><cell>95.5</cell></row><row><cell>Swin-S [18]</cell><cell>49.61</cell><cell>8.7</cell><cell>437</cell><cell>83.3</cell><cell>96.2</cell></row><row><cell>Swin-B [18]</cell><cell>87.77</cell><cell>15.4</cell><cell>278</cell><cell>83.5</cell><cell>96.5</cell></row><row><cell>ResT-Lite (Ours)</cell><cell>10.49</cell><cell>1.4</cell><cell>1246</cell><cell cols="2">77.2 (? 7.5) 93.7 (? 4.6)</cell></row><row><cell>ResT-Small (Ours)</cell><cell>13.66</cell><cell>1.9</cell><cell>1043</cell><cell cols="2">79.6 (? 9.9) 94.9 (? 5.8)</cell></row><row><cell>ResT-Base (Ours)</cell><cell>30.28</cell><cell>4.3</cell><cell>673</cell><cell cols="2">81.6 (? 2.6) 95.7 (? 1.3)</cell></row><row><cell>ResT-Large (Ours)</cell><cell>51.63</cell><cell>7.9</cell><cell>429</cell><cell cols="2">83.6 (? 3.3) 96.3 (? 1.1)</cell></row></table><note>by a large margin. For example, for smaller models, ResT noticeably surpass the counterpart PVT architectures with similar complexities: +4.5% for ResT-Small (79.6%) over PVT-T (75.1%). For larger models, ResT also significantly outperform the counterpart Swin architectures with similar complexities: +0.3% for ResT-Base (81.6%) over Swin-T (81.3%), and +0.3% for ResT-Large (83.6%) over Swin-S(83.3%) using 224 ? 224 input.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Object detection performance on the COCO val2017 split using the RetinaNet framework.</figDesc><table><row><cell>Backbones</cell><cell>AP50:95</cell><cell>AP50</cell><cell>AP75</cell><cell>APs</cell><cell>APm</cell><cell>APl</cell><cell>Param (M)</cell></row><row><cell>R18 [10]</cell><cell>31.8</cell><cell>49.6</cell><cell>33.6</cell><cell>16.3</cell><cell>34.3</cell><cell>43.2</cell><cell>21.3</cell></row><row><cell>PVT-T [28]</cell><cell>36.7</cell><cell>56.9</cell><cell>38.9</cell><cell>22.6</cell><cell>38.8</cell><cell>50.0</cell><cell>23.0</cell></row><row><cell>ResT-Small(Ours)</cell><cell>40.3</cell><cell>61.3</cell><cell>42.7</cell><cell>25.7</cell><cell>43.7</cell><cell>51.2</cell><cell>23.4</cell></row><row><cell>R50 [10]</cell><cell>37.4</cell><cell>56.7</cell><cell>40.3</cell><cell>23.1</cell><cell>41.6</cell><cell>48.3</cell><cell>37.9</cell></row><row><cell>PVT-S [28]</cell><cell>40.4</cell><cell>61.3</cell><cell>43.0</cell><cell>25.0</cell><cell>42.9</cell><cell>55.7</cell><cell>34.2</cell></row><row><cell>Swin-T [18]</cell><cell>41.5</cell><cell>62.1</cell><cell>44.1</cell><cell>27.0</cell><cell>44.2</cell><cell>53.2</cell><cell>38.5</cell></row><row><cell>ResT-Base (Ours)</cell><cell>42.0</cell><cell>63.2</cell><cell>44.8</cell><cell>29.1</cell><cell>45.3</cell><cell>53.3</cell><cell>40.5</cell></row><row><cell>R101 [10]</cell><cell>38.5</cell><cell>57.8</cell><cell>41.2</cell><cell>21.4</cell><cell>42.6</cell><cell>51.1</cell><cell>56.9</cell></row><row><cell>PVT-M [28]</cell><cell>41.9</cell><cell>63.1</cell><cell>44.3</cell><cell>25.0</cell><cell>44.9</cell><cell>57.6</cell><cell>53.9</cell></row><row><cell>Swin-S [18]</cell><cell>44.5</cell><cell>65.7</cell><cell>47.5</cell><cell>27.4</cell><cell>48.0</cell><cell>59.9</cell><cell>59.8</cell></row><row><cell>ResT-Large (Ours)</cell><cell>44.8</cell><cell>66.1</cell><cell>48.0</cell><cell>28.3</cell><cell>48.7</cell><cell>60.3</cell><cell>61.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Object detection and instance segmentation performance on the COCO val2017 split using Mask RCNN framework.</figDesc><table><row><cell>Backbones</cell><cell cols="2">AP box AP box 50</cell><cell>AP box 75</cell><cell cols="2">AP mask AP mask 50</cell><cell>AP mask 75</cell><cell>Param (M)</cell></row><row><cell>R18 [10]</cell><cell>34.0</cell><cell>54.0</cell><cell>36.7</cell><cell>31.2</cell><cell>51.0</cell><cell>32.7</cell><cell>31.2</cell></row><row><cell>PVT-T [28]</cell><cell>36.7</cell><cell>59.2</cell><cell>39.3</cell><cell>35.1</cell><cell>56.7</cell><cell>37.3</cell><cell>32.9</cell></row><row><cell>ResT-Small(Ours)</cell><cell>39.6</cell><cell>62.9</cell><cell>42.3</cell><cell>37.2</cell><cell>59.8</cell><cell>39.7</cell><cell>33.3</cell></row><row><cell>R50 [10]</cell><cell>38.6</cell><cell>59.5</cell><cell>42.1</cell><cell>35.2</cell><cell>56.3</cell><cell>37.5</cell><cell>44.3</cell></row><row><cell>PVT-S [28]</cell><cell>40.4</cell><cell>62.9</cell><cell>43.8</cell><cell>37.8</cell><cell>60.1</cell><cell>40.3</cell><cell>44.1</cell></row><row><cell>ResT-Base(Ours)</cell><cell>41.6</cell><cell>64.9</cell><cell>45.1</cell><cell>38.7</cell><cell>61.6</cell><cell>41.4</cell><cell>49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of various stem modules onResT-Lite. Results show that the proposed stem module is more effective than existing ones in PVT and ResNet.</figDesc><table><row><cell>Stem</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>PVT [28]</cell><cell>71.96</cell><cell>89.87</cell></row><row><cell>ResNet [10]</cell><cell>72.24</cell><cell>90.17</cell></row><row><cell>ResT (Ours)</cell><cell>72.88</cell><cell>90.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different reduction strategies of EMSA on ResT-Lite. Results show that Average Pooling can be an alternative to Depthwise Conv2d to make a trade-off.</figDesc><table><row><cell>Reduction</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>DWConv</cell><cell>72.88</cell><cell>90.62</cell></row><row><cell>Avg Pooling</cell><cell>72.64</cell><cell>90.41</cell></row><row><cell>Max Pooling</cell><cell>72.20</cell><cell>89.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">Ablation study results on the important</cell></row><row><cell cols="3">design elements of EMSA on ResT-Lite, includ-</cell></row><row><cell cols="3">ing the 1 ? 1 convolution operation and Instance</cell></row><row><cell cols="2">Normalization in Eq. 4.</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>origin</cell><cell>72.88</cell><cell>90.62</cell></row><row><cell>w/o IN</cell><cell>71.98</cell><cell>90.32</cell></row><row><cell>w/o Conv-1&amp;IN</cell><cell>71.72</cell><cell>89.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell cols="3">Comparison of various positional encod-</cell></row><row><cell cols="2">ing (PE) strategies on ResT-Lite.</cell><cell></cell></row><row><cell>Encoding</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>w/o position</cell><cell>71.54</cell><cell>89.82</cell></row><row><cell>+ LE</cell><cell>71.98</cell><cell>90.32</cell></row><row><cell>+ GL</cell><cell>72.04</cell><cell>90.41</cell></row><row><cell>+ PA</cell><cell>72.88</cell><cell>90.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparison of MSA and EMSA.</figDesc><table><row><cell cols="6">Model #Params (M) FLOPs (G) Throughput Top-1 (%) Top-5 (%)</cell></row><row><cell>MSA</cell><cell>10.48</cell><cell>1.6</cell><cell>512</cell><cell>72.68</cell><cell>90.46</cell></row><row><cell>EMSA</cell><cell>10.49</cell><cell>1.4</cell><cell>1246</cell><cell>72.88</cell><cell>90.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Object Detection.</figDesc><table><row><cell cols="3">Backbones Setting AP50:95</cell></row><row><cell>ResT-Small</cell><cell>w/o LN w LN</cell><cell>39.5 40.3</cell></row><row><cell>ResT-Base</cell><cell>w/o LN w LN</cell><cell>41.2 42.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>We provide the related work and more experimental results to complete the experimental sections of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Related Work</head><p>Convolutional Networks. As the cornerstone of deep learning computer vision, CNNs have been evolved for years and are becoming more accurate and faster. Among them, the ResNet series <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> are the most famous backbone networks because of their simple design and high performance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/2012.00364</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Geoffrey J. Gordon, David B. Dunson, and Miroslav Dud?k</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04-11" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<idno type="arXiv">arXiv:2104.11227</idno>
		<title level="m">Xiong. Multiscale vision transformers</title>
		<editor>Karttikeya Mangalam Yanghao Li Zhicheng Yan Jitendra Malik Christoph Feichtenhofer Haoqi Fan</editor>
		<meeting><address><addrLine>Bo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>abs/1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How much position information do convolutional neural networks encode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10425" to="10433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>abs/2101.11605</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">FCOS: A simple and strong anchor-free object detector. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1711.07971</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Solov2: Dynamic, faster and stronger. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/2101.11986</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Resnest: Split-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno>abs/2004.08955</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Group-cam: Group score-weighted visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing-Long</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2103.13859</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing-Long</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2102.00240</idno>
		<title level="m">Sa-net: Shuffle attention for deep convolutional neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>abs/2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
