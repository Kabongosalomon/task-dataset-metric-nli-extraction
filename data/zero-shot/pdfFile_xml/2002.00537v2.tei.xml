<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards High Performance Human Keypoint Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
							<email>zhe.chen1@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards High Performance Human Keypoint Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human Pose Estimation ? Deep Nerual Networks ? Sub-pixel Refinement ? Context</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM's representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark. The source code will be released at https://github.com/chaimi2013/CCM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b23">(Lin et al., 2014)</ref><p>. (c) Some examples from the MS COCO dataset, where occluded, underexposed, and blurry person instances are very common. Blue and red dots denote the annotated visible and invisible keypoints, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human keypoint detection is also known as human pose estimation (HPE) refers to detecting human body keypoint location and recognizing their categories for each person instance from a given image. It is very useful in many downstream applications such as activity recognition <ref type="bibr" target="#b31">(Ni et al., 2017;</ref><ref type="bibr" target="#b1">Baradel et al., 2018;</ref><ref type="bibr" target="#b25">Liu et al., 2018)</ref>, human-robot interaction <ref type="bibr" target="#b28">(Mazhar et al., 2018;</ref><ref type="bibr" target="#b55">Zhang and Tao, 2020)</ref>, and video surveillance <ref type="bibr" target="#b12">(Hattori et al., 2018;</ref><ref type="bibr" target="#b45">Varadarajan et al., 2018)</ref>. However, HPE is very challenging even for human annotators. For example, almost 50% of instances 1 in the COCO training dataset <ref type="bibr" target="#b23">(Lin et al., 2014)</ref> have at least six unannotated keypoints <ref type="figure">(Figure 1(a)</ref>) and 35% keypoints are unannotated <ref type="figure">(Figure 1(b)</ref>) due to various factors including occlusion, truncation, under-exposed imaging, blurry appearance and low-resolution of person instances. Some examples from the MS COCO dataset are shown in <ref type="figure">Figure 1</ref>.</p><p>Prior methods have made significant progress in this area with the success of deep convolutional neural networks (DCNN) <ref type="bibr" target="#b44">(Toshev and Szegedy, 2014)</ref>, which can be categorized into two groups: top-down methods and bottom-up methods. Top-down methods are composed of two phases, i.e., person detection and keypoint detection, while bottomup methods directly detect all keypoints from the image and associate them with corresponding person instances. Although bottom-up methods <ref type="bibr" target="#b4">(Cao et al., 2017)</ref> are usually fast, top-down methods still dominate the leaderboard of public benchmark datasets like MS COCO due to their high accuracy. Using heatmaps to represent keypoint locations and fully CNNs with an encoder-decoder structure to learn features has gained prominence in recent studies <ref type="bibr" target="#b29">(Newell et al., 2016;</ref><ref type="bibr" target="#b18">Huang et al., 2017)</ref>, since spatial correspondence can be preserved between feature maps and heatmaps. Recently, to learn strong feature representations at multiple resolutions, pyramid-based networks have been proposed <ref type="bibr" target="#b6">Chen et al., 2018b)</ref>. Although these methods improve detection accuracy by learning multi-scale features, there are still some issues to be addressed.</p><p>First, detecting invisible keypoints is more difficult than visible ones due to ambiguous appearance and inconsistent context bodies. How to effectively model multi-source context information to infer the hard keypoints is still underexplored. Second, external datasets such as the AI Challenger dataset 2 , have been used to learn more discriminative feature representations <ref type="bibr" target="#b42">Sun et al., 2019)</ref>. However, they may have different annotation formats with the target set, for example, 17 keypoints for the MS COCO dataset and 14 keypoints for the AI Challenger dataset, or even have no annotations like the MS COCO unlabeled dataset. How to effectively leverage these external datasets to learn human pose configuration and discriminative feature representations for recognizing diverse poses remains challenging. Third, the ground truth keypoint location is annotated in pixel (or sub-pixel) in the high-resolution image plane, while the regression target is usually in the low-resolution heatmap, e.g., 1/4 size of the input image. This scale mismatch of representation will degrade the keypoint detection performance. To address this issue, sub-pixel <ref type="bibr">1</ref> The instances with at least one annotated keypoint are counted. 2 https://challenger.ai/competition/keypoint/ representation or post-processing techniques <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref> have been proposed. Nonetheless, a systematic study of post-processing techniques is still absent and worth further discovering.</p><p>In this paper, we address these issues to improve human keypoint detection by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, inspired by the keypoint detection process carried out by humans, where the "context" contributes to the perception and inference process, we advance the research by studying the role of context information for human keypoint detection. Specifically, we adopt an encoder-decoder network structure and propose a novel cascaded context mixer (CCM) in the decoder. It can efficiently integrate both spatial and channel context information and progressively refine them. Then, we propose a joint training strategy and a knowledgedistilling approach to exploit abundant unlabeled data. Besides, we also propose a hard-negative person detection mining strategy to migrate the inconsistency of person instances between training and testing. These strategies endow the detection network with the capability of learning discriminative features. Third, we present and comprehensively evaluate four sub-pixel refinement techniques for postprocessing keypoint predictions. Extensive experiments on the MS COCO keypoint detection benchmark validate the effectiveness of the proposed CCM model, the training strategies, and the sub-pixel techniques. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark 3 .</p><p>The contributions of this work can be summarized as follows:</p><p>? We devise an effective cascaded context mixer in the decoder which can learn both spatial and channel context information to infer the human body and hard keypoints.</p><p>? We propose several efficient training strategies to guide our model to deal with false-positive person detections and learn discriminative features from diverse poses.</p><p>? We present some sub-pixel refinement techniques to enhance location accuracy and comprehensively evaluate their performance and complementarity.</p><p>2 Related work HPE methods can be grouped into 2D pose estimation <ref type="bibr" target="#b41">(Rogez et al., 2012;</ref><ref type="bibr" target="#b44">Toshev and Szegedy, 2014;</ref><ref type="bibr" target="#b29">Newell et al., 2016;</ref><ref type="bibr" target="#b9">Fang et al., 2017;</ref><ref type="bibr" target="#b18">Huang et al., 2017;</ref><ref type="bibr" target="#b4">Cao et al., 2017;</ref><ref type="bibr" target="#b50">Yang et al., 2017;</ref><ref type="bibr" target="#b48">Xiao et al., 2018;</ref><ref type="bibr" target="#b43">Sun et al., 2018;</ref><ref type="bibr" target="#b6">Chen et al., 2018b;</ref><ref type="bibr" target="#b42">Sun et al., 2019;</ref><ref type="bibr" target="#b54">Zhang et al., 2019a;</ref><ref type="bibr" target="#b22">Li et al.,</ref>   <ref type="bibr">@MPII Cao et al. (2017)</ref> VGG-19 multi-stage CNN --61. <ref type="bibr">8AP@COCO Newell et al. (2017)</ref> Hourglass --multiscale average 65.5AP@COCO</p><p>Top-down <ref type="bibr" target="#b14">He et al. (2017)</ref> ResNet-50-FPN conv+deconv -offset regression 63. <ref type="bibr">1AP@COCO Fang et al. (2017)</ref> STN+Hourglass --parametric NMS 63. <ref type="bibr">3AP@COCO Papandreou et al. (2018)</ref> ResNet-101 1x1 conv offset regression 68. <ref type="bibr">5AP@COCO Chen et al. (2018b)</ref> ResNet-Inception GlobalNet -flip/GF 72. <ref type="bibr">1AP@COCO Xiao et al. (2018)</ref> ResNet-152 deconv -flip/sub-pixel shift 76. <ref type="bibr">5AP@COCO Sun et al. (2019)</ref> HRNet-w48 1x1 conv -flip/sub-pixel shift 77. <ref type="bibr">0AP@COCO Li et al. (2019)</ref> 4 x ResNet-50 GlobalNet flip/GF/sub-pixel shift 78.1AP@COCO</p><p>Our CCM ResNet-152 CCM sub-pixel refinement 78.9AP@COCO HRNet-w48 2019; <ref type="bibr" target="#b11">Girdhar et al., 2018)</ref> and 3D pose estimation <ref type="bibr" target="#b41">(Rogez et al., 2012;</ref><ref type="bibr">Pavlakos et al., 2018b,a;</ref><ref type="bibr" target="#b40">Rhodin et al., 2018;</ref><ref type="bibr" target="#b16">Hossain and Little, 2018;</ref><ref type="bibr" target="#b51">Yang et al., 2018)</ref> according to the dimension of the coordinates of the keypoint locations. In this paper, we focus on 2D pose estimation, specifically, the multi-person pose estimation (MPPE) problem, which is more challenging than the single-person pose estimation (SPPE) problem. MPPE approaches can be further divided into bottom-up and top-down approaches. A summary of these approaches is presented in <ref type="table" target="#tab_0">Table 1</ref>, where we outline their features according to the network structure, whether using extra data or not, and the postprocessing techniques used to improve detection accuracy. The details are presented in the following part.</p><p>Bottom-up approaches first detect all human keypoints and then associate them with each detected person instance <ref type="bibr" target="#b4">(Cao et al., 2017;</ref><ref type="bibr" target="#b30">Newell et al., 2017;</ref><ref type="bibr" target="#b38">Pishchulin et al., 2016)</ref>. This approach is usually faster than top-down approaches, but the assembly step can become intractable when person instances are ambiguous due to occlusions, blur, etc., degrading accuracy compared to top-down approaches. Top-down approaches first detect all person instances in the image and then apply SPPE on each detected person. Benefiting from recent progress in DCNN-based object detection <ref type="bibr" target="#b39">(Ren et al., 2015;</ref><ref type="bibr" target="#b32">Ouyang et al., 2016;</ref><ref type="bibr" target="#b14">He et al., 2017;</ref><ref type="bibr" target="#b24">Lin et al., 2017;</ref><ref type="bibr" target="#b26">Liu et al., 2020;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>, person detectors have achieved promising detection accuracy. Further, different neural networks have been proposed to learn strong feature representations based on multi-scale feature fusion and multi-level supervisions, e.g., the Pyramid Residual Module in , the Cascaded Pyramid Network in <ref type="bibr" target="#b6">(Chen et al., 2018b)</ref>, the simple baseline model in , and the High-resolution Net in , which detect keypoint locations with high accuracy. Our proposed method follows the top-down scheme and has an encoder-decoder structure. However, in contrast to the above methods, we study the role of context information for human keypoint detection by devising a cascaded context mixer module in the decoder to sequentially capture both spatial and channel context information.</p><p>Deep neural models benefit from a large scale of training data and efficient training strategies. Trained on more examples with diverse poses, the keypoint detection model can learn to infer the occluded or blurry keypoints from similar poses <ref type="bibr" target="#b22">Li et al., 2019)</ref>. For example, all the top entries of the COCO keypoint detection leaderboard 4 leverage external data such as the AI Challenger human keypoint detection dataset. In this paper, we also validate the benefit of using external data. However, instead of using transfer learning, we propose a more effective joint training strategy to harvest external data with heterogeneous labels. Further, we make use of unlabeled data, e.g., the unlabeled MS COCO dataset, referring to the knowledge distilling idea, where the pseudo-labels are generated by a teacher model. Besides, few of the above approaches deal with the mismatch problem of person detections during the training phase and testing phase, i.e., training with ground truth person instances while testing with detected ones, which may be false positives. In this paper, we propose an effective hard-negative person detection mining strategy in the training phase, adapting the model to predict no keypoints for those false person instances.</p><p>Dominant methods adopt a heatmap to represent the keypoint location where a Gaussian density map is placed on the corresponding pixel. However, the heatmap is usually in low-resolution compared with the input, which has a side effect on the location accuracy. Increasing the heatmap resolution means to decode high-resolution features, which may incur extra computational cost and model complexity. Instead, prior methods adopt computationally efficient postprocessing techniques to refine the predictions, for example, shifting the detected location by 0.25 pixels according to the local gradient directions <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref><ref type="bibr" target="#b48">Xiao et al., 2018;</ref><ref type="bibr" target="#b42">Sun et al., 2019)</ref>. In contrast to them, we present a sub-pixel refinement technique using the second-order approximation, it turns out to be more effective than the above one and boost the detection accuracy by a large margin. Besides, we comprehensively study the sub-pixel refinement techniques for postprocessing including the second-order approximation (SOA), the Soft Non-Maximum Suppression (Soft-NMS), the sub-pixel shift of flipped heatmaps (SSP), and the Gaussian filtering on heatmaps (GF). Experiment results validate that these techniques can significantly boost the keypoint detection performance. Moreover, they are complementary to each other, and so the combination of them will boost the performance further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cascaded Context Mixer for Human Keypoint Detection</head><p>In this part, we propose a novel human keypoint detection model based on a cascaded context mixer module to explicitly and simultaneously model spatial and channel context information. To further exploit the representation capacity of the model, we propose three efficient training strategies including a hard-negative person detection mining strategy to migrate the mismatch between training and testing, a joint-training strategy to use abundant unlabeled samples by knowledge distilling, and a joint-training strategy to exploit external data with heterogeneous labels. To improve the detection accuracy, we also present four postprocessing techniques to refine predictions at the sub-pixel level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Since occlusions are ubiquitous in and between human bodies, we take it as an example to show how humans carry out the detection process when dealing with the aforementioned hard cases. As shown in <ref type="figure">Figure 2</ref>(a), occlusions include selfocclusion (A, C, D, E, F), occlusion by others (C, G), and truncation <ref type="bibr">(B, H)</ref>. Note that all the faces are self-occluded in <ref type="figure">Figure 2</ref>(a), i.e., half of each face is invisible. Humans can easily recognize a complete object and its boundaries, even if it is occluded. According to Gestalt psychology in visual perception, we can group fragmented contours under the law of closure <ref type="bibr" target="#b46">(Wagemans et al., 2012)</ref>, e.g., D and E. We have also seen numerous human body positions and acquired the common sense that a body consists of symmetric legs, hands, and face and that different body parts move and form different poses. Therefore, we can easily infer the occluded parts like A, B, C, G, and H. For the more difficult case F, we may infer the invisible arms by judging the boy's intention and rehearsing the same action psychologically.</p><p>The Recognition-by-Components (RBC) theory tells us that humans quickly recognize objects even under occlu- sions by characterizing the object's components <ref type="bibr" target="#b2">(Biederman, 1987)</ref>. One possible path of the keypoint detection process carried out by humans could be divided into four main stages as shown in <ref type="figure">Figure 2</ref>(b). First, we recognize and locate a human body (The top-down approaches follow this paradigm <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref><ref type="bibr" target="#b48">Xiao et al., 2018;</ref><ref type="bibr" target="#b42">Sun et al., 2019)</ref>). Then, we recognize each body part to further locate the keypoints belonging to it (Some graph-based models complete this step explicitly <ref type="bibr" target="#b10">(Felzenszwalb et al., 2008;</ref><ref type="bibr" target="#b15">Holt et al., 2011;</ref><ref type="bibr" target="#b47">Wang and Li, 2013;</ref><ref type="bibr" target="#b52">Yang and Ramanan, 2013)</ref>). Here, we can easily identify some distinct and visible keypoints. Finally, we infer the remaining keypoints which may be invisible or ambiguous. How do we accomplish this? By reviewing the inference process and the occlusion cases in <ref type="figure">Figure 2</ref>(a), we think that the "context" plays an important role when we associate separate body parts into a whole or infer an invisible keypoint <ref type="bibr" target="#b27">Ma et al., 2020)</ref>. Another key factor may be that we have a priori knowledge of human body configurations in all possible poses. The context tells us about the surrounding visible body parts, and the a priori knowledge helps us to determine what the category and location of the invisible part should be. This motivates us to design a context-aware model that can efficiently learn useful feature representation from diverse poses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cascaded Context Mixer-based Decoder</head><p>In this paper, we tackle the multi-person pose estimation problem by following a topdown scheme. First, a human detector is used to detect the bounding box for each person instance. Then, the proposed CCM model detects keypoints for each person instance. After aggregating the detections using Object Keypoint Similarity (OKS)-based Non-Maximum Suppression (NMS), we obtain the final pose estimation. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the CCM model has an encoder-decoder structure where we use ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref> or HRNet  as the backbone encoder network. The decoder consists of three Context Mixer (CM) modules in a cascaded manner. The details of CM are presented as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Context Mixer</head><p>One the one hand, as we know that DCNN is able to learn distributed semantics at each feature channel, which leads to powerful representation ability to recognize a vast number of categories. In <ref type="figure" target="#fig_2">Figure 3</ref>, we visualized some feature channels from the ResNet-50 encoder. As can be seen, the encoder learned to activate one or multiple explicit body parts. On the other hand, as discussed in Section 3.1, context information could be useful to infer invisible keypoints and help to learn human body configuration. Thereby, we exploit two types of context information in this paper, i.e., global context and local context.</p><p>First, since different feature channels correspond to certain body parts <ref type="figure" target="#fig_2">(Figure 3</ref>(a) and <ref type="figure" target="#fig_2">Figure 3</ref>(c)), the relationship between them can be modeled to further enhance the feature representation. To this end, we use a channel attention module to encode the relationship between different channels, which inherits the idea of squeeze-and-excitation (SE) network <ref type="bibr" target="#b17">(Hu et al., 2018)</ref>. The attention vector is used to reweigh features in a channel-wise manner, which is a global operation from the perspective of spatial dimension. In this way, we can extract the global context to enhance the feature representation. Second, since some of the feature responses at different body parts have strong correlations <ref type="figure" target="#fig_2">(Figure 3</ref>(b) and <ref type="figure" target="#fig_2">Figure 3(d)</ref>), we can encode the spatial relationship by using convolution layers with large receptive fields such that they can cover different body parts. To this end, we leverage hybrid-dilated convolutions to capture multi-scale spatial context information within different receptive fields, which inherits the idea of atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b5">(Chen et al., 2018a)</ref>. These two types of context information collaborate with each other to learn discriminative feature representations. Besides, we also use a residual branch to reuse the learned features from the previous stage. Thereby, there are three branches in CM as shown in the middle part of <ref type="figure" target="#fig_3">Figure 4</ref>. We present the details of each branch as follows.</p><p>In the residual branch of the k th CM, feature maps f CM k?1 from the previous stage are first up-sampled two times before being fed into a 1 ? 1 convolutional layer to output feature maps f RES</p><formula xml:id="formula_0">k of size H k ? W k ? C k , i.e., f RES k = ? RES k f CM k?1 ? 2 ,<label>(1)</label></formula><p>where ? 2 denotes the up-sampling operation, ? RES k (?) denotes the function learned by the convolutional layer.</p><p>In the SE branch of the k th CM, the feature maps f CM k?1 first go through a global pooling layer. Then, the obtained feature vector is fed into a bottle-neck layer with 1 ? 1 convolutions. The feature dimension is reduced to 1 ? 1 ? C k /4. Then, it is fed into a subsequent 1 ? 1 convolutional layer to increase the feature dimension to 1 ? 1 ? C k . A sigmoid function is used to squeeze the feature vector f SE k into the range [0, 1], i.e.,</p><formula xml:id="formula_1">? SE k = ? ? SE k GP f CM k?1 ,<label>(2)</label></formula><p>where GP (?) denotes the global pooling operation, ? SE k (?) denotes the function learned by those two 1?1 convolutional layers, and ? (?) denotes the sigmoid activation function.</p><p>In the HDC branch of the k th CM, the feature maps f CM k?1 go through four 3 ? 3 convolutional layers with different dilated rates, i.e., 1, 2, 3, and 4. Each convolutional layer has C k /4 kernels. These feature maps are then concatenated and fed into a deconvolutional layer of stride 2. The output feature maps f HDC</p><formula xml:id="formula_2">k are of size H k ? W k ? C k , i.e., f HDC k = ? HDC k ? d1 k f CM k?1 ; . . . ; ? d4 k f CM k?1 ,<label>(3)</label></formula><p>where ? d1 k (?) ? ? d4 k (?) denote functions learned by the dilated convolutional layers, ? HDC k (?) denotes the function  learned by the deconvolutional layer, and [; ] denotes the concatenation operation. Then, the output of the k th CM is calculated as:</p><formula xml:id="formula_3">f CM k = f HDC k ? SE k + f RES k ? CM k f CM k?1 ,<label>(4)</label></formula><p>where denotes the channel-wise multiplication and ? CM k (?) denotes the mapping function learned by the k th CM. Batch normalization <ref type="bibr" target="#b19">(Ioffe and Szegedy, 2015)</ref> is used after each convolutional layer and deconvolutional layer. ReLU is used after the first convolutional layer in the SE branch, all the convolutional layers in the HDC branch, and the output of each CM module <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cascaded Context Mixer</head><p>To capture the context information at multiple resolutions and learn multi-scale feature representation, we stack K CMs sequentially to decode the features from the encoder step by step and increase their resolutions accordingly. Mathematically, it can be written as:</p><formula xml:id="formula_4">f EN C = ? EN C (img) ,<label>(5)</label></formula><formula xml:id="formula_5">f DEC = ? CM K . . . ? CM 1 f EN C ? DEC f EN C<label>(6)</label></formula><p>where img represents the input image, ? EN C (?) denotes the function learned by the encoder, f EN C is the encoded feature, ? DEC (?) denotes the function learned by the decoder, f DEC is the decoded feature. Note that we denote f CM 0 f EN C for consistency. The decoded feature f DEC is fed into a final 1 ? 1 convolutional layer to predict the target heatmaps:</p><formula xml:id="formula_6">h = ? P RE f DEC ,<label>(7)</label></formula><p>where ? P RE (?) denotes the function learned by the prediction layer and h represents the predicted heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Auxiliary Decoder and Intermediate Supervision</head><p>Deep supervision <ref type="bibr" target="#b21">(Lee et al., 2015)</ref> refers to the technique that adds auxiliary supervision on some intermediate layers within a deep neural network. It facilitates multi-scale and multi-level feature learning by allowing error information back-propagation from multiple paths and alleviating the problem of vanishing gradients in deep neural networks. Leveraging the deep supervision idea, we also add an auxiliary decoder ? DEC aux (?) after the penultimate stage of the encoder. Its structure is identical to ? DEC (?) described above. These two decoders do not share weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training objective</head><p>The ground truth heatmap is constructed by placing a Gaussian peak at each keypoint's location in the image plane. The number of heatmaps is identical to the number of keypoints predefined in the dataset, for example, 17 for the MS COCO dataset <ref type="bibr" target="#b23">(Lin et al., 2014)</ref> and 14 for the AI Challenger dataset. We use an MSE loss to supervise the network during training. Mathematically, it is defined as:</p><formula xml:id="formula_7">L main = 1 H K W K C i,j,c h (i, j, c) ? h GT (i, j, c) 2 ,<label>(8)</label></formula><p>where C is the number of heatmaps, i, j, and c are the spatial and channel index, h and h GT are the predicted and ground truth heatmaps, respectively. Similar to Eq. <ref type="formula" target="#formula_7">(8)</ref>, an extra MSE loss L aux is also added to the auxiliary decoder as an intermediate supervision. The final training objective is defined as the weighted sum of both losses:</p><formula xml:id="formula_8">L = L main + ?L aux ,<label>(9)</label></formula><p>where ? is the weight for the auxiliary loss. 4 Learning from diverse poses with efficient strategies CCM's capacity to model context information and learn discriminative feature representation can be exploited by learning from massive training samples with diverse poses.</p><p>In this paper, we propose a hard-negative person detection mining strategy to migrate the mismatch problem of person detections during the training phase and testing phase, a joint-training strategy on unlabeled samples by knowledge distilling, and a joint-training strategy on external data with heterogeneous labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hard-Negative Person Detection Mining (HNDM)</head><p>Top-down approaches detect person instances before detecting keypoints on them. On the one hand, although modern detection models have achieved a good detection performance, they may still produce some false positive detections due to occlusion, similar appearances, etc. On the other hand, the keypoint detection model is usually trained with ground truth bounding box annotations enclosing exact person instances. It has never seen any false positive detections during the training phase. Therefore, there is a mismatch between training and testing, which may lead to incorrect keypoint predictions for those false positive person detections. To address this issue, we propose a hard-negative person detection mining strategy.</p><p>First, we trained a Mask R-CNN on the MS COCO training set containing only the category of person. ResNeXt152 was used as the backbone network. It achieved a mean average precision (AP) of 60.4 on the COCO minival dataset. Then, we evaluated the training set using the detection model and screened out those detections with sufficiently high scores, e.g., ? 0.5, but no intersections with ground truth person instances. These were treated as hard-negative detections in this paper. Some examples are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. During training CCM, these hard-negative detections could be added to the training set. Their keypoint heatmaps are set to all-zero maps. In this way, CCM learns to predict no keypoints on those false "person instances".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Joint-Training on Unlabeled Samples by Knowledge Distilling</head><p>To increase pose diversity in the training samples, we leverage the MS COCO unlabeled dataset, which contains over 120k images. Since there are no person and keypoint annotations, we generate pseudo labels by referring to the knowledge distilling idea. First, we used the above-trained person detector to detect all possible person instances within the unlabeled images. Then, we screened out those detections with scores above a predefined threshold, which is determined by guaranteeing the number of average person instances per image to be identical to the one calculated from the ground truth annotations of the MS COCO training set. In our case, this threshold was 0.9924. Next, we trained several keypoint detection models using ResNet152 and HRNet-w48 as the backbone encoder network and used them to detect keypoints on the person detections obtained from the previous stage. We fused the predictions by different models, kept all keypoints with scores above 0.9 as the pseudo labels, and treated the rest as unlabeled. In this way, we distill the learned "knowledge" in the keypoint detection model to the pseudo labels of unlabeled training samples and use them to supervise the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint-Training on External Data with Heterogeneous Labels</head><p>Different datasets may not share the same annotation norms, even if they are used for the same purpose. For instance, 17 keypoints are used to define a human skeleton in the MS COCO dataset <ref type="bibr" target="#b23">(Lin et al., 2014)</ref>, but only 14 in AI Challenger (AIC). Five keypoints correspond to the eyes, ears, and nose in MS COCO, while only the "top of head" is annotated in AIC. AIC has a keypoint annotation for the neck, which is absent in MS COCO. The other 12 keypoints corresponding to limbs are the same in both datasets. To use AIC with MS COCO, a common practice is to train a network on AIC and then change the number of channels in the final prediction layer and fine-tune this network on MS COCO. In this paper, we propose a simple but effective joint-training strategy that mixes the training samples in both datasets to leverage the diverse poses simultaneously. To make the training tractable, we align the labels in AIC with the ones in MS COCO by keeping the 12 common annotations and discarding the others.</p><p>To further adapt the trained model to the MS coco dataset, we can also add a finetune stage. In conclusion, the training strategies described above can be summarized as:</p><formula xml:id="formula_9">T rain |? ? F inetune |? ,<label>(10)</label></formula><p>where ? ? {A, AC, ACH, ACHU, CHU }, ? ? {C, CH}, A denotes the AIC training dataset, C denotes the COCO training set, H denotes the hard-negative training samples, and U denotes the reprocessed unlabeled training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Sub-pixel Refinement Techniques for Postprocessing</head><p>To migrate the scale mismatch of representation between the high-resolution ground-truth keypoint location and the corresponding position on the low-resolution heatmap, different sub-pixel refinement techniques have been introduced, e.g., the 0.25-pixel shift of the maximum response pixel <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref><ref type="bibr" target="#b48">Xiao et al., 2018;</ref><ref type="bibr" target="#b42">Sun et al., 2019;</ref><ref type="bibr" target="#b22">Li et al., 2019)</ref>, the one-pixel shift of flipped heatmap <ref type="bibr" target="#b42">Sun et al., 2019)</ref>, and Gaussian filtering of predicted heatmap <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref><ref type="bibr" target="#b22">Li et al., 2019)</ref>. With this regard, we devise four sub-pixel refinement techniques by (1) exploring the second-order approximation to locate the maximum response at the sub-pixel level accuracy, (2) introducing the Soft Non-Maximum Suppression (Soft-NMS) to refine the maximum response's location instead of the original NMS, (3) extending the one-pixel shift of flipped heatmap to a general sub-pixel form, and (4) applying the Gaussian filter on predicted heatmaps. They are complementary (or orthogonal) to each other and can be conducted sequentially at different stages of the inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sub-pixel Refinement by the Second-Order Approximation</head><p>Sub-pixel refinement by the second-order approximation has ever been used in the depth super-resolution and disparity calculation literature <ref type="bibr" target="#b49">(Yang et al., 2007)</ref>. They use the cost volume at different disparities to find the optimal disparity that minimizes the inconsistency between the left and right views. Therefore, they face the issue to estimate the sub-pixel disparity given the costs at integer disparities. Likewise, we can adopt such a technique to estimate the sub-pixel keypoint location given the predicted heatmaps. In contrast to the one-dimensional cost volume, the heatmaps are in the two-dimensional space. Therefore, we present two kinds of second-order approximation by either using a parabola approximation for each dimension or using a paraboloid approximation simultaneously for the two dimensions. The sub-pixel refinement by the second order approximation, i.e., the case of parabola.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">The Parabola Approximation</head><p>As we know that the heatmap is a Gaussian function w.r.t. the pixel dimension, nevertheless, it is reasonable to approximate it with a parabola function in a local neighborhood of the maximum pixel as shown in <ref type="figure">Figure 6</ref>:</p><formula xml:id="formula_10">z (x) = ax 2 + bx + c,<label>(11)</label></formula><p>where a, b, and c are the coefficients of the parabola. The maximum z is subject to the first order condition:</p><formula xml:id="formula_11">?z ?x = 2ax + b = 0.<label>(12)</label></formula><p>Therefore, the maximum z is reached at x * = ? b 2a . Given z 1 is the maximum pixel at x 0 , z 0 and the z 2 is the heatmap response at x 0 ? 1 and x 0 + 1, we can calculate x * as:</p><formula xml:id="formula_12">x * = x 0 + z 0 ? z 2 2 (z 0 + z 2 ? 2z 1 ) .<label>(13)</label></formula><p>The second term in the right-hand side (RHS) of Eq. <ref type="formula" target="#formula_0">(13)</ref> is the sub-pixel shift from the detected maximum response pixel x 0 to the underlying maximum one x * . Similarly, we can calculate the optimal y * along the vertical dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">The Paraboloid Approximation</head><p>Since the heatmap is represented as a two-dimension Gaussian function, we can approximate it with a paraboloid function in a local neighborhood of the maximum pixel as shown in <ref type="figure" target="#fig_6">Figure 7</ref>:</p><formula xml:id="formula_13">z (x, y) = ax 2 + by 2 + cxy + dx + ey + f,<label>(14)</label></formula><p>where a, b, c, d, e, and f are the coefficients of the paraboloid. The maximum z is subject to the first order condition: Therefore, the maximum z is reached at:</p><formula xml:id="formula_14">x * = 2bd?ce c 2 ?4ab y * = 2ae?cd c 2 ?4ab .<label>(16)</label></formula><p>As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, z 11 is the maximum in the heatmap at (x 0 , y 0 ), z 00 ? z 22 are its 8-neighbor heatmap responses.</p><p>Assuming that the coordinate original is at (x 0 , y 0 ), we can calculate the coefficients a ? e as:</p><formula xml:id="formula_15">a = 1 8 [2 (z 12 + z 10 ? 2z 11 ) + (z 02 + z 00 ? 2z 01 ) + (z 22 + z 20 ? 2z 21 )] ,<label>(17)</label></formula><formula xml:id="formula_16">b = 1 8 [2 (z 01 + z 21 ? 2z 11 ) + (z 00 + z 20 ? 2z 10 ) + (z 02 + z 22 ? 2z 12 )] ,<label>(18)</label></formula><formula xml:id="formula_17">c = 1 4 (z 00 + z 22 ? z 02 ? z 20 ) ,<label>(19)</label></formula><formula xml:id="formula_18">d = 1 8 [(z 02 ? z 00 ) + (z 22 ? z 20 ) + 2 (z 12 ? z 10 )] , (20) e = 1 8 [(z 20 ? z 00 ) + (z 22 ? z 02 ) + 2 (z 21 ? z 01 )] ,<label>(21)</label></formula><p>Therefore, the maximum z is reached at:</p><formula xml:id="formula_19">x * = x 0 + 2bd?ce c 2 ?4ab y * = y 0 + 2ae?cd c 2 ?4ab .<label>(22)</label></formula><p>The second terms in the RHS of Eq. <ref type="formula" target="#formula_1">(22)</ref> is the sub-pixel shift from the detected maximum response pixel x 0 (resp. y 0 ) to the underlying maximum one x * (resp. y * ). Given the heatmap, after locating the maximum pixel, we calculate the optimal location according to Eq. (13) or Eq. <ref type="formula" target="#formula_1">(22)</ref>. z=ax 2 +bx+c z=ax 2 +by 2 +cxy+dx+ey+f Soft NMS <ref type="figure">Fig. 8</ref>: The sub-pixel refinement by Soft NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sub-pixel Refinement by Soft-NMS</head><p>During the inference phase of top-down approaches, there may be several bounding boxes detected around a person instance. Consequently, we may estimate several poses on them. Similar to the Non-Maximum Suppression (NMS) postprocessing step used in object detection, NMS is also applied to the detected poses <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref><ref type="bibr" target="#b48">Xiao et al., 2018;</ref><ref type="bibr" target="#b42">Sun et al., 2019)</ref>. Different from the Intersection over Union (IoU) used to compare the overlap between two bounding boxes, the Object Keypoint Similarity (OKS)based IOU (OKS-IOU) is used to compare two poses. The original NMS is to filter out all the poses that have sufficient large OKS-IOUs with top-ranked detection. We argue that those poses can also be treated as reasonable estimates, which can be used to get a more stable result. Therefore, we present Soft-NMS for sub-pixel refinement as follows. As shown in <ref type="figure">Figure 8</ref>, the poses marked in red, blue, and green dots are three estimates and the red pose has the highest score. We can calculate the fusion results by leveraging the OKS-IOU as the fusion weight, i.e.,</p><formula xml:id="formula_20">p * i = j??i IOU OKS ij p j j??i IOU OKS ij ,<label>(23)</label></formula><p>where ? i is the index set of poses to be filtered given the topranked detection p i , IOU OKS ij is the OKS-IOU between p i and p j . Note that we treat i ? ? i and set IOU OKS ii = 1. We can re-write Eq. (23) as:</p><formula xml:id="formula_21">p * i = p i + j??i IOU OKS ij (p j ? p i ) j??i IOU OKS ij ,<label>(24)</label></formula><p>where the second term in the RHS is the sub-pixel shift from the top-ranked detection p i to the underlying best one p * i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Gaussian Filtering on Heatmaps</head><p>The regressed heatmap may not be as smooth as the ground truth Gaussian density map. A Gaussian filter-based post- processing technique is proposed to smooth it and minimize the variance of the prediction <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref><ref type="bibr" target="#b22">Li et al., 2019)</ref>. We evaluate this technique and compare it with other sub-pixel refinement techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sub-pixel Shift of Flipped Heatmaps</head><p>During the inference phase, some methods predict the pose from the flipped image and average the flipped heatmap with the original one to get the final prediction <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref><ref type="bibr" target="#b48">Xiao et al., 2018)</ref>. However, since the flipped heatmap is not aligned with the original one, one common practice is to shift the flipped heatmap by one pixel.</p><p>In this paper, we extend the one-pixel shift to the subpixel shift as shown in <ref type="figure" target="#fig_7">Figure 9</ref>. First, we resize the flipped heatmap by a scale ratio r (r ? 1) along the horizontal axis. Then, we shift it by one pixel to the right. Then, we resize it back to the original size. As can be seen, the effective shift becomes 1/r pixel. We name it as the sub-pixel shift (SSP) in this paper. Note the above process is equivalent to a linear interpolation of the original heatmap and its one-pixel shifted version, i.e.,</p><formula xml:id="formula_22">h * f (i, j) = 1 ? 1 r h f (i, j) + 1 r h f (i, j ? j 0 ) ,<label>(25)</label></formula><p>where h f is the flipped heatmap, j 0 is the maximum shifted pixels, i.e., j 0 = 1 in this paper. h * f is the sub-pixel estimate. It becomes the one-pixel shift technique when r = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We conducted extensive experiments to demonstrate the effectiveness of the proposed model. First, comprehensive ablation studies on the components of CM were presented, followed by the comparative studies of the proposed training strategies and sub-pixel refinement techniques. Next, we compared the proposed model with representative state-ofthe-art methods in terms of detection accuracy, model complexity, and computational cost. Then, we presented some visual examples of the detection results by our model and explained the detection process by inspecting the learned features at each stage. Finally, we empirically studied the impact of visible and invisible annotations in our model and obtained useful some insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental settings</head><p>Datasets: The COCO Keypoint Challenge addresses multiperson pose estimation in challenging uncontrolled conditions <ref type="bibr" target="#b23">(Lin et al., 2014)</ref>. The dataset is split into training, minival, test-dev, and test-challenge sets. The training set includes 118k images and 150k person instances, the minival dataset includes 5000 images, and the test-dev set includes 20k images. It also provides an unlabeled dataset containing 123k images. 110k person instances and corresponding keypoints were detected using the method described in Section 4.2. The external dataset from AIC contains a training set with 237k images and 440k person instances and a validation set with 3000 images. We also evaluated CCM and the baseline model on the recently proposed OCHuman benchmark <ref type="bibr" target="#b55">(Zhang et al., 2019b)</ref> comprising heavilyoccluded human instances to compare their performance on handling occluded cases. This dataset contains 8110 human instances with detailed keypoint annotations like COCO. It is divided into two subsets: OCHuman-Moderate and OCHuman-Hard. The first subset contains instances with MaxIoU in the range of 0.5 and 0.75, while the second contains instances with MaxIoU larger than 0.75. MaxIoU denotes the max IoU of a person with others in an image.</p><p>Evaluation metrics: We report the main results based on the object keypoint similarity (OKS)-based mean average precision (AP) over 10 OKS thresholds, where OKS defines the object keypoint similarity between different human poses. They are calculated as follows <ref type="bibr" target="#b23">(Lin et al., 2014)</ref>: AP = mean AP @(0.50:0.05:0.95) ,</p><formula xml:id="formula_23">AP @s = p ? (OKS p &gt; s) p 1 ,<label>(26)</label></formula><formula xml:id="formula_24">OKS p = i exp ?d 2 pi 2a 2 p ? 2 i ? (v pi &gt; 0) i ? (v pi &gt; 0) ,<label>(27)</label></formula><p>where p is the person instance index, i is the keypoint index, ? (?) is the Kronecker function. ? (?) = 1 if the condition holds, otherwise 0. s is a threshold, d pi is the Euclidean distance between the predicted i th keypoint of the person instance p and its ground truth, a p is the area of the person instance p, ? i is the normalization factor predefined for each keypoint type, and v pi is the visible status. Implementation details: The feature dimension C i of each CM was set to 256 for ResNet-50 and 128, 96, 64 for ResNet-152, 32 for HRNet-w32, and 48 for HRNet-w48. All backbone networks were pre-trained on the ImageNet dataset <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>. Gaussian initialization was used for convolutional and deconvolutional layers in the decoder. The weights and bias in BatchNorm layers were initialized as 1 and 0, respectively. CCM was implemented in Pytorch <ref type="bibr" target="#b35">(Paszke et al., 2017)</ref> and trained on four NVIDIA Tesla V100 GPUs using the Adam optimizer. Hyper-parameters were set by following <ref type="bibr" target="#b42">Sun et al., 2019)</ref>. We used the detection results on the minival set and testdev set released in  for a fair comparison, which were obtained by a faster-RCNN detector <ref type="bibr" target="#b39">(Ren et al., 2015)</ref> with detection AP 56.4 for the person category on COCO val2017. We obtained the final predictions by averaging the heatmaps of the original and flipped image as in <ref type="bibr" target="#b6">(Chen et al., 2018b;</ref><ref type="bibr" target="#b29">Newell et al., 2016;</ref><ref type="bibr" target="#b48">Xiao et al., 2018)</ref>. First, we conducted an ablation study on the components of CM by training different variants on the COCO training set and calculating the AP and AR on the minival set. The backbone network was ResNet-50 (R50) and HRNet-w32, and the input size was 256 ? 192. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. For the ResNet-50 backbone, we used three CM modules in the decoder to keep consistent with the baseline model which used three deconvolutional layers. In this way, the feature map size increased from 8 ? 6 at the end of the encoder to 64 ? 48 at the end of the decoder. As can be seen, each component achieved gains over the baseline model , for example, adding the SE branch or HDC branch improved the detection accuracy by a margin of 1.3 AP or 1.4 AP over the baseline model. The auxiliary decoder and intermediate supervision also benefited the detection model and achieved a gain of 1.7 AP. Using dilated convolutions in the encoder could produce feature maps with 2? higher resolution (i.e., 128?96), thereby benefiting the localization accuracy. As can be seen, it achieved a gain of 2.3 AP over the baseline model. These components are complementary to each other that the combination of them improved the detection accuracy further, i.e., a gain of 0.8?1.8 over the individual component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Ablation Study of the Components of CM</head><p>For the HRNet-w32 backbone, since it could produce high-resolution features (i.e., 64 ? 48), we only attached one CM module after HRNet-w32 and did not use any dilated convolutions, thereby increasing the feature map size to 128 ? 96. Nonetheless, we also investigated whether extra CM modules were useful or not. To this end, we attached one or two extra CM modules (i.e., K=2 or 3) accordingly. To avoid huge computations for processing larger feature maps, we replaced the deconvolutional layers in the extra CM modules with convolutional layers to keep the feature map size. As can be seen from <ref type="table" target="#tab_2">Table 2</ref>, CCM (K=1) outperformed the vanilla HRNet-w32 by a gain of 1.1 AP. Besides, adding extra CM modules only led to marginally better results while the parameters and computations increased from 25.56M and 7.92 GFLOPs (K=1) to 28.58M and 8.16 GFLOPs (K=2), and 28.6M and 8.4 GFLOPs (K=3), respectively. To make a trade-off between accuracy and computational efficiency, we chose K=1 as the default setting for the HRNet-w32 backbone. Besides, to compare the generalization ability of the proposed CCM and the baseline models when dealing with unseen heavily-occluded cases from other benchmarks, e.g., the OCHuman benchmark <ref type="bibr" target="#b55">(Zhang et al., 2019b)</ref> and the PoseTrack benchmark <ref type="bibr" target="#b0">(Andriluka et al., 2018)</ref>, we evaluated them in a zero-shot manner, where all models were trained on the COCO training set without further fine-tuning. It is noteworthy that we only include the results of the baseline models and our model for the following several reasons. Firstly, the goal is of this paper is to investigate the key factors that have strong impacts on the performance of human keypoint detection. Since it has already been evidenced by prior excellent work <ref type="bibr" target="#b42">Sun et al., 2019)</ref> that a better keypoint detector benefits pose tracking more effectively, we only focus on human keypoint detection in this paper. Here, we use the OCHuman dataset and Pose-Track dataset to validate the effectiveness of the proposed method for handling occluded cases. Secondly, the detection and tracking performance heavily depends on the person detector, but the detector as well as the implementation of joint propagation in <ref type="bibr" target="#b42">Sun et al., 2019)</ref> are not available. As a result, it will be unfair to compare with their results in a different setting. We plan to investigate the role of context for pose tracking in our future work, where spatial-, temporal-and channel-wise context could be exploited together to enhance feature representation. Thirdly, the Simple baseline method  and the High-resolution Network method  are already two strong baseline models. It is noteworthy that many recent methods use HRNet as the backbone and have achieved SOTA performance on public datasets or challenges. Thereby, it is representative to compare our model with them on these two datasets. Besides, we use the zero-shot transfer setting to evaluate how the model trained on a representative dataset (e.g., MS COCO) generalizes to unseen samples from different data distributions, especially those samples containing large occlusions.</p><p>The results on the OCHuman benchmark are listed in <ref type="table" target="#tab_3">Table 3</ref>. As can be seen, CCM outperformed the baseline model for handling occlusions by large margins, e.g., 3.3 points of AP gain on the validation set and 2.8 points of AP gain on the test set. The gains mainly arise from the occlusion cases val [0.5?0.75] and test [0.5?0.75] , which contain occluded instances with MaxIoU in the range of 0.5 and 0.75. The results on the PoseTrack validation set are listed in <ref type="table" target="#tab_4">Table 4</ref>. Our model outperforms the the baseline model with a ResNet-50 backbone and the HRNet-w32 model by 1.7 AP and 0.8 AP, confirming the superiority of CCM when dealing with heavily-occluded human instances. For a person instance with occluded body parts, it is challenging to detect both visible and invisible keypoints due to the selfocclusion, incomplete body, and perplexity with adjacent overlapped bodies. The proposed CM enables the detection model to learn discriminative feature representation for diverse poses and help it to recognize occluded keypoints. After the network sees diverse poses, it "memorizes" different poses with/without occlusions in the form of feature mapping. Inferring an occluded keypoint thus becomes easier by associating it with similar poses. More discussions will be presented in Section 6.4 and Section 6.5.</p><p>Remarks: 1) CM has better representative capacity than the plain deconvolution layer in the simple baseline method  because it leverages spatial and channel context information explicitly; 2) CM is also complementary to the high-resolution module in  and improves the performance of the stronger HRNet-w32; and 3) CM effectively handles occlusions by learning context features to infer the occluded keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Comparison of Training Strategies</head><p>Next, we present the results of using different training strategies described in Section 4 in <ref type="table" target="#tab_5">Table 5</ref>, where A?C denotes the transfer learning strategy, AC?C denotes the jointtraining strategy, i.e., training CCM on both AIC and COCO datasets then fine-tuning it on the COCO dataset. Other symbols have a similar meaning. As can be seen, leveraging the external AIC dataset increased the AP by 1.3 compared with the model trained on the COCO dataset in <ref type="table" target="#tab_2">Table 2</ref>. The improvement became 1.5 AP when using the proposed joint-training strategy. The proposed HNDM method increased the AP further by an extra gain of 0.3, while the AR remained the same. This is reasonable since HNDM aims to suppress the keypoints of the false-positive person detections, meaning that it can increase the precision but has little influence on the recall. After exploiting the unlabeled dataset, CCM obtained a final AP of 75.6, a gain of 2.1 over the same model trained on the COCO dataset.</p><p>We also evaluated the impact of the detection score threshold in HNDM. Specifically, we set its value to 0.5, 0.7, and 0.9 in the training strategy "ACH?CH". The results are summarized in <ref type="table" target="#tab_6">Table 6</ref>. As can be seen, the performance decreased with the increase of the threshold. Note that fewer hard negative detections were included in the training set when a larger threshold was used. For example, there were 11,762 detections at the threshold of 0.5 while only 4,359 and 952 detections were left at the threshold of 0.7 and 0.9, respectively. On the one hand, since there were many person detection proposals with low scores from the person detectors to increase the recall, thereby leveraging hard neg-  ative person detections with low scores (e.g., 0.5 ? 0.7) and predicting all-zero heatmaps could reduce false positive keypoints on those person detection proposals. On the other hand, due to the annotation policy, human-like objects such as model, sculpture, and doll (please see <ref type="figure" target="#fig_4">Figure 5</ref>) were not annotated as the person category, which however could be detected with high scores by the person detector. Since they shared similar appearance with real human bodies, only using these hard negative person detections with high scores (e.g., ? 0.9) increased the difficulty for learning discriminative feature representation for keypoint detection. Remarks: 1) The proposed joint-training strategy is more effective than transfer learning by reducing the domain gap during the pretraining phase; 2) HNDM can deal with the false-positive person detections, thereby improving the detection precision; and 3) exploiting extra unlabeled data using knowledge distilling enables the network to learn more discriminative features from abundant and diverse samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Comparison of Sub-pixel Refinement Techniques</head><p>We conducted the contrastive experiments by using different sub-pixel refinement techniques in both the simple baseline method  and the High-Resolution Network. The results are summarized in <ref type="table">Table 7 and Table 8</ref>.</p><p>First, the sub-pixel refinement techniques by secondorder approximation described in Section 5.1 consistently improved the performance of both methods. Note that shifting towards the gradient directions by 0.25 pixels was effective and improved the performance of ResNet-50 and HRNet-w32 by 1.9 AP and 2.1 AP, respectively. Nevertheless, it only used the first-order derivative information and the shift was a fixed value, limiting its performance. In contrast, the proposed SOA refinement further increased the AP from 68.5 to 71.0, and 71.7 to 74.3, for ResNet-50 and HRNet-w32, respectively. With the second-order approximation, SOA could adaptively calculate the shift vector for each heatmap. The paraboloid-based SOA and parabola-based SOA performed similarly. It is reasonable because the target heatmap is a 2D Gaussian density map where the density along the x-axis is independent of the density along the y-axis. The predicted heatmap had a similar pattern. Therefore, the paraboloid-based SOA had no obvious advantage over the parabola-based SOA. Nevertheless, the paraboloid-based SOA may be useful for those scenarios where the joint densities along different axes are not independent of each other, i.e., there is an elliptical response with a bias direction in the heatmap.</p><p>Second, Soft-NMS was effective, which improved the performance by 0.8 AP and 0.9 AP for ResNet-50 and HRNet-w32, respectively. The weighted fusion defined by Eq. (23) shifted the keypoint locations in sub-pixels by considering the reasonable estimations rather than filtering them out as done in standard NMS. Besides, the SOA technique was complementary to Soft-NMS. For example, the parabola-based SOA technique combined with Soft-NMS improved the performance further by 0.4 AP compared with using the parabola-based SOA technique individually and improved the performance further by 2.1 AP compared with using Soft-NMS individually for both baselines.</p><p>Third, the flip test together with the one-pixel shift of flipped heatmaps improved the performance consistently, i.e., by a margin of 1.2 AP and 1.0 AP ResNet-50 and HRNet-w32, respectively. The SOA technique was complementary to the flip test. We leave the analysis on the subpixel shift later.</p><p>Fourth, Gaussian filtering was beneficial for improving detection performance. A gain of 0.3 AP and 0.6 AP was achieved for ResNet-50 and HRNet-w32, respectively. It was complementary to the SOA technique. Using them together outperformed using each of them individually. To show the complementarity among all the techniques, we used them together in both methods. They boosted the vanilla baseline's performance by a large margin, e.g., 4.1 AP for ResNet-50 and 4.0 AP for HRNet-w32.</p><p>We evaluated the influence of the shifted pixel for the flipped heatmap described in Section 5.4. As can be seen from the bottom rows in <ref type="table">Table 7 and Table 8</ref>, the performance dropped significantly without shifting the flipped <ref type="table">Table 7</ref>: Experiments on different sub-pixel refinement techniques using the simple baseline method . SOA: sub-pixel refinement by the second-order approximation. Soft-NMS: Soft Non-Maximum Suppression. SSP: the sub-pixel shift of flipped heatmaps. GF: Gaussian filtering on predicted heatmaps. Backbone network: ResNet-50 (R50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOA</head><p>Soft-NMS SSP GF heatmap, i.e., from 72.6 AP to 70.7 AP for ResNet-50, and from 75.7 AP to 73.8 AP for HRNet-w32, since the flipped heatmap was not aligned with the original one. Generally, increasing the shifted pixel from zero to 0.8 consistently improved the detection accuracy. It saturated at a shift of 0.8 pixels and then dropped at a shift of one pixel. We used the sub-pixel refinement techniques in our submission to the 2019 COCO Keypoint Detection Challenge. Remarks: 1) Each sub-pixel refinement technique has a positive but slightly different influence on the performance, i.e., SOA ? SSP ? Soft-NMS ? GF; 2) the proposed SOA and SSP are much better than their vanilla counterparts due to the closed-form and sub-pixel level approximation; and 3) these techniques are complementary since they are carried out in subsequent steps for unique and explicit purposes.</p><formula xml:id="formula_26">AP AP @.5 AP @.75 AP M AP L AR AR @.5 AR @.75 AR M AR L - - - - 68.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Inference Time Analysis</head><p>We also compared the inference time of different sub-pixel refinement techniques and the proposed CM module. Specifically, we chose HRNet-w32 as the backbone network. We divided the inference process into three parts, i.e., 1) the network forward process (denoting "Network Forward"); 2) the process of getting final keypoint coordinates from predicted heatmaps (denoting "Map ? Coord"); and 3) the process of calculating the evaluation metrics of the keypoint detection results (denoting "Evaluation"), and recorded their inference time separately. For each setting, we ran the model three times and calculated the average inference time for single person instance. The results are summarized in <ref type="table" target="#tab_9">Table 9</ref>.</p><p>As can be seen, the inference time of different postprocessing techniques mainly differs in the process of "Map ? Coord". For example, replacing the NMS in the default setting of the baseline model  with the proposed soft-NMS only increased by 0.04 millisecond (ms). SOA and SSP increased the inference time slightly, i.e., from 3.85 ms to 4.13 ms and from 4.73 ms to 4.99 ms. However, Gaussian filtering required much more computations and increased the inference time from 4.13 ms to 4.73 ms. It is noteworthy that the process of "Map ? Coord" cost more inference time than "Network Forward" since it was mainly carried out on CPU while network forward computation was carried on GPU. Adding a CM module on HRNet-w32 only increased the network forward time slightly, i.e., about 0.05 ms. However, since the heatmaps generated by CCM were two times larger than those by the baseline model, <ref type="table">Table 8</ref>: Experiments on different settings of sub-pixel refinement tricks using the High-Resolution Network  (HRNet-w32). SOA: sub-pixel refinement by the second-order approximation. Soft-NMS: sub-pixel refinement by Soft Non-Maximum Suppression (Soft-NMS). SSP: the sub-pixel shift of flipped heatmaps. GF: Gaussian filtering on heatmaps.  the process of "Map ? Coord" became much slower, i.e., about two times. It is worth further study to find a fast GPU implementation for this process.</p><formula xml:id="formula_27">SOA Soft-NMS SSP GF AP AP @.5 AP @.75 AP M AP L AR AR @.5 AR @.75 AR M AR L - - - - 71.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with state-of-the-art methods</head><p>The results of CCM and SOTA methods on the COCO minival are summarized in <ref type="table" target="#tab_0">Table 10</ref>. CCM was trained on the COCO dataset without using the external AI Challenger dataset. We evaluated the proposed approach on three groups of backbone networks, i.e., the small ones including ShuffleNet-v2 and MobileNet-v2, the medium ones including ResNet-50 and HRNet-w32, and the large ones including ResNet-152 and HRNet-w48, respectively. As can be seen, our small model based on ShuffleNet-v2 and MobileNet-v2 significantly improved the detection accuracy compared with the baseline model , i.e., a gain of 4.0 AP and 4.1 AP, respectively. The improvement is at the cost of 5% ? 10% more parameters and about 15% more GFLOPs, which are affordable. CCM also outperformed the Hourglass model <ref type="bibr" target="#b29">(Newell et al., 2016)</ref> and was comparable with the CPN <ref type="bibr" target="#b6">(Chen et al., 2018b)</ref> based on ResNet-50.</p><p>As for the medium backbone networks, simple baseline method  achieved similar performance using ResNet-50 and ResNeXt-50, but they were inferior to the High-Resolution Network  based on HRNet-w32. The proposed CCM based on ResNet-50 achieved a 74.3 AP and outperformed other models with the same input size and backbone network, for example, a gain of 3.9 AP over the simple baseline method . Replacing ResNet-50 to ResNeXt-50 leads to a slightly better result, i.e., from 74.3 AP to 74.5 AP. When using the HRNet-w32 as the backbone network, our CCM model increased the AP from 74.4 to 76.7 and achieved the best performance among all the models. Note that CCM based on ResNet-50 or ResNeXt-50 has more parameters and GFLOPs than the baseline model since it uses dilated convolutions to increase the feature map size and three CMs in the decoder. As for the one based on HRNet-w32, it has roughly the same amount of parameters and GFLOPs as its counterpart since only one CM was used.</p><p>Our large model based on ResNet-152 with input size 384?288 achieved a gain of 2.4 AP over the simple baseline model  and a gain of 0.4 AP over the recent HRNet-w48 model . For example, CCM outperformed HRNet-w48 by a margin of 0.4 AP and 0.5 AP at the threshold 0.5 and 0.75. However, CCM was inferior to HRNet-w48 for large person instances, i.e., a drop of 0.2 AP. One possible explanation is that HRNet-w48 learned a high-resolution and discriminative feature representation by integrating the features from different scales. We attached a CM to HRNet-w48 and used the sub-pixel refinement techniques for postprocessing. It further improved the detection accuracy of HRNet-w48 from 76.3 AP to 77.5 AP. Besides, the result of large person instances was improved by 0.6 AP. Our CCM model achieved the best performance among all other models based on comparable backbone networks. Besides, both models have nearly the same parameters and GFLOPs as the baseline models since we decreased the number of filters in the CM for ResNet-152 and only used one CM for HRNet-w48. These results demonstrate the effectiveness of the proposed CCM, training strategies, and sub-pixel refinement techniques.</p><p>The results of CCM and SOTA methods on the COCO test-dev set are summarized in <ref type="table" target="#tab_0">Table 11</ref>. The CCM using ResNet-152 as the backbone network trained on the COCO dataset outperformed the baseline model  by 2.1 AP. It also outperformed the recent HRNet-w48 model  by 0.3 AP. Using HRNet-w48 as the backbone network, CCM outperformed the vanilla HRNet-w48 model, MSPN <ref type="bibr" target="#b22">(Li et al., 2019)</ref>, and DARK  by a margin of 1.1 AP, 0.5 AP, and 0.4 AP, respectively. It was even better than the ensemble simple baseline models trained with the external AI Challenger dataset. After joint-training with this external dataset, CCM based on ResNet-152 outperformed both the simple baseline method and HRNet-w48. Besides, replacing the backbone network from ResNet-152 to HRNet-w48, the performance was further improved by 0.7 AP. It was better than the recently proposed method DARK*  by a margin of 0.6 AP using the same person detection results and comparable with the ensemble models MSPN+* <ref type="bibr" target="#b22">(Li et al., 2019)</ref> (the champion of the 2018 COCO Keypoint Challenge), i.e., 78.0 v.s. 78.1. Generally, the external dataset brought about 1.5 AP improvement, which mainly arose from the AP at the larger threshold, demonstrating that training on more diverse poses helped the model to learn discriminative features and improve the location accuracy. Our final ensemble models brought another 0.9 AP and set a new state-of-the-art on this benchmark, i.e., 78.9 AP. It was comparable with RSN+ <ref type="bibr" target="#b3">(Cai et al., 2020)</ref> (the champion of the 2019 COCO Keypoint Challenge), which used a better person detector and had 59.8 AP for the person category on COCO val2017. Remarks: 1) Our CCM model consistently outperforms the baseline models using small, medium, and large backbone networks, but the gain becomes smaller with the increasing of the model capacity, i.e., MobileNet ? ShuffleNet ? ResNet-50 ? ResNeXt-50 ? HRNet-w32 ? ResNet-152 ? HRNet-w48. It is reasonable because "bigger" backbone networks themselves have stronger representation capacity, thereby the impact of CM decreases accordingly; and 2) our CCM model benefits from the effective CM modules to model the context information, the efficient training strategies to learn discriminative features, and the sub-pixel refinement techniques to locate keypoints accurately, in a collaborative and complementary manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Subjective visual inspection and discussion</head><p>We presented some visual examples of the keypoint detection results by using the CCM model based on HRNet-w48 on the COCO minival set and PoseTrack validation set in <ref type="figure" target="#fig_8">Figure 10</ref> and <ref type="figure" target="#fig_9">Figure 11</ref>, respectively. As can be seen from, our model could handle various poses. Besides, it also successfully inferred the occluded keypoints (self-occluded or occluded by other objects). In the bottom two rows, we presented the detection results on multiple person instances within each image, which were also promising. Our model could handle small instances, blurry ones, low-light images as well as various occlusions. To see how CCM achieved the performance, we conducted an experiment to visually inspect the learned features by the network.</p><p>We overlaid the output feature maps from the CMs in CCM on the input images to inspect what has been learned by the network. <ref type="figure">Figure 12</ref> shows the feature maps learned by the CMs at different levels. "HDC k" stands for the feature maps from the kth dilated convolutional layer in the hybrid-dilated convolutional branches. "HDC x SE" stands for the first term in Eq. (4). 'Deconv' stands for the outputted feature maps from the CMs, i.e., the left side of Eq. (4). "Level k" stands for the index of CM in CCM. The keypoints belonging to the left (right) body were connected by red (blue) lines. The left-right symmetric keypoints were connected by yellow lines. The predicted invisible keypoints of occluded body parts were indicated by red arrows.</p><p>As can be seen, HDC paid more attention to the context with the increase of the dilation rate. CCM learned to identify easy keypoints and inferred hard keypoints progressively through the cascaded CMs. Please check the arrows and ellipses. Moreover, 1) CCM probably learned the body configuration by inferring the invisible keypoints and potential poses conditioned on the visible keypoints as shown in the first row; 2) CCM also learned the body symmetry, e.g.,  <ref type="bibr" target="#b23">(Lin et al., 2014)</ref>. there were two legs and arms in the human body, as shown in the left part of the second row; 3) CCM could also handle blurry images and infer the head and right arms as shown in the right part of the second row.</p><p>To illustrate the effectiveness of CCM for handling occlusions, we manually added masks on some body parts, e.g., the head, hip, and ankle as supplements to the existing occlusions, as shown in <ref type="figure" target="#fig_2">Figure 13</ref>. CCM first recognized and located the human bodies using the encoder (Res5), then detected different body parts (Deconv1) to help identify some distinct keypoints (Deconv2). In the final stage (Deconv3), CCM predicted the difficult occluded keypoints using the context information of identified body parts and keypoints. To further investigate the effectiveness of CCM for handling severe occlusions, we manually masked out either the upper or lower body of each person as shown in the first column of <ref type="figure" target="#fig_2">Figure 13</ref>. As can be seen, although the predicted human poses were different from the ground truth annotations at the masked regions, CCM showed the ability to learn human body configuration and infer reasonable invisible keypoints. Moreover, the results also confirm that CCM detected those distinct keypoints in the earlier stage while predicting the difficult occluded ones in the later stage.</p><p>Remarks: 1) Empirically, CCM's detection process follows a "Localization ? Componentization ? Identification ? Prediction" routine, similar to the procedure that humans detect human keypoints in occluded settings (Section 3.1); and 2) CCM probably has learned the body configuration such as symmetric body parts and reasonable distances between adjacent keypoints, evidenced by visual examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Empirical Studies on Annotations</head><p>As we know that occluded keypoints are more difficult to be detected than visible ones, we are wondering whether the invisible keypoint annotations have the same impact on the model or not, compared with the same amount of visible keypoint annotations? To this end, we conducted an experiment to gain some insight into the keypoint annotations.</p><p>We constructed two training sets based on the COCO training set. The first one was COCO-I which was obtained by removing all the annotations of invisible keypoints in the     <ref type="figure">Fig. 12</ref>: Visualization of the feature maps from the CMs in CCM based on ResNet-50. "HDC k" stands for the feature maps from the kth dilated convolutional layer in the hybrid-dilated convolutional branches. "HDC x SE" stands for the first term in Eq. (4). "Deconv" stands for the outputted feature maps from the CMs, i.e., the left side of Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Res5 <ref type="formula" target="#formula_0">Deconv1</ref> Deconv2 Deconv3 <ref type="figure" target="#fig_2">Fig. 13</ref>: Visualization of the feature maps learned by CCM based on ResNet-50 at different stages. "Deconv" stands for the outputted feature maps from the CMs, i.e., the left side of Eq. (4). One or several of keypoints of each person is manually occluded by a mask.  original training set. The second one was COCO-V which was obtained by removing the same amount of visible keypoint annotations randomly. The resulting keypoints without annotations were treated as unlabeled. Then, we trained the proposed CCM using the ResNet-50 as the backbone encoder on COCO-I and COCO-V. They were denoted as "CCM-I" and "CCM-V", respectively. Their results on the COCO minival set are summarized in the first two rows of <ref type="table" target="#tab_0">Table 12</ref>. As a reference, we also listed the model trained on the original COCO training set in the bottom row.</p><p>As can be seen, the scores of CCM-V dropped marginally compared with CCM. However, the scores of CCM-I dropped significantly by a large margin compared with CCM-V and CCM. These results confirm that annotating invisible keypoints matters, i.e.. The invisible keypoint annotations contributed more to the model than the same amount of visible ones. Since it is more difficult to detect invisible keypoints, the invisible keypoint annotations provide stronger supervisory signals to the model than the visible ones did. To infer an invisible keypoint with such supervision, it probably learned useful features from the context since the keypoint itself was invisible. In this way, the model could learn the knowledge of body configuration implicitly, e.g., as a form of discriminative feature for each category of keypoints.</p><p>To further analyze the impact of invisible keypoint annotations, we calculated the indexes on visible and invisible keypoints, respectively. We removed all the invisible keypoint annotations from the COCO minival set. The resulting minival set was used to evaluate the model's performance on the visible keypoints. Similarly, we also removed all the visible keypoint annotations from the COCO minival set to evaluate the model's performance on the invisible keypoints. We used the ground truth bounding boxes as the person detection results. The results are listed in <ref type="table" target="#tab_0">Table 13</ref>. ?? denoted the gain of CCM-V over CCM-I.</p><p>Unsurprisingly, CCM-V achieved better results on invisible keypoints compared with CCM-I, demonstrating that the gains of CCM-V over CCM-I in <ref type="table" target="#tab_0">Table 12</ref> mainly arose from the invisible ones. Note that although CCM-I was trained without using the annotations of invisible keypoints, it still obtained 40.2 AP on them, demonstrating that it had a bit of generalization on predicting the invisible keypoints. Since there were distinct appearance differences between visible keypoints and invisible ones of the same category and the model did not get any supervisory signal from the invisible keypoints, it implies that CCM probably learned useful features from contextual keypoints to infer the invisible ones. Using the invisible annotations, CCM achieved better performance by exploiting its representation capacity.</p><p>We also reported APs of the two models on instances with different numbers of annotated keypoints from the COCO minival set. The results are shown in <ref type="table" target="#tab_0">Table 14</ref>. As can be seen, the gains mainly arose from the occluded instances, for example, instances with less than 10 annotated keypoints. It implies that the invisible keypoint annotations helped the model to learn the body configuration for inferring the invisible keypoints and handling occlusions. Some visual results were presented in <ref type="figure">Figure 12</ref> and <ref type="figure" target="#fig_2">Figure 13</ref>.</p><p>Although annotating invisible keypoints is more difficult than visible ones, the above results confirm that the invisible keypoint annotations are more valuable. Consequently, we could improve our model by 1) annotating more occluded keypoints to train a better model; 2) exploiting the active learning strategy to identify hard keypoints that should be annotated to continuously improve the model; 3) developing effective multi-view learning algorithms to utilize the complementary information between different views of data.</p><p>Remarks: 1) The invisible keypoint annotations have a larger impact on the model than the same amount of visible ones; and 2) even without the invisible annotations, the proposed CCM model is still able to generalize to the invisible keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we address the human keypoint detection problem by devising a new cascaded context mixer-based neural network (CCM), which has a strong representative capacity to simultaneously model the spatial and channel context information. We also propose three efficient training strategies including a hard-negative person detection mining strategy to migrate the mismatch between training and testing, a joint-training strategy to use abundant unlabeled samples by knowledge distilling, and a joint-training strategy to exploit external data with heterogeneous labels. They collaboratively enable CCM to learn discriminative features from abundant and diverse poses. Besides, we present four postprocessing techniques to refine predictions at the sub-pixel level accuracy. These complementary techniques are carried out sequentially during the inference phase for unique and explicit purposes, which further improve the detection accuracy. Our CCM model consistently outperforms public stateof-the-art models with various backbone networks by a large margin. We empirically show that CCM's detection process is similar to humans in occluded settings and probably learns human body configuration. Moreover, we also identify that invisible keypoint annotations have a larger impact on the model than the same amount of visible ones and present some promising research topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a)-(b) Statistics of the annotated keypoints in the MS COCO training dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Ubiquitous occlusions in an image from the MS COCO dataset. (b) Illustration of the keypoint detection process carried out by humans when faced with occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of the feature maps learned by the ResNet-50 encoder on two test images as shown in (a)-(b) and (c)-(d), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The diagram of the proposed human keypoint detection model based on cascaded context mixer (CCM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of the hard-negative detections. Green: ground truth person instances. Red: hard-negative detections. Yellow: low-score false positive detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig. 6: The sub-pixel refinement by the second order approximation, i.e., the case of parabola.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>.Fig. 7 :</head><label>7</label><figDesc>?z ?x = 2ax + cy + d = 0 ?z ?y = 2by + cx + e = 0 The sub-pixel refinement by the second order approximation, i.e., the case of paraboloid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>The sub-pixel shift of flipped heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Some visual examples of the keypoint detection results on the COCO minival set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Some visual examples of the keypoint detection results on the PoseTrack validation set<ref type="bibr" target="#b0">(Andriluka et al., 2018)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>Visualization of the feature maps learned by CCM based on ResNet-50 at different stages. Either the upper or lower body of each person is manually masked out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A summary of the human keypoint detection methods based on DCNN.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>Backbone</cell><cell>Decoder</cell><cell>Extra Data</cell><cell>Post-processing</cell><cell>Performance</cell></row><row><cell></cell><cell>Pishchulin et al. (2016)</cell><cell>VGG</cell><cell>-</cell><cell>-</cell><cell>offset regression</cell><cell>82.4PCK h</cell></row><row><cell>Bottom-up</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the components of CCM. AD: auxiliary decoder, D: dilated convolutions. AP/AR: mean average precision/recall on COCO minival set. Backbone network: ResNet-50 (R50) and HRNet-w32 (HR32).</figDesc><table><row><cell>Method</cell><cell>SE HDC AD</cell><cell>D</cell><cell>AP</cell><cell>AR</cell></row><row><cell>Baseline (R50)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Xiao et al., 2018)</cell><cell></cell><cell></cell><cell cols="2">70.4 76.3</cell></row><row><cell>CCM (R50), K=3</cell><cell></cell><cell></cell><cell cols="2">71.7 77.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">71.8 78.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">72.1 78.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">72.7 78.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">73.5 79.1</cell></row><row><cell>HRNet-w32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Sun et al., 2019)</cell><cell></cell><cell></cell><cell cols="2">74.4 79.8</cell></row><row><cell>CCM (HR32), K=1</cell><cell></cell><cell></cell><cell cols="2">75.5 80.9</cell></row><row><cell>CCM (HR32), K=2</cell><cell></cell><cell></cell><cell cols="2">75.5 81.0</cell></row><row><cell>CCM (HR32), K=3</cell><cell></cell><cell></cell><cell cols="2">75.6 81.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of CCM and the Baseline model on the OCHuman dataset<ref type="bibr" target="#b55">(Zhang et al., 2019b)</ref>. Backbone network: ResNet-50 (R50). AP @.75 AP M AP L</figDesc><table><row><cell cols="4">Method AP @.5 Baseline Dataset AP val 59.9 79.3 val [0.5-0.75] 62.7 81.5 val [0.75-1] 42.9 63.1</cell><cell>66.1 69.4 46.4</cell><cell cols="2">64.7 59.9 64.7 62.7 -42.9</cell></row><row><cell></cell><cell>test</cell><cell>52.1</cell><cell>70.6</cell><cell>57.0</cell><cell cols="2">72.6 52.1</cell></row><row><cell></cell><cell>test [0.5-0.75]</cell><cell>61.1</cell><cell>80.4</cell><cell>66.7</cell><cell cols="2">89.2 61.0</cell></row><row><cell></cell><cell>test [0.75-1]</cell><cell>43.4</cell><cell>60.8</cell><cell>47.4</cell><cell cols="2">40.0 43.4</cell></row><row><cell></cell><cell>val</cell><cell cols="2">63.2 (+3.3) 81.9</cell><cell>68.1</cell><cell cols="2">53.7 63.2</cell></row><row><cell></cell><cell>val [0.5-0.75]</cell><cell cols="2">66.2 (+3.5) 84.5</cell><cell>71.8</cell><cell cols="2">53.7 66.3</cell></row><row><cell>CCM(R50)</cell><cell>val [0.75-1]</cell><cell cols="2">43.5 (+0.6) 66.0</cell><cell>45.6</cell><cell>-</cell><cell>43.5</cell></row><row><cell></cell><cell>test</cell><cell cols="2">54.9 (+2.8) 74.5</cell><cell>59.3</cell><cell cols="2">72.6 54.9</cell></row><row><cell></cell><cell cols="3">test [0.5-0.75] 65.4 (+4.3) 83.7</cell><cell>71.5</cell><cell cols="2">89.2 65.4</cell></row><row><cell></cell><cell>test [0.75-1]</cell><cell cols="2">44.6 (+1.2) 65.0</cell><cell>47.6</cell><cell cols="2">40.0 44.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of CCM and the Baseline model on the PoseTrack validation set<ref type="bibr" target="#b0">(Andriluka et al., 2018)</ref>. Backbone network: ResNet-50 (R50) and HRNet-w32 (HR32). AP @.75 AP M AP L AR</figDesc><table><row><cell cols="3">Method AP @.5 Baseline(R50) AP 69.6 86.4</cell><cell>75.7</cell><cell>40.6</cell><cell>73.8</cell><cell>72.2</cell></row><row><cell>HRNet-w32</cell><cell>73.0</cell><cell>87.6</cell><cell>78.9</cell><cell>42.7</cell><cell>77.3</cell><cell>75.4</cell></row><row><cell>CCM(R50)</cell><cell cols="2">71.3 (+1.7) 87.1</cell><cell>76.5</cell><cell>42.0</cell><cell>75.5</cell><cell>74.1</cell></row><row><cell>CCM(HR32)</cell><cell cols="2">73.8 (+0.8) 87.7</cell><cell>79.0</cell><cell>44.8</cell><cell>78.2</cell><cell>76.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of CCM trained with the different strategies described in Section 4. A: the AIC training dataset; C: the COCO training set; H: the hard-negative training samples; U: the reprocessed unlabeled training set.</figDesc><table><row><cell>Method</cell><cell>Training Strategy</cell><cell>AP</cell><cell>AP @.5</cell><cell>AP @.75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell><cell>AR @.5</cell><cell>AR @.75</cell><cell>AR M</cell><cell>AR L</cell></row><row><cell></cell><cell>A?C</cell><cell>74.8</cell><cell>90.6</cell><cell>81.7</cell><cell>70.5</cell><cell>81.1</cell><cell>80.1</cell><cell>94.0</cell><cell>86.1</cell><cell>75.7</cell><cell>86.5</cell></row><row><cell>CCM (ResNet-50)</cell><cell>AC?C ACH?CH</cell><cell>75.0 75.3</cell><cell>90.3 90.6</cell><cell>82.1 82.1</cell><cell>70.8 71.0</cell><cell>81.3 81.5</cell><cell>80.4 80.4</cell><cell>94.0 94.0</cell><cell>86.7 86.5</cell><cell>76.0 76.0</cell><cell>86.6 86.6</cell></row><row><cell></cell><cell>ACHU?CH</cell><cell>75.6</cell><cell>90.5</cell><cell>82.5</cell><cell>71.4</cell><cell>81.8</cell><cell>80.7</cell><cell>94.0</cell><cell>86.8</cell><cell>76.3</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Study of the threshold setting in HNDM for the training strategy "ACH?CH" described in Section 4.1.</figDesc><table><row><cell>Threshold</cell><cell>AP</cell><cell>AP @.5</cell><cell>AP @.75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell></row><row><cell>0.5</cell><cell>75.3</cell><cell>90.6</cell><cell>82.1</cell><cell>71.0</cell><cell>81.5</cell><cell>80.4</cell></row><row><cell>0.7</cell><cell>75.2</cell><cell>90.9</cell><cell>82.2</cell><cell>71.1</cell><cell>81.3</cell><cell>80.4</cell></row><row><cell>0.9</cell><cell>74.9</cell><cell>90.5</cell><cell>82.2</cell><cell>70.9</cell><cell>81.0</cell><cell>80.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="8">: Inference time (millisecond, i.e., ms) of differ-</cell></row><row><cell cols="8">ent sub-pixel refinement techniques and the proposed CM</cell></row><row><cell cols="8">module. Network Forward: the forward process of different</cell></row><row><cell cols="8">networks. Map ? Coord: the process of getting final key-</cell></row><row><cell cols="8">point coordinates from predicted heatmaps. Evaluation: the</cell></row><row><cell cols="8">process of calculating the evaluation metrics of the keypoint</cell></row><row><cell cols="7">detection results. Backbone network: HRNet-w32.</cell><cell></cell></row><row><cell>Model</cell><cell>SOA</cell><cell>Soft-</cell><cell cols="3">SSP GF Network</cell><cell>Map ?</cell><cell>Eval-</cell></row><row><cell></cell><cell></cell><cell>NMS</cell><cell></cell><cell></cell><cell>Forward</cell><cell>Coord</cell><cell>uation</cell></row><row><cell></cell><cell>parabola</cell><cell></cell><cell>0.4</cell><cell></cell><cell>2.44</cell><cell>4.99</cell><cell>0.43</cell></row><row><cell></cell><cell>parabola</cell><cell></cell><cell>1</cell><cell></cell><cell>2.43</cell><cell>4.73</cell><cell>0.43</cell></row><row><cell>Baseline</cell><cell>parabola</cell><cell></cell><cell>1</cell><cell>-</cell><cell>2.45</cell><cell>4.13</cell><cell>0.43</cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell>1</cell><cell>-</cell><cell>2.46</cell><cell>3.85</cell><cell>0.43</cell></row><row><cell></cell><cell>0.25</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>2.45</cell><cell>3.81</cell><cell>0.41</cell></row><row><cell>CCM</cell><cell>parabola 0.25</cell><cell>-</cell><cell>0.4 1</cell><cell>-</cell><cell>2.50 2.51</cell><cell>9.30 7.53</cell><cell>0.39 0.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Comparisons of CAPE-Net and SOTA methods on the COCO minival set.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone</cell><cell cols="2">#Params GFLOPs</cell><cell>AP</cell><cell>AP @.5</cell><cell>AP @.75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Input size: 256 ? 192</cell><cell></cell><cell></cell></row><row><cell>Baseline (Xiao et al., 2018)</cell><cell></cell><cell>ShuffleNet-v2</cell><cell>8.78M</cell><cell>4.07</cell><cell>63.9</cell><cell>86.6</cell><cell>71.4</cell><cell>60.8</cell><cell>70.1</cell><cell>70.3</cell></row><row><cell>Baseline (Xiao et al., 2018) CCM</cell><cell>Small</cell><cell>MobileNet-v2 ShuffleNet-v2</cell><cell>9.57M 9.72M</cell><cell>4.17 4.75</cell><cell>64.3 67.9</cell><cell>86.3 88.4</cell><cell>72.2 75.3</cell><cell>60.9 63.8</cell><cell>70.9 74.0</cell><cell>70.5 74.2</cell></row><row><cell>CCM</cell><cell></cell><cell>MobileNet-v2</cell><cell>10.1M</cell><cell>4.80</cell><cell>68.4</cell><cell>88.0</cell><cell>75.7</cell><cell>64.2</cell><cell>74.5</cell><cell>74.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Input size: 256 ? 192</cell><cell></cell><cell></cell></row><row><cell>Hourglass (Newell et al., 2016)</cell><cell></cell><cell>8xHourglass</cell><cell>25.1M</cell><cell>14.3</cell><cell>66.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CPN (Chen et al., 2018b)</cell><cell></cell><cell>ResNet-50</cell><cell>27.0M</cell><cell>6.20</cell><cell>69.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline (Xiao et al., 2018)</cell><cell></cell><cell>ResNet-50</cell><cell>34.0M</cell><cell>8.20</cell><cell>70.4</cell><cell>88.6</cell><cell>78.3</cell><cell>67.1</cell><cell>77.2</cell><cell>76.3</cell></row><row><cell>Baseline (Xiao et al., 2018) HRNet (Sun et al., 2019)</cell><cell>Medium</cell><cell>ResNeXt-50 HRNet-w32</cell><cell>33.5M 28.5M</cell><cell>8.35 7.68</cell><cell>70.6 74.4</cell><cell>88.9 90.5</cell><cell>77.9 81.9</cell><cell>67.2 70.8</cell><cell>77.5 81.0</cell><cell>76.5 79.8</cell></row><row><cell>CCM</cell><cell></cell><cell>ResNet-50</cell><cell>40.7M</cell><cell>45.4</cell><cell>74.3</cell><cell>90.3</cell><cell>81.3</cell><cell>70.0</cell><cell>80.6</cell><cell>79.6</cell></row><row><cell>CCM</cell><cell></cell><cell>ResNeXt-50</cell><cell>40.1M</cell><cell>45.4</cell><cell>74.5</cell><cell>90.4</cell><cell>81.2</cell><cell>70.0</cell><cell>81.0</cell><cell>79.6</cell></row><row><cell>CCM</cell><cell></cell><cell>HRNet-w32</cell><cell>28.6M</cell><cell>7.92</cell><cell>76.7</cell><cell>91.1</cell><cell>83.5</cell><cell>72.5</cell><cell>83.0</cell><cell>81.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Input size: 384 ? 288</cell><cell></cell><cell></cell></row><row><cell>Baseline (Xiao et al., 2018)</cell><cell></cell><cell>ResNet-152</cell><cell>68.6M</cell><cell>35.9</cell><cell>74.3</cell><cell>89.6</cell><cell>81.1</cell><cell>70.5</cell><cell>79.7</cell><cell>79.7</cell></row><row><cell>HRNet (Sun et al., 2019) CCM</cell><cell>Large</cell><cell>HRNet-w48 ResNet-152</cell><cell>63.6M 63.5M</cell><cell>35.4 40.1</cell><cell>76.3 76.7</cell><cell>90.8 91.2</cell><cell>82.9 83.4</cell><cell>72.3 72.4</cell><cell>83.4 83.2</cell><cell>81.2 81.7</cell></row><row><cell>CCM</cell><cell></cell><cell>HRNet-w48</cell><cell>63.7M</cell><cell>36.6</cell><cell>77.5</cell><cell>91.2</cell><cell>83.6</cell><cell>73.0</cell><cell>84.0</cell><cell>82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparisons of CCM and SOTA methods on the COCO test-dev set. Input size: 353 ? 257 for G-RMI; 320 ? 256 for RMPE; 384?288 for CPN, Baseline, HRNet, MSPN, DARK, RSN, and CCM. The symbol "*" denotes external data, "+" denotes ensemble models, " ?" and " ?" denote the champion of the 2018 and 2019 COCO Keypoint Challenge, respectively.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">#Params GFLOPs</cell><cell>AP</cell><cell>AP @.5</cell><cell>AP @.75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell></row><row><cell>Mask-RCNN (He et al., 2017)</cell><cell>ResNet-50</cell><cell>-</cell><cell>-</cell><cell>63.1</cell><cell>87.3</cell><cell>68.7</cell><cell>57.8</cell><cell>71.4</cell><cell>-</cell></row><row><cell>G-RMI (Papandreou et al., 2017)</cell><cell>ResNet-101</cell><cell>42.6M</cell><cell>57.0</cell><cell>64.9</cell><cell>85.5</cell><cell>71.3</cell><cell>62.3</cell><cell>70.0</cell><cell>69.7</cell></row><row><cell cols="2">G-RMI* (Papandreou et al., 2017) ResNet-101</cell><cell>42.6M</cell><cell>57.0</cell><cell>68.5</cell><cell>87.1</cell><cell>75.5</cell><cell>65.8</cell><cell>73.3</cell><cell>73.3</cell></row><row><cell>RMPE (Fang et al., 2017)</cell><cell>Hourglass</cell><cell>28.1M</cell><cell>36.7</cell><cell>72.3</cell><cell>89.2</cell><cell>79.1</cell><cell>68.0</cell><cell>78.6</cell><cell>-</cell></row><row><cell>CPN (Chen et al., 2018b)</cell><cell>ResNet-Inception</cell><cell>-</cell><cell>-</cell><cell>72.1</cell><cell>91.4</cell><cell>80.0</cell><cell>68.7</cell><cell>77.2</cell><cell>78.5</cell></row><row><cell>CPN+ (Chen et al., 2018b)</cell><cell>ResNet-Inception</cell><cell>-</cell><cell>-</cell><cell>73.0</cell><cell>91.7</cell><cell>80.9</cell><cell>69.5</cell><cell>78.1</cell><cell>79.0</cell></row><row><cell>Baseline (Xiao et al., 2018)</cell><cell>ResNet-152</cell><cell>68.6M</cell><cell>35.9</cell><cell>73.7</cell><cell>91.9</cell><cell>81.1</cell><cell>70.3</cell><cell>80.0</cell><cell>79.0</cell></row><row><cell>Baseline+* (Xiao et al., 2018)</cell><cell>ResNet-152</cell><cell>-</cell><cell>-</cell><cell>76.5</cell><cell>92.4</cell><cell>84.0</cell><cell>73.0</cell><cell>82.7</cell><cell>81.5</cell></row><row><cell>HRNet (Sun et al., 2019)</cell><cell>HRNet-w48</cell><cell>63.6M</cell><cell>35.4</cell><cell>75.5</cell><cell>92.5</cell><cell>83.3</cell><cell>71.9</cell><cell>81.5</cell><cell>80.5</cell></row><row><cell>HRNet* (Sun et al., 2019)</cell><cell>HRNet-w48</cell><cell>63.6M</cell><cell>35.4</cell><cell>77.0</cell><cell>92.7</cell><cell>84.5</cell><cell>73.4</cell><cell>83.1</cell><cell>82.0</cell></row><row><cell>MSPN (Li et al., 2019)</cell><cell>4xResNet-50</cell><cell>71.9M</cell><cell>58.7</cell><cell>76.1</cell><cell>93.4</cell><cell>83.8</cell><cell>72.3</cell><cell>81.5</cell><cell>81.6</cell></row><row><cell>MSPN* (Li et al., 2019)</cell><cell>4xResNet-50</cell><cell>71.9M</cell><cell>58.7</cell><cell>77.1</cell><cell>93.8</cell><cell>84.6</cell><cell>73.4</cell><cell>82.3</cell><cell>82.3</cell></row><row><cell>MSPN+* (Li et al., 2019) ?</cell><cell>4xResNet-50</cell><cell>-</cell><cell>-</cell><cell>78.1</cell><cell>94.1</cell><cell>85.9</cell><cell>74.5</cell><cell>83.3</cell><cell>83.1</cell></row><row><cell>DARK (Zhang et al., 2020)</cell><cell>HRNet-w48</cell><cell>63.6M</cell><cell>32.9</cell><cell>76.2</cell><cell>92.5</cell><cell>83.6</cell><cell>72.5</cell><cell>82.4</cell><cell>81.1</cell></row><row><cell>DARK* (Zhang et al., 2020)</cell><cell>HRNet-w48</cell><cell>63.6M</cell><cell>32.9</cell><cell>77.4</cell><cell>92.6</cell><cell>84.6</cell><cell>73.6</cell><cell>83.7</cell><cell>82.3</cell></row><row><cell>RSN (Cai et al., 2020)</cell><cell>4xRSN-50</cell><cell>111.8M</cell><cell>65.9</cell><cell>78.6</cell><cell>94.3</cell><cell>86.6</cell><cell>75.5</cell><cell>83.3</cell><cell>83.8</cell></row><row><cell>RSN+ (Cai et al., 2020) ?</cell><cell>4xRSN-50</cell><cell>-</cell><cell>-</cell><cell>79.2</cell><cell>94.4</cell><cell>87.1</cell><cell>76.1</cell><cell>83.8</cell><cell>84.1</cell></row><row><cell>CCM</cell><cell>ResNet-152</cell><cell>63.5M</cell><cell>40.1</cell><cell>75.8</cell><cell>92.7</cell><cell>83.4</cell><cell>71.8</cell><cell>81.5</cell><cell>80.9</cell></row><row><cell>CCM</cell><cell>HRNet-w48</cell><cell>63.7M</cell><cell>36.6</cell><cell>76.6</cell><cell>92.8</cell><cell>84.1</cell><cell>72.6</cell><cell>82.4</cell><cell>81.7</cell></row><row><cell>CCM*</cell><cell>ResNet-152</cell><cell>63.5M</cell><cell>40.1</cell><cell>77.3</cell><cell>93.0</cell><cell>84.8</cell><cell>73.3</cell><cell>83.1</cell><cell>82.3</cell></row><row><cell>CCM*</cell><cell>HRNet-w48</cell><cell>63.7M</cell><cell>36.6</cell><cell>78.0</cell><cell>93.4</cell><cell>85.1</cell><cell>74.0</cell><cell>83.6</cell><cell>83.0</cell></row><row><cell>CCM+*</cell><cell>HRNet-w48</cell><cell>-</cell><cell>-</cell><cell>78.9</cell><cell>93.8</cell><cell>86.0</cell><cell>75.0</cell><cell>84.5</cell><cell>83.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Comparison between CCM-I and CCM-V on the COCO minival set. Please refer to Section 6.5.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>AP</cell><cell>AP @.5</cell><cell>AP @.75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell><cell>AR @.5</cell><cell>AR @.75</cell><cell>AR M</cell><cell>AR L</cell></row><row><cell>CCM-I</cell><cell cols="3">ResNet-50 70.4 89.1</cell><cell>76.7</cell><cell>66.0</cell><cell>77.0</cell><cell cols="2">77.1 93.4</cell><cell>82.9</cell><cell>72.2</cell><cell>83.8</cell></row><row><cell cols="4">CCM-V ResNet-50 73.7 90.0</cell><cell>80.9</cell><cell>69.5</cell><cell>79.9</cell><cell cols="2">79.2 93.5</cell><cell>85.5</cell><cell>74.7</cell><cell>85.5</cell></row><row><cell>CCM</cell><cell cols="3">ResNet-50 73.8 90.2</cell><cell>80.9</cell><cell>69.6</cell><cell>80.1</cell><cell cols="2">79.3 93.7</cell><cell>85.6</cell><cell>74.8</cell><cell>85.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Comparison between CCM-I and CCM-V on the visible and invisible keypoints in the COCO minival set.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell cols="2">Kpt. Type AP</cell><cell>AP @.5</cell><cell>AP @.75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell><cell>AR @.5</cell><cell>AR @.75</cell><cell>AR M</cell><cell>AR L</cell></row><row><cell>CCM-I</cell><cell>ResNet-50</cell><cell>Visible</cell><cell cols="2">79.3 94.6</cell><cell>86.7</cell><cell>77.4</cell><cell>82.2</cell><cell cols="2">82.4 95.5</cell><cell>88.7</cell><cell>80.2</cell><cell>85.5</cell></row><row><cell></cell><cell></cell><cell>Invisible</cell><cell cols="2">40.2 64.2</cell><cell>39.5</cell><cell>40.2</cell><cell>42.3</cell><cell cols="2">47.8 69.8</cell><cell>48.0</cell><cell>45.1</cell><cell>53.2</cell></row><row><cell></cell><cell></cell><cell>Visible</cell><cell cols="2">79.4 94.7</cell><cell>87.0</cell><cell>76.9</cell><cell>82.4</cell><cell cols="2">82.0 95.3</cell><cell>88.1</cell><cell>79.5</cell><cell>85.4</cell></row><row><cell cols="2">CCM-V ResNet-50</cell><cell>??</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>-0.5</cell><cell>0.2</cell><cell>-0.4</cell><cell>-0.2</cell><cell>-0.6</cell><cell>-0.7</cell><cell>-0.1</cell></row><row><cell></cell><cell></cell><cell>Invisible</cell><cell cols="2">55.0 79.4</cell><cell>57.1</cell><cell>54.7</cell><cell>57.8</cell><cell cols="2">61.4 82.1</cell><cell>63.9</cell><cell>58.8</cell><cell>66.9</cell></row><row><cell></cell><cell></cell><cell>??</cell><cell cols="2">14.8 15.2</cell><cell>17.6</cell><cell>14.5</cell><cell>15.5</cell><cell cols="2">13.6 12.3</cell><cell>15.9</cell><cell>13.7</cell><cell>13.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Comparison between CCM-I and CCM-V on different types of instances w.r.t. the numbers of annotated keypoints.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Number of annotated keypoints per instance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Metric 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell></row><row><cell>CCM-I</cell><cell>AP</cell><cell cols="2">32.7 25.0</cell><cell cols="15">33.4 38.9 48.1 52.5 60.2 63.4 67.1 68.0 74.2 77.8 79.0 81.6 82.6 88.8 90.9</cell></row><row><cell>CCM-V</cell><cell>AP ??</cell><cell cols="2">33.9 23.9 1.2 -1.1</cell><cell cols="15">37.6 43.6 48.2 58.1 64.2 70.0 72.6 72.8 77.9 81.5 81.7 83.4 84.1 89.3 91.2 4.2 4.7 0.1 5.6 4.0 6.6 5.5 4.8 3.7 3.7 2.7 1.8 1.5 0.5 0.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A video demo can be found in https://github.com/ chaimi2013/CCM/video</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://cocodataset.org/index.htm# keypoints-leaderboard</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ensafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recursive context routing for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthesizing a scene-specific pedestrian detector and pose estimator for static video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beainy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1027" to="1044" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Putting the pieces together: Connected poselets for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops (ICCVW)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1196" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mri</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3028" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence and statistics</title>
		<imprint>
			<biblScope unit="volume">562</biblScope>
			<biblScope unit="page">570</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>arXiv:190100148</idno>
		<title level="m">Rethinking on multi-stage networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto learning attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards real-time physical human-robot interaction using skeleton information and hand gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mazhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Passama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherubini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning semantic-aligned action representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3715" to="3725" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning mutual visibility relationship for pedestrian detection with a deep model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="27" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">E</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="750" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast human pose detection using randomized hierarchical cascades of rejectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Orrite-Uru?uela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="52" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint estimation of human pose and conversational groups from social scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="410" to="429" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A century of gestalt psychology in visual perception: I. perceptual grouping and figure-ground organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kubovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Der Heydt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1172</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="596" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial-depth super resolution for range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nist?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Distributionaware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Human pose estimation with spatial contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>arXiv:190101760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Empowering things with intelligence: A survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D ;</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Pose2seg: Detection free human instance segmentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
