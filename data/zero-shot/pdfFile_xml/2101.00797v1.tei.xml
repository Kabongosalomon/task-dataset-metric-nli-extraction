<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Low-frequency Information in Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Bo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
							<email>shenhuawei@ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Lab of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Low-frequency Information in Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have been proven to be effective in various network-related tasks. Most existing GNNs usually exploit the low-frequency signals of node features, which gives rise to one fundamental question: is the low-frequency information all we need in the real world applications? In this paper, we first present an experimental investigation assessing the roles of low-frequency and high-frequency signals, where the results clearly show that exploring low-frequency signal only is distant from learning an effective node representation in different scenarios. How can we adaptively learn more information beyond low-frequency information in GNNs? A well-informed answer can help GNNs enhance the adaptability. We tackle this challenge and propose a novel Frequency Adaptation Graph Convolutional Networks (FAGCN) with a selfgating mechanism, which can adaptively integrate different signals in the process of message passing. For a deeper understanding, we theoretically analyze the roles of low-frequency signals and high-frequency signals on learning node representations, which further explains why FAGCN can perform well on different types of networks. Extensive experiments on six real-world networks validate that FAGCN not only alleviates the over-smoothing problem, but also has advantages over the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Networks, such as social networks, citation networks and molecular networks, are ubiquitous in the real world. Recently, the emerging graph neural networks (GNNs) have demonstrated powerful ability to learn node representations by jointly encoding network structures and node features <ref type="bibr" target="#b23">(Wu et al. 2020;</ref><ref type="bibr" target="#b28">Zhang, Cui, and Zhu 2020;</ref><ref type="bibr" target="#b20">Wang et al. 2020a</ref>). This strategy has been proven to be effective in various tasks, including link prediction <ref type="bibr" target="#b27">(Zhang and Chen 2018)</ref>, node classification <ref type="bibr" target="#b7">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b19">Velickovic et al. 2018)</ref> and graph classification <ref type="bibr" target="#b4">(Errica et al. 2020)</ref>.</p><p>In general, GNNs update node representations by aggregating information from neighbors, which can be seen as a special form of low-pass filter <ref type="bibr" target="#b9">Li et al. 2019)</ref>. Some recent studies <ref type="bibr" target="#b13">(NT and Maehara 2019;</ref><ref type="bibr" target="#b24">Xu et al. 2019a)</ref> show that the smoothness of signals, i.e., low-frequency information, are the key to the success of GNNs. However, is the low-frequency information all we need and what roles do other information play in GNNs? This is a fundamental question which motivates us to rethink whether GNNs comprehensively exploit the information in node features when learning node representation.</p><p>Firstly, the low-pass filter in GNNs mainly retains the commonality of node features, which inevitably ignores the difference, so that the learned representations of connected nodes become similar. Thanks to the smoothness of low-frequency information, this mechanism may work well for assortative networks, i.e., similar nodes tend to connect with each other <ref type="bibr" target="#b24">(Xu et al. 2019a</ref>). However, the real-world networks are not always assortative, but sometimes disassortative, i.e., nodes from different classes tend to connect with each other <ref type="bibr" target="#b12">(Newman 2003)</ref>. For example, the chemical interactions in proteins often occur between different types of amino acids ). If we force the representation of connected proteins to be similar by employing low-pass filter, obviously, the performance will be largely hindered. The low-frequency information here is insufficient to support the inference in such networks. Under the circumstances, the high-frequency information, capturing the difference between nodes, may be more suitable. Even the raw features, containing both low-and high-frequency information, are alternative solution <ref type="bibr" target="#b21">(Wang et al. 2020b)</ref>. Secondly, it is well established that the node representation will becomes indistinguishable when we always utilize low-pass filter, causing over-smoothing <ref type="bibr" target="#b14">(Oono and Suzuki 2020)</ref>. This reminds us that low-pass filter of current GNNs is distant from optimal for real world scenarios.</p><p>To provide more evidence for the above analysis, we focus on low-frequency and high-frequency signals as an example, and present experiments to assess their roles (details can be seen in Section 2). The results clearly show that both of them are helpful for learning node representations. Specifically, we find that when a network exhibits disassortativity, highfrequency signals perform much better than low-frequency signals. This implies that the high-frequency information, which is largely eliminated by the current GNNs, is not always useless, and the low-frequency information is not always optimal for the complex networks. Once the weakness of low-frequency information in GNNs is identified, a natural question is how to use signals of different frequencies in GNNs and, at the same time, makes GNNs suitable for different type of networks? To answer this question, two challenges need to be solved: (1) Both the low-frequency and high-frequency signals are the parts of the raw features. Traditional filter is specifically designed for one certain signal, and cannot well extract different frequency signals simultaneously.</p><p>(2) Even we can extract different information, however, the assortativity of real-world networks is usually agnostic and varies greatly, moreover, the correlation between task and different information is very complex, so it is difficult to decide what kind of signals should be used: raw features, low-frequency signals, high-frequency signals or their combination.</p><p>In this paper, we design a general frequency adaptation graph convolutional networks called FAGCN, to adaptively aggregate different signals from neighbors or itself. We first employ the theory of graph signal processing to formally define an enhanced low-pass and high-pass filter to separate the low-frequency and high-frequency signals from the raw features. Then we design a self-gating mechanism to adaptively integrate the low-frequency signals, high-frequency signals and raw features, without knowing the assortativity of network. Theoretical analysis proves that FAGCN is a generalization of most existing GNNs and it has a capability to freely shorten or enlarge the distance between node representations, which further explains why FAGCN can perform well on different types of networks.</p><p>The contribution of this paper is summarized as follows:</p><p>? We study the roles of both low-frequency and highfrequency signals in GNNs and verify that high-frequency signals are useful for disassortative networks.</p><p>? We propose a novel graph convolutional networks FAGCN, which can adaptively change the proportion of lowfrequency and high-frequency signals without knowing the types of networks.</p><p>? We theoretically prove that the expressive power of FAGCN is greater than other GNNs. Moreover, our proposed FAGCN is able to alleviate the over-smoothing problem. Extensive experiments on six real-world networks validate that FAGCN has advantages over state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">An Experimental Investigation</head><p>In this section, taking the low-frequency and high-frequency signals as an example, we analyze their roles in learning node representations. Specifically, we test their performance of node classification on a series of synthetic networks. The main idea is to gradually increase the disassortativity of the synthetic networks, and observe how the performance of these two signals changes. We generate a network with 200 nodes and randomly divide them into 2 classes. For each node in class one, we sample a 20-dimensional feature vector from Gaussian distribution N (0.5, 1), while for the nodes in class two, the distribution is N (?0.5, 1). Besides, the connections in the same class are generated from a Bernoulli distribution with probability p = 0.05, and the probability of connections between two classes q varies from 0.01 to 0.1. When q is small, the network exhibits assortativity; As q increases, the network gradually exhibits disassortativity. We then apply the low-pass and high-pass filters, described in Section 3, to node classification task. Half of the nodes are used for training and the remains are used for testing. <ref type="figure" target="#fig_0">Figure 1</ref>(a) illustrates that with the increase of interconnection q, the accuracy of low-frequency signals decreases, while the accuracy of high-frequency signals increases gradually. This proves that both the low-frequency and high-frequency signals are helpful in learning node representations. The reason why existing GNNs fail when q increases is that, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), they only aggregate low-frequency signals from neighbors, i.e., making the node representations become similar, regardless of whether nodes belong to the same class, thereby losing the discrimination. When the network becomes disassortative, the effectiveness of high-frequency signals appears, but as shown in <ref type="figure" target="#fig_0">Figure  1</ref>(a), a single filter cannot achieve optimal results in all cases. Our proposed FAGCN, which combines the advantages of both low-pass and high-pass filters, can aggregate the lowfrequency signals of neighbors within the same class and high-frequency signals of neighbors from different classes, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c), thereby showing the best performance on every synthetic network.  <ref type="figure">Figure 2</ref>: The relations between eigenvalues and amplitudes in different filters.</p><formula xml:id="formula_0">1 1 + (a) FL 0 1 2 Eigenvalue ( ) 1 0 1 Amplitude (g ) (1 ) 2 (1 + ) 2 (b) F 2 L 0 1 2 Eigenvalue ( ) 1 0 1 Amplitude (g ) 1 1 + (c) FH 0 1 2 Eigenvalue ( ) 1 0 1 Amplitude (g ) ( 1) 2 ( 1 + ) 2 (d) F 2 H</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Model: FAGCN</head><p>Consider an undirected graph G = (V, E) with adjacency matrix A ? R N ?N , where V is a set of nodes with |V | = N and E is a set of edges. The normalized graph Laplacian matrix is defined as</p><formula xml:id="formula_1">L = I n ? D ?1/2 AD ?1/2 , where D ? R N ?N is a diagonal degree matrix with D i,i = j A i,j</formula><p>and I n denotes the identity matrix. Because L is a real symmetric matrix, it has a complete set of orthonormal eigenvectors {u l } n l=1 ? R n , each of which has a corresponding eigenvalue ? l ? [0, 2] <ref type="bibr" target="#b1">(Chung and Graham 1997)</ref>. Through the eigenvalues and eigenvectors, we have</p><formula xml:id="formula_2">L = U ?U , where ? = diag([? 1 , ? 2 , ? ? ? , ? n ]).</formula><p>Graph Fourier Transform. According to theory of graph signal processing <ref type="bibr" target="#b17">(Shuman et al. 2013)</ref>, we can treat the eigenvectors of normalized Laplacian matrix as the bases in graph Fourier transform. Given a signal x ? R n , the graph Fourier transform is defined asx = U x, and the inverse graph Fourier transform is x = Ux. Thus, the convolutional * G between the signal x and convolution kernel f is:</p><formula xml:id="formula_3">f * G x = U U f U x = U g ? U x,<label>(1)</label></formula><p>where denotes the element-wise product of vectors and g ? is a diagonal matrix, which represents the convolutional kernel in the spectral domain, replacing U f . Spectral CNN <ref type="bibr" target="#b0">(Bruna et al. 2014</ref>) uses a non-parametric convolutional kernel g ? = diag({? i } n i=1 ). ChebNet <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref> parameterizes convolutional kernel with a polynomial expansion g ? = K?1 k=0 ? k ? k . GCN defines the convolutional kernel as g ? = I ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Separation</head><p>As discussed in Section 2, both the low-frequency and highfrequency signals are helpful for learning node representations. To make full use of them, we design a low-pass filter F L and a high-pass filter F H to separate the low-frequency and high-frequency signals from the node features:</p><formula xml:id="formula_4">F L = ?I + D ?1/2 AD ?1/2 = (? + 1)I ? L, F H = ?I ? D ?1/2 AD ?1/2 = (? ? 1)I + L,<label>(2)</label></formula><p>where ? is a scaling hyper-parameter limited in [0, 1]. If we use F L and F H to replace the convolutional kernel f in Equation 1. The signal x is filtered by F L and F H as:</p><formula xml:id="formula_5">F L * G x = U [(? + 1)I ? ?]U x = F L ? x, F H * G x = U [(? ? 1)I + ?]U x = F H ? x.<label>(3)</label></formula><p>Therefore, the convolutional kernel of F L is g ? = (? + 1)I ? ?, rewritten as g ? (? i ) = ? + 1 ? ? i , shown in <ref type="figure">Figure 2</ref>(a). When ? i &gt; 1 + ?, g ? (? i ) &lt; 0, which gives a negative amplitude. To avoid this, we consider the second-order convolution</p><formula xml:id="formula_6">kernel F 2 L with g ? (? i ) = (? + 1 ? ? i ) 2 , shown in Figure 2(b). When ? i = 0, g ? (? i ) = (? + 1) 2 &gt; 1 and when ? i = 2, g ? (? i ) = (? ? 1) 2 &lt; 1,</formula><p>which amplifies the low-frequency signals and restrains the high-frequency signals. Remark 1. (Enhanced filters) As in <ref type="figure">Figure 2</ref>, compared with traditional low-pass filters, e.g., GCN and SGC </p><formula xml:id="formula_7">), F L is an enhanced low-pass filter. Convolutional kernel of second-order GCN is g ? (? i ) = (1 ? ? i ) 2 . When ? i = 0, the amplitude of GCN is g ? (? i ) = 1 &lt; (1 + ?) 2 .</formula><p>Hence, the value of F L is greater than GCN in low-pass filtering. Similarly, F H is an enhanced high-pass filter, which provides a greater value for the high-frequency signals.</p><p>Separating the low-frequency and high-frequency signals from the node features provides a feasible way to deal with different networks, e.g., low-frequency signals for assortative networks and high-frequency signals for disassortative networks. However, this way has two disadvantages: One is that selecting signals requires a priori knowledge, i.e., we actually do not know whether a network is assortative or disassortative beforehand. The other is that, as in Equation 3, it requires matrix multiplication, which is undesirable for large graphs <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref>. Therefore, an efficient method that can adaptively aggregate low-frequency and high-frequency signals is desired. Remark 2. (Concrete meaning of signals) In Equation 2, we have F L = ?I + D ?1/2 AD ?1/2 and F H = ?I ? D ?1/2 AD ?1/2 . Therefore, the concrete meaning of lowfrequency signal F L ? x is the sum of node features and neighborhood features in spatial domain, while high-frequency signal F H ? x represents the difference between node features and neighborhood features in spatial domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Aggregation</head><p>Before introducing the details, we first compare the aggregation process of existing GNNs and FAGCN in <ref type="figure">Figure 3</ref>. The left shows that existing GNNs consider learning the importance (? ij ) of each node in aggregation. The right is FAGCN that uses two coefficients (? L ij and ? H ij ) to aggregate lowfrequency and high-frequency signals from the neighbors, respectively.</p><formula xml:id="formula_8">h ig h ? ! ? " ? # lo w low high ? ! $ Avg/Cat !" # !" $ !% # !% $ ? ! ? " ? # ? ! $ Avg/Cat !% !! !"</formula><p>Figure 3: Left: The aggregation process of existing GNNs, and ? ij indicates the importance of node j to node i. Right:</p><p>The aggregation process of FAGCN, and ? L ij , ? H ij denote the proportions of low-frequency and high-frequency signals of node j to node i, respectively.</p><p>The input of our model are the node features, H = {h 1 , h 2 , ? ? ? , h N } ? R N ?F , where F is the dimension of the node features. For the purpose of frequency adaptation, a basic idea is to use the attention mechanism to learn the proportion of low-frequency and high-frequency signals:</p><formula xml:id="formula_9">h i = ? L ij (F L ?H) i +? H ij (F H ?H) i = ?h i + j?Ni ? L ij ? ? H ij d i d j h j ,<label>(4)</label></formula><p>whereh i is the aggregated representation of node i. N i and d i denote the neighbor set and degree of node i, respectively. ? L ij and ? H ij represent the proportions of node j's low-frequency and high-frequency signals to node i. We set ?</p><formula xml:id="formula_10">L ij + ? H ij = 1 and ? G ij = ? L ij ? ? H ij .</formula><p>In the following, we show that ? G ij can be interpreted from two perspectives.</p><p>Remark 3. (Two perspectives of ? G ij ) One is that ? G ij indirectly represents the proportion of low-frequency and highfrequency signals. ? G ij &gt; 0, i.e., ? L ij &gt; ? H ij , means that low-frequency signals dominate the representations and vice versa. Based on ? G ij , we can calculate the value of ? L ij and ? H ij , so as to achieve the proportions of signals. Another is that ? G ij denotes the coefficients of neighbors in aggregation. ? G ij &gt; 0 represents the sum of node features and neighborhood features, i.e., h i + h j , while ? G ij &lt; 0 represents the difference between them, i.e., h i ?h j , as explained in Remark 2. Besides, when ? G ij ? 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.</p><p>In order to learn the coefficients ? G ij effectively, we need to consider the features of both the node itself and its neighbors. Therefore, we propose a shared self-gating mechanism R F ? R F ? R to learn the coefficients:</p><formula xml:id="formula_11">? G ij = tanh g [h i h j ] ,<label>(5)</label></formula><p>where denotes the concatenation operation, g ? R 2F can be seen as a shared convolutional kernel <ref type="bibr" target="#b19">(Velickovic et al. 2018</ref>) and tanh(?) is the hyperbolic tangent function, which can naturally limits the value of ? G ij in [?1, 1]. Besides, to make use of the structural information, we only calculate the coefficients among the node and its first-order neighbors N i .</p><p>After calculating ? G ij , we can aggregate the representations of neighbors:</p><formula xml:id="formula_12">h i = ?h i + j?Ni ? G ij d i d j h j ,<label>(6)</label></formula><p>where h i denotes the aggregated representation of node i. Note that when aggregating information from neighbors, the degrees are used to normalize the coefficients, thus preventing the aggregated representations from being too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Whole Architecture of FAGCN</head><p>In the previous section, we introduce the message passing process of FAGCN. Here, we formally define the whole architecture of FAGCN. Some recent studies <ref type="bibr" target="#b2">Cui et al. 2020</ref>) emphasize that the entanglement of filters and weight matrices may be harmful to the performance and robustness of the model. Motivated by this, we first use a multilayer perceptron (MLP) to apply the non-linear transform to the raw features. Then we propagate the representations through Eq. 6. The mathematical expression of FAGCN is defined as: h</p><formula xml:id="formula_13">(0) i = ?(W 1 h i ) ? R F ?1 h (l) i = ?h (0) i + j?Ni ? G ij d i d j h (l?1) j ? R F ?1 h out = W 2 h (L) i ? R K?1 ,<label>(7)</label></formula><p>where W 1 ? R F ?F , W 2 ? R F ?K are the weight matrices, ? is the activation function, F denotes the dimension of hidden layers, l indicates the layers, ranging from 1 to L, and K represents the number of classes. The complexity of a single layer FAGCN is O((N + |E|) ? F ), which is approximately linear with the number of edges and nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Connection to Existing GNNs</head><p>FAGCN is a generalization of most existing GNNs. Specifically, when we set the coefficients ? G ij to 1, FAGCN acts like GCN and when we use softmax function to normalize ? G ij , FAGCN becomes GAT. Therefore, as indicated in Remark 2, because the coefficients in GCN and GAT are both greater than zero, they prefer to aggregate the low-frequency signals. However, FAGCN can learn a coefficient that can be positive or negative, to adaptively aggregate low-frequency and high-frequency signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expressive Power of FAGCN</head><p>We analyze the expressive power of FAGCN from the perspective of the distance between node representations. Assume that (u, v) is a pair of connected nodes, and h u , h v are the node features. Let D, D L , D H be the distance of node features, low-frequency signals of node features and highfrequency signals of node features, respectively. Proposition 1. Low-pass filtering makes the representations become similar, while high-pass filtering makes the representations become discriminative.</p><formula xml:id="formula_14">D = h u ? h v 2 . D L = (?h u + h v ) ? (?h v + h u ) 2 = |1 ? ?|D. D H = (?h u ? h v ) ? (?h v ? h u ) 2 = |1 + ?|D.</formula><p>Proof. It is easy to see that D H &gt; D &gt; D L . This indicates that compared with the original distance D, the distance D L induced by low-frequency signals is smaller, implying that low-frequency signals can make the representations of connected nodes become similar. While the distance D H induced by high-frequency signals is larger, implying that high-frequency signals can make the representations of connected nodes become discriminative.</p><p>We have analyzed the roles of low-frequency and highfrequency signals in representation learning. Obviously, FAGCN can choose to shorten or enlarge the distance between node representations flexibly, while most existing GNNs cannot. Proposition 2. Most existing GNNs, e.g., GCN, only have the capability to make representations of nodes become similar.</p><p>Proof. The filter used in GCN is: (D + I) ?1/2 (A + I)(D + I) ?1/2 . Hence, the distance of representations learned by GCN is:</p><formula xml:id="formula_15">D G ? ( 1 du h u +h v )?( 1 dv h v +h u ) 2 ? |1? 1 d |D &lt; D (s.t. d u ? d v ? d).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Assortative datasets. We choose the commonly used citation networks, e.g., Cora, Citeseer and Pubmed for assortative datasets. Edges in these networks represent the citation relationship between two papers (undirected), node features are the bag-of-words vector of the papers and labels are the fields of papers. In each network, we use 20 labeled nodes per class for training, 500 nodes for validation and 1000 nodes for testing. Details can be found in <ref type="bibr" target="#b7">(Kipf and Welling 2017)</ref>. Disassortative datasets. We consider the Wikipedia networks 1 and Actor co-occurrence network <ref type="bibr" target="#b18">(Tang et al. 2009</ref>) for disassortative datasets. Chameleon and Squirrel are two Wikipedia networks. Edges represent the hyperlinks between two pages, node features are some informative nouns in the pages and labels correspond to the traffic of the pages. In Actor co-occurrence network, each node represents an actor, and the edges denote the collaborations of them. Node features are the keywords in Wikipedia and labels are the types of actors. Since there is no standard division for these 1 http://snap.stanford.edu/data/wikipedia-article-networks.html networks. To verify the effectiveness and robustness, we use 20% for validation, 20% for testing and change the training ratio from 10% to 60%. More detailed characteristics of the datasets can be found in <ref type="table" target="#tab_0">Table 1</ref>. Note that a higher value of the second column represents a more obvious assortativity <ref type="bibr" target="#b12">(Newman 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We compare FAGCN with two types of representative GNNs: Spectral-based methods, i.e., SGC , GCN <ref type="bibr" target="#b7">(Kipf and Welling 2017)</ref>, ChebNet <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref> and GWNN <ref type="bibr" target="#b25">(Xu et al. 2019b</ref>); Spatialbased methods, i.e., GIN <ref type="bibr" target="#b26">(Xu et al. 2019c</ref>), GAT <ref type="bibr" target="#b19">(Velickovic et al. 2018)</ref>, MoNet <ref type="bibr" target="#b11">(Monti et al. 2017)</ref>, GraphSAGE (Hamilton, Ying, and Leskovec 2017) and APPNP <ref type="bibr" target="#b8">(Klicpera, Bojchevski, and G?nnemann 2019)</ref>. For disassortative networks, we add Geom-GCN <ref type="bibr" target="#b15">(Pei et al. 2020</ref>) and MLP as new benchmarks. All methods were implemented in Pytorch with Adam optimizer <ref type="bibr" target="#b6">(Kingma and Ba 2015)</ref>. We run 10 times and report the mean values with standard deviation. The hidden unit is fixed at 16 in assortative networks and 32 in disassortative networks. The hyper-parameter search space is: learning rate in {0.01, 0.005}, dropout in {0.4, 0.5, 0.6}, weight decay in {1E-3, 5E-4, 5E-5}, number of layers in {1, 2, ? ? ? , 8}, ? in {0.1, ? ? ? , 1.0}.</p><p>In assortative datasets, we use hyper-parameters in previous literature for baselines. For FAGCN, the hyper-parameter setting is: learning rate = 0.01, dropout = 0.6, weight decay = 1E-3, layers = 4. ? = 0.2, 0.3, 0.3 for Cora, Citeseer and Pubmed. The patience of early stop is set to 100. In disassortative datasets, the hyper-parameter for FAGCN is: learning rate = 0.01, dropout = 0.5, weight decay = 5E-5, layers = 2. ? = 0.4, 0.3, 0.5 for Chameleon, Squirrel and Actor, respectively. Besides, we run 500 epochs and choose the model with highest validation accuracy for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Classification on Different Types of Networks</head><p>The performance of different methods on assortative networks is summarized in <ref type="table" target="#tab_1">Table 2</ref>. GraphHeat designs a lowpass filer through heat kernel, which can better capture the low-frequency information than GCN <ref type="bibr" target="#b24">(Xu et al. 2019a</ref>  Therefore, it performs best in the baselines. But we can see that FAGCN exceed the benchmarks on most networks due to the enhanced low-pass filter, which validates the importance of low-pass filters in the assortative networks. Besides, the performance on disassortative networks is illustrated in <ref type="figure">Fig. 4</ref>. Note that we do not choose all baselines because the methods focus on low-pass filtering have poor performance, and we use GCN and GAT as representatives. In addition, APPNP leverages residual connection to preserve the information of raw features, ChebNet uses ChebNet polynomials to approximate arbitrary convolution kernels and Geom-GCN is the state-of-the-art on disassortative networks. Therefore, comparing FAGCN with these baselines can reflect the superiority of FAGCN. <ref type="figure">From Fig. 4</ref>, we can see that GCN and GAT perform worse than other methods, which indicates that only using low-pass filters is not suitable for disassortative networks. APPNP and ChebNet perform better than GCN and GAT, which shows that the raw features and polynomials can preserve the high-frequency information to some extent. Finally, FAGCN performs best in most datasets and label rates, which reflects the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Alleviating Over-smoothing Problem</head><p>To validate whether FAGCN can alleviate the over-smoothing problem, we compare the performance of GCN and FAGCN under different model depth. The results are shown in <ref type="figure">Fig. 5</ref>. It can be seen that GCN achieves the best performance at two layers. As the number of layers increases, the performance of GCN drops rapidly, which indicates that GCN suffers from over-smoothing seriously. Instead, the results of FAGCN are stable and significantly higher than GCN on different types of networks. The reasons are two-folds: one is that in Section 4.2 we show that negative weights can prevents node representations from being too similar, which benefits deeper network architecture. Another is that we add the raw features, containing both low-frequency and high-frequency information, to each layer, which further keeps node representations from becoming indistinguishable. Through these two designs, when the model going deep, the performance of FAGCN is significantly better than GCN, which indicates that FAGCN has a good capability to alleviate over-smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Visualization of Edge Coefficients</head><p>In order to verify whether FAGCN can learn different edge coefficients to adapt to different networks, we visualize the coefficients ? G ij , extracted from the last layer of FAGCN. Specifically, we divide the edges into intra-edges and interedges based on whether two connected nodes have the same label. It can be seen from <ref type="figure" target="#fig_3">Fig. 6(a)</ref> that in the networks with large assortativity, i.e., Cora, Citeseer and Pubmed, all edges are concentrated at the positive weights, which implies that the low-pass filter plays a major role in classification. However, in <ref type="figure" target="#fig_3">Fig. 6(b)</ref> and 6(c), a lot of inter-edges are distributed in negative weights, which shows that in the network with small assortativity, the high-frequency signal plays an  important role in node classification. Moreover, there is an interesting phenomenon that in <ref type="figure" target="#fig_3">Fig. 6(d)</ref> the coefficients of edges are concentrated at zero. One possible reason is that the assortativity of Actor is quite small, which implies that the structures contributes less to the results of node classification, instead, the raw features dominate the classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Details of Wikipedia Networks</head><p>In this section, we aim to give more details of Wikipedia networks. First of all, Chameleon and Squirrel were originally collected for regression task, i.e., traffic prediction (Rozemberczki, Allen, and Sarkar 2019). We divide the traffic into three categories: less than 1000, between 1000 and 10000 and more than 10000, so that they can be applied to node classification task. Secondly, labels of Chameleon and Squirrel are different from those in Geom-GCN. The reason is that in the disassortative networks provided by <ref type="bibr" target="#b15">(Pei et al. 2020</ref>), i.e., Cham-5 and Squi-5 in <ref type="table" target="#tab_3">Table 3</ref>, we find that GCN performs much better than MLP. This is a strange phenomenon, because MLP uses raw features as input, which contains highfrequency information, so its performance should be better than GCN . Therefore, we redivide the labels based on traffic, i.e., Cham-3 and Squi-3 in <ref type="table" target="#tab_3">Table 3</ref>, where the performance of GCN and MLP is more reasonable. Besides, we can see that FAGCN perform best on all four datasets, so its effectiveness is still guaranteed across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Spectral Graph Neural Networks. Spectral GNNs aim to define the convolution kernel in spectral domain, by leveraging the theory of graph signal processing. Spectral CNN <ref type="bibr" target="#b0">(Bruna et al. 2014</ref>) treats the convolution kernel as a trainable diagonal matrix and directly learns the amplitudes of signals.</p><p>However, it requires the decomposition of Laplacian matrix, which is inefficient. To deal with this problem, ChebNet <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref> uses the polynomial of Laplacian matrix to approximate the convolution kernel and make better performance. GCN <ref type="bibr" target="#b7">(Kipf and Welling 2017)</ref> is the first-order approximation of ChebNet with a self-loop mechanism. GraphHeat <ref type="bibr" target="#b24">(Xu et al. 2019a</ref>) designs a more powerful low-pass filter through heat kernel. Besides, GWNN <ref type="bibr" target="#b25">(Xu et al. 2019b</ref>) replaces the eigenvectors with wavelet bases so as to further improve the efficiency of the model. Generally, spectral methods have good interpretability for the signal processing on graph, but lack generalization <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref>. Spatial Graph Neural Networks. Spatial GNNs focus on the design of aggregation function. GraphSAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref> designs a permutation-invariant aggregator for message passing; GAT <ref type="bibr" target="#b19">(Velickovic et al. 2018</ref>) employs self-attention to calculate the coefficients of neighbors in aggregation; MoNet <ref type="bibr" target="#b11">(Monti et al. 2017</ref>) provides a unified generalization of graph convolutional architectures in spatial domain; PPNP <ref type="bibr" target="#b8">(Klicpera, Bojchevski, and G?nnemann 2019)</ref> incorporates personalized PageRank to the aggregation function; Geom-GCN <ref type="bibr" target="#b15">(Pei et al. 2020</ref>) utilizes the structural similarity to capture the long-range dependencies in disassortative graphs. H2GNN  separates the raw features and aggregated features so as to preserve both high-frequency and low-frequency information, but it lacks adaptability; Non-Local GNN <ref type="bibr" target="#b10">(Liu, Wang, and Ji 2020)</ref> designs an attention-guided sorting mechanism to transform the disassortative networks into assortative networks, which costs a lot of computations. Generally, spatial methods are more flexible and scalable, but lack interpretability. It is worth noting that FAGCN is a spatial method, but it still has good interpretability, which combines the advantages of both spectral and spatial methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we make the attempt to study the roles of lowfrequency and high-frequency signals in GNNs and show that both of them are helpful in learning node representations. Based on this observation, we design a novel frequency adaptation graph convolutional network to adaptively combine the low-frequency and high-frequency signals. Theoretical analysis shows that the expressive power of our model is greater than most existing GNNs. An important direction of future work is to use more signals with different frequencies, e.g., the intermediate frequency signals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Classification accuracy of low-frequency signals, high-frequency signals and our model FAGCN. X-axis denotes probability of inter-connection q. (b) Existing GNNs aggregate the low-frequency signals of neighbors. (c) FAGCN aggregates the low-frequency signals of neighbors within the same class and high-frequency signals of neighbors from different classes, where the color indicates the node label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Classification accuracy of different methods under different label rates on disassortative networks. Classification accuracy with different model depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of edge coefficients on different networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of datasets</figDesc><table><row><cell>Dataset</cell><cell cols="2">Assortivity Nodes</cell><cell cols="3">Edges Classes Features</cell></row><row><cell>Cora</cell><cell>0.771</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>0.671</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell></row><row><cell>Pubmed</cell><cell>0.686</cell><cell>19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell></row><row><cell>Chameleon</cell><cell>0.180</cell><cell>2,277</cell><cell>36,101</cell><cell>3</cell><cell>2,325</cell></row><row><cell>Squirrel</cell><cell>0.018</cell><cell cols="2">5,201 217,073</cell><cell>3</cell><cell>2,089</cell></row><row><cell>Actor</cell><cell>0.003</cell><cell>7,600</cell><cell>33,544</cell><cell>5</cell><cell>931</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of node classification results (in percent).</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>SGC</cell><cell>81.0%</cell><cell>71.9%</cell><cell>78.9%</cell></row><row><cell>GCN</cell><cell>81.5%</cell><cell>70.3%</cell><cell>79.0%</cell></row><row><cell>GWNN</cell><cell>82.8%</cell><cell>71.7%</cell><cell>79.1%</cell></row><row><cell>ChebNet</cell><cell>81.2%</cell><cell>69.8%</cell><cell>74.4%</cell></row><row><cell>GraphHeat</cell><cell>83.7%</cell><cell>72.5%</cell><cell>80.5%</cell></row><row><cell>GIN</cell><cell>77.6%</cell><cell>66.1%</cell><cell>77.0%</cell></row><row><cell>GAT</cell><cell>83.0%</cell><cell>72.5%</cell><cell>79.0%</cell></row><row><cell>MoNet</cell><cell>81.7%</cell><cell>-</cell><cell>78.8%</cell></row><row><cell>APPNP</cell><cell>83.7%</cell><cell>72.1%</cell><cell>79.2%</cell></row><row><cell>GraphSAGE</cell><cell>82.3%</cell><cell>71.2%</cell><cell>78.5%</cell></row><row><cell>FAGCN</cell><cell cols="3">84.1?0.5% 72.7?0.8% 79.4?0.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Accuracy</cell><cell>0.68 0.70 0.72 0.74 0.76</cell><cell>MLP GCN GAT APPNP ChebNet Geom-GCN FAGCN</cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.54 0.56 0.58 0.60 0.62 0.64 0.66</cell><cell></cell><cell></cell><cell></cell><cell>MLP GCN Geom-GCN FAGCN ChebNet APPNP GAT</cell><cell>Accuracy</cell><cell>0.28 0.30 0.32 0.34 0.36</cell><cell>MLP GCN GAT APPNP ChebNet Geom-GCN FAGCN</cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell>20%</cell><cell>30% Label Rate 40%</cell><cell>50%</cell><cell>60%</cell><cell>10%</cell><cell>20%</cell><cell>30% Label Rate 40%</cell><cell>50%</cell><cell>60%</cell><cell></cell><cell>10%</cell><cell>20%</cell><cell>30% Label Rate 40%</cell><cell>50%</cell><cell>60%</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) Chameleon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Squirrel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) Actor</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy with different label division.</figDesc><table><row><cell></cell><cell>This paper</cell><cell>(Pei et al. 2020)</cell></row><row><cell>Method</cell><cell cols="2">Cham-3 Squi-3 Cham-5 Squi-5</cell></row><row><cell>FAGCN</cell><cell>76.1% 66.7%</cell><cell>61.7% 39.7%</cell></row><row><cell>Geom-GCN</cell><cell>73.2% 63.3%</cell><cell>60.9% 38.1%</cell></row><row><cell>GCN</cell><cell>72.3% 61.9%</cell><cell>59.8% 36.9%</cell></row><row><cell>MLP</cell><cell>74.3% 63.1%</cell><cell>46.4% 29.7%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>This work is supported in part by the National Natural Science Foundation of China (No. U1936220, U1936104, 61772082, 61702296, 62002029, 61972442), Meituan-Dianping Group and BUPT Excellent Ph.D. Students Foundation (No. CX2020115). Huawei Shen is funded by Beijing Academy of Artificial Intelligence (BAAI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Spectral graph theory</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive Graph Encoder for Attributed Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="976" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Fair Comparison of Graph Neural Networks for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Label Efficient Semi-Supervised Learning via Graph Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9582" to="9591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Non-Local Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno>abs/2005.14612</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mixing patterns in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26126</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno>abs/1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph Neural Networks Exponentially Lose Expressive Power for Node Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multiscale Attributed Node Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<idno>abs/1909.13021</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="83" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/2011.14867</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AM-GCN: Adaptive Multi-channel Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H S</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<title level="m">A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph Convolutional Networks using Heat Kernel for Semisupervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1928" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph Wavelet Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5171" to="5181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
