<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Align Deep Features for Oriented Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Gui-Song</forename><forename type="middle">Xia</forename></persName>
						</author>
						<title level="a" type="main">Align Deep Features for Oriented Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object detection</term>
					<term>aerial images</term>
					<term>deep learning</term>
					<term>feature alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The past decade has witnessed significant progress on detecting objects in aerial images that are often distributed with large scale variations and arbitrary orientations. However most of existing methods rely on heuristically defined anchors with different scales, angles and aspect ratios and usually suffer from severe misalignment between anchor boxes and axis-aligned convolutional features, which leads to the common inconsistency between the classification score and localization accuracy. To address this issue, we propose a Single-shot Alignment Network (S 2 A-Net) consisting of two modules: a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The FAM can generate high-quality anchors with an Anchor Refinement Network and adaptively align the convolutional features according to the anchor boxes with a novel Alignment Convolution. The ODM first adopts active rotating filters to encode the orientation information and then produces orientation-sensitive and orientation-invariant features to alleviate the inconsistency between classification score and localization accuracy. Besides, we further explore the approach to detect objects in large-size images, which leads to a better trade-off between speed and accuracy. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on two commonly used aerial objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>general pipeline, RPN is used to generate high-quality Region of Interests (RoIs) from horizontal anchors, then an RoI Pooling operator is adopted to extract accurate features from RoIs. Finally, R-CNN is employed to regress the bounding boxes and classify them into different categories. However, it is worth noticing that horizontal RoIs often result in severe misalignment between bounding boxes and oriented objects <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref>. For example, a horizontal RoI usually contains several instances due to oriented and densely packed objects in aerial images. A natural solution is employing oriented bounding boxes as anchors to alleviate this issue <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. As a consequence, welldesigned anchors with different angles, scales and aspect ratios are required, which however leads to massive computations and memory footprint. Recently, RoI Transformer <ref type="bibr" target="#b3">[4]</ref> was proposed to address this issue by transforming horizontal RoIs into rotated RoIs, avoiding a large number of anchors, but it still needs heuristically defined anchors and complex RoI operation.</p><p>In contrast with R-CNN based detectors, one-stage detectors regress the bounding boxes and classify them directly with regular and densely sampling anchors. This architecture enjoys high computational efficiency but often lags behind in accuracy <ref type="bibr" target="#b2">[3]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, we argue that severe misalignment in one-stage detectors matters:</p><p>-Heuristically defined anchors are with low-quality and cannot cover the objects, leading a misalignment between objects and anchors. For example, the aspect ratio of a bridge usually ranges from 1/3 to 1/30, and only a few or even no anchors can be assigned to it. This misalignment usually aggravates the foreground-background class imbalance and hinders the performance. -The convolutional features from the backbone network are usually axis-aligned with fixed receptive field, while objects in aerial images are distributed with arbitrary orientations and variant appearances. Even an anchor box is assigned to an instance with high confidence (i.e., Intersection over Union (IoU)), there is still a misalignment between anchor boxes and convolutional features. In other words, the corresponding feature of an anchor box is hard to represent the whole object to some extent. As a result, the final classification score can not accurately reflect the localization accuracy, which also hinders the detection performance in post-processing phases (e.g., non-maximum suppression (NMS)). To address these issues in one-stage detectors, we propose a Single-Shot Alignment Network (S 2 A-Net) which consists of two modules: a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The FAM can generate high-quality anchors with an Anchor Refinement Network To alleviate this issue, we first refine the initial anchor into a rotated one (orange bounding box), and then adjust the feature sampling locations (orange points) with the guide of the refined anchor box to extract aligned deep features. The green box denotes the ground truth. (b) Performance comparisons of different methods under the same settings: ResNet50 (in small markers) and ResNet101 (in big markers) backbones, 1024 ? 1024 input size of images, without data augmentation. Faster R-CNN (FR-CNN) <ref type="bibr" target="#b9">[10]</ref>, Mask R-CNN <ref type="bibr" target="#b10">[11]</ref>, RetinaNet <ref type="bibr" target="#b11">[12]</ref>, Hybird Task Cascade (HTC) <ref type="bibr" target="#b12">[13]</ref> and RoI Transformer (RoITrans) <ref type="bibr" target="#b3">[4]</ref> are tested. The speed of all methods is reported on the V100 GPU in terms of Frames Per Second (FPS). Note that Mask R-CNN, HTC and RoITrans are tested based on the AerialDetection 1 project. RoITrans * indicates an official re-implementation.</p><p>(ARN) and adaptively align the feature according to the corresponding anchor boxes <ref type="figure" target="#fig_0">(Fig 1(a)</ref>) with an Alignment Convolution (AlignConv). Different from other methods with densely sampling anchors, we employ only one squared anchor for each location in the feature map, and the ARN refines them into high-quality rotated anchors. Then the AlignConv, a variant of convolution, adaptively aligns the feature according to the shapes, sizes and orientations of its corresponding anchors. In the ODM, we first adopt active rotating filters (ARF) <ref type="bibr" target="#b13">[14]</ref> to encode the orientation information and produce orientationsensitive features, and then extract orientation-invariant features by pooling the orientation-sensitive features. Finally, we feed the features into a regression sub-network and a classification sub-network to yield the final predictions. Besides, we also explore the approach to detect objects on large-size images (e.g., 4000 ? 4000) rather than on chip images, which significantly reduces the overall inference time with negligible loss of accuracy. Extensive experiments on commonly used datasets, i.e., DOTA <ref type="bibr" target="#b2">[3]</ref> and HRSC2016 <ref type="bibr" target="#b14">[15]</ref>, demonstrate that our proposed method can achieve state-of-the-art performance while keeping high efficiency, see <ref type="figure" target="#fig_0">Fig 1 (b)</ref>.</p><p>Our main contributions are summarized as follows:</p><p>? We propose a novel Alignment Convolution to alleviate the misalignment between axis-aligned convolutional features and arbitrary oriented objects in a fully convolutional way. Note AlignConv has negligible extra consuming time compared with standard convolution and can be embedded into many detectors with little modification. ? With the Alignment Convolution embedded, we design a light Single-Shot Alignment Network which enables us to generate high-quality anchors and aligned features for accurate object detection in aerial images. ? We report 79.42% mAP on the oriented object detection task on the DOTA dataset, achieving the state-of-the-art in both speed and accuracy. The rest of the paper is organized as follows. Section II introduces the related works. Section III introduces the details of our proposed S 2 A-Net. In Section IV, the experimental results and analysis are reported on challenging DOTA and HRSC2016 datasets. Finally, the conclusion is made in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>With the advance of machine learning, especially deep learning, object detection has made significant progress in recent years, which can be roughly divided into two groups: twostage detectors and one-stage detectors. Two-stage detectors <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> first generate a sparse set of RoIs in the first stage, and perform an RoI-wise bounding box regression and object classification in the second one. One-stage detectors, e.g., YOLO <ref type="bibr" target="#b16">[17]</ref> and SSD <ref type="bibr" target="#b17">[18]</ref>, detect objects directly and do not require the RoI generation stage. Generally, the performance of one-stage detectors usually lag behind two-stage detectors due to extreme foreground-background class imbalance. To address this problem, the Focal Loss <ref type="bibr" target="#b11">[12]</ref> can be used, and anchor-free detectors <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> alternatively formulate object detection as a points detection problem to avoid complex computations related to anchors and usually run faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object Detection in Aerial Images</head><p>Objects in aerial images are often crowded, distribute with large scale variations and appear at arbitrary orientations. Generic object detection methods with horizontal anchors <ref type="bibr" target="#b2">[3]</ref> usually suffer from severe misalignment in such scenarios: one anchor/RoI may contain several instances. Some methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> adopt rotated anchors with different angles, scales and aspect ratios to alleviate this issue, while involving heavy computations related to anchors (e.g., bounding box transform and ground truth matching). Ding et al. <ref type="bibr" target="#b3">[4]</ref> propose RoI Transformer to transform horizontal RoIs into rotated RoIs, which avoids a large number of anchors and alleviates the misalignment issue. However, it still needs heuristically defined anchors and complex RoI operations. Instead of employing rotated anchors, Xu et al. <ref type="bibr" target="#b6">[7]</ref> glide the vertex of the horizontal bounding box to accurately describe an oriented object. But the corresponding feature of a RoI is still horizontal and suffers from the misalignment issue. Recently proposed R 3 Det <ref type="bibr" target="#b23">[24]</ref> samples features from five locations (e.g., center and corners) of the corresponding anchor box and sum them up to reencode the position information. In contrast with the above methods, the proposed S 2 A-Net in this paper gets ride of  <ref type="figure">Fig. 2</ref>. Architecture of the proposed S 2 A-Net. S 2 A-Net consists of a backbone network, a Feature Pyramid Network <ref type="bibr" target="#b11">[12]</ref>, a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The FAM and ODM make up the detection head which is applied to each scale of the feature pyramid. In FAM, the Anchor Refinement Network (ARN) is proposed to generate high-quality rotated anchors. Then we feed the anchors and input features into the Alignment Convolution Layer (ACL) to extract aligned features. Note we only visualize the regression (reg.) branch of ARN and ignore the classification (cls.) branch for simplification. In ODM, we first adopt active rotating filters (ARF) <ref type="bibr" target="#b13">[14]</ref> to generate orientation-sensitive features, and pool the features to extract orientation-invariant features. Then the cls. branch and reg. branch are applied to produce the final detections.</p><p>heuristically defined anchors and can generate high-quality anchors by refining horizontal anchors into rotated anchors. Besides, the proposed FAM module enables to achieve feature alignment in a fully convolutional way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Alignment in Object Detection</head><p>Feature alignment usually refers to the alignment between convolution features and anchor boxes/RoIs, which is important for both two-stage and one-stage detectors. Detectors relying on misaligned features are hard to obtain accurate detections. In two-stage detectors, an RoI operator (e.g., RoIPooling <ref type="bibr" target="#b15">[16]</ref>, RoIAlign <ref type="bibr" target="#b10">[11]</ref> and Deformable RoIPooling <ref type="bibr" target="#b24">[25]</ref>) is adopted to extract fixed-length features inside the RoIs which can approximately represent the location of objects. RoIPooling first divides an RoI into a grid of sub-regions and then maxpools each sub-region into the corresponding output grid cell. However, RoIPooling quantizes the floating-number boundary of an RoI into integer, which introduces misalignment between the RoI and the feature. To avoid the quantization of RoIPooling, RoIAlign adopts bilinear interpolation to compute the extract values at each sampling location in sub-regions, significantly boosting the performance of localization. Meanwhile, Deformable RoIPooling adds an offset to each sub-region of an RoI, enabling adaptive feature selection. However, the RoI operator usually involves massive region-wise operation, e.g., feature warping and feature interpolation, which becomes a bottleneck toward fast object detection.</p><p>Recently, Guided Anchoring <ref type="bibr" target="#b25">[26]</ref> tries to align features with the guide of anchor shapes. It learns an offset field from the anchor prediction map and then guides the Deformable Convolution (DeformConv) to extract aligned features. Align-Det <ref type="bibr" target="#b26">[27]</ref> designs an RoI Convolution to obtain the same effect as RoIAlign in one-stage detector. Both <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref> achieve feature alignment in a fully convolutional way and enjoy high efficiency. These methods work well for objects in nature images but often lose their performance when detecting objects that are oriented and densely packed in aerial images, although some of them (e.g., Rotated RoIPooling <ref type="bibr" target="#b22">[23]</ref> and Rotated Position Sensitive RoIAlign <ref type="bibr" target="#b3">[4]</ref>) have been adopted to achieve feature alignment in oriented object detection. Different from the aforementioned methods, our proposed method aims at alleviating the misalignment between axis-aligned convolutional features and arbitrary oriented objects, which adjusts the feature sampling locations with the guide of anchor boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inconsistency between Regression and Classification</head><p>An object detector usually consists of two parallel tasks: bounding-box regression and object classification, which share the same features from the backbone network. And the classification score is used to reflect the localization accuracy in a post-processing phase (e.g., NMS). However, as discussed in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b28">[29]</ref>, there is a common inconsistency between classification score and localization accuracy. Detections with high classification scores may produce bounding boxes with low localization accuracy. While other nearby detections with high localization accuracy may be suppressed in the NMS step. To address this issue, IoU-Net <ref type="bibr" target="#b27">[28]</ref> proposed to learn to predict the IoU of a detection as the localization confidence and then combine the classification score and localization confidence as the final probability of a detection. Double-Head R-CNN <ref type="bibr" target="#b28">[29]</ref> adopts different head architectures for different tasks, i.e., fully connected head for classification and convolution head for regression. In our methods, we aim to improve the classification score by extracting aligned features for each instance. Especially when detecting densely packed objects in aerial images, accurate features are important to robust classification and precise localization. Besides, as discussed in <ref type="bibr" target="#b28">[29]</ref>, shared features from the backbone are not suitable for both classification and localization. Inspired by <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b29">[30]</ref>, we first adopt active rotating filters to encode the orientation information and then extract orientation-sensitive features and orientation-invariant features for regression and classification, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we first enable RetinaNet for oriented object detection and select it as our baseline in Section III-A. Then, we detail the Alignment Convolution in Section III-B. The architectures of Feature Alignment Module and Oriented Detection Module are presented in Section III-C and Section III-D, respectively. Finally, we show details of the proposed S 2 A-Net in both training and inference phases. The overall architecture is shown in <ref type="figure">Fig. 2</ref>, and the code is available at https://github.com/csuhan/s2anet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RetinaNet as Baseline</head><p>We choose a representative single-shot detector, Reti-naNet <ref type="bibr" target="#b11">[12]</ref> as our baseline. It consists of a backbone network and two task-specific subnetworks. Feature pyramid network (FPN) <ref type="bibr" target="#b30">[31]</ref> is adopted as the backbone network to extract multi-scale features. Classification and regression subnetworks are fully convolutional networks with several (i.e., 4) stacked convolution layers. Moreover, Focal loss is proposed to address the extreme foreground-background class imbalance during training.</p><p>Note that RetinaNet is designed for generic object detection, outputting horizontal bounding box ( <ref type="figure">Fig. 3 (a)</ref>) represented as,</p><p>x, w, h , with x = (x 1 , x 2 ) as the center of the bounding box. In order to be compatible with oriented object detection, we replace the regression output of the RetinaNet with oriented bounding box ( <ref type="figure">Fig. 3 (b)</ref>) as,</p><formula xml:id="formula_0">x, w, h, ? , where ? ? [? ? 4 , 3? 4</formula><p>] denotes the angle from the position direction of x 1 to the direction of the width w <ref type="bibr" target="#b3">[4]</ref>. All other settings keep unchanged with original RetinaNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Alignment Convolution</head><p>In a standard 2D convolution, we first sample over the input feature map X defined on ? = {0, 1, . . . , H ? 1} ? {0, 1, . . . , W ? 1} by a regular grid R = {(r x , r y )}, and then sum up the sampled values weighted by W. For example, the grid R = {(?1, ?1), (?1, 0), . . . , (0, 1), (1, 1)} represents a kernel size 3 ? 3 and dilation 1. For each location p ? ? on the output feature map Y, we have</p><formula xml:id="formula_1">Y(p) = r?R W(r) ? X(p + r).<label>(1)</label></formula><p>Compared with standard convolution, Alignment Convolution (AlignConv) adds an additional offset field O for each location p, that is</p><formula xml:id="formula_2">Y(p) = r?R; o?O W(r) ? X(p + r + o).<label>(2)</label></formula><p>As shown in <ref type="figure">Fig. 4</ref> (c) and (d), for location p, the offset field O is calculated as the difference between anchor-based sampling locations and regular sampling locations (i.e., p + r).</p><p>Let (x, w, h, ?) represent the corresponding anchor box at location p. For each r ? R, the anchor-based sampling location L r p can be defined as</p><formula xml:id="formula_3">L r p = 1 S x + 1 k (w, h) ? r ? R T (?) ,<label>(3)</label></formula><p>where k indicates the kernel size, S denotes the stride of the feature map, and R(?) = (cos ?, ? sin ?; sin ?, cos ?) T is the rotation matrix, respectively. The offset field O at location p is</p><formula xml:id="formula_4">O = {L r p ? p ? r} r?R .<label>(4)</label></formula><p>In this way, we can transform the axis-aligned convolutional features X(p) of a given location p into arbitrary oriented ones based on the corresponding anchor box.</p><p>Comparisons with other convolutions. As shown in <ref type="figure">Fig. 4</ref>, standard convolution samples over the feature map by a regular grid. DeformConv learns an offset field to augment the spatial sampling locations. However, it may sample from wrong locations with weak supervision, especially for densely packed objects. Our proposed AlignConv extracts grid-distributed features with the guide of anchor boxes by adding an additional offset field. Different from DeformConv, the offset field in AlignConv is inferred from the anchor boxes directly. The examples in <ref type="figure">Fig. 4</ref> (c) and (d) illustrate that our AlignConv can extract accurate features inside the anchor boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Alignment Module (FAM)</head><p>This section introduces the FAM that consists of an Anchor Refinement Network and an Alignment Convolution Layer, illustrated in <ref type="figure">Fig. 2 (c)</ref>.</p><p>Anchor Refinement Network. The Anchor Refinement Network (ARN) is a light network with two parallel branches: an anchor classification branch (not shown in the figure) and an anchor regression branch. The anchor classification branch classifies anchors into different categories and the anchor regression branch refines horizontal anchors into rotated anchors with high-quality. By default, since we only need the regressed anchor boxes to adjust the sampling locations in AlignConv, the classification branch is discarded in the inference phase to speed up the model. But for a fast version of S 2 A-Net, for which the output of ARN is adopted to produce the final predictions (see Section IV-D), the classification branch is reserved. Following the one-to-one fashion in anchorfree detectors, we preset one squared anchor for each location in the feature map. And we do not filter out the predictions with low confidence because we notice that some negative predictions turn to positive in the final predictions.</p><p>Alignment Convolution Layer. With AlignConv embedded, we forms an Alignment Convolution Layer (ACL) which is shown in <ref type="figure">Fig. 5</ref>. Specifically, for each location in the H ?W ?5 anchor prediction map, we first decode it into absolute anchor boxes (x, w, h, ?). Then the offset field calculated by Eq. (4) along with the input feature are fed into AlignConv to extract aligned features. Note for each anchor box (5-dimension), we regularly sample 9 (3 rows and 3 columns) points to obtain the 18-dimension offset field (i.e., the x-offset and y-offset of 9 points, see the blue arrows in <ref type="figure">Fig. 4 (c) and (d)</ref>). Besides, it should be emphasized that ACL is a light convolution layer with negligible speed latency in offset field calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Oriented Detection Module (ODM)</head><p>As shown in <ref type="figure">Fig. 2 (d)</ref>, the Oriented Detection Module (ODM) is proposed to alleviate the inconsistency between classification score and localization accuracy and then performs accurate object detection. We first adopt active rotating filters (ARF) <ref type="bibr" target="#b13">[14]</ref> to encode the orientation information. An ARF is a k ? k ? N filter that actively rotates N ? 1 times during convolution to produce a feature map with N orientation channels (N is 8 by default). For a feature map X and an ARF F, the i-th orientation output of Y can be denoted as</p><formula xml:id="formula_5">Y (i) = N ?1 n=0 F (n) ?i ? X (n) , ? i = i 2? N , i = 0, . . . , N ? 1,<label>(5)</label></formula><p>where F ?i is the clockwise ? i -rotated version of F, F (n) ?i and X (n) are the n-th orientation channel of F ?i and X, respectively. Applying ARF to the convolution layer, we can obtain orientation-sensitive features with explicitly encoded orientation informations. The bounding box regression task benefits from the orientation-sensitive features, while the object classification task requires invariant features. Following <ref type="bibr" target="#b13">[14]</ref>, we aims to extract orientation-invariant features by pooling the orientation-sensitive features. This is simply done by choosing the orientation channel with strongest response as the output featureX.X = max X (n) , 0 &lt; n &lt; N ? 1.</p><p>In this way, we can align the feature of objects with different orientations, toward robust object classification. Compared with the orientation-sensitive feature, the orientation-invariant feature is efficient with fewer parameters. For example, an H ?W ?256 feature map with 8 orientation channels becomes H ? W ? 32 after pooling. Finally, we feed the orientation-sensitive feature and orientation-invariant feature into two subnetworks to regress the bounding boxes and classify the categories, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Single-Shot Alignment Network</head><p>We adopt RetinaNet as the baseline, including its network architecture and most parameter settings, and form S 2 A-Net based on the combination of FAM and ODM. In the following, we detail S 2 A-Net in both training and inference phases.</p><p>Regression targets. Following previous works, we give the parameterized regression targets as:</p><formula xml:id="formula_7">?x g = x g ? x R(?) ? ( 1 w , 1 h ),</formula><p>(?w g , ?h g ) = log(w g , h g ) ? log(w, h),</p><formula xml:id="formula_8">?? g = 1 ? (? g ? ? + k?),<label>(7)</label></formula><p>where x g , x are for the ground-truth box and the anchor box respectively (likewise for w, h, ?). And k is an integer to ensure (? g ? ? + k?) ? [? ? 4 , 3? 4 ] (see <ref type="figure">Fig. 3</ref>). In FAM, we set ? = 0 to represent a horizontal anchor. Then the regression targets can be expressed by Eq. <ref type="bibr" target="#b6">(7)</ref>. In ODM, we first decode the output of FAM and then re-compute the regression targets by Eq. <ref type="formula" target="#formula_8">(7)</ref>.</p><p>Matching strategy. We adopt IoU as the metrics, and an anchor box can be assigned to positive (or negative) if its IoU is greater than a foreground threshold (or less than a background threshold, respectively). Different from the IoU between horizontal bounding boxes, we calculate the IoU between two oriented bounding boxes. By default, we set the foreground threshold as 0.5 and the background threshold as 0.4 in both FAM and ODM.</p><p>Loss function. The loss of S 2 A-Net is a multi-task one which consists of two parts, i.e., the loss of FAM and the loss of ODM. For each part, we assign a class label to each anchor/refined anchor and regress its location. The loss function can be defined as:</p><formula xml:id="formula_9">L = 1 N F i L c (c F i , l * i ) + i 1 [l * i ?1] L r (x F i , g * i ) + ? N O i L c (c O i , l * i ) + i 1 [l * i ?1] L r (x O i , g * i ) ,<label>(8)</label></formula><p>where ? is a loss balance parameter, 1 <ref type="bibr">[?]</ref> is an indicator function, N F and N O are the numbers of positive samples in the FAM and ODM respectively, i is the index of a sample in a minibatch. c F i and x F i are the predicted category and refined locations of the anchor i in FAM. c O i and x O i are the predicted object category and locations of the bounding box in ODM. l * i and g * i are the ground-truth category and locations of the anchor i. The Focal loss <ref type="bibr" target="#b11">[12]</ref> and smooth L1 loss are adopted as the classification loss L c and the regression loss L r , respectively.</p><p>Inference. S 2 A-Net is a fully convolutional network and we can simply forward an image through the network without complex RoI operation. Specifically, we pass the input image to the backbone network to extract pyramid features. Then the pyramid features are fed into FAM to produce refined anchors and aligned features. After that, ODM encodes the orientation information to produce the predictions with high confidence. Finally, we choose top-k (i.e., 2000) predictions and adopt NMS to yield the final detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS A. Datasets</head><p>DOTA <ref type="bibr" target="#b2">[3]</ref>. It is a large aerial image dataset for oriented objects detection which contains 2806 images with the size ranges from 800 ? 800 to 4000 ? 4000 and 188282 instances Both training and validation sets are used for training, and the testing set is used for testing. Following <ref type="bibr" target="#b2">[3]</ref>, we crop a series of 1024 ? 1024 patches from original images with a stride of 824. We only adopt random horizontal flipping during training to avoid over-fitting and no other tricks are utilized if not specified. For fair comparison with other methods, we adopt data augmentation (i.e., random rotation) in the training phase. For multi-scale experiments, we firstly resize original images at three scales (0.5, 1.0 and 1.5) and then crop them into 1024?1024 patches with a stride of 512.</p><p>HRSC2016 <ref type="bibr" target="#b14">[15]</ref>. It is a high resolution ship recognition dataset annotated with oriented bounding boxes which contains 1061 images, and the image size ranges from 300 ? 300 to 1500 ? 900. We use the training (436 images) and validation (181 images) sets for training and the testing set (444 images) for testing. All images are resized to (800, 512) without changing the aspect ratio. Horizontal flipping is applied during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We adopt ResNet101 FPN as the backbone network for fair comparison with other methods, and ResNet50 FPN is adopted for other experiments if not specified. For each level of pyramid features (i.e., P 3 to P 7 ), we preset one squared anchor per location with a scale of 4 times the total stride size (i.e., 32, 64, 128, 256, 512). The loss balance parameter ? is set to 1. The hyperparameters of Focal loss L c are set to  ? = 0.25 and ? = 2.0. We adopt the same training schedules as mmdetection <ref type="bibr" target="#b31">[32]</ref>. We train all models in 12 epochs for DOTA and 36 epochs for HRSC2016. SGD optimizer is adopted with an initial learning rate of 0.01 and the learning rate is divided by 10 at each decay step. The momentum and weight decay are 0.9 and 0.0001, respectively. We adopt learning rate warmup for 500 iterations. We use 4 V100 GPUs with a total batch size of 8 for training and a single V100 GPU for inference by default. The time of post-processing (e.g., NMS) is included in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>In this section, we conduct a series of experiments on the testing set of DOTA to validate the effectiveness of our method. ResNet50 FPN is adopted as the backbone in all experiments. Note that we extend the flops_counter tool in mmdetection <ref type="bibr" target="#b31">[32]</ref> to calculate the FLOPs of our method.</p><p>RetinaNet as baseline. As a single-shot detector, RetinaNet is fast enough. However, any module added to it will introduce more computations. We experiment different architectures and settings on RetinaNet. As shown in Effectiveness of AlignConv. As discussed in Section III-B, we compare AlignConv with other methods to validate its effectiveness. We only replace AlignConv with other convolution methods and keep other settings unchanged. Besides, we also add comparison with Guided Anchoring DeformConv (GA-DeformConv) <ref type="bibr" target="#b25">[26]</ref>. Note that the offset field of GA-DeformConv is learned from the anchor prediction map in ARN by a 1 ? 1 convolution.</p><p>As shown in <ref type="table" target="#tab_0">Table II</ref>, AlignConv surpasses other methods by a big margin. Compared with the standard convolution, AlignConv improves about 3% mAP while only introduces  Besides, AlignConv improves the performance for almost all categories, especially for those categories with large aspect ratios (e.g., bridge), densely distribution (e.g., small vehicles and large vehicles) and fewer instances (e.g., helicopters). On the contrary, DeformConv and GA-DeformConv only achieve 71.71% and 71.33% mAP, respectively. The qualitative comparison in <ref type="figure" target="#fig_5">Fig. 6</ref> shows that AlignConv achieves accurate bounding box regression in detecting densely packed and arbitrary oriented objects, while other methods with implicit learning get poor performance. Effectiveness of ARN and ARF. To evaluate the effectiveness of ARN and ARF, we experiment different settings of S 2 A-Net. If ARN is discarded, then FAM and ODM share the same initial anchors without refinement. If ARF is discarded, we replace the ARF layer with the standard convolution layer. As shown in <ref type="table" target="#tab_0">Table III</ref>, without ARN, ACL and ARF, our method achieves 68.26% mAP, about 1.26% mAP higher than the baseline method. This is mainly because we add supervisions in both FAM and ODM. With the participation of ARN, we obtain 71.17% mAP, showing that anchor refinement is important to the final predictions in ODM.</p><p>Besides, we find ARF dose nothing for performance improvement without the participation of ACL, i.e., applying ARF or the combination of ARN and ARF to our method only achieve 68.35% and 71.11% mAP, respectively. However, if we put ACL and ARF together, there is an obvious improvement, from 73.24% to 74.12%. We argue that CNNs are not rotation-invariant, and even we can extract accurate features to represent the object, the corresponding features are still rotation-sensitive. So the participation of ARF augments the orientation information explicitly, leading to better regression and classification. Network design. As shown in <ref type="table" target="#tab_0">Table IV</ref>, we explore different network designs in FAM and ODM. Compared with the baseline method in Table IV (a), we can conclude that S 2 A-Net is not only an effective detector with high detection accuracy, but also an efficient detector in both speed and parameters. The results in Table IV (b)-(f) show that our proposed method is insensitive to the depth of the network and the performance improvements mainly come from our novel alignment mechanism. Besides, as the number of layers increases, there is a performance drop from Table IV (d) to (f). We hypothesize that deeper networks with a larger receptive field may hinder the detection performance of small size objects. Moreover, the setting (d), for which the number of layers in FAM and ODM is the same, obtains the highest mAP among (c)-(e), while (c) and (e) have a significant drop in mAP, showing that similar receptive field in FAM and ODM is more balancing for high quality object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detecting on large-size images</head><p>The size of aerial image often ranges from thousands to tens of thousands, which means more computations and memory footprint. Many previous works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> adopt a detection on chips strategy to alleviate this challenge, even if a chip does not contain any object. ClusDet <ref type="bibr" target="#b32">[33]</ref> tries to address this issue by generating clustered chips, while introducing more complex operations (e.g., chip generation and results merge) and significant performance drop. As our proposed S 2 A-Net is efficient and the architecture is flexible, we aims to detect objects on large-size images directly. We first explore different settings of the input size and cropping stride, and report the mAP and overall time during inference <ref type="table" target="#tab_4">(Table V)</ref>. We first crop the images into 1024 ? 1024 chips, and the mAP improves from 71.20% to 74.62% when the stride decreases from 1024 to 512. However, the number of chip images increases from 8143 to 20012, and the overall inference time increases about 135%. If we detect on the original large-size images without cropping, the inference time has reduced by 50% with negligible loss of accuracy. We argue that the cropping strategy makes it hard to detect objects around the boundary <ref type="figure" target="#fig_6">(Fig. 7)</ref>. Besides, if we adopt the output of FAM for detection and Floating-Point 16 (FP16) to speed up the inference, we can reduce the inference time to 97 seconds with a mAP of 70.85%. Compared our S 2 A-Net with ClusDet <ref type="bibr" target="#b32">[33]</ref>  <ref type="table" target="#tab_0">(Table VI)</ref>, our method only process 458 images and outperforms ClusDet by a large margin. If we adopt the output of FAM for evaluation, we still achieve 42.7% mAP .5?.95 and 72.7% mAP .5 . The result demonstrates that out method is efficient and effective, and our detection strategy can achieve better speed-accuracy trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparisons with the State-of-the-Art</head><p>In this section, we compare our proposed S 2 A-Net with other state-of-the-art methods on two aerial detection datasets, DOTA and HRSC2016. The settings have been introduced in Section IV-A and Section IV-B.</p><p>Results on DOTA 2 . Note that RetinaNet is our reimplemented version referred in Sec III-A. As shown in <ref type="table" target="#tab_0">Table VII</ref>, we achieve 74.01% mAP in 22.6 FPS with ResNet-50-FPN backbone and without any data augmentation (e.g., random rotation). Note that the FPS is an average FPS and we obtain it by calculating the overall inference time and the number of chip images (i.e., 10833). Besides, we achieve state-of-the-art 76.11% mAP with a ResNet101 FPN backbone, outperforming all two-stage and one-stage methods. In multiscale experiments, our S 2 A-Net achieves 79.42% and 79.15% mAP with a ResNet-50-FPN and ResNet-101-FPN backbone, respectively. And we achieve best results in 10/15 categories, especially for some hard categories (e.g., bridge, soccer-ball field, swimming pool, helicopter). Qualitative detection results of the baseline method (i.e., RetinaNet) and our S 2 A-Net are visualized in <ref type="figure" target="#fig_7">Fig 8.</ref> Compared with RetinaNet, our S 2 A-Net produces less false predictions when detecting on the object with dense distribution and large scale variations.</p><p>Results on HRSC2016. Note that DRN <ref type="bibr" target="#b34">[35]</ref> and CenterMap-Net <ref type="bibr" target="#b5">[6]</ref> are evaluated under PASCAL VOC2012 metrics while other methods are evaluated under PASCAL VOC2007 metrics, and the performance under VOC2012 metrics is better than that under VOC2007 metrics. As shown in <ref type="table" target="#tab_0">Table VIII</ref>, our proposed S 2 A-Net achieves 90.17% and 95.01% mAP under VOC2007 and VOC2012 metrics respectively, outperforming all other methods. The objects in HRSC2016 have large aspect ratios and arbitrary orientations, and previous methods often set more anchors for better performance, e.g., 20 in RoI Trans. and 21 in R 3 Det. Compared with the previous best result 89.26% (VOC2007) by R 3 Det and 92.8% (VOC2012) by CenterMap-Net, we improve 0.91% and 2.21% mAP respectively with only one anchor, which effectively get ride of heuristically defined anchors. Some qualitative results are shown in <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a simple and effective Single-Shot Alignment Network (S 2 A-Net) for oriented object detection in aerial images. With the proposed Feature Alignment Module and Oriented Detection Module, our S 2 A-Net realizes full feature alignment and alleviates the inconsistency between regression ans classification. Besides, we explore the approach to detect on large-size images for better speed-accuracy tradeoff. Extensive experiments demonstrate that our S 2 A-Net can achieve state-of-the-art performance on both DOTA and HRSC2016.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Speed vs. accuracy (mAP) on DOTA (a) The misalignment (red arrows) between an anchor box (blue bounding box) and convolutional features (light blue rectangle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Fig. 3 .</head><label>23</label><figDesc>Two types of bounding box. (a) Horizontal bounding box x, w, h with center point x = (x 1 , x 2 ), width w and height h. (b) Oriented bounding box x, w, h, ? . x denotes the center point. w and h represent the long side and short side of a bounding box, respectively. ? means the angle from the position direction of x 1 to the direction of w where ? ? [? ? 4 , 3? 4 ]. And an oriented bounding box turns to a horizontal one when ? = 0, e.g., x, w, h, 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>(a) standard 2D convolution (b) Deformable convolution (c) AlignConv w. horizontal AB (d) AlignConv w. rotated AB Illustration of the sampling locations in different methods with 3?3 kernel. (a) is the standard 2D convolution with regular sampling locations (in green dots). (b) is Deformable Convolution [25] with deformable sampling locations (in blue dots). (c) and (d) are two examples of our proposed AlignConv with horizontal and rotated anchor box (AB), respectively (in orange rectangle). The blue arrows mean the offset field. Alignment Convolution Layer. It takes the input feature and the anchor prediction (pred.) map as inputs and produces aligned features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>of 15 common object categories includes: Plane (PL), Baseball diamond (BD), Bridge (BR), Ground track field (GTF), Small vehicle (SV), Large vehicle (LV), Ship (SH), Tennis court (TC), Basketball court (BC), Storage tank (ST), Soccer-ball field (SBF), Roundabout (RA), Harbor (HA), Swimming pool (SP), and Helicopter (HC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative comparison of different convolution methods. The blue bounding box indicates the prediction of large vehicle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative comparison of detection results. We crop a large-size image into 1024 ? 1024 chip images with a stride of 824. The large-size image and chip images are fed into the same network to produce detection results (e.g., planes in red boxes) without resizing. Instances with the same number are corresponding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Some detection results on DOTA by different methods. For each image pair, the upper is the baseline method while the bottom is by S 2 A-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESULTS</head><label>I</label><figDesc>OF DIFFERENT RETINANET ON DOTA. DEPTH INDICATES THE NUMBER OF CONVOLUTION LAYER IN TWO SUBNETWORKS OF RETINANET.</figDesc><table><row><cell></cell><cell>Model</cell><cell>#Anchor</cell><cell cols="2">Depth</cell><cell>mAP</cell><cell cols="2">GFLOPs</cell><cell>Param</cell></row><row><cell>(a)</cell><cell>RetinaNet</cell><cell>9</cell><cell>4</cell><cell></cell><cell>68.05</cell><cell cols="2">215.92</cell><cell>36.42 M</cell></row><row><cell cols="2">(b) RetinaNet</cell><cell>9</cell><cell>2</cell><cell></cell><cell>67.64</cell><cell cols="2">164.38</cell><cell>34.06 M</cell></row><row><cell>(c)</cell><cell>RetinaNet</cell><cell>1</cell><cell>2</cell><cell></cell><cell>67.00</cell><cell cols="2">156.33</cell><cell>33.69 M</cell></row><row><cell></cell><cell>Conv</cell><cell cols="2">DeformConv</cell><cell cols="3">GA-DeformConv</cell><cell>AlignConv</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I</head><label>I</label><figDesc>(a), RetinaNet achieves a mAP of 68.05% with 215.92 GFLOPs and 36.42 M parameters, indicating that our baseline is solid. If the depth of RetinaNet head changes from 4 to 2, the mAP drops 0.41% and the FLOPs (resp. parameters) reduce 51.54 G (resp. 2.36 M). Furthermore, if we set one anchor per location (Table I (c)), the FLOPs reduces 28% with a accuracy drop of 1.5% compared with Table I (a). The results show that a light detection head and few anchors can also achieve competitive performance and better speed-accuracy trade-off.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARING</head><label>II</label><figDesc>ALIGNMENT CONVOLUTION (ALIGNCONV) WITH OTHER CONVOLUTION METHODS. WE COMPARE OUR ALIGNCONV WITH THE STANDARD CONVOLUTION (CONV), DEFORMABLE CONVOLUTION (DEFORMCONV) AND GUIDED ANCHORING DEFORMABLE CONVOLUTION (GA-DEFORMCONV).</figDesc><table><row><cell cols="2">Methods</cell><cell>PL</cell><cell>BD</cell><cell></cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell><cell>GFLOPs</cell></row><row><cell>Conv</cell><cell></cell><cell>88.87</cell><cell cols="2">76.34</cell><cell cols="3">46.42 67.53 77.21</cell><cell cols="6">74.80 82.27 90.79 81.22 85.02 50.99</cell><cell cols="4">61.10 63.54 67.24 53.25</cell><cell>71.11</cell><cell>196.62</cell></row><row><cell cols="2">DeformConv</cell><cell>88.96</cell><cell cols="2">80.23</cell><cell cols="3">45.92 67.51 77.10</cell><cell cols="6">74.23 84.28 90.81 81.47 85.56 54.19</cell><cell>64.11</cell><cell cols="3">64.85 68.13 48.34</cell><cell>71.71</cell><cell>198.02</cell></row><row><cell cols="2">GA-DeformConv</cell><cell>88.72</cell><cell cols="2">79.56</cell><cell cols="3">46.19 65.41 76.86</cell><cell cols="6">74.96 79.44 90.78 80.99 84.73 55.31</cell><cell cols="4">63.17 62.07 67.69 54.12</cell><cell>71.33</cell><cell>197.92</cell></row><row><cell cols="2">AlignConv</cell><cell>89.11</cell><cell cols="2">82.84</cell><cell cols="3">48.37 71.11 78.11</cell><cell cols="6">78.39 87.25 90.83 84.90 85.64 60.36</cell><cell>62.60</cell><cell cols="3">65.26 69.13 57.94</cell><cell>74.12</cell><cell>198.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">ABLATION STUDIES. WE CHOOSE A LIGHT RETINANET (SHOWN IN TABLE I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">(C)) AS THE BASELINE, AND EXPERIMENT DIFFERENT SETTINGS OF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">S 2 A-NET, i.e., ANCHOR REFINEMENT NETWORK (ARN), ALIGNMENT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">CONVOLUTION LAYER (ACL) AND ACTIVE ROTATING FILTERS (ARF).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Baseline</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Different Settings of S 2 A-Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ARN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ARF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP</cell><cell>67.00</cell><cell cols="2">68.26</cell><cell cols="2">71.17</cell><cell>68.35</cell><cell>71.11</cell><cell cols="2">73.24 74.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV EXPERIMENTS</head><label>IV</label><figDesc>OF DIFFERENT NETWORK DESIGNS. WE EXPLORE THE NETWORK DESIGN IN FAM AND ODM WITH DIFFERENT NUMBER OF LAYERS. SETTING (D) IS THE DEFAULT SETTING OF OUR PROPOSED METHOD SHOWN IN FIG. 2.</figDesc><table><row><cell></cell><cell>Model</cell><cell>FAM</cell><cell>ODM</cell><cell>mAP</cell><cell>GFLOPs</cell><cell>Param</cell></row><row><cell>(a)</cell><cell>RetinaNet</cell><cell>-</cell><cell>-</cell><cell>68.05</cell><cell>215.92</cell><cell>36.42 M</cell></row><row><cell>(b)</cell><cell>S 2 A-Net</cell><cell>1</cell><cell>1</cell><cell>73.04</cell><cell>159.27</cell><cell>33.25 M</cell></row><row><cell>(c)</cell><cell>S 2 A-Net</cell><cell>1</cell><cell>3</cell><cell>72.89</cell><cell>210.81</cell><cell>35.61 M</cell></row><row><cell>(d)</cell><cell>S 2 A-Net</cell><cell>2</cell><cell>2</cell><cell>74.12</cell><cell>198.03</cell><cell>35.02 M</cell></row><row><cell>(e)</cell><cell>S 2 A-Net</cell><cell>1</cell><cell>3</cell><cell>72.86</cell><cell>185.04</cell><cell>34.43 M</cell></row><row><cell>(f)</cell><cell>S 2 A-Net</cell><cell>4</cell><cell>4</cell><cell>73.30</cell><cell>275.22</cell><cell>38.57 M</cell></row></table><note>1.41 GFLOPs computation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF DIFFERENT SETTINGS DETECTING ON LARGE IMAGES IN DOTA. Stride IS THE CROPPING STRIDE REFERRED IN SECTION IV-A. #Image MEANS THE NUMBER OF IMAGES OR CHIPS. Output INDICATES THE MODULE (i.e., FAM OR ODM) USED FOR TESTING. WE SHOW THE INFERENCE TIME REQUIRED FOR ENTIRE DATASET USING FP32/FP16 WITH 4 V100 GPUS. A-NET WITH CLUSDET [33] ON DOTA VALIDATION SET. FOLLOWING [33], WE REPORT THE ACCURACY OF FIVE CATEGORIES (i.e., PL, SV, LV, SH AND HC) WITH DIFFERENT IOU THRESHOLDS (i.e., MAP .5 , MAP .75 AND MAP .5?.95 ). THE RESULTS OF RETINANET AND S 2 A-NET ARE CALCULATED FROM THE AXIS-ALIGNED BOUNDING BOXES OF THE OUTPUT. #IMAGE MEANS THE NUMBER OF IMAGES OR CHIPS. ? INDICATES THAT THE OUTPUT OF FAM IS ADOPTED FOR THE FINAL RESULTS.</figDesc><table><row><cell>Input Size</cell><cell>Stride</cell><cell cols="3">#Image Output</cell><cell>mAP</cell><cell>Time (s)</cell></row><row><cell>1024 ? 1024</cell><cell>1024</cell><cell></cell><cell>8143</cell><cell>ODM</cell><cell>71.20</cell><cell>150 / 126</cell></row><row><cell>1024 ? 1024</cell><cell>824</cell><cell cols="2">10833</cell><cell>ODM</cell><cell>74.12</cell><cell>246 / 160</cell></row><row><cell>1024 ? 1024</cell><cell>512</cell><cell cols="2">20012</cell><cell>ODM</cell><cell>74.62</cell><cell>352 / 308</cell></row><row><cell>Original</cell><cell>-</cell><cell></cell><cell>937</cell><cell>ODM</cell><cell>74.01</cell><cell>120 / 103</cell></row><row><cell>Original</cell><cell>-</cell><cell></cell><cell>937</cell><cell>FAM</cell><cell>70.85</cell><cell>104 / 97</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell></row><row><cell>COMPARING S 2 Methods</cell><cell cols="2">#Image</cell><cell cols="2">mAP .5?.95</cell><cell>mAP .5</cell><cell>mAP .75</cell></row><row><cell>ClusDet [33]</cell><cell>1055</cell><cell></cell><cell></cell><cell>32.2</cell><cell>47.6</cell><cell>39.2</cell></row><row><cell>RetinaNet</cell><cell>458</cell><cell></cell><cell></cell><cell>41.6</cell><cell>70.5</cell><cell>44.2</cell></row><row><cell>S 2 A-Net  ? (Ours)</cell><cell>458</cell><cell></cell><cell></cell><cell>42.7</cell><cell>72.7</cell><cell>45.3</cell></row><row><cell>S 2 A-Net (Ours)</cell><cell>458</cell><cell></cell><cell></cell><cell>43.9</cell><cell>75.8</cell><cell>46.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII COMPARISONS</head><label>VII</label><figDesc>WITH STATE-OF-THE-ART METHODS ON DOTA. R-101-FPN STANDS FOR RESNET 101 WITH FPN (LIKEWISE R-50-FPN), AND H-104 STANDS FOR HOURGLASS 104. ? INDICATES TRAINING AND TESTING WITHOUT DATA AUGMENTATION. ? DENOTES THE INPUT IS THE ORIGINAL IMAGES OTHER THAN CHIP IMAGES. * MEANS MULTI-SCALE TRAINING AND TESTING. 64.05 35.30 38.02 37.16 89.41 69.64 59.28 50.30 52.91 47.89 47.40 46.30 54.13 -Azimi et al. [34] R-101-FPN 81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.20 53.64 62.90 67.02 64.17 50.23 68.16 -RoI Trans. * [4] R-101-FPN 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61 -Xu et al. [7] R-101-FPN 89.64 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 66.55 78.10 88.83 77.80 83.61 49.36 66.19 72.10 72.36 58.70 78.32 87.19 90.66 84.89 85.27 56.46 69.23 74.13 71.56 66.06 55.82 72.77 90.55 82.83 76.30 54.19 63.64 63.71 69.73 53.37 70.69 84.94 90.14 83.85 84.11 50.12 58.41 67.62 68.60 52.50 74.43 85.84 90.57 86.18 84.89 57.65 61.93 69.30 69.63 58.48</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell><cell>FPS</cell></row><row><cell>two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FR-O [3]</cell><cell>R-101</cell><cell>79.42</cell><cell>77.13</cell><cell cols="14">17.70 69.56</cell><cell>5.9</cell></row><row><cell>CADNet [5]</cell><cell cols="2">R-101-FPN 87.80</cell><cell>82.40</cell><cell cols="2">49.40 73.50</cell><cell cols="3">71.10 63.50 76.60</cell><cell cols="8">90.90 79.20 73.30 48.40 60.90 62.00 67.00 62.20</cell><cell>69.90</cell><cell>-</cell></row><row><cell>SCRDet [8]</cell><cell cols="2">R-101-FPN 89.98</cell><cell>80.65</cell><cell cols="2">52.09 68.36</cell><cell cols="8">68.36 70.91</cell><cell cols="3">72.94 70.86 57.32</cell><cell>75.02</cell><cell>10.0</cell></row><row><cell>CenterMap-Net [6]</cell><cell>R-50-FPN</cell><cell>88.88</cell><cell>81.24</cell><cell cols="2">53.15 60.65</cell><cell cols="12">78.62 71.74</cell><cell>-</cell></row><row><cell>CenterMap-Net  *  [6]</cell><cell cols="2">R-101-FPN 89.83</cell><cell>84.41</cell><cell cols="2">54.60 70.25</cell><cell cols="12">77.66 76.03</cell><cell>-</cell></row><row><cell>one-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RetinaNet [12]</cell><cell cols="2">R-101-FPN 88.82</cell><cell>81.74</cell><cell cols="2">44.44 65.72</cell><cell cols="12">67.11 68.72</cell><cell>12.7</cell></row><row><cell>DRN [35]</cell><cell>H-104</cell><cell>88.91</cell><cell>80.22</cell><cell cols="2">43.52 63.35</cell><cell cols="12">73.48 70.70</cell><cell>-</cell></row><row><cell>DRN  *  [35]</cell><cell>H-104</cell><cell>89.71</cell><cell>82.34</cell><cell cols="2">47.22 64.10</cell><cell cols="12">76.22 73.23</cell><cell>-</cell></row><row><cell>R 3 Det [24]</cell><cell cols="2">R-101-FPN 89.54</cell><cell>81.99</cell><cell cols="2">48.46 62.52</cell><cell cols="11">70.48 74.29 77.54 90.80 81.39 83.54 61.97 59.82 65.44 67.46 60.05</cell><cell>71.69</cell><cell>-</cell></row><row><cell>R 3 Det [24]</cell><cell cols="2">R-152-FPN 89.49</cell><cell>81.17</cell><cell cols="2">50.53 66.10</cell><cell cols="11">70.92 78.66 78.21 90.81 85.26 84.23 61.81 63.77 68.16 69.83 67.17</cell><cell>73.74</cell><cell>-</cell></row><row><cell>S 2 A-Net  ? (Ours)</cell><cell>R-50-FPN</cell><cell>89.11</cell><cell>82.84</cell><cell cols="2">48.37 71.11</cell><cell cols="11">78.11 78.39 87.25 90.83 84.90 85.64 60.36 62.60 65.26 69.13 57.94</cell><cell>74.12</cell><cell>16.0</cell></row><row><cell>S 2 A-Net  ? ? (Ours)</cell><cell>R-50-FPN</cell><cell>89.11</cell><cell>81.51</cell><cell cols="2">48.75 72.85</cell><cell cols="11">78.23 76.77 86.95 90.84 83.59 85.52 62.70 61.63 66.55 68.94 56.24</cell><cell>74.01</cell><cell>22.6</cell></row><row><cell>S 2 A-Net (Ours)</cell><cell cols="2">R-101-FPN 88.70</cell><cell>81.41</cell><cell cols="2">54.28 69.75</cell><cell cols="11">78.04 80.54 88.04 90.69 84.75 86.22 65.03 65.81 76.16 73.37 58.86</cell><cell>76.11</cell><cell>12.7</cell></row><row><cell>S 2 A-Net  *  (Ours)</cell><cell>R-50-FPN</cell><cell>88.89</cell><cell>83.60</cell><cell cols="2">57.74 81.95</cell><cell>79.94</cell><cell>83.19</cell><cell cols="4">89.11 90.78 84.87 87.81</cell><cell cols="4">70.30 68.25 78.30 77.01</cell><cell>69.58</cell><cell>79.42</cell><cell>16.0</cell></row><row><cell>S 2 A-Net  *  (Ours)</cell><cell cols="2">R-101-FPN 89.28</cell><cell>84.11</cell><cell cols="2">56.95 79.21</cell><cell>80.18</cell><cell>82.93</cell><cell>89.21</cell><cell cols="4">90.86 84.66 87.61 71.66</cell><cell>68.23</cell><cell cols="2">78.58 78.20</cell><cell>65.55</cell><cell>79.15</cell><cell>12.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII COMPARISONS</head><label>VIII</label><figDesc>OF STATE-OF-THE-ART METHODS ON HRSC2016. #ANCHOR MEANS THE NUMBER OF ANCHORS AT EACH LOCATION OF THE FEATURE MAP. INDICATES THAT THE RESULT IS EVALUATED UNDER PASCAL VOC2012 METRICS.</figDesc><table><row><cell>Methods</cell><cell cols="3">RC2 [36] R 2 PN  *  [37] RRD [30]</cell><cell>RoI Trans. [4]</cell><cell cols="2">Xu et al. [7] R 3 Det [24]</cell><cell cols="4">DRN [35] CenterMap-Net [6] S 2 A-Net (Ours)</cell></row><row><cell>#Anchor</cell><cell>-</cell><cell>24</cell><cell>13</cell><cell>20</cell><cell>20</cell><cell>21</cell><cell>-</cell><cell>15</cell><cell></cell><cell>1</cell></row><row><cell>mAP Baseline S 2 A-Net Baseline S 2 A-Net</cell><cell cols="8">75.7 92.8  PL 79.6 84.3 86.2 88.2 89.26 92.7  *  BD BR GTF SV LV SH TC BC ST HA RA SBF</cell><cell>SP</cell><cell>HC</cell></row></table><note>** 90.17 / 95.01*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The result is available at https://captain-whu.github.io/DOTA/results.html with setting name hanjiaming. Note that, to concentrate on studying the algorithmic problem of ODAI, this setting is without using model fusions which can further improve the detection performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ship Rotated Bounding Box Space for Ship Extraction From High-Resolution Optical Satellite Images With Complex Backgrounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3974" to="3983" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cad-net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning center probability map for detecting objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICCV</title>
		<imprint>
			<biblScope unit="page" from="8231" to="8240" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4969" to="4978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4961" to="4970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPRAM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Some detection results on HRSC2016 with the proposed S 2 A-Net. once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
	<note>You only look Fig. 9</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9656" to="9665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rotated region based cnn for ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2960" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Revisiting feature alignment for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01570</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking classification and localization for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06493</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Clustered object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8310" to="8319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACCV</title>
		<imprint>
			<biblScope unit="page" from="150" to="165" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning a rotation invariant detector with rotatable bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<idno>arXiv preprint:1711.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Icpr2018 contest on object detection in aerial images (odai-18),&quot; in ICPR</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Truncation cross entropy loss for remote sensing image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scene classification with recurrent attention of vhr remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1155" to="1167" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4905" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Soft-NMS -Improving Object Detection with One Line of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6154" to="6162" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4203" to="4212" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cascade rpn: Delving into high-quality region proposal network with adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9626" to="9635" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
