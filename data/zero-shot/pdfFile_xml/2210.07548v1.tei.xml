<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reconstructed Student-Teacher and Discriminative Networks for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Yamada</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Kamiya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiro</forename><surname>Hotta</surname></persName>
						</author>
						<title level="a" type="main">Reconstructed Student-Teacher and Discriminative Networks for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection is an important problem in computer vision; however, the scarcity of anomalous samples makes this task difficult. Thus, recent anomaly detection methods have used only "normal images" with no abnormal areas for training. In this work, a powerful anomaly detection method is proposed based on student-teacher feature pyramid matching (STPM), which consists of a student and teacher network. Generative models are another approach to anomaly detection. They reconstruct normal images from an input and compute the difference between the predicted normal and the input. Unfortunately, STPM does not have the ability to generate normal images. To improve the accuracy of STPM, this work uses a student network, as in generative models, to reconstruct normal features. This improves the accuracy; however, the anomaly maps for normal images are not clean because STPM does not use anomaly images for training, which decreases the accuracy of the image-level anomaly detection. To further improve accuracy, a discriminative network trained with pseudo-anomalies from anomaly maps is used in our method, which consists of two pairs of student-teacher networks and a discriminative network. The method displayed high accuracy on the MVTec anomaly detection dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Anomaly detection is the identification of samples that deviate from normal patterns in the data. In recent years, convolutional neural networks (CNNs) have been applied to anomaly detection for the visual inspection of industrial products <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, surveillance of locate intruders <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and pathological diagnosis of medical images <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In the past, image-level anomaly detection methods that classify each image as an anomaly have been studied, but image-level anomaly detection methods are not suitable for the inspection of products that require anomaly detection at the pixel level. Famous image-level anomaly detection methods rely on generative models to accurately reconstruct normal images. The generative model, based on the generative adversarial network (GAN) <ref type="bibr" target="#b10">[11]</ref> and autoencoders <ref type="bibr" target="#b12">[13]</ref>, can be trained using only normal images which allow the reconstruction of normal regions although anomalous regions cannot be reconstructed because the model is trained using only normal images. Anomaly detection using generative models identifies anomalous regions by visualizing regions with poor reconstruction capabilities. However, it is difficult for these methods to identify anomalous regions <ref type="bibr" target="#b0">1</ref> Meijo University, 1-501 Shiogamaguchi, Tempaku-ku, Nagoya 468-8502, Japan, 160442146@ccalumni.meijo-u.ac.jp <ref type="bibr" target="#b1">2</ref> Meijo University, 1-501 Shiogamaguchi, Tempaku-ku, Nagoya 468-8502, Japan, 180442042@ccalumni.meijo-u.ac.jp <ref type="bibr" target="#b2">3</ref> Meijo University, 1-501 Shiogamaguchi, Tempaku-ku, Nagoya 468-8502, Japan, kazuhotta@meijo-u.ac.jp unless the generated normal regions can be reconstructed with high accuracy <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>The use of pretrained models has been proposed as a new approach for anomaly detection, such as STPM <ref type="bibr" target="#b13">[14]</ref> which uses a student-teacher network. In STPM, the teacher network is the pretrained ResNet18, and the student network is the untrained ResNet18. Only normal images are used for STPM training. The student network learns, thereby making feature maps similar to those in the teacher network. Because training is performed only on normal images, the student network can only output the features of normal regions. In contrast, the teacher network is a model pre-trained on ImageNet, so it can represent features of abnormal regions well. The difference in the feature representation between the student and teacher networks is the anomaly region.</p><p>In student ResNet18, STPM detects anomalous regions using anomaly maps at three different resolutions. However, STPMs have several limitations. STPM uses the difference between feature maps at three different resolutions in student and teacher networks. In the validation phase, the final anomaly map is calculated by multiplying the three maps. When all three anomaly maps detect the same abnormal area, highly accurate abnormality detection is possible. However, if one of the three maps fails to detect an abnormal region, the anomaly region cannot be detected because the three maps are multiplied.</p><p>Therefore, each of the three anomaly maps must be improved. Observing that STPM does not have the ability to reconstruct normal images similar to generative models which are effective in anomaly detection, we introduced a new student network that has the ability to reconstruct normal features. Conventional STPM uses ResNet18 as the teacher network. If the same teacher network is used for a new student, it is possible that similar anomaly maps can be obtained. To obtain anomaly maps from different viewpoints, the pre-trained ResNet50 was used as the teacher network for the student network for reconstruction. However, because the architecture of the student network is ResNet18, knowledge distillation for student-teacher pairs with different structures is more challenging than learning student-teacher pairs as in STPM with the same structure. Therefore, in the proposed method, to proceed with the learning, an attention mechanism is used with the structure to propagate some features of the teacher network to the student network. Because only normal images are used for training in the proposed method, the attention mechanism reconstructs normal regions with higher accuracy. Knowledge distillation is ensured by passing hints through the attention mechanism.</p><p>Although the use of a student-teacher pair for reconstruction improves the accuracy of anomaly detection, a discriminative network is added to further improve the accuracy of anomaly detection. Because anomaly detection with knowledge distillation uses only normal images for training, it cannot discriminate whether a product is an anomaly. Thus, there are cases in which anomaly maps have large values for the normal regions, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Anomaly maps can detect abnormal areas, but they also detect normal areas as anomalies. This decreases the accuracy of anomaly detection.</p><p>Therefore, the anomaly maps obtained by our STPM, with two pairs of student-teacher networks, are input into a discriminative network, to reconsider the anomaly map when pseudo-abnormal images are also fed into our STPM. The discriminative network thus learns to produce a more accurate anomaly map. The accuracy of anomaly detection is improved by multiplying the anomaly map obtained from the discriminant network by that obtained from the STPM with two pairs of networks.</p><p>The proposed method was evaluated on an MVTec anomaly detection dataset <ref type="bibr" target="#b1">[2]</ref>. The pixel-level AUC of the proposed method outperformed that of the original STPM in many categories. The image-level AUC of the proposed method was also better than that of the original STPM. In addition, the effectiveness of different teacher networks and attention mechanisms was demonstrated in ablation studies.</p><p>The rest of this paper is organized as follows. In Section II, related works are described. In Section III, the proposed method is explained in detail. In Section IV, the experimental results are presented. Finally, the study is concluded in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anomaly Detection</head><p>There are two types of anomaly detection: image-level and pixel-level. The goal of image-level anomaly detection is to classify anomalous samples correctly. There are three approaches for image-level anomaly detection: generative models, the distribution of data such as feature spaces, and classification. The generative model detects anomalies based on the degree of reconstruction loss <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The distribution-based method considers samples that deviate from the normal data distribution as abnormal. When generating a probability distribution of only normal products, the probability density of abnormal products is low <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, allowing the classification of abnormal data. A method based on classification is a method for anomaly detection that combines geometric transformation and classification <ref type="bibr" target="#b20">[21]</ref>. It detects anomalous samples based on the idea that classification accuracy for unknown anomalous data is poor. However, these methods are unsuitable for anomaly detection at the pixel level.</p><p>Pixel-level anomaly detection is a method used for the detection of anomalies at each pixel. The difficulty of detecting anomalies at the pixel level is higher than that of detecting anomalies at the image level because the number of inspection targets increases. The mainstream method for detecting anomalies at the pixel level is based on generative models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, such as GAN and autoencoders. In recent years, methods that combine GAN and autoencoders have been studied <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The generative model must reconstruct normal images with high accuracy; otherwise, the accuracy of anomaly detection decreases. SPADE <ref type="bibr" target="#b25">[26]</ref> was proposed as a new approach for pixellevel anomaly detection. SPADE compares the features of normal and abnormal images based on a pre-trained model, and k-means clustering was used to detect abnormalities. In addition to the pre-trained model, uninformed students <ref type="bibr" target="#b0">[1]</ref> using knowledge distillation with a student-teacher structure have also been proposed. Uninformed students train the student network using only normal data. Anomaly maps are then calculated from two perspectives: (i) the difference between the outputs of student and teacher networks, (ii) the uncertainty of multiple student networks. STPM <ref type="bibr" target="#b13">[14]</ref> was proposed as a method for developing both SPADE and uninformed students. In STPM, because the training is performed only on normal data, the student network represents only the features of the normal region. In contrast, the teacher network is pre-trained on ImageNet, so it can represent the features of anomalous regions. The difference in the feature representation between the student and teacher networks is the anomaly region. STPM uses knowledge distillation for feature maps at three different resolutions and outputs three anomaly maps. The final anomaly map is computed by multiplying the three anomaly maps. However, if one of the three anomaly maps is unable to detect an anomalous region, the final anomaly map cannot detect the anomalous region, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. To address this problem, each anomaly map is improved by using a new student network for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism</head><p>Various attention mechanisms have been proposed for use in image recognition. The residual attention network <ref type="bibr" target="#b26">[27]</ref> solved the vanishing gradient problem using a structure similar to that of a residual block. Squeeze-and-excitation network (SENet) <ref type="bibr" target="#b27">[28]</ref> introduced an attention mechanism that emphasizes important channels in feature maps. A transformer <ref type="bibr" target="#b28">[29]</ref> was proposed for language translation using only the attention mechanism. Several image recognition methods using self-attention, a type of transformer, have also been proposed <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. An attention branch network <ref type="bibr" target="#b32">[33]</ref> proposes an attention map for classification by aggregating multiple feature maps. An attention map can be used to visualize the basis of decisions.</p><p>In our method, the purpose of the attention mechanism is to leak the features of the teacher network to the student network to effectively reconstruct the features of normal data. Because anomalies must be detected at the pixel level, an attention mechanism that can emphasize and suppress pixels is more suitable than an attention mechanism that emphasizes channels such as SENet. If almost all features in the teacher network are leaked into the student network, there will be no difference between the student and teacher networks. Therefore, by aggregating the features in the teacher network into one channel, pixels can be emphasized and suppressed without providing the entire information. Therefore, we used an attention map generated from the teacher network. Because only normal data are used for training the student network, the attention mechanism is used to reconstruct the normal regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The details of the proposed method are described in this section. The proposed method makes four contributions: the student network for reconstruction, attention from the teacher network to the student network, different teacher networks from the original STPM, and the discriminative network.</p><p>In Section III.A, the original STPM and its problems, in Section III.B, the student network for reconstruction and its teacher network, in Section III.C, the attention mechanism from the teacher to the student network, and in Section III.D, a discriminative network that revisits anomaly maps obtained by STPM with two pairs of student-teacher networks are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem of STPM</head><p>STPM is an effective anomaly detection method. STPM multiplies the three anomaly maps at different resolutions. The accuracy of the anomaly detection was improved by multiplying the three anomaly maps. However, if one of the three anomaly maps is unable to detect the anomalous region, the anomalous region cannot be detected after multiplying the three maps. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example in which STPM failed to detect anomalous regions because of such a problem. Multiplying the three anomaly maps improved the accuracy of anomaly detection with respect to the detection accuracy at each resolution, but there were cases of failure after multiplication. To address this problem, it was necessary to improve each anomaly map at three different resolutions. As shown in the network structure of STPM in <ref type="figure" target="#fig_1">Figure 2</ref>, STPM does not use feature reconstruction such as generative models. Adding reconstruction abilities, such as generative models, to new student networks was considered, to improve the accuracy of anomaly detection. The proposed network detects anomalies using two pairs of student-teacher networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Student Network for Reconstruction</head><p>In this work, a new student network is introduced to reconstruct normal features. There are three main contributions in the new student network. The first is the reconstruction of normal features. Second, a teacher network different from that of the original STPM is used for the new student network. The third is the attention mechanism from the teacher to student network, as explained in the next section. <ref type="figure">Fig. 4</ref>. Attention mechanism in the proposed method. The student network can emphasize the important pixels for reconstruction.  <ref type="figure" target="#fig_2">Figure 3</ref> presents an overview of the proposed method. The input for the new student network is the feature map at the lowest resolution in the pre-trained ResNet18, whereby the student network reconstructs the features of normal data. The structure of the new student network is similar to that of ResNet18. Therefore, if the same teacher network is used for student2 as in the original STPM, the same mistakes as in the original STPM will be made. Thus, ResNet50 pre-trained on ImageNet was used as the teacher network for student2 because ResNet50 has features and structures that are different from ResNet18, which is teacher1. With a perspective different from student1, such features will improve anomaly detection accuracy.</p><p>The equation for knowledge distillation is the same as that for conventional STPM. A normal image is defined as I k ? R w?h?c , where h is the height, w is the width, and c is the number of channels. The features in the teacher and student networks are normalized along the channel dimensions as,</p><formula xml:id="formula_0">F l t (I k ) ij = F l t (I k ) ij ||F l t (I k ) ij || 2 2 ,F l s (I k ) ij = F l s (I k ) ij ||F l s (I k ) ij || 2 2<label>(1)</label></formula><p>where F l t and F l s represent the features of the teacher and student networks, respectively; l represents the resolution of the feature map; and (i, j) is the position. The student network learns to obtain the same normalized output. The loss function is defined as follows:</p><formula xml:id="formula_1">l (I k ) ij = 1 2 ||F l t (I k ) ij ?F l s (I k ) ij || 2 2<label>(2)</label></formula><p>Equation 2 represents the pixel loss. For the entire image, the loss function is defined as,</p><formula xml:id="formula_2">l (I k ) = 1 w l h l w l i=1 h l j=1 l (I k ) ij<label>(3)</label></formula><p>The same loss function is used for feature maps at three resolutions in the two-student networks. The numbers in <ref type="figure" target="#fig_2">Fig.  3</ref> indicate the locations where the loss function was applied. The final loss function is the sum of the losses, defined as,</p><formula xml:id="formula_3">(I k ) = L l=1 l (I k )<label>(4)</label></formula><p>where L is the number of knowledge distillations. In this study, L = 6 because the loss was calculated at six locations, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. In the test phase, six anomaly maps {? 1 (J), ..., ? 6 (J)} were obtained using student1 and student2. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the anomaly maps at the same resolution are summed, while the anomaly maps at different resolutions are multiplied to generate the final anomaly map. When multiplying the three anomaly maps, the size of each anomaly map is made the same using bilinear interpolation. Anomaly detection is performed using the final anomaly map.</p><formula xml:id="formula_4">?(J) = {? 1 (J) + ? 6 (J)} {? 2 (J) + ? 5 (J)} {? 3 (J) + ? 4 (J)}<label>(5)</label></formula><p>where J ? R w?h?c denotes the test image. is the elementwise multiplication operation. The input for student2 is the feature maps with a size of 1/32 of the input image, and student2 reconstructs the feature maps up to 1/4 of the size of the input image. Anomaly maps from ? 1 to ? 3 in <ref type="figure" target="#fig_2">Figure 3</ref> were obtained from the original STPM. Anomaly maps from ? 4 to ? 6 obtained from the reconstruction network are also shown. Anomaly map ? 4 has the lowest resolution. When the anomaly maps of the original STPM are compared with those of the reconstructed network, the accuracy of the reconstructed network is better than that of the original STPM. In particular, when the anomaly maps ? 1 are compared with ? 6 at the highest resolution, it is observed that the anomaly map ? 6 of the reconstructed network was able to detect anomalies that were not detected by the original STPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Mechanism for feature propagation</head><p>The third contribution of the proposed method is the attention mechanism from the teacher network to a new student network for reconstruction. The teacher network consisted of 50 layers, whereas the student network consisted of 18 layers. Knowledge distillation between networks with different structures is more difficult to learn than those between networks with the same structure <ref type="bibr" target="#b33">[34]</ref>. It is likely that learning will not proceed as intended. Thus, an attention mechanism was used to ensure that learning proceeded well. <ref type="figure">Figure 4</ref> shows the structure of the attention mechanism. The attention mechanism receives the feature maps in the teacher network and aggregates them into one channel. The <ref type="figure">Fig. 6</ref>. Overview of the proposed network. The discriminant network uses pseudo-anomalies to reconsider the anomaly map obtained from STPM with two pairs of student-teacher networks. In the test phase, the anomaly map from the STPM is multiplied by that from the discriminant network.</p><p>attention mechanism was also used to teach hints to the student network to facilitate learning. Because only normal data are used for training, with the attention mechanism, the features in the student network for the normal region become more similar to the features in the teacher network. If most of the features in the teacher network are leaked into the student network, there will be no difference between the student and teacher networks. Thus, an attention map with only one channel was used to limit the amount of information provided. Considering an attention mechanism that emphasizes the channel would not be effective in detecting anomalies because it would not be able to learn important locations for pixel-level anomaly detection. Thus, considering a single attention map would be better because it would emphasize important pixels. The computational graph between the attention mechanism and the teacher network is not connected; therefore, the teacher network is not updated. Because only normal products are used as training data, the attention mechanism learns to successfully reconstruct the normal regions. <ref type="figure" target="#fig_3">Figure 5</ref> shows examples of attention maps. The attention maps at the highest resolution, which are feature map 6, are displayed in <ref type="figure" target="#fig_2">Figure 3</ref>. <ref type="figure" target="#fig_3">Figure 5</ref> shows the attention maps of the normal and abnormal data with three categories along with the ground truth labels corresponding to the abnormal regions. Attention maps differ between the normal and abnormal areas. The network was trained using only the normal data. Therefore, it behaves differently in normal regions when the input is an abnormal region that has never been seen during training. The attention mechanism was also introduced into the reconstruction network in which knowledge distillation is performed. The usage of the attention mechanism encourages the student network to represent the features in normal regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discriminative Network</head><p>In this study, the anomaly detection accuracy of the conventional STPM was improved by adding a student network for reconstruction. The STPM with two pairs of studentteacher networks, uses only normal images for training. An anomaly map is calculated only by taking the difference in features between the student-teacher networks, and there are cases where the anomaly map has large values for normal regions. To address this problem, a discriminative network with U-Net was used. In an ideal anomaly map, the values for the normal regions are zero and only the abnormal regions have large values. Thus, a discriminative network was trained to learn pseudo-anomalies to make the anomaly map closer to the ideal anomaly map. <ref type="figure">Figure 6</ref> shows an overview of the proposed method with a discriminative network. Pseudo-anomalies are input into the trained STPM with two pairs of student-teacher networks and three anomaly maps are obtained. The anomaly maps at three resolutions, {? 1 (I a )+? 6 (I a )}, {? 2 (I a )+? 5 (I a )}, and {? 3 (I a ) + ? 4 (I a )}, are fed into the discriminant network. Note that I a ? R w?h?c is a pseudo-anomalous image. The discriminative network identifies pseudo-anomalies from the input anomaly maps. The focal loss used in training the difficult samples is defined as,</p><formula xml:id="formula_5">L seg (P, T ) = 1 hw h i=1 w j=1 ?T ij (1 ? P ij ) ? log P ij<label>(6)</label></formula><p>where P is the anomaly map for the pseudo-anomaly predicted by the discriminative network; T is the ground truth; and h and w denote the height and width of the input image, respectively. ? was set to two. Because the discriminative network learns to discriminate whether the regions are anomalous, anomaly values for normal regions are closer to zero, and anomaly regions have values closer to one. The anomaly map from our STPM was combined with that of the discriminative network to achieve more accurate anomaly detection. The final anomaly map was calculated as follows:</p><formula xml:id="formula_6">?(J) = {? 1 (J) + ? 6 (J)} {? 2 (J) + ? 5 (J)} {? 3 (J) + ? 4 (J)} {? 7 (J)} (7)</formula><p>where ? 7 is the anomaly map of the discriminant network and J is a test image. is the element-wise multiplication operation.</p><p>The pseudo-anomalies proposed in DRAEM <ref type="bibr" target="#b34">[35]</ref> were used to train the discriminative network. DRAEM can generate various types of pseudo-anomalies using Perlin noise <ref type="bibr" target="#b35">[36]</ref> as well as parameters that determine the concentration of anomalous regions. In this study, DRAEM pseudo-anomalies were used to prevent overfitting of pseudo-anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, the experimental results are presented. In Section IV.A, the datasets and experimental conditions are described. In Section IV.B, the experimental results for the MVTec dataset are presented. In Section IV.C, the results of the ablation study are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The MVTec anomaly datasets <ref type="bibr" target="#b1">[2]</ref> was used for the evaluation. The MVTecAD includes 15 categories of industrial products. The training data consisted of only the normal products. The image size was different for each category, and all images were resized to 256 ? 256 pixels. The STPM was trained with two pairs of student-teacher networks and separately with the discriminative network. The numbers of epochs for training the STPM and discriminative network were 100 and 300, respectively. The SGD optimizer was used with a momentum of 0.9. The learning rate was set to 0.4, batch size to 32, and weight decay to 1 ? 10 ?4 . The optimization method was Adam in the discriminative network, and the learning rate was set to 0.0001.</p><p>The pixel-level and image-level areas under the ROC curve (AUC) were used as the evaluation measures. Per-regionoverlap (PRO) <ref type="bibr" target="#b0">[1]</ref> was also used to evaluate the anomaly maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on Anomaly detection</head><p>First, pixel-level AUC and PRO metrics are described. The pixel-level AUC and PRO metrics indicate whether the anomaly map correctly detects the anomalous area. <ref type="table" target="#tab_0">Table  I</ref> displays the accuracy. Our (Student-Tescher) method in <ref type="table" target="#tab_0">Table I</ref> shows the results using only the network in <ref type="figure" target="#fig_2">Figure  3</ref>. Our (Discriminative) method shows the accuracy when only the output ? 7 of the discriminative network is used. It is observed from the table that the reconstruction network when combined with the conventional STPM improves the accuracy, and the discriminative network alone outperforms the conventional STPM. The anomaly detection accuracy of the proposed method using both the STPM with two pairs of student-teacher networks and the discriminative network, as shown in <ref type="figure">Figure 6</ref>, provided further improvement. The pixel-level AUC and PRO scores of the proposed method outperformed the accuracies of the DRAEM using pseudoanomalies and Padim using a pre-trained model. The proposed method can accurately detect anomalous regions because the reconstruction network improves the conventional STPM and the discriminative network assists the STPM using the reconstruction network.</p><p>Next, the results of image-level AUC scores are presented. The image-level AUC is a score that indicates whether the input image can be correctly classified as abnormal. <ref type="table" target="#tab_0">Table II</ref> lists the image-level AUCs. The accuracy of anomaly detection using the proposed methods outperformed conventional STPM. In the Table, the anomaly classification accuracies of Padim, STPM, and our (student-teacher) method which learn only normal products are lower than those of DRAEM and our (discriminative) method trained using pseudo-anomalies. The accuracy of anomaly classification can be improved using pseudo-anomalies. The proposed method achieved the best accuracy by combining the reconstruction and discriminative networks.</p><p>The qualitative results are shown in <ref type="figure" target="#fig_4">Figure 7</ref>. The anomaly maps of the proposed method displayed in <ref type="figure" target="#fig_4">Figure 7</ref>, are significantly better than those of conventional STPM. Conventional STPM cannot accurately detect anomaly areas of "Cable", "Pill", and "Capsule". In contrast, the proposed method can accurately detect anomalous regions that STPM cannot. STPM detects the abnormal area of "Screw" and "Transistor," but there are large anomaly values in normal areas. The proposed method can accurately detect these anomalies. These results demonstrate the effectiveness of the proposed method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study</head><p>In the proposed method, student network1 and student network2 have different teacher networks, as shown in <ref type="figure" target="#fig_2">Figure  3</ref> and 6, respectively. An attention mechanism was also used from the teacher network to the student network. First, the influence of changes in teacher networks is discussed. In this study, ResNet50 was used as a teacher for student reconstruction. The proposed method was compared with the method in which ResNet18 was used as a teacher. Note that all the methods use an attention mechanism from the teacherto-student network. <ref type="table" target="#tab_0">Table III</ref> lists the accuracy associated with the teacher network changes. Different teacher networks lead to a perspective different from that of the conventional method, improving anomaly detection accuracy.</p><p>Next, the influence of the attention mechanism is discussed. In this study, an attention mechanism was used only between Student2 and Teacher2. The proposed method is compared with the method without the attention mechanism and the method using attention for both Student1-Teacher1 and Student2-Teacher2. <ref type="table" target="#tab_0">Table IV</ref> shows that it is better to use the attention mechanism only between Student2 and Teacher2. We believe that the attention mechanism allows us to learn better. Attention between Student1 and Teacher1 did not improve the accuracy. Because Student1 and Teacher1 have the same structure, knowledge distillation is easy. Therefore, if the attention mechanism is used for student1 and teacher1, the representational ability of the student network is high, possibly leading to a small difference in features between students and teachers. The attention mechanism was appropriate for use in Student2 and Teacher2, providing the expected improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this study, the STPM anomaly detection method was improved using a new student network for reconstruction. The teacher network was modified for a new student network and the attention mechanism from teacher to student was used to ensure successful learning. To further improve the accuracy of anomaly detection, discriminative networks were used to reconsider the anomaly maps. By using pseudoanomalies in training a discriminative network, a more accurate anomaly map was generated for normal regions. The proposed method, using both STPM with two pairs of student-teacher networks and a discriminative network, achieved high anomaly detection accuracy compared with conventional methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The problem of STPM-based methods. The anomaly map has large values for both abnormal and normal regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>STPM and its problems. STPM detects anomalies by multiplying the anomaly maps at three different resolutions. However, if one of the three anomaly maps fails to detect an anomaly location, the anomaly detection fails. The bottom images show examples of failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of STPM with two pairs of student-teacher networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of attention map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Examples of anomaly map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ANOMALY</head><label>I</label><figDesc>LOCALIZATION ON THE MVTEC (PIXEL LEVEL AUROC / PRO AUROC)</figDesc><table><row><cell></cell><cell cols="2">category</cell><cell cols="2">PaDim[37]</cell><cell>DRAEM</cell><cell></cell><cell>STPM</cell><cell>Ours (Student-Teacher)</cell><cell>Ours (Discriminative)</cell><cell>Ours</cell></row><row><cell></cell><cell cols="2">Carpet</cell><cell cols="2">0.991 / 0.962</cell><cell>0.955 / -</cell><cell cols="2">0.988 / 0.958</cell><cell>0.988 / 0.952</cell><cell>0.985 / 0.952</cell><cell>0.992 / 0.968</cell></row><row><cell>Textures</cell><cell cols="2">Grid Leather Tile</cell><cell cols="2">0.973 / 0.946 0.992 / 0.978 0.941 / 0.860</cell><cell>0.997 / -0.986 / -0.992 / -</cell><cell cols="2">0.990 / 0.966 0.993 / 0.980 0.974 / 0.921</cell><cell>0.994 / 0.967 0.984 / 0.951 0.970 / 0.910</cell><cell>0.996 / 0.974 0.996 / 0.984 0.987 / 0.958</cell><cell>0.996 / 0.975 0.996 / 0.984 0.988 / 0.962</cell></row><row><cell></cell><cell>Wood</cell><cell></cell><cell cols="2">0.949 / 0.911</cell><cell>0.964 / -</cell><cell cols="2">0.972 / 0.936</cell><cell>0.970 / 0.942</cell><cell>0.978 / 0.960</cell><cell>0.981 / 0.967</cell></row><row><cell></cell><cell cols="2">Bottle</cell><cell cols="2">0.983 / 0.948</cell><cell>0.991 / -</cell><cell cols="2">0.988 / 0.951</cell><cell>0.990 / 0.965</cell><cell>0.993 / 0.962</cell><cell>0.993 / 0.973</cell></row><row><cell></cell><cell>Cable</cell><cell></cell><cell cols="2">0.967 / 0.888</cell><cell>0.943 / -</cell><cell cols="2">0.955 / 0.877</cell><cell>0.973 / 0.924</cell><cell>0.979 / 0.926</cell><cell>0.983 / 0.948</cell></row><row><cell></cell><cell cols="2">Capsule</cell><cell cols="2">0.985 / 0.935</cell><cell>0.943 / -</cell><cell cols="2">0.983 / 0.922</cell><cell>0.985 / 0.917</cell><cell>0.972 / 0.874</cell><cell>0.985 / 0.910</cell></row><row><cell>Objects</cell><cell cols="2">Hazel nut Metal nut Pill Screw</cell><cell cols="2">0.982 / 0.926 0.972 / 0.856 0.957 / 0.927 0.985 / 0.944</cell><cell>0.997 / -0.995 / -0.976 / -0.976 / -</cell><cell cols="2">0.985 / 0.943 0.976 / 0.945 0.978 / 0.965 0.983 / 0.930</cell><cell>0.991 / 0.948 0.982 / 0.947 0.972 / 0.965 0.993 / 0.958</cell><cell>0.995 / 0.949 0.989 / 0.958 0.990 / 0.961 0.989 / 0.935</cell><cell>0.995 / 0.956 0.989 / 0.963 0.987 / 0.974 0.993 / 0.955</cell></row><row><cell></cell><cell cols="4">Toothbrush 0.988 / 0.931</cell><cell>0.981 / -</cell><cell cols="2">0.989 / 0.922</cell><cell>0.989 / 0.905</cell><cell>0.994 / 0.925</cell><cell>0.993 / 0.927</cell></row><row><cell></cell><cell cols="2">Transistor</cell><cell cols="2">0.975 / 0.845</cell><cell>0.909 / -</cell><cell cols="2">0.825 / 0.695</cell><cell>0.898 / 0.817</cell><cell>0.872 / 0.803</cell><cell>0.907 / 0.830</cell></row><row><cell></cell><cell cols="2">Zipper</cell><cell cols="2">0.985 / 0.959</cell><cell>0.988 / -</cell><cell cols="2">0.985 / 0.952</cell><cell>0.985 / 0.955</cell><cell>0.988 / 0.957</cell><cell>0.992 / 0.972</cell></row><row><cell></cell><cell>Mean</cell><cell></cell><cell cols="2">0.975 / 0.921</cell><cell>0.973 / -</cell><cell cols="2">0.970 / 0.921</cell><cell>0.977 / 0.935</cell><cell>0.980 / 0.939</cell><cell>0.985 / 0.951</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">RESULT OF IMAGE LEVEL AUC ON THE MVTEC</cell></row><row><cell></cell><cell></cell><cell cols="2">category</cell><cell cols="4">PaDim[37] DRAEM STPM</cell><cell>Ours (Student-Teacher)</cell><cell>Ours (Discriminative)</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell cols="2">Carpet</cell><cell>0.998</cell><cell cols="2">0.970</cell><cell>-</cell><cell>0.981</cell><cell>0.964</cell><cell>0.987</cell></row><row><cell></cell><cell>Textures</cell><cell cols="2">Grid Leather Tile</cell><cell>0.967 1.000 0.981</cell><cell cols="2">0.999 1.000 0.996</cell><cell>---</cell><cell>0.984 0.998 0.953</cell><cell>1.000 1.000 0.986</cell><cell>1.000 1.000 0.999</cell></row><row><cell></cell><cell></cell><cell cols="2">Wood</cell><cell>0.992</cell><cell cols="2">0.991</cell><cell>-</cell><cell>0.992</cell><cell>0.966</cell><cell>0.993</cell></row><row><cell></cell><cell></cell><cell cols="2">Bottle</cell><cell>0.999</cell><cell cols="2">0.992</cell><cell>-</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell></cell><cell></cell><cell cols="2">Cable</cell><cell>0.927</cell><cell cols="2">0.918</cell><cell>-</cell><cell>0.967</cell><cell>0.989</cell><cell>0.996</cell></row><row><cell></cell><cell></cell><cell cols="2">Capsule</cell><cell>0.913</cell><cell cols="2">0.985</cell><cell>-</cell><cell>0.873</cell><cell>0.945</cell><cell>0.930</cell></row><row><cell></cell><cell>Objects</cell><cell cols="2">Hazel nut Metal nut Pill Screw</cell><cell>0.920 0.987 0.933 0.858</cell><cell cols="2">1.000 0.987 0.989 0.939</cell><cell>----</cell><cell>1.000 1.000 0.967 0.948</cell><cell>0.987 0.999 0.980 0.941</cell><cell>0.998 1.000 0.981 0.968</cell></row><row><cell></cell><cell></cell><cell cols="2">Toothbrush</cell><cell>0.961</cell><cell cols="2">1.000</cell><cell>-</cell><cell>0.900</cell><cell>0.987</cell><cell>0.979</cell></row><row><cell></cell><cell></cell><cell cols="2">Transistor</cell><cell>0.974</cell><cell cols="2">0.931</cell><cell>-</cell><cell>0.975</cell><cell>0.978</cell><cell>0.983</cell></row><row><cell></cell><cell></cell><cell cols="2">Zipper</cell><cell>0.903</cell><cell cols="2">1.000</cell><cell>-</cell><cell>0.898</cell><cell>0.998</cell><cell>0.993</cell></row><row><cell></cell><cell></cell><cell cols="2">Mean</cell><cell>0.955</cell><cell cols="2">0.980</cell><cell>0.955</cell><cell>0.962</cell><cell>0.981</cell><cell>0.987</cell></row><row><cell></cell><cell cols="3">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">ACCURACY WHILE CHANGING TEACHER NETWORK FOR</cell><cell></cell><cell></cell></row><row><cell cols="5">RECONSTRUCTION STUDENT</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">ResNet18 ResNet50(Ours)</cell><cell></cell><cell></cell></row><row><cell cols="2">Image Level AUC</cell><cell>0.982</cell><cell></cell><cell cols="2">0.987</cell><cell></cell><cell></cell></row><row><cell cols="2">Pixel Level AUC</cell><cell>0.979</cell><cell></cell><cell cols="2">0.985</cell><cell></cell><cell></cell></row><row><cell>PRO</cell><cell></cell><cell>0.937</cell><cell></cell><cell cols="2">0.951</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="3">ACCURACY WITH/WITHOUT ATTENTION MECHANISM</cell><cell></cell></row><row><cell></cell><cell cols="2">Without Attention All Students-Teachers</cell><cell>Ours</cell></row><row><cell>Image Level AUC</cell><cell>0.985</cell><cell>0.985</cell><cell>0.987</cell></row><row><cell>Pixel Level AUC</cell><cell>0.983</cell><cell>0.983</cell><cell>0.985</cell></row><row><cell>PRO</cell><cell>0.944</cell><cell>0.946</cell><cell>0.951</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mvtec-ad real real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Anomaly detection in nano brous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">209</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Informed democracy: voting-based novelty detection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brain lesion Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention based glaucoma detection: a large-scale database and CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10571" to="10580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">f-anoGAN: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Q-space novelty detection with variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meissner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sgarlata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tomassini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Diffusion MRI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="113" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Student-teacher feature pyramid matching for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04257</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09136</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">t know? arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarially learned anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="727" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reconstruction by inpainting for visual anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ganomaly: Semisupervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection neural network with dual auto-encoders GAN and its industrial inspection applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3336</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention branch network : Learning of attention mechanism for visual explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10705" to="10714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DRAEM-A discriminatively trained reconstruction embedding for surface anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An image synthesizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Siggraph Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="296" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Padim: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
