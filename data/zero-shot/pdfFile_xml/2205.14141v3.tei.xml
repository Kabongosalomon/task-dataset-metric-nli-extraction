<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
							<email>t-yixuanwei@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>yuecao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked image modeling (MIM) learns representations with remarkably good finetuning performances, overshadowing previous prevalent pre-training approaches such as image classification, instance contrastive learning, and image-text alignment. In this paper, we show that the inferior fine-tuning performance of these pre-training approaches can be significantly improved by a simple post-processing in the form of feature distillation (FD). The feature distillation converts the old representations to new representations that have a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by a set of attention-and optimization-related diagnosis tools. With these properties, the new representations show strong fine-tuning performance. Specifically, the contrastive self-supervised learning methods are made as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is also significantly improved, with a CLIP ViT-L model reaching 89.0% top-1 accuracy on ImageNet-1K classification. On the 3-billion-parameter SwinV2-G model, the fine-tuning accuracy is improved by +1.5 mIoU / +1.1 mAP to 61.4 mIoU / 64.2 mAP on ADE20K semantic segmentation and COCO object detection, respectively, creating new records on both benchmarks. More importantly, our work provides a way for the future research to focus more effort on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness since it can be enhanced rather easily. The code will be available at https://github.com/SwinTransformer/Feature-Distillation. * Equal. ? Correspondence. Yixuan and Zhenda are long-term interns at MSRA.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The paradigm of pre-training and fine-tuning has played a key role in the development of deep learning methods in the field of computer vision. In 2006, the autoencoder based pre-training <ref type="bibr" target="#b18">[19]</ref> is a pioneer work that largely triggered the burst of deep learning. Furthermore, since AlexNet <ref type="bibr" target="#b24">[25]</ref> achieved revolutionary recognition accuracy on ImageNet-1K image classification <ref type="bibr" target="#b8">[9]</ref> in 2012, model pre-training using the image classification task has become a standard practice for a variety of down-stream computer vision tasks, including object detection <ref type="bibr" target="#b13">[14]</ref> and semantic segmentation <ref type="bibr" target="#b30">[31]</ref>.</p><p>For representation learning in computer vision, two notable approaches have been quite successful: the instance contrastive learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref> and the image-text alignment methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22]</ref>. The former learns representation in a self-supervised manner, and achieves impressive linear evaluation performance on image classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3]</ref>. The latter, represented by the CLIP approach <ref type="bibr" target="#b33">[34]</ref>, is notable for opening up the field of zero-shot recognition, allowing visual recognition models  <ref type="bibr" target="#b39">[40]</ref> ViT-L 336 2 87.1 --DINO <ref type="bibr" target="#b2">[3]</ref> ViT  to classify almost any category. However, when fine-tuned on down-stream vision tasks, their performance is generally not superior to other methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref> and thus limits their wider adoption.</p><p>Recently, the masked image modeling (MIM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b16">17]</ref> has achieved remarkable performance in fine-tuning evaluations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b16">17]</ref> and attracted widespread attention. The success of MIM begs for the question: why does MIM perform so much better in fine-tuning? In other words, are there key ingredients that can be added to other pre-training approaches to make them as successful as MIM in fine-tuning?</p><p>In this paper, we show that a simple feature distillation method can generally improve the fine-tuning performance of various pre-training methods, including contrastive based self-supervised learning approaches such as DINO <ref type="bibr" target="#b2">[3]</ref> and EsViT <ref type="bibr" target="#b25">[26]</ref>, visual-language models such as CLIP <ref type="bibr" target="#b33">[34]</ref>, and image classification methods such as DeiT <ref type="bibr" target="#b36">[37]</ref>, as shown in <ref type="table" target="#tab_0">Table 1</ref>. In our feature distillation method, the already learnt representations are distilled into new features that are trained from scratch. For the distillation targets, we advocate feature maps rather than logits, which allows the FD approach to handle features obtained by arbitrary pre-training methods and leads to better fine-tuning accuracy. We also propose useful designs that are beneficial for the successive fine-tuning process, including whitened distillation targets, shared relative position bias, and asymmetric drop path rates. With this approach and the careful design, contrastive based self-supervised pre-training approaches such as DINO and EsViT become competitive in the fine-tuning evaluation as the masked image modeling approaches, or even slightly better. The CLIP pre-trained ViT-L model achieves an 89.0% top-1 accuracy on ImageNet-1K image classification, a new state-of-the-art result with ViT-L. On the 3-billion-parameter SwinV2-G model, the fine-tuning accuracy is improved by +1.5 mIoU / +1.1  mAP to 61.4 mIoU / 64.2 mAP on ADE20K semantic segmentation and COCO object detection, respectively, creating new records on both benchmarks (see <ref type="table" target="#tab_2">Table 2</ref>). We analyze the model properties before and after feature distillation with a set of attention-and optimization-related diagnostic tools. We observe that the feature distillation converts the old representations to new ones embodied a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by the attention-and optimization-related diagnosis tools, including average attention distances <ref type="bibr" target="#b9">[10]</ref>, average attention maps, attention similarities between heads <ref type="bibr" target="#b54">[55]</ref>, and normalized loss landscape <ref type="bibr" target="#b27">[28]</ref>. These tools show that the models after distillation have more diverse attention heads, and account more on relative positions than the absolute positions. These are desirable properties that encourage flatter loss landscapes in fine-tuning as well as better final accuracy. Note that representations learnt by the masked image modeling methods already have nice optimization friendliness properties and hence adding an additional feature distillation postprocessing yield little gains. Such results suggest that optimization friendly representations are the reasons behind the superior fine-tuning performance of masked image modeling methods.</p><p>Our work provides a way for the future research to focus more attention on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness. The generality and scalability are critical because they not only make the pre-training suitable for broad visual tasks but also allow the trained network to take full advantage of larger model capacity and larger data. In existing research, the goal of generality and scalability is often intertwined with that of optimization friendliness. The feature distillation approach sheds lights on how we might be able to decouple these two goals and allows more effort devoted to the critical issue of generality and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Feature Distillation Method</head><p>With an already pre-trained model, our goal is to obtain a new representation that distills knowledge from the already pre-trained model while being more friendly to fine-tuning. We achieve this by a feature distillation method, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref> (left). In this method, the already pre-trained model plays the teacher, and the new model plays the student. We consider the following designs to make this method both generic and effective.</p><p>Distilling feature maps so as to be generic Instead of distilling logits as most previous distillation works have done <ref type="bibr" target="#b19">[20]</ref>, we adopt the output feature map of the pre-trained model as the distillation target. Using the feature map as the distilling target allows us to work with any pre-trained model that may not have a logit output. In addition to being more general, distilling the feature map also shows higher fine-tuning accuracy than using logits or the reduced single feature vector (see <ref type="table" target="#tab_5">Table 3</ref>).</p><p>To make feature maps of the teacher and student comparable, we adopt the same augmentation view for each original image. We also apply a 1 ? 1 convolution layer on top of the student network to allow different dimensions of output feature maps between the teacher and student, such that the method can be further generalized.</p><p>Whitening teacher features for distillation Different pre-trained models may have very different orders of feature magnitudes, which will make difficulties in hyper-parameter tuning for different pre-training approaches. To solve this problem, we normalize the output feature map of the teacher network by a whitening operation, which is implemented by a non-parametric layer normalization operator without scaling and bias.</p><p>In distillation, we employ a smooth ? 1 loss between the student and teacher feature maps:</p><formula xml:id="formula_0">L distill (s, t) = 1 2 (g(s) ? whiten(t)) 2 /?, |g(s) ? whiten(t)| ? ? (|g(s) ? whiten(t)| ? 1 2 ?), otherwise ,<label>(1)</label></formula><p>where ? is set 2.0 by default; s and t are output feature vectors of the student and teacher networks, respectively; g is a 1 ? 1 convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared relative position bias</head><p>In the original ViT <ref type="bibr" target="#b9">[10]</ref>, relative position bias (RPB) did not show any benefit over the absolute position encoding (APE), and so absolute position encoding (APE) is usually used for ViT architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>In the feature distillation framework, we re-examine the effects of position encoding configuration in the student architecture, including absolute position encoding (APE) and relative position bias (RPB) <ref type="bibr" target="#b29">[30]</ref>. We also consider a shared RPB configuration, where all layers share the same relative positional bias matrices. We find that the shared RPB performed best overall, as shown in <ref type="table" target="#tab_7">Table 5</ref>. We find that the shared RPB can diversify the attention distances of heads, especially for the deeper layers (see <ref type="figure">figure 3</ref> and 7), which may cause its slightly better fine-tuning accuracy. We use the shared RPB by default in our experiments.</p><p>Asymmetric drop path rates The two-branch manner in the feature distillation framework allows us to use asymmetric regularization for the teacher and student networks. We find that a strategy of asymmetric drop path <ref type="bibr" target="#b20">[21]</ref> rates is beneficial for learning better representations. Specifically, on ViT-B, the strategy of applying a drop path rate of 0.1-0.3 on the student branch, and no drop path regularization on the teacher branch works best, as shown in <ref type="table" target="#tab_8">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representations before and after Feature Distillation</head><p>In this section, we delve into the feature distillation mechanism introduced in the previous section through a set of attention-and optimization-related diagnostic tools, including the average attention distance per head <ref type="bibr" target="#b9">[10]</ref>, average cosine similarities between attention maps of heads <ref type="bibr" target="#b54">[55]</ref>, average attention maps for each layer, and normalized loss landscapes <ref type="bibr" target="#b27">[28]</ref>. We perform these analyses using 50,000 ImageNet-1K validation images and diagnose the models both before and after applying the distillation method. Different property behaviors of learnt representations are observed before and after feature distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature distillation diversifies attention heads</head><p>We examine the attention diversity of heads. <ref type="figure" target="#fig_2">Figure 2</ref> shows the average attention distance per head and layer depth using the ViT-B architectures pretrained by DINO, DeiT, and CLIP, respectively. The average attention distance is introduced in <ref type="bibr" target="#b9">[10]</ref>, which can partially reflect the receptive field size for each attention head, computed according to the attention weights. It can be seen that: For all pre-trained representations before distillation, the attention distances of different heads in deeper layers collapse to locate within a very small distance range. This suggests that different heads learn very similar visual cues and may be wasting model capacity. After the feature distillation process, all representations become more diverse or more evenly distributed regarding the attention distance, especially for deeper layers. This observation is also reflected by <ref type="figure">Figure 3</ref>, which calculates the average cosine similarity between attention heads of each layer. <ref type="figure">Figure 4</ref> shows the average attention maps before (left) and after (right) feature distillation. There are two obvious patterns in the attention maps: diagonal and column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changes on attention patterns</head><p>The diagonal pattern corresponds to the relationship between image patches in some fixed relative positions, while the column pattern represents the effect of image patches in certain absolution positions to all other locations.  It can be seen that representations after feature distillation have much more diagonal patterns, which means the model relies more on visual cues that encode relationship of relative locations. It suggests better translational invariance of the model, which is often a beneficial property for various visual tasks. Noting the student network has included shared relative position bias (RPB), in order to study its effects, we also tried to use the absolute position encoding (APE) in the student architecture, whose attention maps are shown in <ref type="figure" target="#fig_5">Figure 7</ref>. In this configuration, the representations after feature distillation also rely more on relative locations, for example, Layer 0 and Layer 7, and the fine-tuning accuracy is also quite high (see <ref type="table" target="#tab_7">Table 5</ref>). This suggests that the more diagonal patterns are primarily caused by the feature distillation algorithm itself.</p><p>Feature distillation gets better loss / accuracy landscapes We use the method in <ref type="bibr" target="#b27">[28]</ref> to visualize the loss / accuracy landscapes of different models. In this visualization method, the model weights are perturbed by a series of Gaussian noises with varying degrees. Following <ref type="bibr" target="#b27">[28]</ref>, each noise level is defined normalized to the ? 2 norm of each filter to account for the effects of varying weight amplitudes of different models. <ref type="figure">Figure 5</ref> visualizes the loss / accuracy landscapes of different pretrained models before and after feature distillation. It turns out that the loss / accuracy landscapes of most representations after feature distillation have become flatter than the ones by the representations before distillation, which is consistent with their better fine-tuning accuracy.</p><p>On masked image modeling (MIM) <ref type="figure">Figure 6</ref> shows the average attention distance and the loss / accuracy landscapes of an MIM-based approach, MAE <ref type="bibr" target="#b16">[17]</ref>, before and after the feature distillation process. It can be seen that the representations pre-trained using MAE have learnt diverse heads, and that the loss / accuracy landscapes are relatively flat. In fact, an additional conversion of the old MAE representations to new ones by the feature distillation method only brings slight gains of +0.2% (83.8% versus 83.6%). These results may suggest that the good fine-tuning performance brought by the feature distillation post-processing have certain overlap in functionality with the masked image modeling (MIM) method. Evaluation settings We consider 3 evaluation benchmarks: ImageNet-1K fine-tuning, ImageNet-1K linear probe, and ADE20K semantic segmentation.</p><p>ImageNet-1K fine-tuning. We follow <ref type="bibr" target="#b0">[1]</ref> to use the AdamW optimizer <ref type="bibr" target="#b23">[24]</ref> with layer decayed learning rates. For ViT-B, we fine-tune it by 100 epochs, and our default hyper-parameters are: batch size 2,048, learning rate 5e-3, weight decay 0.05, layer decay 0.65, and drop path rate 0.3. For ViT-L, we fine-tune it by 50 epochs, and our default hyper-parameters are: batch size 2,048, learning rate 1e-3, layer decay 0.75, and drop path rate 0.4.</p><p>ImageNet-1K linear probe. We follow <ref type="bibr" target="#b16">[17]</ref> to use the LARS optimizer <ref type="bibr" target="#b47">[48]</ref> with a base learning rate of 0.1 and a weight decay of 0. For ViT-B, we train for 90 epochs. For ViT-L, we train for 50 epochs.</p><p>ADE20K semantic segmentation. We follow <ref type="bibr" target="#b29">[30]</ref> to use an UPerNet framework <ref type="bibr" target="#b41">[42]</ref> for experiments. The AdamW <ref type="bibr" target="#b23">[24]</ref>   <ref type="table" target="#tab_0">Table 1</ref> shows the main results. With the feature distillation method, all listed pre-trained models are improved by 1.0%?2.0% on ImageNet-1K fine-tuning and 1.0?3.3 mIoU on ADE20K semantic segmentation. Particularly, by this approach, we improve the CLIP ViT-L model to reach 89.0% on ImageNet-1K classification, which is +1.9% higher than a sophisticated fine-tuning approach dedicated for CLIP models <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We also improve the 3-billion-parameter SwinV2-G to achieve 61.4 mIoU and 64.2 mAP on ADE20K semantic segmentation and COCO object detection (using the same UperNet / HTC++ framework and the same evaluation settings as the original Swin V2 paper <ref type="bibr" target="#b28">[29]</ref>), creating new records that is +0.6 mIoU and +0.9 mAP higher than previous state-of-the-art reported in (Mask) DINO <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b26">27]</ref>, respectively, as shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>These results suggest the general applicability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>In this section, we ablate the designs described in Section 2. All experiments were performed on ImageNet-1K training images using ViT-B and 100-epoch training. <ref type="table" target="#tab_5">Table 3</ref> ablates the effects of different distilling targets. The use of full feature map performs best for all pre-training methods. The distillation of the full feature map can maintain more information involved in the teacher model than using other reduced features, which may lead to its superior performance. We also experimented with the classical logit distillation method on the DeiT pretrained model. It turns out that logit distillation has only +0.1% improvements compared to the original representation. In addition, for pre-training models that do not rely on classification, the logit distillation method will not be applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On distilling targets</head><p>The effect of normalizing teacher features <ref type="table" target="#tab_6">Table 4</ref> ablates the effect of whether and how to perform teacher feature map normalization. Whitening the teacher feature map brings +0.8%, +0.2%, and +1.0% improvements over using the original feature maps, for CLIP, DINO, and DeiT, respectively. Comparing two normalization approaches of ? 2 and whitening, the whitening approach performs notably better (+0.4%, +0.0%, and +1.1% for CLIP, DINO, and DeiT, respectively). Using feature map normalization also makes hyper-parameters insensitive to the pre-training models.</p><p>On position encoding configurations <ref type="table" target="#tab_7">Table 5</ref> ablates the effect of varying position encoding configurations in the student network. The shared relative position bias (RPB) configuration performs best overall. <ref type="figure" target="#fig_5">Figure 4 and 7</ref> shows that the shared RPB configuration has the most diverse attention heads, which may result in the best accuracy. Also note that all of these configurations perform quite well, suggesting that the primary factor for success comes not from the proper position encoding configuration, but from the feature distillation algorithm itself.</p><p>On asymmetric drop path rates <ref type="table" target="#tab_8">Table 6</ref> ablates the effect of different degrees of drop path regularization. Moderately increasing the drop path rate of the student network would be beneficial, possibly due to the relief of over-fitting. The optimal student drop path rate for CLIP, DINO, and DeiT are 0.1, 0.2, and 0.2, respectively. For teacher networks, adding drop path regularization often     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion in the Context of Related Works</head><p>Representation learning is a major theme in computer vision. In this section, we will discuss its goals, evaluations, property requirements, existing methods, and some reflections based on the findings of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goals and evaluations of representation learning</head><p>The aims and common evaluations for representation learning include:</p><p>? Fine-tuning purpose. The pioneer work <ref type="bibr" target="#b18">[19]</ref> discovered that the layer-by-layer greedily learnt model weights can play good initialization for the final joint learning with all layers, which has actually triggered the renaissance of deep learning in 2006. Since 2012, the use of representation learning for fine-tuning has become more de facto standard, where commonly initialized using model weights trained from the ImageNet-1K image classification task has been a key factor in generalizing deep learning into a wide range of visual tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>. This usage is so important that solid improvements in this direction can usually attract significant attention, which is right the case for the recent masked image modeling methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>? Linear evaluation. Early self-supervised learning methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18]</ref> are usually evaluated using this protocol, which fixes the visual backbone and learns a linear classifier on top of the fixed backbone. The protocol mainly reflects the linear separability of learnt features.</p><p>Since it is not directly targeted at real-world scenarios, it is not as popular as it used to be.</p><p>? Few-shot scenarios. Human beings are very good at few-shot learning. Few-shot learning also finds numerous applications to help us to quickly build new capabilities with limited data. Self-supervised approaches such as instance contrastive learning and visual-text contrastive learning have been proven to be good for few-shot learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref>. ? Zero-shot / prompt-aided inference. Another use of representation learning is zero-shot or prompt-aided inference. This allows to use pre-trained models without additional training on down-stream data <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b45">46]</ref>. This use of representation learning makes possible for a single model to cope with all tasks, and it is now attracting more and more attention from the community.</p><p>This paper is mainly concerned with the fine-tuning usage of representation learning. We will discuss:</p><p>What are nice properties for fine-tuning? Few studies have attempted to study which properties are good for fine-tuning. In this discussion, we'll divide the potentially nice properties as:</p><p>? Optimization friendliness. The pre-trained model is used as the initialization of the finetuning tasks. Good initialization can help the optimization process in fine-tuning. Given the same optimizer in fine-tuning, a better initialization may result in flatter loss landscape, and better accuracy <ref type="bibr" target="#b27">[28]</ref>. ? Encoding of scalable and generalizable knowledge. Pre-trained models are supposed to encode knowledge that can well accomplish the pre-training task. The knowledge encoded in the pre-trained models may be generally beneficial for down-stream tasks, for example, the semantics encoded in the CLIP models turn out to be significantly helpful for down-stream tasks such as ImageNet-1K classification and ADE20K segmentation tasks. Another nice property is the scalability to encode the knowledge: Can models encode well the richer knowledge when we have larger data? Can larger models be well driven by the learning task? This is often discussed in the context of scaling law <ref type="bibr" target="#b22">[23]</ref>, which has become a corner belief behind the recent remarkable success of natural language models. We expect this also applies to computer vision.</p><p>The feature distillation approach introduced in this paper mainly improves optimization friendliness as analyzed throughout this paper. We hope it to provide a way for the study of representation learning to focus more effort on the second nice property of generality and scalability. In the following, we will make some reflections on existing representation learning approaches in the context that there has been a general method such as feature distillation to improve the optimization friendliness.</p><p>Existing representation learning approaches and our reflections There are four notable representation learning approaches, including image classification <ref type="bibr" target="#b24">[25]</ref> on ImageNet-1K <ref type="bibr" target="#b8">[9]</ref>, instance contrastive learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>, visual-text contrastive learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22]</ref>, and masked image modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b16">17]</ref>. We made the following reflections to these approaches:</p><p>? Masked image modeling (MIM). Masked image modeling has attracted a lot of attention for its excellence in fine-tuning evaluations. As this paper suggests, the excellent performance may come primarily from the optimization friendliness of the learnt representation. In terms of its scalability, the ability of MIM tasks to train large-capacity models has been well demonstrated in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b28">29]</ref>, however, the ability to benefit from larger data currently sounds negative, as shown in <ref type="bibr" target="#b11">[12]</ref>. We think that this issue, if not well addressed, could hinder its further popularity. ? Instance contrastive learning. The instance contrastive learning method performs pretraining in a self-supervised manner, which has attracted a lot of attention since it surpassed the supervised classification method on multiple down-stream tasks <ref type="bibr" target="#b17">[18]</ref>. Moreover, it achieved impressive accuracy using linear and few-shot evaluations <ref type="bibr" target="#b5">[6]</ref>. However, when it comes to vision Transformer backbones <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>, the fine-tuning performance becomes inferior to others <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b25">26]</ref>, which may have hindered its appeal for now. Another issue is that its scalability to model capacity and data size is rarely studied, or is only performed in a linear evaluation setup <ref type="bibr" target="#b34">[35]</ref>. Therefore, its actual behavior on the scalability property is unclear. We hope that our significant improvements to its fine-tuning performance will encourage the community to reinvigorate the research on this method. It will be interesting to see a solid study about its scalability to model capacity and data size.</p><p>? Image classification and visual-text contrastive learning. Image classification has been the standard upstream pre-training task for nearly a decade since AlexNet <ref type="bibr" target="#b24">[25]</ref>. The visual-text alignment task opens up the field of zero-shot recognition. We now discuss them within a single item because they can basically be formulated with the same objective <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47]</ref>, and they possess some similar properties. We first examine existing studies about its scalability to model capacity and data size. In fact, both of these tasks have been shown to perform scalable <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22]</ref>. In general, the data for the visual-text tasks is more readily available and thus is more friendly for the data-hungry large model training. Moreover, the fine-tuning performance is also shown able to be significantly enhanced with the aid of our feature distillation method, which would further make this pre-training task more attractive. In these considerations, this task would be a good choice for large-scale representation learning. It will be also interesting to see if it can be combined with other representation learning methods to improve its data mining efficiency.</p><p>There are many other representation learning approaches such as: gray-scale image colorization <ref type="bibr" target="#b52">[53]</ref>, jigsaw puzzle solving <ref type="bibr" target="#b31">[32]</ref>, split-brain auto-encoding <ref type="bibr" target="#b53">[54]</ref>, rotation prediction <ref type="bibr" target="#b12">[13]</ref>, learning to cluster <ref type="bibr" target="#b1">[2]</ref>, or predicting values of some channels with one or two other channels as input <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>, and pixel-level contrast <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b37">38]</ref>. In the context of our feature distillation approach, some of them may worth re-examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper has introduced a simple feature distillation approach that can generally improve the finetuning performance of many visual pre-training models. It made the contrastive based self-supervised learning methods as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM). It also improved a CLIP pre-trained ViT-L model to reach 89.0% top-1 accuracy on ImageNet-1K classification. While our analysis through a set of attention-and optimization-related diagnosis tools suggests that our feature distillation approach mainly improves the optimization friendliness of learnt representations, we hope the findings can provide a way for the future research to focus more effort on the generality and scalability of the learnt representations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters for Feature Distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Representation Properties of Masked Image Modeling before and after Distillation</head><p>In <ref type="figure">Figure 6</ref>, we show the average attention distance and the loss accuracy landscapes of MAE before and after the feature distillation process. We further include the other two diagnosis tools, average attention maps in <ref type="figure">Figure ?</ref>? and average attention head cosine similarity in <ref type="figure" target="#fig_1">Figure 11</ref>. These two diagnosis tools also show similar behaviors before and after the feature distillation process, which also suggests that the feature distillation and masked image modeling method have certain overlap in functionality.</p><p>(a) Before feature distillation (b) After feature distillation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Training Cost by Feature Distillation Compared to that of Pre-training</head><p>All experiments in <ref type="table" target="#tab_0">Table 1</ref> use 300-epoch ImageNet-1K training. This means that the additional training cost is constant. The additional training overhead could small if the pre-training methods are costly, e.g., about +3% for the CLIP model. <ref type="table" target="#tab_11">Table 9</ref> showed the equivalent training epochs of the original pre-training methods counted according to the feature distillation epochs, as well as the performance gains on ImageNet-1K image classification. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Left: Illustration of the feature distillation approach. Right: Average attention distances per layer and head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Average attention distances per head at each layer depth before (left) and after (right) feature distillation using ViT-B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Average cosine similarity of attention maps between heads per layer before (left) and after (right) feature distillation using ViT-B. Average attention maps per layer using the CLIP ViT-B model before and after feature distillation. The image patches are indexed starting from top-left to bottom-right. The 12 layers' average attention maps (Layer 0-11) are visualized from top-left to bottom-right. The average attention maps for other pre-training approaches will be shown in appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>The loss / accuracy landscapes<ref type="bibr" target="#b27">[28]</ref> of different pre-trained models before (top) and after (bottom) feature distillation. Each plot has 5 landscapes using 5 randomly generated directions.(a) Average attention distance (b) Accuracy and loss landscapes The average attention distances and loss / accuracy landscapes before (left) and after (right) feature distillation on a masked image modeling approach, MAE<ref type="bibr" target="#b16">[17]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The average attention distance and average attention maps with different position encoding configurations on the student network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Average attention maps per layer using the MAE ViT-B model before and after feature distillation. The image patches are indexed starting from top-left to bottom-right. The 12 layers' average attention maps (Layer 0-11) are visualized from top-left to bottom-right. (a) Before feature distillation (b) After feature distillation Average cosine similarity of attention maps between heads per layer before (left) and after (right) feature distillation using the MAE ViT-B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Feature distillation improves fine-tuning performance. * uses the same model in the upper row, but with an additional inter-mediate fine-tuning step on ImageNet-22K image classification.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>res.</cell><cell>F. D.</cell><cell cols="2">IN-1K f.t. linear</cell><cell>ADE20K</cell></row><row><cell>BEiT [1]</cell><cell>ViT-B</cell><cell>224 2</cell><cell></cell><cell>83.2</cell><cell>37.6</cell><cell>47.1</cell></row><row><cell>MAE [17]</cell><cell>ViT-B</cell><cell>224 2</cell><cell></cell><cell>83.6</cell><cell>68.0</cell><cell>48.1</cell></row><row><cell>SimMIM [45]</cell><cell>ViT-B</cell><cell>224 2</cell><cell></cell><cell>83.8</cell><cell>56.7</cell><cell>47.6</cell></row><row><cell>SimMIM [45]</cell><cell>Swin-B</cell><cell>224 2</cell><cell></cell><cell>84.8</cell><cell>24.8</cell><cell>48.3</cell></row><row><cell>WiSE-FT CLIP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Feature distillation improves the state-of-the-art SwinV2-G model.</figDesc><table><row><cell>mIoU)</cell></row></table><note>Method IN-1K (%) COCO (AP box ) COCO (AP mask ) ADE20K (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The learning rate, weight decay, and batch size are set default as 1.2e-3, 0.05, and 2,048, respectively. For drop path rate, we select the best of {0.1, 0.2, 0.3, 0.4}.</figDesc><table><row><cell>4 Experiments</cell></row></table><note>4.1 Experimental Settings Distillation implementation and dataset For all experiments, we perform feature distillation using 1.28M ImageNet-1K training images. In ablation, we distill 100 epochs for all experiments. In system-level comparison such as Table 1, we adopt 300-epoch training. The AdamW optimizer [24] is used.We consider 5 pre-training methods of DINO [3], EsViT [26], CLIP [34], DeiT [36], and MAE [17], using their public checkpoints. For DINO, EsViT, and DeiT, we use their largest available models, ViT-B or Swin-B. For CLIP method, we tried both ViT-B and ViT-L.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>optimizer is employed with the training length of 160K, a batch size of 16, and a weight decay of 0.05. For ViT-B, other hyper-parameters are set as: learning rate 4e-4, layer decay 0.65, and drop path rate 0.2. For ViT-L, other hyper-parameters are set as: learning rate 1e-4, layer decay 0.75, and drop path rate 0.4.In training, the input image size is 512 ? 512. In inference, we follow the single-scale testing of<ref type="bibr" target="#b29">[30]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Ablation on distilling targets.</cell></row><row><cell>targets</cell><cell cols="3">CLIP DINO DeiT</cell></row><row><cell>before distill</cell><cell>82.9</cell><cell>82.8</cell><cell>81.8</cell></row><row><cell>logit</cell><cell>-</cell><cell>-</cell><cell>81.9</cell></row><row><cell>CLS token</cell><cell>81.7</cell><cell>82.2</cell><cell>78.0</cell></row><row><cell>GAP feature</cell><cell>83.4</cell><cell>82.8</cell><cell>82.0</cell></row><row><cell cols="2">Full Feature map 84.3</cell><cell>83.6</cell><cell>82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The effect of normalizing teacher features.</figDesc><table><row><cell cols="2">normalize method CLIP DINO DeiT</cell></row><row><cell>none</cell><cell>83.5 83.4 81.7</cell></row><row><cell>? 2 norm</cell><cell>83.9 83.6 81.6</cell></row><row><cell>whiten</cell><cell>84.3 83.6 82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">Ablation on position encoding configura-</cell></row><row><cell>tions.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Position config. CLIP DINO DeiT</cell></row><row><cell>before distill</cell><cell>82.9</cell><cell>82.8</cell><cell>81.8</cell></row><row><cell>APE</cell><cell>84.0</cell><cell>83.4</cell><cell>82.3</cell></row><row><cell cols="2">non-shared RPB 83.9</cell><cell>83.6</cell><cell>82.6</cell></row><row><cell>shared RPB</cell><cell>84.3</cell><cell>83.6</cell><cell>82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation on asymmetric drop path rate.</figDesc><table><row><cell cols="5">stu. d.p.r. tea. d.p.r. CLIP DINO DeiT</cell></row><row><cell>0.1</cell><cell>0</cell><cell cols="3">84.3 83.2 82.4</cell></row><row><cell>0.2</cell><cell>0</cell><cell cols="3">84.0 83.6 82.7</cell></row><row><cell>0.3</cell><cell>0</cell><cell>-</cell><cell cols="2">83.6 81.9</cell></row><row><cell>best</cell><cell>0.1</cell><cell cols="2">84.0 83.6</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for feature distillation on ImageNet-1K.</figDesc><table><row><cell>Hyperparameters</cell><cell>Base Size</cell><cell>Large Size</cell></row><row><cell>Patch size</cell><cell>16 ? 16</cell><cell>14 ? 14</cell></row><row><cell>Layers</cell><cell>12</cell><cell>24</cell></row><row><cell>Hidden size</cell><cell>768</cell><cell>1024</cell></row><row><cell>FFN inner hidden size</cell><cell>3072</cell><cell>4096</cell></row><row><cell>Attention heads</cell><cell>12</cell><cell>16</cell></row><row><cell>Attention head size</cell><cell>64</cell><cell></cell></row><row><cell>Training epochs</cell><cell>300</cell><cell></cell></row><row><cell>Batch size</cell><cell cols="2">2048</cell></row><row><cell>Adam ?</cell><cell>1e-8</cell><cell></cell></row><row><cell>Adam ?</cell><cell cols="2">(0.9, 0.999)</cell></row><row><cell>Peak learning rate</cell><cell cols="2">1.2e-3</cell></row><row><cell>Minimal learning rate</cell><cell>2e-5</cell><cell></cell></row><row><cell>Learning rate schedule</cell><cell cols="2">Cosine</cell></row><row><cell>Warmup epochs</cell><cell>10</cell><cell></cell></row><row><cell>Gradient clipping</cell><cell>3.0</cell><cell></cell></row><row><cell>Dropout</cell><cell>?</cell><cell></cell></row><row><cell>Weight decay</cell><cell>0.05</cell><cell></cell></row><row><cell>Stoch. depth</cell><cell>{0.1,0.2,0.3}</cell><cell>0.3</cell></row><row><cell>Data Augment</cell><cell cols="2">RandomResizeAndCrop 0.08-1</cell></row><row><cell>Input resolution</cell><cell cols="2">224 ? 224</cell></row><row><cell cols="2">C Hyperparameters for Fine-tuning</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters for fine-tuning on ImageNet-1K.</figDesc><table><row><cell>Hyperparameters</cell><cell>Base Size</cell><cell>Large Size</cell></row><row><cell>Peak learning rate</cell><cell>{5e-3, 6e-3}</cell><cell>1e-3</cell></row><row><cell>Fine-tuning epochs</cell><cell>100</cell><cell>50</cell></row><row><cell>Warmup epochs</cell><cell>20</cell><cell>5</cell></row><row><cell>Layer-wise learning rate decay</cell><cell>{0.6, 0.65}</cell><cell>0.75</cell></row><row><cell>Batch size</cell><cell cols="2">2048</cell></row><row><cell>Adam ?</cell><cell>1e-8</cell><cell></cell></row><row><cell>Adam ?</cell><cell cols="2">(0.9, 0.999)</cell></row><row><cell>Minimal learning rate</cell><cell>2e-6</cell><cell></cell></row><row><cell>Learning rate schedule</cell><cell cols="2">Cosine</cell></row><row><cell>Repeated Aug</cell><cell>?</cell><cell></cell></row><row><cell>Weight decay</cell><cell>0.05</cell><cell></cell></row><row><cell>Label smoothing ?</cell><cell>0.1</cell><cell></cell></row><row><cell>Stoch. depth</cell><cell>{0.1,0.2,0.3}</cell><cell>0.4</cell></row><row><cell>Dropout</cell><cell>?</cell><cell></cell></row><row><cell>Gradient clipping</cell><cell>5.0</cell><cell></cell></row><row><cell>Erasing prob.</cell><cell>0.25</cell><cell></cell></row><row><cell>Input resolution</cell><cell cols="2">224 ? 224</cell></row><row><cell>Rand Augment</cell><cell cols="2">9/0.5</cell></row><row><cell>Mixup prob.</cell><cell>0.8</cell><cell></cell></row><row><cell>Cutmix prob.</cell><cell>1.0</cell><cell></cell></row><row><cell>Color jitter</cell><cell>0.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparing the equivalent pre-training epochs of different models to the feature distillation cost. The top-1 accuracy gains on ImageNet-1K image classification are also listed for reference.Method Equivalent #. pre-training epochs #. Feature distillation epochs Accuracy gain</figDesc><table><row><cell>DeiT</cell><cell>300</cell><cell>100 300</cell><cell>+0.9 +1.2</cell></row><row><cell>DINO</cell><cell>1,535</cell><cell>100 300</cell><cell>+0.8 +1.0</cell></row><row><cell>EsViT</cell><cell>1,535</cell><cell>100 300</cell><cell>+0.9 +1.2</cell></row><row><cell>CLIP</cell><cell>?10,000</cell><cell>100 300</cell><cell>+1.4 +2.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>All SwinV2-G experiments are conducted by Ze Liu.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Average Attention Maps for DINO and DeiT</head><p>In the main paper, we present the average attention maps of the CLIP pre-trained ViT-B model before and after feature distillation. In <ref type="figure">Figure 8</ref> and 9, we illustrate the maps for DINO and DeiT pre-trained ViT-B models, respectively. Similar to the observations for the CLIP pre-trained model, DINO and DeiT models after feature distillation are also observed with more diagonal patterns than before. This indicates that our feature distillation approach can generally enhance the translational invariance properties for various pre-training models.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08534</idno>
		<title level="m">Vision transformer adapter for dense predictions</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Are large-scale datasets necessary for self-supervised pre-training?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10740</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient self-supervised vision transformers for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mask dino: Towards a unified transformer-based framework for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09883</idno>
		<title level="m">Swin transformer v2: Scaling up capacity and resolution</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Divide and contrast: Self-supervised learning from uncurated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10063" to="10074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10760</idno>
		<title level="m">icar: Bridging image classification and image-text alignment for visual recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Robust fine-tuning of zero-shot models</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<title level="m">Self-supervised learning with swin transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16684" to="16693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14757</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03610</idno>
		<title level="m">Unified contrastive learning in image-text-label space</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<title level="m">Large batch training of convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">Florence: A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Dino: Detr with improved denoising anchor boxes for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Glipv2: Unifying localization and vision-language understanding</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
