<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyeon</forename><surname>Kim</surname></persName>
							<email>doyeonkim@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Division of Future Vehicle</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonghyun</forename><surname>Ka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pyunghwan</forename><surname>Ahn</surname></persName>
							<email>p.ahn@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Division of Future Vehicle</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Division of Future Vehicle</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewhan</forename><surname>Chun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
							<email>junmo.kim@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Division of Future Vehicle</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset NYU Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalization ability and robustness than other comparative models. The code will be available soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Depth estimation is a challenging area that has been actively researched for many years. In particular, monocular depth estimation, which uses a single image to predict depth, is an illposed problem due to its inherent ambiguity. With the advent of convolutional neural networks (CNNs), many CNN-based approaches have been proposed for depth estimation and have yielded promising results <ref type="bibr" target="#b0">[Bhat et al., 2021;</ref><ref type="bibr" target="#b8">Lee et al., 2019;</ref><ref type="bibr" target="#b4">Fu et al., 2018]</ref>. This paper proposes a new architecture and training strategy to further improve the performance by focusing on the essential properties of monocular depth estimation.</p><p>As many previous papers have claimed <ref type="bibr" target="#b1">[Chen et al., 2019;</ref><ref type="bibr">Kim et al., 2020]</ref>, understanding both global and local contexts is crucial for successful depth estimation. There are many cues in monocular depth estimation that require understanding the scene on a global scale, such as the location of objects or the vanishing point. In addition, local connectivity of features is important because adjacent pixels tend to have similar values owing to their coplanar surfaces. Therefore, we propose a new global-local path network to fully extract meaningful features on diverse scales and effectively deliver them throughout the network. First, we adopt a hierarchical transformer as the encoder to model long-range dependencies and capture multi-scale context features. In prior studies, it is observed that the transformer enables the network to enlarge the size of the receptive field <ref type="bibr" target="#b13">[Xie et al., 2021]</ref>. Motivated by this knowledge, we leverage the global relationships explicitly by building the global path with multiple transformer blocks. Second, we design a highly utilized decoder with an effective fusion module to enable local features to produce a fine depth map while preserving structural details. Contrary to the transformer, skip connections tend to create smaller receptive fields and help to focus on shortdistance information <ref type="bibr" target="#b8">[Luo et al., 2016]</ref>. Thus, the proposed architecture is intended to take the complementary advantages of both transformer and skip connections. This is enabled by aggregating the encoded and the decoded features using an input-dependent fusion module, called selective feature fusion (SFF). The SFF module aids the model to selectively focus on the salient regions by estimating the attention map for both features with a very low computational burden. Compared to other decoders, our decoder achieves superior performance with much lower complexity. Furthermore, we train the network with an additional task specific data augmentation technique to boost the model capability. Data augmentation plays an important role in optimizing the network and can accelerate the model performance without additional computational costs. Nevertheless, data augmentation for depth estimation has been rarely adopted unlike for other tasks. To the best of our knowledge, Cut-Depth <ref type="bibr" target="#b7">[Ishii and Yamashita, 2021]</ref> is the first attempted data augmentation method specifically for depth estimation. We revisit CutDepth with the discovery that the vertical position of an object plays an essential role in monocular depth estimation <ref type="bibr" target="#b2">[Dijk and Croon, 2019]</ref>  <ref type="bibr" target="#b11">[Silberman et al., 2012]</ref> and exhibit the stateof-the-art performance. We validate the model through extensive quantitative and qualitative experiments, and the suggested architecture and data augmentation method demonstrate their effectiveness. In addition, we observe that our network can generalize well under cross-dataset validation and shows robust performance against image corruption.</p><p>To summarize, our contributions are as follows:</p><p>? We propose a novel global-local path architecture for monocular depth estimation.</p><p>? We suggest an improved depth-specific data augmentation method to boost the performance of the model.</p><p>? Our network achieves state-of-the-art performance on the most popular dataset NYU Depth V2 and shows higher generalization ability and robustness than previously developed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Monocular depth estimation is a computer vision task that predicts corresponding depth maps with given input images.</p><p>Learning-based monocular depth estimation has been studied following the seminal work of <ref type="bibr" target="#b10">[Saxena et al., 2008]</ref>   <ref type="bibr">et al., 2017]</ref> adopts a self-attention mechanism with multi-layer perceptron (MLP) to overcome the limitation of previous RNN for natural language processing. Since the emergence of the transformer, it has gained considerable attention in various fields. In the field of computer vision, a vision transformer (ViT) <ref type="bibr" target="#b3">[Dosovitskiy et al., 2020]</ref> first uses a transformer to solve image classification tasks. The success of ViT in the image classification task accelerates the introduction of the transformer into other tasks. SETR <ref type="bibr" target="#b15">[Zheng et al., 2021]</ref> first employs ViT as a backbone and demonstrates the potential of the transformer in dense prediction tasks by achieving new state-of-the-art performance. <ref type="bibr" target="#b13">[Xie et al., 2021]</ref> proposed SegFormer, which is a transformer-based segmentation framework, with a simple lightweight MLP decoder. However, very few attempts have been made to employ a transformer for monocular depth estimation. Adabins <ref type="bibr" target="#b0">[Bhat et al., 2021]</ref> uses a minimized version of a vision transformer (mini-ViT) to calculate bin width in an adaptive manner. DPT <ref type="bibr" target="#b9">[Ranftl et al., 2021]</ref> employs ViT as an encoder to obtain a global receptive field at different stages and attaches a convolutional decoder to make a dense prediction. However, both Adabins and DPT use CNN-based encoders and transformers simultaneously which increases the computational complexity. In addition, DPT is trained with an extra large-scale dataset. In contrast to these studies, our method use only one encoder and does not require an additional dataset to accomplish state-of-the-art performance. Data augmentation plays an important role in preventing overfitting by increasing the effective amount of training data. Therefore, common methods, such as flipping, color space transformation, cropping, and rotation, are used in several tasks to improve the network performance. However, although various methods, such as CutMix <ref type="bibr" target="#b15">[Yun et al., 2019]</ref>, Copy-Paste <ref type="bibr" target="#b6">[Ghiasi et al., 2021]</ref> and CutBlur <ref type="bibr" target="#b15">[Yoo et al., 2020]</ref>, have been actively proposed in diverse tasks, the depth-specific data augmentation method has rarely been studied. To the best of our knowledge, CutDepth <ref type="bibr" target="#b7">[Ishii and Yamashita, 2021]</ref> is the first approach that attempts to augment the data in depth estimation. We accelerate the performance of this depth-specific data augmentation method by emphasizing the vertical location in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Global-Local Path Networks</head><p>Our depth estimation framework aims to predict the depth map? ? R H?W ?1 with a given RGB image I ? R H?W ?3 . Thus, we suggest a new architecture with global and local feature paths through the entire network to generate? . The overall structure of our framework is depicted in <ref type="figure">Figure 1</ref>. Our transformer encoder <ref type="bibr" target="#b13">[Xie et al., 2021]</ref> enables the model to learn global dependencies, and the proposed decoder successfully recovers the extracted feature into the target depth map by constructing the local path through skip connection and the feature fusion module. We detail the proposed architecture in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder</head><p>In the encoding phase, we aim to leverage rich global information from an RGB image. To achieve this, we adopt a hierarchical transformer as the encoder. First, the input image I is embedded as a sequence of patches with a 3 ? 3 convolution operation. Then, the embedded patches are used as an input of the transformer block, which comprises of multiple sets of self-attention and the MLP-Conv-MLP layer with a residual skip. To reduce the computational cost in the selfattention layer, the dimension of each attention head is reduced with ratio R i for each ith block. With a given output, we perform patch merging with overlapped convolution. This process allows us to generate multi-scale features during the encoding phase and can be utilized in the decoding phase. We use four transformer blocks and each block generates 1 4 ,</p><formula xml:id="formula_0">1 8 , 1 16 , 1 32 scale feature with [C 1 , C 2 , C 3 , C 4 ] dimensions.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lightweight Decoder</head><p>The encoder transforms the input image I into the bottleneck feature F 4 E with the size of 1 32 H ? 1 32 W ? C 4 . To obtain the estimated depth map, we construct a lightweight and effective decoder to restore the bottleneck feature into the size of H ? W ? 1. Most of the previous studies have conventionally stacked multiple bilinear upsampling with convolution or deconvolution layers to recover the original size. However, we empirically observe that the model can achieve better performance with much fewer convolution and bilinear upsampling layers of the decoder if we design our restoring path effectively. First, we reduce the channel dimension of the bottleneck feature into N C with 1 ? 1 convolution to avoid computational complexity. Then we use consecutive bilinear upsampling to enlarge the feature into size of H ? W ? N C . Finally, the output is passed through two convolution layers and a sigmoid function to predict depth map H ? W ? 1. And depth map is multiplied with the maximum depth value to scale in meter. This simple decoder can generate as precise a depth map as other baseline structures. However, to further exploit the local structures with fine details, we add skip connection with the proposed fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Selective Feature Fusion</head><p>We propose a Selective Feature Fusion (SFF) module to adaptively select and integrate local and global features by attaining an attention map for each feature. The detailed structure of SFF is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. To match the dimensions of the decoded features F D and F E , we first reduce the dimensions of multi-scale local context features to N C with the convolution layer. Then, these features are concatenated along the channel dimension and passed through two 3 ? 3 Convbatch normalization-ReLU layers. The final convolution and sigmoid layers produce a two-channel attention map, where each local and global feature is multiplied with each channel to focus on the significant location. Then these multiplied features are added element-wise to construct a hybrid feature H D . To strengthen the local continuity we do not reduce the dimension on the 1 4 scale feature. We will verify the effectiveness of the proposed decoder in section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Vertical CutDepth</head><p>Recently, a depth-specific data augmentation method named CutDepth has been proposed <ref type="bibr" target="#b7">[Ishii and Yamashita, 2021]</ref>, which replaces a part of the input image with the groundtruth depth map to provide diversity to the input image and enable the network to focus on the high-frequency area. In CutDepth, the coordinates (l, u) and size (w, h) of the cut region are randomly chosen. However, we believe that the vertical and horizontal directions should not be regarded equally for depth estimation based on the following discovery. A previous study <ref type="bibr" target="#b2">[Dijk and Croon, 2019]</ref> suggested that the depth estimation networks mainly use vertical position in the image rather than apparent size or texture to predict the depth of arbitrary obstacles. This motivates us to propose vertical Cut-Depth, which enhances the original CutDepth by preserving the vertical geometric information. In vertical CutDepth, the ground-truth depth map replaces an area on I with the same location of Y , but the crop is not applied along the vertical direction. Therefore, the coordinate of the replacement region (l, u) and size (w, h) are calculated as follows:</p><formula xml:id="formula_1">(l, u) = (? ? W, 0) (w, h) = (max((W ? ? ? W ) ? ? ? p, 1), H)<label>(1)</label></formula><p>where ? and ? are U(0, 1). p is a hyperparameter that is set at a value of (0, 1]. By maintaining the vertical range of the input RGB image, the network can capture the long-range vertical direction for better prediction, as shown in the results. We set the value of p to 0.75 by performing various settings of p (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training Loss</head><p>In order to calculate the distance between predicted output Y and ground truth depth map Y , we use scale-invariant log scale loss <ref type="bibr">[Eigen et al., 2014</ref>] to train the model. y i * and y i indicates ith pixel in? and Y . The equation of training loss is as follows:</p><formula xml:id="formula_2">L = 1 n i d i 2 ? 1 2n 2 i d i 2<label>(2)</label></formula><p>where d i = log y i ? log y i * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To validate our approach, we perform several experiments on the NYU Depth V2 and SUN RGB-D datasets. We compare our model with existing methods through quantitative and qualitative evaluation, and an ablation study is conducted to show the effectiveness of each contribution. Additionally, we provide other results on additional dataset in supplementary material.  and segmentation maps. We use this dataset for evaluating pre-trained models; thus, only the official test set of 5050 images is used. Image sizes are not constant throughout this dataset, and thus we resize the image to the largest multiple of 32 below the image size, and then pass the resized image to predict the depth map, which is then resized to the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement the proposed network using the PyTorch framework. For training, we use the one-cycle learning rate strategy with an Adam optimizer. The learning rate increases from 3e-5 to 1e-4 following a poly LR schedule with a factor of 0.9 in the first half of the total iteration, and then decreases from 1e-4 to 3e-5 in the last half. The total number of epochs is set to 25 with a batch size of 12. We use pre-trained weights from the MiT-b4 <ref type="bibr" target="#b13">[Xie et al., 2021]</ref> network and initialize our encoder. The values of N C , R i and C i are 64, [8, 4, 2, 1] and [64, 128, 320, 512], respectively. In the case of data augmentation, the following strategies are used with the proposed vertical CutDepth with 50% probability: horizontal flips, random brightness(?0.2), contrast(?0.2), gamma(?20), hue(?20), saturation(?30), and value(?20). We apply p = 0.75 for vertical CutDepth with 25% possibility.  <ref type="table">Table 1</ref> presents the performance comparison of the NYU Depth V2 dataset. DPT <ref type="bibr" target="#b9">[Ranftl et al., 2021]</ref> uses a much larger dataset of 1.4M images for training the model. As listed in the table, the proposed model shows state-of-the-art performance in most of the evaluation metrics which we attribute to the proposed architecture and enhanced depth-specific data augmentation method. Furthermore, our model achieves higher performance than the recently developed state-of-the-art models (Adabins, DPT) with lesser parameters. This suggests that the combination of the trans- former encoder and the proposed compact decoder clearly makes an important contribution to estimate accurate depth maps in an efficient manner. The visualized results are shown in <ref type="figure">Figure 3</ref>. In the figure, our model shows an accurate estimation of depth values for the provided example images and is more robust to various illumination conditions as compared to other methods. SUN RGB-D. We test our network on an additional indoor dataset SUN RGB-D to show the generalization performance.  The network is trained on the NYU Depth V2 dataset and evaluated with a test set of SUN RGB-D without any finetuning process. <ref type="table" target="#tab_4">Table 2</ref> compares the results with those obtained by comparative studies. The proposed approach outperforms competing methods in all metrics. As shown in <ref type="figure">Figure 4</ref>, reasonable result depth maps are generated through our model without additional training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this subsection, we validate the effectiveness of our approach through several experiments conducted on the NYU Depth V2 dataset.</p><p>Comparison with different decoder designs.   study aims to avoid computationally demanding decoders, we construct simple baselines and compare them with ours. Baseline-Dconv consists of consecutive deconvolution-batch normalization-ReLU blocks to obtain the desired depth map. In addition, Baseline-UNet is an improved structure from Baseline-Dconv that has skip connections between the encoder and decoder. As detailed in the table, our decoder achieves better performance than the baselines. Even without an SFF, it already shows better performance than other decoders while having fewer parameters. The powerful encoding ability of our encoder and the effectively designed decoder enables the network to produce a finely detailed depth map. In addition, our proposed SFF leverages additional performance of our model. We additionally provide comparison with existing decoder architectures which integrates multi-scale features, in the bottom part of <ref type="table" target="#tab_6">Table 3</ref>. Despite the compactness of the proposed decoder, our network outperforms other networks. Our decoder has only 0.66M parameters while the MLPdecoder <ref type="bibr" target="#b13">[Xie et al., 2021]</ref>, <ref type="bibr">BTS [Lee et al., 2019]</ref> and DPT <ref type="bibr" target="#b9">[Ranftl et al., 2021]</ref> have 3.19M, 5.79M and 14.15M parameters, respectively, and thus, are highly heavier than ours. This indicates that we have effectively designed the restoring path for our encoder, which enables the proposed model to record fine performance with very few parameters. Effectiveness of the vertical CutDepth. We perform an ablation study on the data augmentation method used to train the network. The results are shown in <ref type="table" target="#tab_9">Table 5</ref>. The first row of the table represents the baseline, which is only trained with traditional data augmentation except for CutDepth, and the second row shows the result obtained from adopting the basic CutDepth method. Then, we apply the proposed vertical CutDepth with different choices of hyperparameter p. As detailed in the table, CutDepth helps the model to achieve slightly better performance than the baseline. However, by applying vertical CutDepth, the network shows further improvement. This proves that utilizing vertical features enhances accurate depth estimation as compared to the case of simply cropping the random area. In addition, the model achieves the best performance with a setting of p = 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Robustness of the model</head><p>In this subsection, we demonstrate the robustness of the proposed method against natural image corruptions. Model robustness for depth estimation is essential because real world images always have a high possibility of being corrupted to a certain degree. Under these circumstances, it is beneficial to design a robust model so that it can perform the given task without being critically corrupted. Following the previous study on the robustness of CNNs <ref type="bibr" target="#b6">[Hendrycks and Dietterich, 2018]</ref>, we test our model on images that are corrupted by 16 different methods. Each corruption is applied with five different intensities, and the performance is averaged over all test images and all five intensities. <ref type="table" target="#tab_8">Table 4</ref> presents the depth estimation results for the corrupted images of the NYU Depth V2 test set. Due to space constraints, we provide the complete table in the supplementary material and present results on a few corruption types in <ref type="table" target="#tab_8">Table 4</ref>. The results show that our model is clearly more robust to all types of corruption than the compared models. The experimental results indicate that our model shows stronger robustness and thus is more appropriate for safety-critical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a new architecture for monocular depth estimation to deliver meaningful global and local features and generate a precisely estimated depth map. We further exploit the depth-specific data augmentation technique to improve the performance of the model by considering the knowledge that the use of vertical position is a crucial property of depth estimation. The proposed method shows improvement over state-of-the-art performance for the NYU Depth V2 dataset. Moreover, extensive experimental results demonstrate the effectiveness and generalization ability of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix: Additional dataset results</head><p>In this section, we provide additional results on <ref type="bibr">KITTI [Geiger et al., 2013]</ref> and iBims-1 <ref type="bibr" target="#b8">[Koch et al., 2018]</ref> datasets. KITTI is an outdoor depth estimation dataset and iBims-1 is an indoor dataset. <ref type="bibr">KITTI [Geiger et al., 2013]</ref> contains stereo camera images and corresponding 3D LiDAR scans of various driving scenes acquired by car mounted sensors. The size of RGB images is around 1224 ? 368. We train our network using approximately 23K images on a random crop of 704 ? 352 and test on 697 images. To compare our performance with previous works, we use the crop as defined by Garg <ref type="bibr" target="#b5">[Garg et al., 2016]</ref> and a maximum value of 80m for evaluation. The results on the KITTI dataset are shown in <ref type="table" target="#tab_11">Table 6</ref>. As shown in the table, our model outperforms other previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">KITTI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Params     </p><formula xml:id="formula_3">(M) ? 1 ? ? 2 ? ? 3 ? AbsRel ? RMSE ? RMSE log ? Fu [Fu</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Detailed description of the SFF module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Qualitative comparison with previous works on the NYU Depth V2 dataset. Examples of estimated depth maps on the SUN RGB-D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>AbsRel ? RMSE ? log 10 ?</figDesc><table><row><cell cols="3">Method ? 3 ? Eigen [Eigen et al., 2014] Params (M) ? 1 ? ? 2 ? 141 0.769 0.950 0.988</cell><cell>0.158</cell><cell>0.641</cell><cell>-</cell></row><row><cell>Fu [Fu et al., 2018]</cell><cell>110</cell><cell>0.828 0.965 0.992</cell><cell>0.115</cell><cell>0.509</cell><cell>0.051</cell></row><row><cell>Yin [Yin et al., 2019]</cell><cell>114</cell><cell>0.875 0.976 0.994</cell><cell>0.108</cell><cell>0.416</cell><cell>0.048</cell></row><row><cell>DAV [Huynh et al., 2020]</cell><cell>25</cell><cell>0.882 0.980 0.996</cell><cell>0.108</cell><cell>0.412</cell><cell>-</cell></row><row><cell>BTS [Lee et al., 2019]</cell><cell>47</cell><cell>0.885 0.978 0.994</cell><cell>0.110</cell><cell>0.392</cell><cell>0.047</cell></row><row><cell>Adabins[Bhat et al., 2021]</cell><cell>78</cell><cell>0.903 0.984 0.997</cell><cell>0.103</cell><cell>0.364</cell><cell>0.044</cell></row><row><cell>DPT* [Ranftl et al., 2021]</cell><cell>123</cell><cell>0.904 0.988 0.998</cell><cell>0.110</cell><cell>0.357</cell><cell>0.045</cell></row><row><cell>Ours</cell><cell>62</cell><cell>0.915 0.988 0.997</cell><cell>0.098</cell><cell>0.344</cell><cell>0.042</cell></row></table><note>Table 1: Performance on the NYU Depth V2 dataset. DPT* is trained with an extra dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b11">Silberman et al., 2012]</ref> contains 640 ? 480 images and corresponding depth maps of various indoor scenes acquired using a Microsoft Kinect camera. We train our network using approximately 24K images on a random crop of 576 ? 448 and test on 654 images. To facilitate a fair comparison, we perform the evaluation on a pre-defined center cropping by Eigen[Eigen et al., 2014]  with a maximum range of 10 m.</figDesc><table><row><cell>4.1 Dataset NYU Depth V2 [Method Yin [Yin et al., 2019] BTS [Lee et al., 2019] Adabins [Bhat et al., 2021] 0.771 0.944 0.983 ? 1 ? ? 2 ? ? 3 ? AbsRel ? RMSE ? log10 ? 0.696 0.912 0.973 0.183 0.541 0.082 0.740 0.933 0.980 0.172 0.515 0.075 0.159 0.476 0.068 Ours 0.814 0.964 0.991 0.144 0.418 0.061</cell></row></table><note>SUN RGB-D [Song et al., 2015] contains approximately 10K RGB-D images of various indoor scenes captured by four different sensors, along with the corresponding depth</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance on the SUN RGB-D dataset with the NYU Depth V2 trained model. We test the model without any fine-tuning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison with multiple decoders. All results in this table are obtained from the same encoder.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>demonstrates the comparison results with different decoder</cell></row><row><cell>design. In this experiment, vertical CutDepth is omitted to</cell></row><row><cell>solely compare the effectiveness of the decoder. As our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Robustness experiment results on corrupted images of NYU Depth V2 datasets. The results of BTS and Adabins are obtained from distributed pre-trained weights.</figDesc><table><row><cell>Method</cell><cell>?1 ?</cell><cell>?2 ?</cell><cell>?3 ?</cell><cell cols="2">AbsRel ? RMSE ? log10 ?</cell></row><row><cell>Baseline</cell><cell cols="3">0.908 0.987 0.997</cell><cell>0.101</cell><cell>0.351</cell><cell>0.043</cell></row><row><cell>+ CutDepth</cell><cell cols="3">0.909 0.986 0.997</cell><cell>0.102</cell><cell>0.348</cell><cell>0.042</cell></row><row><cell>+ Ours (p=0.25)</cell><cell cols="3">0.911 0.988 0.997</cell><cell>0.102</cell><cell>0.354</cell><cell>0.043</cell></row><row><cell>+ Ours (p=0.50)</cell><cell cols="3">0.911 0.988 0.997</cell><cell>0.100</cell><cell>0.348</cell><cell>0.042</cell></row><row><cell cols="4">+ Ours (p=0.75) 0.915 0.988 0.998</cell><cell>0.098</cell><cell>0.343</cell><cell>0.042</cell></row><row><cell>+ Ours (p=1.00)</cell><cell cols="3">0.910 0.988 0.997</cell><cell>0.101</cell><cell>0.351</cell><cell>0.043</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Experimental results with data augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Performance on the KITTI dataset. DPT* is trained with extra dataset.<ref type="bibr" target="#b8">Koch et al., 2018]</ref> (independent Benchmark images and matched scans version 1) is a high quality RGB-D dataset acquired using a digital single-lens reflex (DSLR) camera and high-precision laser scanner. iBims-1 can be characterized by accurate edges and planar regions, consistent depth values, and accurate absolute distances. We evaluate with our NYU Depth V2 trained model without any fine-tuning. Results on iBims-1 dataset are listed inTable 7.Method ? 1 ? ? 2 ? ? 3 ? AbsRel ? RMSE ? log10 ?</figDesc><table><row><cell>6.2 iBims-1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>iBims-1 [VNL [Yin et al., 2019]</cell><cell>0.54 0.84 0.93</cell><cell>0.24</cell><cell>1.06</cell><cell>0.11</cell></row><row><cell>BTS [Lee et al., 2019]</cell><cell>0.53 0.84 0.94</cell><cell>0.24</cell><cell>1.10</cell><cell>0.12</cell></row><row><cell>DORN [Fu et al., 2018]</cell><cell>0.55 0.81 0.92</cell><cell>0.24</cell><cell>1.13</cell><cell>0.12</cell></row><row><cell>AdaBins [Bhat et al., 2021]</cell><cell>0.55 0.86 0.95</cell><cell>0.22</cell><cell>1.07</cell><cell>0.11</cell></row><row><cell cols="2">SharpNet [Ramamonjisoa and Lepetit, 2019] 0.59 0.84 0.94</cell><cell>0.26</cell><cell>1.07</cell><cell>0.11</cell></row><row><cell>ACED [Swami et al., 2020]</cell><cell>0.60 0.87 0.95</cell><cell>0.20</cell><cell>1.03</cell><cell>0.10</cell></row><row><cell>Ours</cell><cell>0.61 0.89 0.96</cell><cell>0.20</cell><cell>1.01</cell><cell>0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Performance on the iBims-1 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>7 Appendix: Robustness of the Model In Table 8, we report a full table of the results on the corrupted NYU Depth V2 dataset. (Section 4.5 of the main paper) AbsRel ? SqRel ? RMSE ? RMSElog ?</figDesc><table><row><cell cols="3">Corruption Type ? 3 ? Clean Method ? 1 ? ? 2 ? BTS 0.885 0.978 0.994 Adabins 0.903 0.984 0.997</cell><cell>0.110 0.103</cell><cell>0.066 0.057</cell><cell>0.392 0.364</cell><cell>0.142 0.130</cell></row><row><cell></cell><cell>Ours</cell><cell>0.915 0.988 0.997</cell><cell>0.098</cell><cell>0.049</cell><cell>0.344</cell><cell>0.124</cell></row><row><cell></cell><cell>BTS</cell><cell>0.223 0.384 0.543</cell><cell>0.435</cell><cell>0.824</cell><cell>1.589</cell><cell>0.743</cell></row><row><cell>Gaussian Noise</cell><cell cols="2">Adabins 0.347 0.553 0.708</cell><cell>0.343</cell><cell>0.578</cell><cell>1.299</cell><cell>0.544</cell></row><row><cell></cell><cell>Ours</cell><cell>0.775 0.940 0.983</cell><cell>0.161</cell><cell>0.126</cell><cell>0.541</cell><cell>0.198</cell></row><row><cell></cell><cell>BTS</cell><cell>0.280 0.448 0.600</cell><cell>0.399</cell><cell>0.736</cell><cell>1.482</cell><cell>0.669</cell></row><row><cell>Shot</cell><cell cols="2">Adabins 0.436 0.653 0.791</cell><cell>0.293</cell><cell>0.460</cell><cell>1.141</cell><cell>0.454</cell></row><row><cell>Noise</cell><cell>Ours</cell><cell>0.791 0.949 0.986</cell><cell>0.152</cell><cell>0.114</cell><cell>0.523</cell><cell>0.189</cell></row><row><cell></cell><cell>BTS</cell><cell>0.116 0.249 0.420</cell><cell>0.504</cell><cell>1.006</cell><cell>1.818</cell><cell>0.875</cell></row><row><cell>Impulse</cell><cell cols="2">Adabins 0.377 0.589 0.736</cell><cell>0.327</cell><cell>0.541</cell><cell>1.246</cell><cell>0.518</cell></row><row><cell></cell><cell>Ours</cell><cell>0.760 0.938 0.984</cell><cell>0.167</cell><cell>0.131</cell><cell>0.556</cell><cell>0.204</cell></row><row><cell></cell><cell>BTS</cell><cell>0.456 0.633 0.756</cell><cell>0.302</cell><cell>0.500</cell><cell>1.159</cell><cell>0.492</cell></row><row><cell>Speckle</cell><cell cols="2">Adabins 0.639 0.834 0.918</cell><cell>0.200</cell><cell>0.244</cell><cell>0.805</cell><cell>0.294</cell></row><row><cell></cell><cell>Ours</cell><cell>0.830 0.965 0.991</cell><cell>0.136</cell><cell>0.091</cell><cell>0.467</cell><cell>0.168</cell></row><row><cell></cell><cell>BTS</cell><cell>0.677 0.850 0.922</cell><cell>0.189</cell><cell>0.207</cell><cell>0.701</cell><cell>0.279</cell></row><row><cell>Motion</cell><cell cols="2">Adabins 0.697 0.859 0.927</cell><cell>0.180</cell><cell>0.182</cell><cell>0.643</cell><cell>0.262</cell></row><row><cell></cell><cell>Ours</cell><cell>0.807 0.946 0.981</cell><cell>0.139</cell><cell>0.103</cell><cell>0.494</cell><cell>0.183</cell></row><row><cell></cell><cell>BTS</cell><cell>0.511 0.684 0.786</cell><cell>0.276</cell><cell>0.415</cell><cell>1.002</cell><cell>0.436</cell></row><row><cell>Defocus</cell><cell cols="2">Adabins 0.599 0.769 0.859</cell><cell>0.227</cell><cell>0.277</cell><cell>0.793</cell><cell>0.341</cell></row><row><cell>Blur</cell><cell>Ours</cell><cell>0.728 0.897 0.954</cell><cell>0.166</cell><cell>0.155</cell><cell>0.605</cell><cell>0.228</cell></row><row><cell></cell><cell>BTS</cell><cell>0.671 0.855 0.927</cell><cell>0.193</cell><cell>0.224</cell><cell>0.747</cell><cell>0.285</cell></row><row><cell>Glass</cell><cell cols="2">Adabins 0.743 0.914 0.967</cell><cell>0.165</cell><cell>0.149</cell><cell>0.619</cell><cell>0.223</cell></row><row><cell></cell><cell>Ours</cell><cell>0.770 0.914 0.978</cell><cell>0.155</cell><cell>0.132</cell><cell>0.573</cell><cell>0.202</cell></row><row><cell></cell><cell>BTS</cell><cell>0.530 0.688 0.779</cell><cell>0.274</cell><cell>0.422</cell><cell>0.989</cell><cell>0.437</cell></row><row><cell>Gaussian</cell><cell cols="2">Adabins 0.595 0.738 0.814</cell><cell>0.244</cell><cell>0.341</cell><cell>0.847</cell><cell>0.379</cell></row><row><cell></cell><cell>Ours</cell><cell>0.716 0.865 0.926</cell><cell>0.177</cell><cell>0.190</cell><cell>0.641</cell><cell>0.248</cell></row><row><cell></cell><cell>BTS</cell><cell>0.842 0.965 0.990</cell><cell>0.124</cell><cell>0.084</cell><cell>0.457</cell><cell>0.166</cell></row><row><cell>Brightness</cell><cell cols="2">Adabins 0.862 0.972 0.994</cell><cell>0.117</cell><cell>0.073</cell><cell>0.427</cell><cell>0.152</cell></row><row><cell></cell><cell>Ours</cell><cell>0.899 0.984 0.997</cell><cell>0.104</cell><cell>0.055</cell><cell>0.369</cell><cell>0.133</cell></row><row><cell></cell><cell>BTS</cell><cell>0.697 0.864 0.932</cell><cell>0.181</cell><cell>0.198</cell><cell>0.689</cell><cell>0.263</cell></row><row><cell>Contrast</cell><cell cols="2">Adabins 0.654 0.836 0.917</cell><cell>0.198</cell><cell>0.234</cell><cell>0.752</cell><cell>0.283</cell></row><row><cell>Digital</cell><cell>Ours</cell><cell>0.860 0.971 0.992</cell><cell>0.117</cell><cell>0.074</cell><cell>0.427</cell><cell>0.152</cell></row><row><cell></cell><cell>BTS</cell><cell>0.814 0.950 0.983</cell><cell>0.135</cell><cell>0.103</cell><cell>0.505</cell><cell>0.182</cell></row><row><cell>Saturation</cell><cell cols="2">Adabins 0.839 0.965 0.991</cell><cell>0.125</cell><cell>0.086</cell><cell>0.465</cell><cell>0.162</cell></row><row><cell></cell><cell>Ours</cell><cell>0.896 0.984 0.996</cell><cell>0.107</cell><cell>0.058</cell><cell>0.374</cell><cell>0.134</cell></row><row><cell></cell><cell>BTS</cell><cell>0.786 0.942 0.983</cell><cell>0.154</cell><cell>0.124</cell><cell>0.532</cell><cell>0.195</cell></row><row><cell>JPEG Compression</cell><cell cols="2">Adabins 0.804 0.954 0.988</cell><cell>0.153</cell><cell>0.115</cell><cell>0.493</cell><cell>0.182</cell></row><row><cell></cell><cell>Ours</cell><cell>0.860 0.973 0.994</cell><cell>0.123</cell><cell>0.073</cell><cell>0.413</cell><cell>0.153</cell></row><row><cell></cell><cell>BTS</cell><cell>0.410 0.649 0.803</cell><cell>0.298</cell><cell>0.423</cell><cell>1.114</cell><cell>0.458</cell></row><row><cell>Snow</cell><cell cols="2">Adabins 0.410 0.656 0.817</cell><cell>0.292</cell><cell>0.410</cell><cell>1.094</cell><cell>0.440</cell></row><row><cell></cell><cell>Ours</cell><cell>0.723 0.926 0.981</cell><cell>0.170</cell><cell>0.138</cell><cell>0.598</cell><cell>0.217</cell></row><row><cell></cell><cell>BTS</cell><cell>0.705 0.878 0.945</cell><cell>0.176</cell><cell>0.168</cell><cell>0.642</cell><cell>0.250</cell></row><row><cell>Spatter</cell><cell cols="2">Adabins 0.699 0.890 0.964</cell><cell>0.173</cell><cell>0.155</cell><cell>0.625</cell><cell>0.234</cell></row><row><cell>Weather</cell><cell>Ours</cell><cell>0.835 0.971 0.994</cell><cell>0.134</cell><cell>0.083</cell><cell>0.445</cell><cell>0.162</cell></row><row><cell></cell><cell>BTS</cell><cell>0.588 0.798 0.893</cell><cell>0.227</cell><cell>0.273</cell><cell>0.835</cell><cell>0.332</cell></row><row><cell>Fog</cell><cell cols="2">Adabins 0.523 0.748 0.873</cell><cell>0.252</cell><cell>0.308</cell><cell>0.898</cell><cell>0.357</cell></row><row><cell></cell><cell>Ours</cell><cell>0.759 0.928 0.978</cell><cell>0.153</cell><cell>0.125</cell><cell>0.559</cell><cell>0.204</cell></row><row><cell></cell><cell>BTS</cell><cell>0.515 0.734 0.850</cell><cell>0.261</cell><cell>0.359</cell><cell>0.996</cell><cell>0.400</cell></row><row><cell>Frost</cell><cell cols="2">Adabins 0.439 0.691 0.842</cell><cell>0.280</cell><cell>0.398</cell><cell>1.074</cell><cell>0.413</cell></row><row><cell></cell><cell>Ours</cell><cell>0.736 0.929 0.983</cell><cell>0.163</cell><cell>0.130</cell><cell>0.576</cell><cell>0.209</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Robustness experiment results on corrupted images of NYU Depth V2 datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We illustrate the detailed structure of Baseline-DConv and Baseline-UNet in <ref type="figure">Figure 5</ref>. We use transposed convolution with K = 3, S = 2, P = 1 parameters to upscale the given feature into 2x size. For Baseline-UNet, features from encoder F 3 E , F 2 E , F 1 E are concatenated in channel dimension.</p><p>(a)  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4009" to="4018" />
		</imprint>
	</monogr>
	<note>Adabins: Depth estimation using adaptive bins</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06023</idno>
		<title level="m">Structure-aware residual pyramid network for monocular depth estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How do neural networks see depth in single images?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Tom Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2183" to="2191" />
		</imprint>
	</monogr>
	<note>Dijk and Croon</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<publisher>Christian Puhrsch, and Rob Fergus</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Depth map prediction from a single image using a multi-scale deep network</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<editor>Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun</editor>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
		</imprint>
	</monogr>
	<note>Vision meets robotics: The kitti dataset</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ghiasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<editor>Huynh et al., 2020] Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Esa Rahtu, and Janne Heikkil?</editor>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="581" to="597" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging contextual information for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayoshi</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yamashita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07684</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<editor>Kim et al., 2020] Doyeon Kim, Sihaeng Lee, Janghyeon Lee, and Junmo Kim</editor>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="147808" to="147817" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cutdepth: Edge-aware data augmentation in depth estimation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sharpnet: Fast and accurate recovery of occluding contours in monocular depth estimation</title>
		<idno type="arXiv">arXiv:1907.10326</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ren? Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ranftl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13413</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
	<note>Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aced: Accurate and edge-consistent monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<editor>Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ?ukasz Kaiser, and Illia Polosukhin</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking data augmentation for image superresolution: A comprehensive analysis and a new strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1905" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
