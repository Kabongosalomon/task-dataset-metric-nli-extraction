<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Light-weight and Real-time Line Segment Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NAVER/LINE Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NAVER/LINE Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seounghyun</forename><surname>Go</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NAVER/LINE Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Hyun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NAVER/LINE Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingeun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NAVER/LINE Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minchul</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NAVER/LINE Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Light-weight and Real-time Line Segment Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>To the best of our knowledge, this is the first real-time deep LSD available on mobile devices. Our code is available 1 .</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous deep learning-based line segment detection (LSD)   suffers from the immense model size and high computational cost for line prediction. This constrains them from real-time inference on computationally restricted environments. In this paper, we propose a real-time and light-weight line segment detector for resource-constrained environments named Mobile LSD (M-LSD). We design an extremely efficient LSD architecture by minimizing the backbone network and removing the typical multi-module process for line prediction found in previous methods. To maintain competitive performance with a light-weight network, we present novel training schemes: Segments of Line segment (SoL) augmentation, matching and geometric loss. SoL augmentation splits a line segment into multiple subparts, which are used to provide auxiliary line data during the training process. Moreover, the matching and geometric loss allow a model to capture additional geometric cues. Compared with TP-LSD-Lite, previously the best real-time LSD method, our model (M-LSDtiny) achieves competitive performance with 2.5% of model size and an increase of 130.5% in inference speed on GPU. Furthermore, our model runs at 56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Line segments and junctions are crucial visual features in low-level vision, which provide fundamental information to the higher level vision tasks, such as pose estimation <ref type="bibr" target="#b18">(P?ibyl, Zem??k, and?ad?k 2017;</ref><ref type="bibr" target="#b27">Xu et al. 2016)</ref>, structure from motion <ref type="bibr" target="#b1">(Bartoli and Sturm 2005;</ref><ref type="bibr" target="#b17">Micusik and Wildenauer 2017)</ref>, 3D reconstruction <ref type="bibr" target="#b3">(Denis, Elder, and Estrada 2008;</ref><ref type="bibr" target="#b4">Faugeras et al. 1992)</ref>, image matching <ref type="bibr" target="#b31">(Xue et al. 2017</ref>), wireframe to image translation <ref type="bibr" target="#b32">(Xue, Zhou, and Huang 2019)</ref> and image rectification <ref type="bibr" target="#b33">(Xue et al. 2019b</ref>). Moreover, the growing demand for performing such vision tasks on resource constraint platforms, like mobile or embedded devices, has made real-time line segment detection (LSD) an essential but challenging task. The difficulty arises from  <ref type="table" target="#tab_2">Table 2.</ref> the limited computational power and model size when finding the best accuracy and resource-efficiency trade-offs to achieve real-time inference.</p><p>With the advent of deep neural networks, deep learningbased LSD architectures have adopted models to learn various geometric cues of line segments and have proved to show improvements in performance. As described in <ref type="figure">Figure 2</ref>, we have summarized multiple strategies that use deep learning models for LSD. The top-down strategy <ref type="bibr" target="#b29">(Xue et al. 2019a</ref>) first detects regions of line segment with attraction field maps and then squeezes these regions into line segments to make predictions. In contrast, the bottom-up strategy first detects junctions, then arranges them into line segments, and lastly verifies the line segments by using an extra classifier <ref type="bibr" target="#b34">Zhang et al. 2019)</ref> or a merging algorithm . Recently,  proposes Tri-Points (TP) representation for a simpler process of line prediction without the time-consuming steps of line proposal and verification.</p><p>Although previous efforts of using deep networks have made remarkable achievements, real-time inference for LSD on resource-constraint platforms still remains limited. There <ref type="figure">Figure 2</ref>: (a) Previous LSD methods exploit multi-module processing for line segment prediction. In contrast, our method directly predicts line segments from feature maps with a single module. (b) Our method shows superior speed on backbone and line prediction by employing a light-weight network with a single module of line prediction.</p><p>have been attempts to present real-time LSD ), but they still depend on server-class GPUs. This is mainly because the models that are used exploit heavy backbone networks, such as dilated ResNet50-based FPN , stacked hourglass network , and atrous residual U-net <ref type="bibr" target="#b29">(Xue et al. 2019a)</ref>, which require large memory and high computational power. In addition, as shown in <ref type="figure">Figure 2</ref>, the line prediction process consists of multiple modules, which include line proposal <ref type="bibr" target="#b29">(Xue et al. 2019a;</ref><ref type="bibr" target="#b34">Zhang et al. 2019;</ref>, line verification networks ) and mixture of convolution module . As the size of the model and the number of modules for line prediction increase, the overall inference speed of LSD can become slower, as shown in <ref type="figure">Figure 2b</ref>, while demanding higher computation. Thus, increases in computational cost make it difficult to deploy LSD on resource-constraint platforms.</p><p>In this paper, we propose a real-time and light-weight LSD for resource-constrained environments, named Mobile LSD (M-LSD). For the network, we design a significantly efficient architecture with a single module to predict line segments. By minimizing the network size and removing the multi-module process from previous methods, M-LSD is extremely light and fast. To maintain competitive performance even with a light-weight network, we present novel training schemes: SoL augmentation, matching and geometric loss. SoL augmentation divides a line segment into subparts, which are further used to provide augmented line data during the training phase. Matching and geometric loss train a model with additional geometric information, including relation between line segments, junction and line segmentation, length and degree regression. As a result, our model is able to capture extra geometric information during training to make more accurate line predictions. Moreover, the proposed training schemes can be used with existing methods to further improve performance in a plug-and-play manner.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our methods achieve competitive performance and faster inference speed with a much smaller model size. M-LSD outperforms previously the real-time method, TP-LSD-Lite , with only 6.3% of the model size but gaining an increase of 32.5% in inference speed. Moreover, M-LSD-tiny runs in real-time at 56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices, respectively. To the best of our knowledge, this is the first real-time LSD method available on mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Deep Line Segment Detection. There have been active studies on deep learning-based LSD. In junction-based methods, DWP  includes two parallel branches to predict line and junction heatmaps, followed by a merging process. PPGNet ) and L-CNN  utilize junction-based line segment representations with an extra classifier to verify whether a pair of points belongs to the same line segment. Another approach uses dense prediction. AFM <ref type="bibr" target="#b29">(Xue et al. 2019a</ref>) predicts attraction field maps that contain 2-D projection vectors representing associated line segments, followed by a squeeze module to recover line segments. HAWP ) is presented as a hybrid model of AFM and L-CNN. Recently, ) devises the TP line representation to remove the use of extra classifiers or heuristic post-processing found in previous methods and proposes TP-LSD network with two branches: TP extraction and line segmentation branches. Other approaches include the use of transformers  or Hough transform with deep networks <ref type="bibr" target="#b13">(Lin, Pintea, and van Gemert 2020)</ref>. However, it is commonly observed that the aforementioned multi-module processes restrict existing LSD to run on resource-constrained environments.</p><p>Real-time Object Detectors. Real-time object detection has been an important task for deep learning-based object detection. Object detectors proposed in earlier days, such as RCNN-series <ref type="bibr" target="#b6">(Girshick et al. 2014;</ref><ref type="bibr" target="#b5">Girshick 2015;</ref><ref type="bibr" target="#b22">Ren et al. 2015)</ref>, consist of two-stage architecture: generating proposals in the first stage, then classifying the proposals in the second stage. These two-stage detectors typically suffer from slow inference speed and difficulty in optimization. To handle this problem, one-stage detectors, such as YOLO-series <ref type="bibr" target="#b19">(Redmon et al. 2016;</ref><ref type="bibr">Farhadi 2017, 2018)</ref> and SSD <ref type="bibr" target="#b14">(Liu et al. 2016)</ref>, are proposed to achieve GPU real-time inference by reducing backbone size and simplifying the two-stage process into one. This onestage architecture has been further studied and improved to run in real-time on mobile devices <ref type="bibr" target="#b7">(Howard et al. 2017;</ref><ref type="bibr" target="#b26">Wang, Li, and Ling 2018;</ref><ref type="bibr" target="#b12">Li et al. 2018)</ref>. Motivated by the transition from two-stage to one-stage architecture in object detection, we argue that the complicated multi-module processing in previous LSD can be disregarded. We simplify the line prediction process with a single module for faster inference speed and enhance the performance by the efficient training strategies; SoL augmentation, matching and geometric loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">M-LSD for Line Segment Detection</head><p>In this section, we present the details of M-LSD. Our design mainly focuses on efficiency while retaining competitive performance. Firstly, we exploit a light-weight backbone and reduce the modules involved in processing line predictions for better efficiency. Next, we apply additional training schemes, including SoL augmentation, matching and geometric loss, to capture extra geometric cues. As a result, M-LSD is able to balance the trade-off between accuracy and efficiency to be well suited for mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>We design light (M-LSD) and lighter (M-LSD-tiny) models as popular encoder-decoder architectures. In efforts to build a light-weight LSD model, our encoder networks are based on MobileNetV2  which is well-known to run in real-time on mobile environments. The encoder network uses parts of MobileNetV2 to make it even lighter. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, the encoder of M-LSD includes an input to 96-channel of bottleneck blocks. The number of parameters in the encoder network is 0.56M (16.5% of Mo-bileNetV2), while the total parameters of MobileNetV2 are 3.4M. For M-LSD-tiny, a slightly smaller yet faster model, the encoder network also uses parts of MobileNetV2, including an input to 64-channel of bottleneck blocks which results in a number of 0.25M (7.4% of MobileNetV2). The decoder network is designed using a combination of block types A, B, and C. The expansive path consists of concatenation of feature maps from the skip connection and upscale from block type A, followed by two 3 ? 3 convolutions with a residual connection in-between from block type B. Similarly, block type C performs two 3 ? 3 convolutions, the first being a dilated convolution, followed by a 1 ? 1 convolution. Please refer to the supplementary material for further details on the network architectures. As shown in <ref type="figure">Figure 2b</ref>, we observe that one of the most critical bottlenecks in inference speed has been the prediction process, which contains multi-module processing from previous methods. In this paper, we argue that the complicated multi-module can be disregarded. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, we generate line segments directly from the final feature maps in a single module process. In the final feature maps, each feature map channel serves its own purpose: 1) TP maps have seven feature maps, including one length map, one degree map, one center map, and four displacement maps. 2) SoL maps have seven feature maps with the same configuration as TP maps. 3) Segmentation maps have two feature maps, including junction and line maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Line Segment Representation</head><p>Line segment representation determines how line segment predictions are generated and ultimately affects the ef-  <ref type="figure">Figure 4</ref>: Tri-Points (TP) representation and Segments of Line segment (SoL) augmentation. l s , l c , and l e denote start, center, and end points, respectively. d s and d e are displacement vectors to start and end points. l 0 ? l 2 indicates internally dividing points of the line segment l s l e . ficiency of LSD. Hence, we employ the TP representation ) which has been introduced to have a simple line generation process and shown to perform realtime LSD using GPUs. TP representation uses three keypoints to depict a line segment: start, center, and end points. As illustrated in <ref type="figure" target="#fig_7">Figure 4a</ref>, the start l s and end l e points are represented by using two displacement vectors (d s , d e ) with respect to the center l c point. The line generation process, which is to convert center point and displacement vectors to a vectorized line segment, is performed as:</p><formula xml:id="formula_0">(x ls , y ls ) = (x lc , y lc ) + d s (x lc , y lc ), (x le , y le ) = (x lc , y lc ) + d e (x lc , y lc ),<label>(1)</label></formula><p>where (x ? , y ? ) denotes coordinates of an arbitrary ? point. d s (x lc , y lc ) and d e (x lc , y lc ) indicate 2D displacements from the center point l c to the corresponding start l s and end l e points. The center point and displacement vectors are trained with one center map and four displacement maps (one for each x and y value of the displacement vectors d s and d e ).</p><p>In the line generation process, we extract the exact center point position by applying non-maximum suppression on the center map. Next, we generate line segments with the extracted center points and the corresponding displacement vectors using a simple arithmetic operation as expressed in Equation 1; thus, making inference efficient and fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Matching Loss</head><p>Following , we use the weighted binary cross-entropy (WBCE) loss and smooth L1 loss as center loss L center and displacement loss L disp , which are for training the center and displacement map, respectively. The line segments under the TP representation are decoupled into center points and displacement vectors, which are optimized separately. However, the coupled information of the line segment is under-utilized in the objective functions.</p><p>To resolve this problem, we present a matching loss, which leverages the coupled information w.r.t. the ground truth. As illustrated in <ref type="figure" target="#fig_3">Figure 5a</ref>, matching loss considers relation between line segments by guiding the generated line segments to be similar to the matched GT. We first take the endpoints of each prediction, which can be calculated via the line generation process, and measure the Euclidean distance d(?) to the endpoints of the GT. Next, these distances are used to match predicted line segmentsl with GT line segments l that are under a threshold ?:</p><formula xml:id="formula_1">d(l s ,l s ) &lt; ? and d(l e ,l e ) &lt; ?,<label>(2)</label></formula><p>where l s and l e are the start and end points of the line l, and ? is set to 5 pixels. Then, we obtain a set M of matched line segments (l,l) that satisfies this condition. Finally, the L1 loss is used for the matching loss, which aims to minimize the geometric distance of the matched line segments w.r.t the start, end, and center points as follows:</p><formula xml:id="formula_2">L match = 1 | M | (l,l)?M l s ?l s 1 + l e ?l e 1 + C (l) ? (l s + l e )/2 1 ,<label>(3)</label></formula><p>whereC(l) is the center point of linel from the center map. The total loss function for the TP map can be formulated as</p><formula xml:id="formula_3">L T P = L center + L disp + L match .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SoL Augmentation</head><p>We propose Segments of Line segment (SoL) augmentation that increases the number of line segments with wider varieties of length for training. Learning line segments with center points and displacement vectors can be insufficient in certain circumstances where a line segment may be too long to manage within the receptive field size or the center points of two distinct line segments may be too close to each other. To address these issues and provide auxiliary information to the TP representation, SoL explicitly splits line segments into multiple subparts with overlapping portions of each other. An overlap between each split is enforced to preserve connectivity among the subparts. As described in <ref type="figure">Figure 4b</ref>, we compute k internally dividing points (l 0 , l 1 , ? ? ? , l k ) and separate the line segment l s l e into k subparts (l s l 1 , l 0 l 2 , ? ? ? , l k?1 l e ). Expressed in TP representation, each subpart is trained as if it is a typical line segment. The number of internally dividing points  <ref type="table" target="#tab_8">Table 1</ref>: Ablation study of M-LSD-tiny on Wireframe. The baseline is M-LSD-tiny trained with only TP representation. M denotes model number.</p><p>k is determined by the length of the line segment as k = r(l)/(?/2) ? 1, where r(l) denotes the length of line segment l, and ? is the base length of subparts. Note that when k ? 1, we do not split the line segment. The resulting length of each subpart can be similar to ? with small margins of error due to the rounding function ? , and we empirically set ? = input size ? 0.125. The loss function of L SoL follows the same configuration as L T P , while each subpart is treated as an individual line segment. Note that the line generation process is only done in TP maps, not in SoL maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning with Geometric Information</head><p>To boost the quality of predictions, we incorporate various geometric information about line segments which helps the overall learning process. In this section, we present learning LSD with junction and line segmentation, and length and degree regression for additional geometric information.</p><p>Junction and Line Segmentation Center point and displacement vectors are highly related to pixel-wise junctions and line segments in the segmentation maps of <ref type="figure" target="#fig_1">Figure 3</ref>. For example, end points, derived from the center point and displacement vectors, should be the junction points. Also, center points must be localized on the pixel-wise line segment. Thus, learning the segmentation maps of junctions and line segments works as a spatial attention cue for LSD. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, M-LSD contains segmentation maps, including a junction map and a line map. We construct the junction GT map by scaling with Gaussian kernel as the center map, while using a binary map for line GT map. The total segmentation loss is defined as L seg = L junc +L line , where we use WBCE loss for both L junc and L line .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Length and Degree Regression</head><p>As displacement vectors can be derived from the length and degree of line segments, they can be additional geometric cues to support the displacement maps. We compute the length and degree from the ground truth and mark the values on the center of line segments in each GT map. Next, these values are extrapolated to a 3 ? 3 window so that all neighboring pixels of a given pixel contain the same value. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we maintain predicted length and degree maps for both TP and SoL maps, where TP uses the original line segment and SoL uses augmented subparts. As the ranges of length and degree are wide, we divide each length by the diagonal length of the input image for normalization. For degree, we divide each degree by 2? and add 0.5. The total regression loss can be formulated as L reg = L length + L degree , where we use smooth L1 loss for both L length and L degree . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Final Loss Functions</head><p>The geometric loss function is defined as the sum of segmentation and regression loss:</p><formula xml:id="formula_4">L Geo = L seg + L reg .<label>(4)</label></formula><p>The loss function for SoL maps L SoL follows the same formulation as L T P but with SoL augmented GT. Finally, we obtain the final loss function to train M-LSD as follows:</p><formula xml:id="formula_5">L total = L T P + L SoL + L Geo .<label>(5)</label></formula><p>Please refer to the supplementary material for further details on the feature maps and losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct extensive ablation studies, quantitative and qualitative analysis of the proposed method. For better understanding, we add extended experiments in the supplementary material, including ablation study of architecture, SoL augmentation, application example and so on.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study and Interpretability</head><p>We conduct a series of ablation experiments to analyze our proposed method. M-LSD-tiny is trained and tested on the Wireframe dataset with an input size of 512 ? 512. As shown in <ref type="table" target="#tab_8">Table 1</ref>, all the proposed schemes contribute to a significant performance improvement. In addition, we include saliency map visualizations generated from each feature map to analyze networks learned from each training scheme in <ref type="figure">Figure 6</ref> using GradCam <ref type="bibr" target="#b24">(Selvaraju et al. 2017</ref>). The saliency map interprets important regions and importance levels on the input image by computing the gradients from each feature map. Matching Loss. Integrating matching loss shows performance boosts on both pixel localization accuracy and line prediction quality. We observe weak attention on center points from the baseline saliency maps in <ref type="figure" target="#fig_7">Figure 6a</ref>, while w/ matching loss amplifies the attention on center points in <ref type="figure">Figure 6b</ref>. This demonstrates that training with coupled information of center points and displacement vectors allows the model to learn with more line-awareness features.</p><p>Geometric Loss. Adding geometric loss gives performance boosts in every metric. Moreover, the saliency map of <ref type="figure">Figure 6c</ref> shows more distinct and stronger attention on cen-ter points and line segments as compared to that of saliency maps w/ matching loss in <ref type="figure">Figure 6b</ref>. It shows that geometric information work as spatial attention cues for training.</p><p>SoL Augmentation. Integrating SoL augmentation shows significant performance boost. In the saliency maps of <ref type="figure">Figure 6c</ref>, w/ geometric loss shows strong but vague attention on center points with disconnected line attention for long line segments. This can be a problem because the entire line information is essential to compute the center point. In contrast, w/ SoL augmentation in <ref type="figure" target="#fig_10">Figure 6d</ref> shows more precise center point attention as well as clearly connected line attention. This demonstrates that augmenting line segments by the number and length guides the model to be more robust in pixel-based and line matching-based qualities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Other Methods</head><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, we conduct experiments that combine the proposed training schemes (SoL augmentation, matching and geometric loss) with existing methods. Finally, we compare our proposed M-LSD and M-LSD-tiny with the previous state-of-the-art methods.</p><p>Existing methods with M-LSD Training Schemes. As our proposed training schemes can be used with existing LSD methods, we demonstrate this using L-CNN and HAWP following Deep Hough Transform (HT) (Lin, Pintea, and van Gemert 2020), a recently proposed combinable method. L-CNN + HT (HT-L-CNN) shows a performance boost of 1.4% while L-CNN + M-LSD-s shows a boost of 0.9% in sAP 10 . HAWP + HT (HT-HAWP) shows 0.1% of performance boost, while HAWP + M-LSD-s shows 0.6% of performance boost in sAP 10 , which makes the combination one of the state-of-the-art performance. Thus, it demonstrates that the proposed training schemes are flexible and powerful to use with existing LSD methods.</p><p>M-LSD and M-LSD-tiny. Our proposed models achieve competitive performance and the fastest inference speed even with a limited model size. In comparison with the previous fastest model, TP-LSD-Lite, M-LSD with input size of 512 shows higher performance and an increase of 32.5% in inference speed with only 6.3% of the model size. Our fastest model, M-LSD-tiny with 320 input size, has a slightly lower performance than that of TP-LSD-Lite, but achieves an increase of 130.5% in inference speed with only 2.5% of the model size. Compared to the previous lightest model TP-LSD-HG, M-LSD with 512 input size outperforms on sAP 5 , sAP 10 and LAP with an increase of 136.0% in inference speed with 20.3% of the model size. Our lightest model, M-LSD-tiny with 320 input size, shows an increase of 310.6% in the inference speed with 8.1% of the model size compared to TP-LSD-HG. Previous methods can be deployed as real-time line segment detectors on server-class GPUs, but not on resource-constrained environments either because the model size is too large or the inference speed is too slow. Although M-LSD does not achieve state-of-theart performance, it shows competitive performance and the fastest inference speed with the smallest model size, offering the potential to be used in real-time applications on resourceconstrained environments, such as mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>We visualize outputs of M-LSD and M-LSD-tiny in <ref type="figure" target="#fig_5">Figure 7</ref>. Junctions and line segments are colored with cyan blue and orange, respectively. Compared to the GT, both models are capable of identifying junctions and line segments with high precision even in complicated low contrast environments such as (a) and (c). Although the results of M-LSD-tiny may have a few small line segments missing and junctions incorrectly connected, the fundamental line segments to identify the environmental structure are accurate.</p><p>The goal of our model is to detect the structural line segments as  while avoiding texture and photometric line segments. However, we observe that some are included in our results, such as texture on the floor in (b) and  shadow on the wall in (d). We acknowledge this to be a common problem for existing methods, and considering texture and photometric features for training would be great future work. We include more visualizations with a comparison of existing methods in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Deployment on Mobile Devices</head><p>We deploy M-LSD on mobile devices and evaluate the memory usage and inference speed. We use iPhone 12 Pro with A14 bionic chipset and Galaxy S20 Ultra with Snapdragon 865 ARM chipset. As shown in <ref type="table" target="#tab_4">Table 3</ref>, M-LSD-tiny and M-LSD are small enough to be deployed on mobile devices where memory requirements range between 78MB and 508MB. The inference speed of M-LSD-tiny is fast enough to be real-time on mobile devices where it ranges from a minimum of 17.9 FPS to a maximum of 56.8 FPS. M-LSD still can be real-time with 320 input size, however, with 512 input size, FP16 may be required for a faster FPS over 10.</p><p>Overall, as all our models have small memory requirements and fast inference speed on mobile devices, the exceptional efficiency allows M-LSD variants to be used in real-world applications. To the best of our knowledge, this is the first and the fastest real-time line segment detector on mobile devices ever reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce M-LSD, a light-weight and real-time line segment detector for resource-constrained environments. Our model is designed with a significantly efficient network architecture and a single module process to predict line segments. To maintain competitive performance even with a light-weight network, we present novel training schemes: SoL augmentation, matching and geometric loss. As a result, our proposed method achieves competitive performance and the fastest inference speed with the lightest model size.</p><p>Moreover, we show that M-LSD is deployable on mobile devices in real-time, which demonstrates the potential to be used in real-time mobile applications.   first uses the probabilistic Hough method to identify optimal lines; then, localize the line segments that generated the peak in the Hough map. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Feature Maps and Losses</head><p>In , the weighted binary cross-entropy (WBCE) loss is used to train the center map. However, we observe that the number of positive (foreground) pixels is much less than that of negative (background) pixels, and such foreground-background class imbalance degrades the performance of the WBCE loss. This is because the majority of pixels are easy negatives that contribute no useful learning signals. Thus, we separate positive and negative terms of the binary cross-entropy loss to have the same scale, and reformulate a separate binary classification loss as follows: <ref type="figure">?(F (p)</ref>)), (ii) cls (F ) = ? pos ? pos (F ) + ? neg ? neg (F ), (iii) where I(p) outputs 1 if the pixel p of the GT map is nonzero, otherwise 0, ? denotes a sigmoid function, and W (p) and F (p) are pixel values in the GT and feature map, respectively. For the GT of the center map, positions of the center point are marked on a zero map, which is then scaled using a Gaussian kernel with 5 stdev, truncated by a 3 ? 3 window. We use the center loss as L center = cls (C), where C denotes the center map and weights (? pos , ? neg ) set to <ref type="bibr">(1,</ref><ref type="bibr">30)</ref>.</p><formula xml:id="formula_6">pos (F ) = ?1 p I(p) p W (p) ? log?(F (p)), (i) neg (F ) = ?1 p 1?I(p) p (1 ? I(p)) ? log(1 ?</formula><p>For the displacement maps, we compute displacement vectors from the ground truth (GT) and mark those values on   the center of line segment in the GT map. Next, these values are extrapolated to a 3?3 window (center blob) so that all neighboring pixels of a given pixel contain the same value. For the displacement, length, and degree maps, we use the smooth L1 loss for regression learning. The regression loss can be formulated as follows:</p><formula xml:id="formula_7">reg (F ) = 1 p H(p) p H(p) ? L smooth 1 (F (p),F (p)), (iv)</formula><p>where F (p) andF (p) denote values of pixel p in the feature map F and the GT mapF , and H(p) outputs 1 if the pixel p of the GT map is on the center blob (extrapolated 3?3 window). We use the displacement loss L disp = reg (D), where D denotes the displacement map. The length and degree losses are L length = reg (?(L)) and L degree = reg (?(G)), where ?(L) and ?(G) are sigmoid functions ? applied to length and degree maps. Note that only the GT points and its neighboring pixels in 3 ? 3 window are used for the loss computation. In the line generation process, the center map is applied with a sigmoid function to output a probability value, while the displacement map uses the original values. Then, we extract the exact center point position by non-maximum suppression ) on the center map to remove duplicates around correct predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Usage of Final Feature Maps</head><p>In the training phase, M-LSD and M-LSD-tiny outputs final feature maps of 16 channels, which include 7 channels for TP maps, 7 channels for SoL maps, and 2 channels for segmentation maps as illustrated in <ref type="figure" target="#fig_7">Figure Ba</ref>. However, as the line generation process only requires the center and displacement maps of TP maps, operations for the other auxiliary maps are unnecessary in the inference phase. Thus, we disregard these operations and output only 5 channels of TP maps in the inference phase, including 1 center map and 4 displacement maps, as shown in <ref type="figure">Figure Bb</ref>. As a result, we can minimize computational cost and maximize the inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Extended Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Ablation Study of Architecture</head><p>We run a series of ablation experiments to investigate various encoder and decoder architectures. As shown in Table Ba, we vary the parts used from the MobileNetV2 on the encoder architecture. As the encoder size increases, we add block types A and B to the decoder structure by following the structural format in <ref type="table" target="#tab_8">Table Aa</ref>. Model 1 ? 3 exploit bigger and deeper encoder architectures, which result in larger model parameters and slower inference speed. The performance turns out to be slightly higher than that of M-LSD. However, we choose 'Input ? 96-channel' of Mo-bileNetV2 as the encoder for M-LSD because increasing the encoder size causes larger amounts of model parameters to be used and decreases the inference speed with a negligible performance boost. Therefore, we observe that 'Input ? 96-channel' is the largest model that can run on a mobile device in real-time. In contrast, when performing real-time LSD on GPUs, model 1 ? 3 are good candidates as they outperform TP-LSD-Lite , previously the best real-time LSD, with faster inference speed and lighter model size.</p><p>In <ref type="table" target="#tab_8">Table Bb</ref>, we vary the block types used in the decoder architecture. Model 4 changes every 1 ? 1 convolution to a 3 ? 3 convolution in block type A, while model 5 changes the residual connection from being in between the convolutions <ref type="bibr">('pre-residual')</ref> to the end of the convolutions ('postresidual') for block type B. These changes result in an in-   <ref type="table" target="#tab_8">Table D</ref>: Impact of ratio in SoL augmentation with M-LSD-tiny on Wireframe dataset. = 0.0 is the baseline with no SoL augmentation applied. The base length of subpart ? is computed by ? = input size ? . '# origin', '# aug', and '# total' denote the number of original, augmented, and total line segments.</p><p>crease in model size and a decrease in inference speed because 'post-residual' requires twice the number of output channels than that of 'pre-residual'. However, the performance remains similar to that of M-LSD-tiny. For models 6 and 7, the dilated rate of the first convolution in block type C is changed to 1 and 3, respectively. Here we observe that by decreasing the dilated rate can improve the inference speed but conversely decrease the performance. This is because the dilated convolution can effectively manage long line segments, which require large receptive fields. Thus, we choose to use 1 ? 1 convolution in block type A, 'pre-residual' in block type B, and the dilated rate of 5 in block type C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Needs of Offset Maps</head><p>In some of the previous LSD methods , offset maps are used to estimate offsets between the predicted map and input image because the predicted map has a smaller resolution than the input image. We perform experiments and evaluate the effectiveness of offset maps with M-LSD-tiny. When we apply offset maps to M-LSD-tiny, we need two offset maps for the center point (one for each coordinate). As shown in <ref type="table" target="#tab_8">Table C</ref>, w/ offset maps increase in model parameters and decrease in inference speed, while the performance does not change. This demonstrates that offset maps are unnecessary for M-LSD-tiny because the resolution of the input image is two times the size of the resolution of predicted maps, which is minor. Thus, we disregard offset maps in M-LSD architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Impact of SoL Augmentation</head><p>In SoL augmentation, the number of internally dividing points k is based on the length of the line segment and computed as k = r(l)/(?/2) ?1, where r(l) denotes the length  of line segment l, and ? is the base length of the subparts. Note that when k ? 1, we do not split the line segment. When dividing the line segment, the base length of subparts ? is determined by ? = input size ? . We conduct an experiment to investigate the impact of ratio in <ref type="table" target="#tab_8">Table D</ref>. Small ratio will split line segments into a shorter length while producing a greater number of subparts, and vice versa when using a large ratio . As shown in <ref type="table" target="#tab_8">Table D</ref>, although a small ratio produces a large number of augmented line segments, performance improvement is small. This is because the center and end points of small subparts are too close to each other to be distinguished, and thus become distractions for the model. Using a large ratio also shows small performance improvement because not only does the amount of augmented line segments decrease, but also these subparts result to resemble the original line segment. We observe the proper ratio is 0.125, which produces enough number of augmented line segments with different lengths and location from the originals.</p><p>When applying SoL augmentation, we split line segments into multiple subparts with overlapping portions with each other. To see the impact of retaining such overlap in SoL augmentation, we conduct an experiment as shown in Table E. W/o overlap shows a smaller performance boost than that of w/ overlap. Hence we conclude that using a larger number of augmented lines and preserving connectivity among subparts with overlaps can yield higher performance than without overlaps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Threshold of Matching Loss</head><p>In the matching loss, the threshold ? decides whether to match the predicted and GT line segments. When ? is small, the matching condition becomes strict, where the predicted line would be matched only with a highly similar GT line. When ? is large, the matching condition becomes lenient, where the predicted line would be easily matched with the GT line even if it is not similar. We conduct an experiment to see the impact of the threshold ? in matching loss. As shown in <ref type="table" target="#tab_8">Table F</ref>, when the threshold is high (? ? 10.0), the matching condition is too broad, and poses a higher chance of predicted lines matching with non-similar GT lines. This becomes a distraction and shows performance degradation. On the other hand, when the threshold is too low (? = 2.5), the matching condition is strict and consequently restrains the effect of the matching loss to be minor due to the small number of matched lines. We observe that a value around 5.0 is the proper threshold ?, which provides the optimal balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Ablation Study of Geometric Loss</head><p>We conduct extended ablation experiments to analyze how each geometric information contributes to the model performance in <ref type="table" target="#tab_8">Table G</ref>. Moreover, we include saliency map visualizations, which are generated from each feature map of geometric information as illustrated in <ref type="figure">Figure C</ref>. Line and Junction Segmentation. In <ref type="table" target="#tab_8">Table Ga</ref>, adding line and junction segmentation gives performance boosts in the following metrics: 0.8 in F H , 1.5 in sAP 10 and 2.1 in LAP . Moreover, the junction and line attention on saliency maps of <ref type="figure" target="#fig_7">Figure Ca</ref> and Cb are precise, which shows that junction and line segmentations work as spatial attention cues for LSD.</p><p>Length and Degree Regression. In <ref type="table" target="#tab_8">Table Ga,</ref>   Then, box candidates are computed from post-processing as (c), and finally we obtain box detection by a ranking process as (d).</p><p>prediction quality improves 1.4 in sAP 10 and 0.7 in LAP by adding length and degree regression, while the pixel localization accuracy F H remains the same. The length saliency map in <ref type="figure">Figure Cc</ref> contains highlights on the entire line, and the degree saliency map in <ref type="figure" target="#fig_10">Figure Cd</ref> has highlights on the center points. We speculate that computing length needs the entire line information whereas computing the degree only needs parts of the line. Overall, learning with additional geometric information of line segments, such as length and degree, further increases the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of Each Training Scheme</head><p>We conduct an additional ablation study by adding each training scheme to the baseline separately in <ref type="table" target="#tab_8">Table Gb</ref>. The proposed training schemes in order of highest performance boost is matching loss, SoL augmentation and geometric loss. Overall, every training scheme gives a significant performance boost to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 HAWP Line Segment Representation</head><p>We conduct an experiment using HAWP  line segment representation with our M-LSD backbones and training schemes. As shown in <ref type="table" target="#tab_8">Table H</ref>   FPS is low due to the complexity of the HAWP line segment representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Applications</head><p>As line segments are fundamental low-level visual features, there are various real-world applications that use LSD. We show an example with real-time box detection on a mobile device as described in <ref type="figure" target="#fig_10">Figure D</ref>. We implement a box detector on a mobile device by using the M-LSD-tiny model. Since the application consists of line detection and postprocessing, a model for the line detection has to be light and fast enough for real-time usage, when M-LSD-tiny is playing a sufficient role. The potential of real-time LSD on a mobile device can further be extended to other real-world applications like a book scanner, wireframe to image translation, and SLAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 Precision and Recall Curve</head><p>We include Precision-Recall (PR) curves of sAP 10 for L-CNN , HAWP ), TP-LSD , and M-LSD (ours). <ref type="figure">Figure E</ref> shows comparisons of PR curves on Wireframe and YorkUrban datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.9 Visualization</head><p>We include more visualization results on Wireframe and YorkUrban datasets in <ref type="figure">Figure F</ref>  , and ground-truth. We use an input size of 512 for every method except that 320 is used for AFM, and the image is resized with the shortest side at least 1100 pixels for LETR. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of M-LSD and existing LSD methods on Wireframe dataset. Inference speed (FPS) is computed on Tesla V100 GPU. Size and value of circles indicate the number of model parameters (Millions). M-LSD achieves competitive performance with the lightest model size and the fastest inference speed. Details are in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The overall architecture of M-LSD. In the feature extractor, block 1 ? 14 are parts of MobileNetV2, and block 15 ? 23 are designed as a top-down architecture. The predicted line segments are generated with center and displacement maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Matching and geometric loss. (a) Given a matched pair of a predicted linel and a GT line l, matching loss (L match ) optimizes the predicted start, end, and center points. (b) Given a line segment, M-LSD learns various geometric cues: junction (L junc ) and line (L line ) segmentation, length (L length ) and degree (L degree ) regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Baseline (M1) (b) w/ matching loss (M2) (c) w/ geometric loss (M3) (d) w/ SoL augmentation (M4)Figure 6: Saliency maps generated from TP center map. Model numbers (M1?4) are fromTable 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative evaluation of M-LSD-tiny and M-LSD on WireFrame dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>A: Architecture details of M-LSD and M-LSD-tiny. Each line describes a sequence of 1 or repeating n identical layers where each layer in the same sequence has the same c output channels. Block numbers ('Block') and block type A?C in 'Operator' are fromFigure 3andFigure A. 'SC input' denotes a skip connection input and the bottleneck operation is from MobileNetV2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A :</head><label>A</label><figDesc>The overall architecture of M-LSD-tiny. In the feature extractor, block 1 ? 11 are parts of MobileNetV2, and block 12 ? 16 are designed as a top-down architecture. The final feature maps are simply generated by upscale. The predicted line segments are generated by merging center points and displacement vectors from the TP maps.(a) Final feature maps in the training phase (b) Final feature maps in the inference phase Figure B: Final feature maps in the training and inference phase. (a) In the training phase, the final feature maps include TP, SoL, and segmentation maps with a total of 16 channels. (b) For better efficiency in the inference phase, we disregard unnecessary convolutions and maintain only the center and displacement maps in the TP maps with a total of 5 channels.B Details of M-LSD B.1 Network ArchitectureThe detailed architecture of M-LSD and M-LSD-tiny is described inTable A. M-LSD includes an encoder structure from MobileNetV2 in block 1?14 and designed decoder structure in block 15?final. M-LSDtiny also includes an encoder structure from MobileNetV2 in block 1?11 and a custom decoder structure in block 12?final, which is illustrated inFigure A. The final feature maps in M-LSD-tiny are generated by upscaling with H/2? W/2 ? 16 tensors when the input image is H ? W ? 3. For the upscale operation, we use bilinear interpolation. On the other hand, M-LSD uses the feature map from block type C as a final feature map with the same size of H/2?W/2?16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(b) Ablation study by varying block types for the decoder architecture. Performance is reported on Wireframe dataset with M-LSD-tiny as the baseline. Block type A ? B are from Figure 3 and Figure A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>the line (a) Junction segmentation map (b) Line segmentation map (c) TP length regression map (d) TP degree regression map Figure C: Saliency maps generated from each feature map. M-LSD-tiny (M7 in Table Ga) model is used for generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure D :</head><label>D</label><figDesc>Real-time box detection using M-LSD-tiny on a mobile device. Given an image as input to the mobile device as (a), line segments are detected using M-LSD-tiny as (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>H: M-LSD with HAWP line segment representation (Rep.). Precision-Recall (PR) curves of sAP 10 on Wireframe and YorkUrban datasets. (320) and (512) denote input image size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>. We compare our M-LSD model with AFM (Xue et al. 2019), L-CNN (Zhou, Qi, and Ma 2019), HAWP (Xue et al. 2020), TP-LSD-Res34 (Huang et al. 2020), LETR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>of line segment detection methods. The columns are the results from AFM, LCNN, HAWP, TP-LSD-Res34, LETR, M-LSD (ours), and ground-truth. The top four rows are the results from Wireframe test set and bottom four rows are the results from YorkUrban test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>SoL augmentation 77.2 (+1.0) 58.0 (+2.9) 57.9 (+2.6)</figDesc><table><row><cell>M Schemes</cell><cell>F H</cell><cell>sAP 10</cell><cell>LAP</cell></row><row><cell>1 Baseline 2 + Matching loss 3 + Geometric loss 4 +</cell><cell cols="3">74.3 75.4 (+1.1) 52.2 (+3.3) 52.5 (+4.4) 48.9 48.1 76.2 (+0.8) 55.1 (+2.9) 55.3 (+2.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Dataset and Evaluation Metrics. We evaluate our model with two famous LSD datasets: Wireframe and YorkUrban<ref type="bibr" target="#b3">(Denis, Elder, and Estrada 2008)</ref>. The Wireframe dataset consists of 5,000 training and 462 test images of man-made environments, while the YorkUrban dataset has 102 test images. Following the typical training and test protocol, we train our model with the training set from the Wireframe dataset and test with both Wireframe and YorkUrban datasets. We evaluate our models using prevalent metrics for LSD<ref type="bibr" target="#b34">Zhang et al. 2019;</ref><ref type="bibr" target="#b29">Xue et al. 2019a;</ref> that include: heatmap-based metric F H , structural average precision (sAP), and line matching average precision (LAP).Optimization. We train our model on Tesla V100 GPU. We use the TensorFlow<ref type="bibr" target="#b0">(Abadi et al. 2016</ref>) framework for model training and TFLite 2 for porting models to mobile</figDesc><table><row><cell>Methods</cell><cell>Input</cell><cell>F H</cell><cell cols="3">Wireframe sAP 5 sAP 10 LAP F H</cell><cell cols="3">YorkUrban sAP 5 sAP 10 LAP</cell><cell>Params(M)</cell><cell>FPS</cell></row><row><cell>LSD (Von Gioi et al. 2008) DWP (Huang et al. 2018) AFM (Xue et al. 2019a) LGNN (Meng et al. 2020) LGNN-lite (Meng et al. 2020) TP-LSD-Lite (Huang et al. 2020) TP-LSD-Res34 (Huang et al. 2020) TP-LSD-Res34 (Huang et al. 2020) TP-LSD-HG (Huang et al. 2020) LETR (Xu et al. 2021) L-CNN (Zhou, Qi, and Ma 2019) HAWP (Xue et al. 2020) HT-L-CNN (Lin, Pintea, and van Gemert 2020) HT-HAWP (Lin, Pintea, and van Gemert 2020)</cell><cell cols="3">320 512 320 512 512 320 320 512 512 1100  *  82.6 59.2 64.1 6.7 72.7 3.7 77.3 18.3 ----80.4 56.4 81.6 57.5 80.6 57.6 82.0 50.9 512 77.5 58.9 512 80.3 62.5 512 -60.3 512 -62.9</cell><cell>8.8 5.1 23.9 62.3 57.6 59.7 60.6 57.2 57.0 65.6 62.8 66.5 64.2 66.6</cell><cell cols="2">18.7 60.6 6.6 65.2 36.7 66.3 ----59.7 68.1 24.8 7.5 2.8 7.0 --60.6 67.4 25.3 61.3 67.2 27.6 55.1 67.3 18.9 65.1 66.6 24.0 59.8 64.6 25.9 62.9 64.8 26.1 --25.7 --25.0</cell><cell>9.2 2.6 9.1 --26.8 27.4 27.7 22.0 27.6 28.2 28.5 28.0 27.4</cell><cell>16.1 3.1 17.5 --31.2 31.1 34.3 24.6 32.5 32.0 30.4 --</cell><cell>-33.0 43.0 --23.9 23.9 23.9 7.4 121.2 9.8 10.4 9.3 10.5</cell><cell>100.0  ? 2.2 14.1 15.8  ? 34.0  ? 87.1 45.8 20.0 48.9 5.4 16.6 32.9 7.5  ? 12.2  ?</cell></row><row><cell>L-CNN + M-LSD-s HAWP + M-LSD-s M-LSD-tiny M-LSD-tiny M-LSD M-LSD</cell><cell>512 512 320 512 320 512</cell><cell cols="2">80.7 59.4 82.5 63.3 76.8 43.0 77.2 52.3 78.7 48.2 80.0 56.4</cell><cell>63.7 67.1 51.3 58.0 55.5 62.1</cell><cell cols="2">63.8 66.5 27.5 64.2 66.7 27.5 50.1 61.9 17.4 57.9 62.4 22.1 55.7 63.4 20.2 61.5 64.2 24.6</cell><cell>28.1 28.5 21.3 25.0 23.9 27.3</cell><cell>31.7 32.4 23.7 28.3 27.7 30.7</cell><cell>9.8 10.4 0.6 0.6 1.5 1.5</cell><cell>16.6 32.9 200.8 164.1 138.2 115.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparisons with existing LSD methods. FPS is evaluated in Tesla V100 GPU, where ? denotes CPU FPS and ? denotes the values from the corresponding paper due to no published or incomplete implementation. * denotes resizing the image with the shortest side at least 1100 pixels. M-LSD-s indicates the proposed training schemes. The best scores among previous methods, our models, and all together are marked in blue, red, and bold, respectively.devices. Input images are resized to 320 ? 320 or 512 ? 512 in both training and testing, which are specified in each experiment. The input augmentation consists of horizontal and vertical flips, shearing, rotation, and scaling. We use ImageNet<ref type="bibr" target="#b2">(Deng et al. 2009</ref>) pre-trained weights on the parts of</figDesc><table /><note>MobileNetV2 (Sandler et al. 2018) in M-LSD and M-LSD-tiny. Our model is trained using the Adam opti- mizer (Kingma and Ba 2014) with a learning rate of 0.01. We use linear learning rate warm-up for 5 epochs and cosine learning rate decay (Loshchilov and Hutter 2016) from 70 epoch to 150 epoch. We train the model for a total of 150 epochs with a batch size of 64.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Inference speed and memory usage on iPhone (A14 Bionic chipset) and Android phone (Snapdragon 865 chipset). FP denotes floating point.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Network Architecture . . . . . . . . . . . . 2 B.2 Feature Maps and Losses . . . . . . . . . . 2 B.3 Usage of Final Feature Maps . . . . . . . . 3 Ablation Study of Architecture . . . . . . . 3 C.2 Needs of Offset Maps . . . . . . . . . . . . 4 C.3 Impact of SoL Augmentation . . . . . . . . 4 C.4 Threshold of Matching Loss . . . . . . . . 5 C.5 Ablation Study of Geometric Loss . . . . . 5 C.6 HAWP Line Segment Representation . . . . 5 C.7 Applications . . . . . . . . . . . . . . . . . 6 C.8 Precision and Recall Curve . . . . . . . . . 6 C.9 Visualization . . . . . . . . . . . . . . . .</figDesc><table><row><cell cols="3">Supplementary Material</cell></row><row><cell cols="3">github.com/navervision/mlsd</cell></row><row><cell>Contents A Additional Related Works B Details of M-LSD B.1 C Extended Experiments C.1 6 1 2 3 References 8</cell><cell cols="2">Block 1 2 3?4 5?7 8?11 12?14 H/16?W/16?64 Input H?W?3 H/2?W/2?32 H/2?W/2?16 H/4?W/4?24 H/8?W/8?32 15 H/16?W/16?96 H/16?W/16?64 block type A 128 1 SC input Operator c n -conv2d 32 1 -bottleneck 16 1 -bottleneck 24 2 -bottleneck 32 3 -bottleneck 64 4 -bottleneck 96 3 16 H/16?W/16?128 -block type B 64 1 17 H/16?W/16?64 H/8?W/8?32 block type A 128 1 18 H/8?W/8?128 -block type B 64 1 19 H/8?W/8?64 H/4?W/4?24 block type A 128 1 20 H/4?W/4?128 -block type B 64 1 21 H/4?W/4?64 H/2?W/2?16 block type A 128 1 22 H/2?W/2?128 -block type B 64 1 23 H/2?W/2?64 -block type C 16 1 Final H/2?W/2?16 ----(a) M-LSD Block Input SC input Operator c n 1 H?W?3 -conv2d 32 1 2 H/2?W/2?32 -bottleneck 16 1 3?4 H/2?W/2?16 -bottleneck 24 2 5?7 H/4?W/4?24 -bottleneck 32 3 8?11 H/8?W/8?32 -bottleneck 64 4</cell></row><row><cell>A Additional Related Works Hand-crafted Feature-based Methods. For a long pe-riod of time, hand-crafted low-level features, especially line gradients, have been used for LSD. These conven-tional approaches can be categorized into edge map based and perceptual grouping methods. Edge map based meth-has been an attempt to merge both approaches. The method challenge in these methods. In (Almazan et al. 2017), there priate threshold to discriminate true line segments remains a into line segment candidates. However, choosing an appro-exploit the image gradients as geometry cues to group pixels Yuille, and Lee 2017; Burns, Hanson, and Riseman 1986) Perceptual grouping methods (Von Gioi et al. 2008; Cho, identify endpoints of the line segment (Elder et al. 2017). line predictions. A key challenge of these methods is to image to a parameter map by Hough transform to sort out and Klette 2014) convert the pixel-wise feature map of an nagawa 2003; Matas, Galambos, and Kittler 2000; Xu, Shin, ods (Kamat-Sadekar and Ganesan 1998; Furukawa and Shi-</cell><cell>12 13 14 15 16 -Final</cell><cell>H/16?W/16?64 H/8?W/8?32 block type A 128 1 H/8?W/8?128 -block type B 64 1 H/8?W/8?64 H/4?W/4?24 block type A 64 1 H/4?W/4?64 -block type B 64 1 H/4?W/4?64 -block type C 16 1 H/4?W/4?16 -upscale 16 1 H/2?W/2?16 ----(b) M-LSD-tiny</cell></row></table><note>Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table B :</head><label>B</label><figDesc>Ablation study on encoder and decoder architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table C :</head><label>C</label><figDesc>Experiments of w/o and w/ offset maps in M-LSDtiny on Wireframe dataset.</figDesc><table><row><cell>?</cell><cell># origin</cell><cell># aug</cell><cell># total</cell><cell cols="2">F H sAP 10 LAP</cell></row><row><cell cols="5">0.000 0.050 25.6 -0.100 51.2 0.125 64.0 0.150 76.8 0.200 102.4 374884 374884 374884 851555 1226439 76.2 0 374884 76.2 374884 251952 626836 76.4 374884 151804 526688 77.2 374884 102719 477603 77.0 47500 422384 76.6 0.300 153.6 374884 12123 387007 76.6 0.400 204.8 374884 3250 378134 76.4 0.500 256.0 374884 170 375054 76.2</cell><cell>55.1 56.2 57.2 58.0 57.5 56.8 56.1 55.5 55.0</cell><cell>55.3 56.3 57.3 57.9 57.9 56.5 56.7 56.1 55.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table E :</head><label>E</label><figDesc>Impact of overlapping in SoL augmentation with M-LSD-tiny on Wireframe dataset. The baseline is not trained with SoL augmentation. '# origin', '# aug', and '# total' denote the number of original, augmented, and total line segments.</figDesc><table><row><cell>?</cell><cell cols="5">Input size 320 F H sAP 10 LAP F H sAP 10 LAP Input size 512</cell></row><row><cell cols="2">0.0 2.5 5.0 7.5 10.0 75.0 75.9 76.2 76.8 76.0 12.5 74.1 15.0 74.2 20.0 73.6</cell><cell>47.1 50.4 51.3 49.0 45.1 43.1 42.7 41.4</cell><cell>44.9 76.1 48.9 76.5 50.1 77.2 48.5 76.8 45.0 76.8 43.2 76.2 42.8 75.7 42.1 75.1</cell><cell>55.1 57.2 58.0 58.5 57.8 56.7 54.0 51.0</cell><cell>54.8 57.2 57.9 57.2 56.7 55.8 53.2 50.6</cell></row><row><cell cols="6">Table F: Impact of matching loss threshold ? with M-LSD-tiny on Wireframe dataset. ? = 0.0 is the baseline with no matching loss applied.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table G</head><label>G</label><figDesc></figDesc><table><row><cell>: Extended ablation study of M-LSD-tiny on Wire-frame. The baseline is trained with M-LSD-tiny backbone including only TP representation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>, both M-LSD backbones and training schemes work well with HAWP line segment representation and produce competitive performance. However, the model parameters are relatively larger and the F H sAP 10 LAP F H sAP 10 LAP</figDesc><table><row><cell cols="2">Setup Rep. MLSD-s M-LSD-tiny HAWP Backbone</cell><cell>69.9</cell><cell>Wireframe 61.5</cell><cell>58.8 57.8</cell><cell>York 26.0</cell><cell>28.6</cell><cell>Params (M) 4.0</cell><cell>FPS 47.3</cell></row><row><cell cols="2">M-LSD-tiny HAWP M-LSD HAWP</cell><cell>75.1 73.0</cell><cell>63.0 64.0</cell><cell>60.2 58.8 60.5 60.3</cell><cell>27.1 28.3</cell><cell>28.4 30.2</cell><cell>4.0 5.0</cell><cell>47.3 38.4</cell></row><row><cell>M-LSD</cell><cell>HAWP</cell><cell>77.5</cell><cell>65.7</cell><cell>61.1 60.8</cell><cell>28.4</cell><cell>30.5</cell><cell>5.0</cell><cell>38.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">www.tensorflow.org/lite</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Structure-from-motion using lines: Representation, triangulation, and bundle adjustment. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="416" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The depth and motion analysis machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Image Processing</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="143" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wireframe parsing with guidance of distance map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="141036" to="141044" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05505</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">TP-LSD: Tri-Points Based Line Segment Detector. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11013</idno>
		<title level="m">Tiny-DSOD: Lightweight object detection for resource-restricted usages</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep hough-transform line priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="323" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LGNN: A Context-aware Line Segment Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4364" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structure from motion with line segments under relaxed endpoint constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wildenauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="79" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Absolute pose estimation from line correspondences using direct linear transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?ibyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zem??k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And?ad?k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="130" to="144" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LSD: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06882</idno>
		<title level="m">Pelee: A real-time object detection system on mobile devices</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pose estimation from line correspondences: A complete analysis and a series of solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1209" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line segment detection using transformers without edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4257" to="4266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1595" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Holistically-attracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2788" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anisotropic-scale junction detection and matching for indoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="91" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03840</idno>
		<title level="m">Neural Wireframe Renderer: Learning Wireframe to Image Translations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to calibrate straight lines for fisheye image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1643" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ppgnet: Learning pointpair graph for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="962" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mcmlsd: A dynamic programming approach to line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2031" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Extracting straight lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="425" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A novel lineletbased representation for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1195" to="1208" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MCMLSD: A Probabilistic Algorithm and Evaluation Framework for Line Segment Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accurate and robust line segment extraction by analyzing distribution around peaks in Hough space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shinagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05505</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">TP-LSD: Tri-Points Based Line Segment Detector. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Complete description of multiple line segments using the Hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kamat-Sadekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="597" to="613" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust detection of lines using the progressive probabilistic hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="137" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">LGNN: A Context-aware Line Segment Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4364" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">LSD: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Line segment detection using transformers without edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4257" to="4266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Accurate and robust line segment extraction using minimum entropy with Hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="822" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1595" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Holistically-attracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2788" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="962" to="971" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
