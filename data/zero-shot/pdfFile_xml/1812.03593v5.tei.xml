<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SDNET: CONTEXTUALIZED ATTENTION-BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN- SWERING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<email>chezhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Microsoft Speech and Dialogue Research Group</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
							<email>nzeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Microsoft Speech and Dialogue Research Group</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Microsoft Speech and Dialogue Research Group</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SDNET: CONTEXTUALIZED ATTENTION-BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN- SWERING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational question answering (CQA) is a novel QA task that requires understanding of dialogue context. Different from traditional single-turn machine reading comprehension (MRC) tasks, CQA includes passage comprehension, coreference resolution, and contextual understanding. In this paper, we propose an innovated contextualized attention-based deep neural network, SDNet, to fuse context into traditional MRC models. Our model leverages both inter-attention and self-attention to comprehend conversation context and extract relevant information from passage. Furthermore, we demonstrated a novel method to integrate the latest BERT contextual model. Empirical results show the effectiveness of our model, which sets the new state of the art result in CoQA leaderboard, outperforming the previous best model by 1.6% F 1 . Our ensemble model further improves the result by 2.7% F 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Traditional machine reading comprehension (MRC) tasks share the single-turn setting of answering a single question related to a passage. There is usually no connection between different questions and answers to the same passage. However, the most natural way humans seek answers is via conversation, which carries over context through the dialogue flow.</p><p>To incorporate conversation into reading comprehension, recently there are several public datasets that evaluate QA model's efficacy in conversational setting, such as CoQA <ref type="bibr" target="#b10">(Reddy et al., 2018)</ref>, QuAC  and QBLink <ref type="bibr" target="#b3">(Elgohary et al., 2018)</ref>. In these datasets, to generate correct responses, models are required to fully understand the given passage as well as the context of previous questions and answers. Thus, traditional neural MRC models are not suitable to be directly applied to this scenario. Existing approaches to conversational QA tasks include BiDAF++ <ref type="bibr" target="#b1">(Yatskar, 2018)</ref>, <ref type="bibr">FlowQA (Huang et al., 2018)</ref>, DrQA+PGNet <ref type="bibr" target="#b10">(Reddy et al., 2018)</ref>, which all try to find the optimal answer span given the passage and dialogue history.</p><p>In this paper, we propose SDNet, a contextual attention-based deep neural network for the task of conversational question answering. Our network stems from machine reading comprehension models, but has several unique characteristics to tackle contextual understanding during conversation. Firstly, we apply both inter-attention and self-attention on passage and question to obtain a more effective understanding of the passage and dialogue history. Secondly, SDNet leverages the latest breakthrough in NLP: BERT contextual embedding <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>. Different from the canonical way of appending a thin layer after BERT structure according to <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>, we innovatively employed a weighted sum of BERT layer outputs, with locked BERT parameters. Thirdly, we prepend previous rounds of questions and answers to the current question to incorporate contextual information. Empirical results show that each of these components has substantial gains in prediction accuracy.</p><p>We evaluated SDNet on CoQA dataset, which improves the previous state-of-the-art model's result by 1.6% (from 75.0% to 76.6%) overall F 1 score. The ensemble model further increase the F 1 score to 79.3%. Moreover, SDNet is the first model ever to pass 80% on CoQA's in-domain dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">APPROACH</head><p>In this section, we propose the neural model, SDNet, for the conversational question answering task, which is formulated as follows. Given a passage C, and history question and answer utterances Q 1 , A 1 , Q 2 , A 2 , ..., Q k?1 , A k?1 , the task is to generate response A k given the latest question Q k . The response is dependent on both the passage and history utterances.</p><p>To incorporate conversation history into response generation, we employ the idea from DrQA+PGNet <ref type="bibr" target="#b10">(Reddy et al., 2018)</ref> to prepend the latest N rounds of utterances to the current question Q k . The problem is then converted into a machine reading comprehension task. In other words, the reformulate question is Q k = {Q k?N ; A k?N ; ..., Q k?1 ; A k?1 ; Q k }. To differentiate between question and answering, we add symbol Q before each question and A before each answer in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MODEL OVERVIEW</head><p>Encoding layer encodes each token in passage and question into a fixed-length vector, which includes both word embeddings and contextualized embeddings. For contextualized embedding, we utilize the latest result from BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>. Different from previous work, we fix the parameters in BERT model and use the linear combination of embeddings from different layers in BERT.</p><p>Integration layer uses multi-layer recurrent neural networks (RNN) to capture contextual information within passage and question. To characterize the relationship between passage and question, we conduct word-level attention from question to passage both before and after the RNNs. We employ the idea of history-of-word from FusionNet <ref type="bibr" target="#b5">(Huang et al., 2017)</ref> to reduce the dimension of output hidden vectors. Furthermore, we conduct self-attention to extract relationship between words at different positions of context and question.</p><p>Output layer computes the final answer span. It uses attention to condense the question into a fixedlength vector, which is then used in a bilinear projection to obtain the probability that the answer should start and end at each position.</p><p>An illustration of our model SDNet is in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ENCODING LAYER</head><p>We use 300-dim GloVe <ref type="bibr" target="#b9">(Pennington et al., 2014)</ref> embedding and contextualized embedding for each word in context and question. We employ BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> as contextualized embedding. Instead of adding a scoring layer to BERT structure as proposed in <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>, we use the transformer output from BERT as contextualized embedding in our encoding layer. BERT generates L layers of hidden states for all BPE tokens <ref type="bibr" target="#b12">(Sennrich et al., 2015)</ref> in a sentence/passage and we employ a weighted sum of these hidden states to obtain contextualized embedding. Furthermore, we lock BERT's internal weights, setting their gradients to zero. In ablation studies, we will show that this weighted sum and weight-locking mechanism can significantly boost the model's performance.</p><p>In detail, suppose a word w is tokenized to s BPE tokens w = {b 1 , b 2 , ..., b s }, and BERT generates L hidden states for each BPE token, h l t , 1 ? l ? L, 1 ? t ? s. The contextual embedding BERT w for word w is then a per-layer weighted sum of average BERT embedding, with weights ? 1 , ..., ? L .</p><formula xml:id="formula_0">BERT w = L l=1 ? l s t=1 h l t s 2.3 INTEGRATION LAYER</formula><p>Word-level Inter-Attention. We conduct attention from question to context (passage) based on GloVe word embeddings. Suppose the context word embeddings are {h C 1 , ..., h C m } ? R d , and the question word embeddings are {h Q 1 , ..., h Q n } ? R d . Then the attended vectors from question to context are {? C 1 , ...,? C m }, defined as,</p><formula xml:id="formula_1">S ij = ReLU(U h C i )D ReLU(U h Q j ), ? ij ? exp(S ij ), h C i = j ? ij h Q j ,</formula><p>where D ? R k?k is a diagonal matrix and U ? R d?k , k is the attention hidden size.</p><p>To simplify notation, we define the attention function above as Attn(A, B, C), meaning we compute the attention score ? ij based on two sets of vectors A and B, and use that to linearly combine vector set C. So the word-level attention above can be simplified as</p><formula xml:id="formula_2">Attn({h C i } m i=1 , {h Q i } n i=1 }, {h Q i } n i=1 })</formula><p>. For each context word in C, we also include a feature vector f w including 12-dim POS embedding, 8-dim NER embedding, a 3-dim exact matching vector em i indicating whether each context word appears in the question, and a normalized term frequency, following the approach in DrQA .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Therefore, the input vector for each context word isw</head><formula xml:id="formula_3">C i = [GloVe(w C i ); BERT w C i ;? C i ; f w C i ]; the input vector for each question word isw Q i = [GloVe(w Q i ); BERT w Q i ].</formula><p>RNN. In this component, we use two separate bidirectional RNNs (BiLSTMs <ref type="bibr" target="#b4">(Hochreiter &amp; Schmidhuber, 1997)</ref>) to form the contextualized understanding for C and Q.</p><formula xml:id="formula_4">h C,k 1 , ..., h C,k m = BiLSTM (h C,k?1 1 , ..., h C,k?1 m ), h Q,k 1 , ..., h Q,k n = BiLSTM (h Q,k?1 1 , ..., h Q,k?1 n ), h C,0 i =w C i , h Q,0 i =w Q i ,</formula><p>where 1 ? k ? K and K is the number of RNN layers. We use variational dropout <ref type="bibr" target="#b8">(Kingma et al., 2015)</ref> for input vector to each layer of RNN, i.e. the dropout mask is shared over different timesteps.</p><p>Question Understanding. For each question word in Q, we employ one more layer of RNN to generate a higher level of understanding of the question.</p><formula xml:id="formula_5">h Q,K+1 1 , ..., h Q,K+1 n = BiLSTM (h Q 1 , ..., h Q n ), h Q i = [h Q,1 i ; ...; h Q,K i ]</formula><p>Self-Attention on Question. As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution. We thus employ self-attention on question. The formula is the same as word-level attention, except that we are attending a question to itself:</p><formula xml:id="formula_6">{u Q i } n i=1 = Attn({h Q,K+1 i } n i=1 , {h Q,K+1 i } n i=1 , {h Q,K+1 i } n i=1 ). The final question representa- tion is thus {u Q i } n i=1</formula><p>. Multilevel Inter-Attention. After multiple layers of RNN extract different levels of understanding of each word, we conduct multilevel attention from question to context based on all layers of generated representations.</p><p>However, the aggregated dimensions can be very large, which is computationally inefficient. We thus leverage the history-of-word idea from FusionNet <ref type="bibr" target="#b5">(Huang et al., 2017)</ref>: we use all previous levels to compute attentions scores, but only linearly combine RNN outputs.</p><p>In detail, we conduct K + 1 times of multilevel attention from each RNN layer output of question to context.</p><formula xml:id="formula_7">{m (k),C i } m i=1 = Attn({HoW C i } m i=1 , {HoW Q i } n i=1 , {h Q,k i } n i=1 ), 1 ? k ? K + 1</formula><p>where history-of-word vectors are defined as</p><formula xml:id="formula_8">HoW C i = [GloVe(w C i ); BERT w C i ; h C,1 i ; ..., h C,k i ], HoW Q i = [GloVe(w Q i ); BERT w Q i ; h Q,1 i ; ..., h Q,k i</formula><p>].</p><p>An additional RNN layer is applied to obtain the contextualized representation v C i for each word in C.</p><p>y</p><formula xml:id="formula_9">C i = [h C,1 i ; ...; h C,k i ; m (1),C i ; ...; m (K+1),C i ], v C 1 , ..., v C m = BiLSTM (y C 1 , ..., y C n ),</formula><p>Self Attention on Context. Similar to questions, we conduct self attention on context to establish direct correlations between all pairs of words in C. Again, we use the history of word concept to reduce the output dimension by linearly combining v C i .</p><formula xml:id="formula_10">s C i =[GloVe(w C i ); BERT w C i ; h C,1 i ; ...; h C,k i ; m (1),Q i ; ...; m (K+1),Q i ; v C i ] {? C i } m i=1 = Attn({s C i } m i=1 , {s C i } m i=1 , {v C i } m i=1 )</formula><p>The self-attention is followed by an additional layer of RNN to generate the final representation of context:</p><formula xml:id="formula_11">{u C i } m i=1 = BiLSTM ([v C 1 ;? C 1 ], ..., [v C m ;? C m ])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">OUTPUT LAYER</head><p>Generating Answer Span. This component is to generate two scores for each context word corresponding to the probability that the answer starts and ends at this word, respectively.</p><p>Firstly, we condense the question representation into one vector:</p><formula xml:id="formula_12">u Q = i ? i u Q i , where ? i ? exp (w T u Q i )</formula><p>and w is a parametrized vector. Secondly, we compute the probability that the answer span should start at the i-th word:</p><formula xml:id="formula_13">P S i ? exp ((u Q ) T W S u C i ),</formula><p>where W S is a parametrized matrix. We further fuse the start-position probability into the computation of end-position probability via a GRU, t Q = GRU (u Q , i P S i u C i ). Thus, the probability that the answer span should end at the i-th word is:</p><formula xml:id="formula_14">P E i ? exp ((t Q ) T W E u C i ), where W E is another parametrized matrix.</formula><p>For CoQA dataset, the answer could be affirmation "yes", negation "no" or no answer "unknown". We separately generate three probabilities corresponding to these three scenarios, P Y , P N , P U , respectively. For instance, to generate the probability that the answer is "yes", P Y , we use:</p><formula xml:id="formula_15">P Y i ? exp ((u Q ) T W Y u C i ), P Y = ( i P Y i u C i ) T w Y ,</formula><p>where W Y and w Y are parametrized matrix and vector, respectively.</p><p>Training. For training, we use all questions/answers for one passage as a batch. The goal is to maximize the probability of the ground-truth answer, including span start/end position, affirmation, negation and no-answer situations. Equivalently, we minimize the negative log-likelihood function L:</p><formula xml:id="formula_16">L = k I S k (log(P S i s k ) + log(P E i e k )) + I Y k logP Y k + I N k logP N k + I U k logP U k ,</formula><p>where i s k and i e k are the ground-truth span start and end position for the k-th question.</p><formula xml:id="formula_17">I S k , I Y k , I N k , I U k</formula><p>indicate whether the k-th ground-truth answer is a passage span, "yes", "no" and "unknown", respectively. More implementation details are in Appendix.</p><p>Prediction. During inference, we pick the largest span/yes/no/unknown probability. The span is constrained to have a maximum length of 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We evaluated our model on CoQA <ref type="bibr" target="#b10">(Reddy et al., 2018)</ref>, a large-scale conversational question answering dataset. In CoQA, many questions require understanding of both the passage and previous questions and answers, which poses challenge to conventional machine reading models. <ref type="table" target="#tab_1">Table 1</ref> summarizes the domain distribution in CoQA. As shown, CoQA contains passages from multiple domains, and the average number of question answering turns is more than 15 per passage. Many questions require contextual understanding to generate the correct answer.  Baseline models and metrics. We compare SDNet with the following baseline models: PGNet (Seq2Seq with copy mechanism) <ref type="bibr" target="#b11">(See et al., 2017)</ref>, DrQA , DrQA+PGNet <ref type="bibr" target="#b10">(Reddy et al., 2018)</ref>, BiDAF++ (Yatskar, 2018) and FlowQA <ref type="bibr" target="#b6">(Huang et al., 2018)</ref>. Aligned with the official leaderboard, we use F 1 as the evaluation metric, which is the harmonic mean of precision and recall at word level between the predicted answer and ground truth. 1</p><p>Results. <ref type="table" target="#tab_2">Table 2</ref> report the performance of SDNet and baseline models. 2 As shown, SDNet achieves significantly better results than baseline models. In detail, the single SDNet model improves overall F 1 by 1.6%, compared with previous state-of-art model on CoQA, FlowQA. Ensemble SDNet model further improves overall F 1 score by 2.7%, and it's the first model to achieve over 80% F 1 score on in-domain datasets (80.7%). <ref type="figure" target="#fig_1">Figure 2</ref> shows the F 1 score on development set over epochs. As seen, SDNet overpasses all but one baseline models after the second epoch, and achieves state-of-the-art results only after 8 epochs.</p><p>Ablation Studies. We conduct ablation studies on SDNet model and display the results in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>The results show that removing BERT can reduce the F 1 score on development set by 7.15%. Our proposed weight sum of per-layer output from BERT is crucial, which can boost the performance by 1.75%, compared with using only last layer's output. This shows that the output from each layer in BERT is useful in downstream tasks. This technique can also be applied to other NLP tasks. Using BERT-base instead of BERT-large pretrained model hurts the F 1 score by 2.61%, which manifests the superiority of BERT-large model. Variational dropout and self attention can each improve the performance by 0.24% and 0.75%, respectively.</p><p>Contextual history. In SDNet, we utilize conversation history via prepending the current question with previous N rounds of questions and ground-truth answers. We experimented the effect of N and show the result in <ref type="table" target="#tab_4">Table 4</ref>. Excluding dialogue history (N = 0) can reduce the F 1 score by as  Our future work is to apply this model to open-domain multiturn QA problem with large corpus or knowledge base, where the target passage may not be directly available. This will be an even more realistic setting to human question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mark Yatskar.</head><p>A qualitative comparison of coqa, squad 2.0 and quac. arXiv preprint arXiv:1809.10735, 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS</head><p>We use spaCy for tokenization. As BERT use BPE as the tokenizer, we did BPE tokenization for each token generated by spaCy. In case a token in spaCy corresponds to multiple BPE sub-tokens, we average the BERT embeddings of these BPE sub-tokens as the embedding for the token. We fix the BERT weights and use the BERT-Large-Uncased model.</p><p>During training, we use a dropout rate of 0.4 for BERT layer outputs and 0.3 for other layers. We use variational dropout <ref type="bibr" target="#b8">(Kingma et al., 2015)</ref>, which shares the dropout mask over timesteps in RNN. We batch the data according to passages, so all questions and answers from the same passage make one batch.</p><p>We use Adamax (Kingma &amp; Ba, 2014) as the optimizer, with a learning rate of ? = 0.002, ? = (0.9, 0.999) and = 10 ?8 . We train the model using 30 epochs, with each epoch going over the data once. We clip the gradient at length 10.</p><p>The word-level attention has a hidden size of 300. The flow module has a hidden size of 300. The question self attention has a hidden size of 300. The RNN for both question and context has K = 2 layers and each layer has a hidden size of 125. The multilevel attention from question to context has a hidden size of 250. The context self attention has a hidden size of 250. The final layer of RNN for context has a hidden size of 125.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>SDNet model structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>F 1 score on CoQA dev set over training epochs. For BERT base model, as there is no associated paper, we use the number on test set from the leaderboard. much as 8.56%, showing the importance of contextual information in conversational QA task. The performance of our model peaks when N = 2, which was used in the final SDNet model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Domain distribution in CoQA dataset.</figDesc><table><row><cell>Domain</cell><cell>#Passage</cell><cell>#QA turn</cell></row><row><cell>Child Story</cell><cell>750</cell><cell>14.0</cell></row><row><cell>Literature</cell><cell>1,815</cell><cell>15.6</cell></row><row><cell>Mid/High Sc.</cell><cell>1,911</cell><cell>15.0</cell></row><row><cell>News</cell><cell>1,902</cell><cell>15.1</cell></row><row><cell>Wikipedia</cell><cell>1,821</cell><cell>15.4</cell></row><row><cell></cell><cell>Out of domain</cell><cell></cell></row><row><cell>Science</cell><cell>100</cell><cell>15.3</cell></row><row><cell>Reddit</cell><cell>100</cell><cell>16.6</cell></row><row><cell>Total</cell><cell>8,399</cell><cell>15.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model and human performance (% in F1 score) on the CoQA test set. Child. Liter. Mid-High. News Wiki Reddit Science Overall</figDesc><table><row><cell>PGNet</cell><cell>49.0 43.3</cell><cell>47.5</cell><cell>47.5 45.1 38.6</cell><cell>38.1</cell><cell>44.1</cell></row><row><cell>DrQA</cell><cell>46.7 53.9</cell><cell>54.1</cell><cell>57.8 59.4 45.0</cell><cell>51.0</cell><cell>52.6</cell></row><row><cell>DrQA+PGNet</cell><cell>64.2 63.7</cell><cell>67.1</cell><cell>68.3 71.4 57.8</cell><cell>63.1</cell><cell>65.1</cell></row><row><cell>BiDAF++</cell><cell>66.5 65.7</cell><cell>70.2</cell><cell>71.6 72.6 60.8</cell><cell>67.1</cell><cell>67.8</cell></row><row><cell>FlowQA</cell><cell>73.7 71.6</cell><cell>76.8</cell><cell>79.0 80.2 67.8</cell><cell>76.1</cell><cell>75.0</cell></row><row><cell>SDNet (single)</cell><cell>75.4 73.9</cell><cell>77.1</cell><cell>80.3 83.1 69.8</cell><cell>76.8</cell><cell>76.6</cell></row><row><cell cols="2">SDNet (ensemble) 78.7 77.1</cell><cell>80.2</cell><cell>81.9 85.2 72.3</cell><cell>79.7</cell><cell>79.3</cell></row><row><cell>Human</cell><cell>90.2 88.4</cell><cell>89.8</cell><cell>88.6 89.9 86.7</cell><cell>88.1</cell><cell>88.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of SDNet on CoQA development dataset. For each in-domain dataset, 100 passages are in the development set, and 100 passages are in the test set. The rest in-domain dataset are in the training set. The test set also includes all of the out-of-domain passages.</figDesc><table><row><cell>Model</cell><cell>F 1</cell></row><row><cell>SDNet</cell><cell>77.99</cell></row><row><cell>-Variational dropout</cell><cell>77.75</cell></row><row><cell>-Question self attention</cell><cell>77.24</cell></row><row><cell>Using last layer of BERT output</cell><cell></cell></row><row><cell>(no weighted sum)</cell><cell>76.24</cell></row><row><cell>BERT-base</cell><cell>75.38</cell></row><row><cell>-BERT</cell><cell>70.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of SDNet on development set when prepending different number of history questions and answers to the question. The model uses BERT-Large contextual embedding and fixes BERT's weights. , we propose a novel contextual attention-based deep neural network, SDNet, to tackle conversational question answering task. By leveraging inter-attention and self-attention on passage and conversation history, the model is able to comprehend dialogue flow and fuse it with the digestion of passage content. Furthermore, we incorporate the latest breakthrough in NLP, BERT, and leverage it in an innovative way. SDNet achieves superior results over previous approaches. On the public dataset CoQA, SDNet outperforms previous state-of-the-art model by 1.6% in overall F 1 metric.</figDesc><table><row><cell>#previous QA rounds N</cell><cell>F 1</cell></row><row><cell>0</cell><cell>69.43</cell></row><row><cell>1</cell><cell>76.70</cell></row><row><cell>2</cell><cell>77.99</cell></row><row><cell>3</cell><cell>77.39</cell></row><row><cell>4 CONCLUSIONS</cell><cell></cell></row><row><cell>In this paper</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">According to official evaluation of CoQA, when there are more than one ground-truth answers, the final score is the average of max F1 against all-but-one ground-truth answers.2 Result was taken from official CoQA leaderboard on Nov. 30, 2018.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07036</idno>
		<title level="m">Quac: Question answering in context</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A dataset and baselines for sequential open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1077" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fusionnet: Fusing via fullyaware attention with application to machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsin-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07341</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsin-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06683</idno>
		<title level="m">Flowqa: Grasping flow in history for conversational machine comprehension</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07042</idno>
		<title level="m">Coqa: A conversational question answering challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
