<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Killing Two Birds with One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
							<email>jiankangdeng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
							<email>guojia@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyong</forename><surname>Feng</surname></persName>
							<email>ziyongfeng@deepglint.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
							<email>tongliang.liu@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepglint</forename><forename type="middle">2</forename><surname>Huawei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Insightface</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng Laboratory</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Killing Two Birds with One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning discriminative deep feature embeddings by using million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the largescale training data inevitably suffers from inter-class conflict and long-tailed distribution. In this paper, we propose a sparsely updating variant of the FC layer, named Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class centers are still maintained throughout the whole training process, but only a subset is selected and updated in each iteration. Therefore, the computing requirement, the probability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Extensive experiments across different training data and backbones (e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the proposed PFC. The source code is available at https://github.com/deepinsight/ insightface/tree/master/recognition. * corresponding author. InsightFace is a nonprofit Github project for 2D and 3D face analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition is playing an increasingly important role in modern life and has been widely used in many real-world applications, such as face authentication on mobile devices. Recently, face recognition has witnessed great advance along with the collection of large-scale training datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50]</ref>, the evolution of network architectures <ref type="figure">Figure 1</ref>. PFC picks the positive center by using the label and randomly selects a significantly reduced number of negative centers to calculate partial image-to-class similarities. PFC kills two birds (efficiency and robustness) with one stone (partial sampling). <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref>, and the design of margin-based and mining-based loss functions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Even though the softmax loss <ref type="bibr" target="#b2">[3]</ref> and its margin-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> or mining-based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref> variants achieved state-of-the-art performance on deep face recognition, the training difficulty accumulates along with the growth of identities in the training data, as the memory and computing consumption of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. When there are large-scale identities in the training dataset, the cost of storage and calculation of the final linear matrix can easily exceed the capabilities of current GPUs, resulting in tremendous training time or even a training failure.</p><p>To break the computing resource constraint, the most straightforward solution is to reduce the number of classes used during training. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> propose to use a hashing forest to partition the space of class weights into small cells but the complexity of walking through the forest to find the closest cell is O(logN). Li et al. <ref type="bibr" target="#b21">[22]</ref> randomly split training identities into groups and identities from each group share one anchor, which is used to construct the virtual fully-connected layer. Even though Virtual FC reduces the FC parameters by more than 100 times, there is an obvious performance drop compared to the conventional FC solution. SST <ref type="bibr" target="#b10">[11]</ref> and DCQ <ref type="bibr" target="#b20">[21]</ref> directly abandon the FC layer and employ a momentum-updated network to produce class weights. However, the negative class number is constrained to past several hundred steps and two networks need to be maintained in the GPU.</p><p>Besides the training difficulty on large-scale datasets, celebrity images gathered from the internet and cleaned by automatic methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50]</ref> exhibit long-tailed distribution <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref> as well as label noise <ref type="bibr" target="#b33">[34]</ref>. Some well-known celebrities have abundant images (head classes) from the search engine while most celebrities have only a few images (tail classes) on the web. To keep hard training samples, the thresholds used in intra-class and inter-class cleaning steps in <ref type="bibr" target="#b49">[50]</ref> are relatively relaxed, leaving label flip noises in the WebFace42M dataset. Wang et al. <ref type="bibr" target="#b33">[34]</ref> point out that label flips deteriorate the model's performance heavier than outliers as the margin-based softmax loss can not easily handle inter-class conflict during training.</p><p>To alleviate the above-motioned problems, we propose a sparsely updated fully connected layer, named Partial FC (PFC), for training large-scale face recognition. In the proposed PFC, the conventional FC layer is still maintained throughout the whole training process but the updating frequency is significantly decreased as we only sample parts of negative class centers in each iteration. As illustrated in <ref type="figure">Fig. 1</ref>, positive class centers are selected and a subset of negative class centers are randomly selected to compute the margin-based softmax loss. As only a subset of inter classes is selected for each iteration, the computing requirement, the frequency of passive update on tail class centers, and the probability of inter-class conflict are dramatically reduced. Extensive experiments across different training datasets and backbones (e.g. CNN <ref type="bibr" target="#b12">[13]</ref> and ViT <ref type="bibr" target="#b9">[10]</ref>) confirm the effectiveness, robustness and efficiency of the proposed PFC under a large range of sampling ratios. The advantages of the proposed PFC can be summarized as follows:</p><p>? Efficient. Under the high-performance mode, PFC-0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Margin-based Deep Face Recognition. The pioneering margin-based face recognition network <ref type="bibr" target="#b29">[30]</ref> uses the triplet loss in the Euclidean space. However, the training procedure is very challenging due to the combinatorial explosion in the number of triplets. By contrast, margin-based softmax methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> focus on incorporating margin penalty into a more feasible framework, softmax loss, and achieve impressive performance. To further improve the margin-based softmax loss, recent works focus on the exploration of adaptive parameters <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, inter-class regularization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">47]</ref>, mining <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>, grouping <ref type="bibr" target="#b18">[19]</ref>, etc.</p><p>To accelerate margin-based softmax loss, a virtual fullyconnected layer is proposed by <ref type="bibr" target="#b21">[22]</ref> to reduce the FC parameters by more than 100 times. In addition, DCQ <ref type="bibr" target="#b20">[21]</ref> directly abandons the FC layer and employs a momentumupdated network to produce class weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Face Recognition Training under Noise and</head><p>Long-tail Distribution. Most of the face recognition datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50]</ref> are downloaded from the Internet by searching a pre-defined celebrity list, and the original labels are likely to be ambiguous and inaccurate <ref type="bibr" target="#b33">[34]</ref>. As accurate manual annotations are expensive <ref type="bibr" target="#b33">[34]</ref>, learning with massive noisy data has recently drawn much attention in face recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. To improve robustness under noises, recent methods attempt to design noise-tolerant loss functions (e.g. computing time-varying weights for samples <ref type="bibr" target="#b14">[15]</ref>, designing piece-wise loss functions <ref type="bibr" target="#b47">[48]</ref> according to model's predictions, and relaxing the constraint of intra-class compactness <ref type="bibr" target="#b6">[7]</ref>), explore consistent predictions from twin networks <ref type="bibr" target="#b36">[37]</ref>, and employ meta-supervision for adaptive label noise cleaning <ref type="bibr" target="#b45">[46]</ref>. Besides the label noise, web data are usually long-tail distributed. To alleviate long-tailed distribution, recent methods attempt to either improve the margin values for the tail classes <ref type="bibr" target="#b23">[24]</ref> or recall the benefit from sample-to-sample comparisons <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b48">49</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section starts with the limitation analysis of the conventional FC Layer. Then, these limitations motivate the proposal of a more efficient and robust training method, called Partial FC (PFC). Through learning dynamic analysis on both clean and noisy training data, we finally achieve a deeper understanding of the role of inter-class interaction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting FC Layer</head><p>In this sub-section, we first discuss the optimization procedure of the FC layer. Then, we discuss three drawbacks of the FC layer based on the gradient analysis.</p><p>The most widely used classification loss function for face recognition, i.e., softmax loss, is presented as follow:</p><formula xml:id="formula_0">L = ? 1 B B i=1 log e W T y i xi e W T y i xi + C j=1,j? =yi e W T j xi ,<label>(1)</label></formula><p>where W j ? R D denotes the j-th column of class-wise centers, x i ? R D denotes the feature of the i-th sample belonging to the y i -th class, D is the feature dimension, C is the class number, and B is the batch size. From the view of features, the network will be updated towards a direction that the feature will be close to the ground-truth center and far apart from all other centers. To illustrate the feature's gradient in a more straightforward way, we denote the probability and center of the ground truth as p + and W + while other negative probabilities and centers as p ? j and W ? j :</p><formula xml:id="formula_1">?L ?x i = ?((1 ? p + )W + ? C j=1,j? =yi p ? j W ? j ).<label>(2)</label></formula><p>From the view of centers, the center W j belonging to j-th class will be updated towards a direction that is close to sample features of j-th class and far apart from sample features of other classes:</p><formula xml:id="formula_2">W t j = W t?1 j + ?( i?B + (1 ? p + i )x + i ? i?B ? p ? i x ? i ),<label>(3)</label></formula><p>where ? is the learning rate, t is the iteration number, B + represents all samples belonging to j-th class, B ? stands for all samples of other classes, and |B + | + |B ? | equals to the batch size B. Even though the softmax loss and its margin-based or mining-based variants achieved state-of-the-art performance on deep face recognition, the fully connected layer in the softmax loss has the following three drawbacks when applied to the large-scale web data <ref type="bibr" target="#b49">[50]</ref>.  The first limitation is the gradient confusion under interclass conflict. As shown in <ref type="figure" target="#fig_0">Fig. 2a</ref>, there are many class pairs from WebFace42M <ref type="bibr" target="#b49">[50]</ref> showing high cosine similarities (e.g. &gt; 0.4), indicating inter-class conflict still exists in this automatically cleaned web data. Here, inter-class conflict refers to images of one person being wrongly distributed to different classes. If a large number of conflicted classes exist, the network optimization will suffer from gradient confusion on both features and centers, as the negative class center W j in Eq. 2 and the negative features x ? i in Eq. 3 could be from the positive class.</p><p>The second limitation is that centers of tail classes undergo too many passive updates. As shown in <ref type="figure" target="#fig_0">Fig. 2b</ref>, the identities of WebFace42M <ref type="bibr" target="#b49">[50]</ref> are long-tail distributed, 44.57% of identities containing less than 10 images. Under the training scenario of million-level identities and thousand-level batch size, B + in Eq. 3 is empty in most iterations for a particular class j, especially for tail classes. When there is an inter-class penalty from training samples of other classes, W j is pushed away from the features of these negative samples, gradually drifting from the direction of its representing class <ref type="bibr" target="#b10">[11]</ref>. Therefore, there may exist a discrepancy between the class-wise feature center predicted by the embedding network and the corresponding center updated by SGD.</p><p>The third limitation is that the storage and calculation of the FC layer can easily exceed current GPU capabilities.</p><p>In ArcFace <ref type="bibr" target="#b7">[8]</ref>, the center matrix W ? R D?C is equally partitioned onto K GPUs. During the forward step, each GPU first gathers all embedding features (i.e. X ? R D?B ) from all GPUs. Then, sample-to-class similarities and their exponential mappings are calculated independently on each GPU. To calculate the denominator of Eq. 1 to normalize all similarity values, the local sum on each GPU is calculated, and then the global sum is computed through cross-GPU communication. Finally, the normalized probabilities are used in Eq. 2 and Eq. 3 to calculate the feature's gradient and center's gradient. Even though the model parallelization can completely solve the storage problems of W through adding more GPUs at negligible communica- Meanwhile, partial centers are copied from each CPU to GPU. Positive class centers are picked through labels while partial negative class centers (in grey) are randomly selected to fill the buffer. After the inner product between gathered features and partial centers on each GPU, we obtain the partial logits. PFC is memory-saving and efficient because it reduces the GPU memory consumption and computation cost on the FC layer. PFC is also robust under inter-class conflict due to decreased inter-class interaction during training. On the dataset with extremely heavy inter-class conflict, abnormal inter-class high similarities can be filtered by a fixed threshold (i.e. 0.4) in PFC to further enhance the robustness.</p><p>tion cost, the storage of predicted logits can not be easily solved by increasing the GPU number.</p><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3a</ref>, the identity number C goes up from 1M to 8M, and we accordingly increase the GPU number K from 8 to 64 to keep C/K consistent. However, the memory consumption of logits (C/K ?B) still significantly increase as the batch size rises synchronously with K, even surpassing the memory usage of the backbone. Besides the memory consumption of the FC layer, the computation cost in the forward and backward steps is also tremendous. As shown in <ref type="figure" target="#fig_2">Fig. 3b</ref>, the throughput can not be improved as the increased GPUs are used for the calculation of the enlarged FC layer. Therefore, simply stacking more GPUs can not efficiently solve large-scale face recognition training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Partial FC</head><p>To alleviate the drawbacks of the FC layer, we propose PFC, a sparsely updating variant of the fully connected layer for training large-scale face recognition models. As illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, we maintain all of the class centers during training but randomly sample a small part of negative class centers to calculate the margin-based softmax loss instead of using all of the negative class centers in every iteration. More specifically, face feature embeddings and labels are first gathered from each GPU, and then the combined features and labels are distributed to all GPUs. In order to equalize the memory usage and computation cost of each GPU, we set a memory buffer for each GPU. The size of the memory buffer is decided by the total number of classes and the sampling rate of the negative class centers. On each GPU, positive class centers are first picked through labels and put in the buffer, then a small part of negative class centers are randomly selected to fill the rest of the buffer to ensure load balance. After the inner product between gathered embeddings and the partial center matrix on each GPU, we simultaneously obtain all partial similarity matrices to calculate the margin-based softmax loss.</p><p>In PFC, the network will be updated towards a direction making the feature x i close to the positive class center W + and away from part of negative class centers W ? j .</p><formula xml:id="formula_3">?L ?x i = ?((1 ? p + )W + ? j?S,j? =yi p ? j W ? j ),<label>(4)</label></formula><p>where S is a subset of all negative classes and one positive class, |S| = C * r, and r is the sampling ratio. By comparing Eq. 2 and Eq. 4, we can easily find that PFC directly decreases the possibility of inter-class conflict by r. In addition, only positive centers and part of negative centers will be updated by Eq. 3 in each iteration. Therefore, the frequency of gradient update on W j also decreases from 1.0 to r, thus avoiding excessive passive update on tail class centers. In <ref type="figure" target="#fig_2">Fig. 3a</ref> and <ref type="figure" target="#fig_2">Fig. 3b</ref>, PFC saves a large amount of GPU memory used by softmax logits, thus the model training can benefit from stacking more GPUs to increase the throughput on large-scale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Rethinking Inter-class Interaction</head><p>In Eq. 4, inter-class interaction between features and centers is significantly decreased by the sampling ratio. To figure out the impact of the inter-class sampling, we define three metrics to evaluate the real-time intra-class compactness, real-time inter-class discrepancy, and final inter-class distribution.</p><p>More specifically, we define the Average Positive Cosine Similarity (APCS) between x i and positive class center  intra-class optimization status on the training data. We also define the Average Maximum Negative Cosine Similarity (AMNCS) between x i and closest negative class center W j as AMNCS </p><formula xml:id="formula_4">W yi as APCS = 1/B B i=1 W T yi x i /(?W yi ? ?x i ?) where B is</formula><formula xml:id="formula_5">= 1/B B i=1 max j? =i W T j x i /(?W j ? ?x i ?),</formula><formula xml:id="formula_6">W T i W j /(?W i ? ?W j ?).</formula><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we compare the intra-class and inter-class status under different sampling ratios. We train a series of ResNet50 models on the WebFace12M dataset <ref type="bibr" target="#b49">[50]</ref> by using margin-based softmax loss <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>. The minimum sampling ratio is the batch size divided by the identity number, that is 1024/600K ? 0.0017, indicating that only withinbatch negative centers are used to construct the marginbased softmax loss. As illustrated in <ref type="figure" target="#fig_4">Fig. 5b</ref>, the inter-class similarities obviously increase when the sampling ratio decreases from 1.0 to 0.0017. As the updating frequency of W j in Eq. 3 is decreased, the network training weakens the inter-class optimization and focuses more on the intra-class optimization. Therefore, PFC achieves higher intra-class similarities during training as shown in <ref type="figure" target="#fig_4">Fig. 5a</ref>.</p><p>Even though the inter-class discrepancy deteriorates on the training data when the sampling ratio is dropping, the verification accuracy on IJB-C <ref type="bibr" target="#b26">[27]</ref> and MFR-All <ref type="bibr" target="#b5">[6]</ref> can be still maintained when the sampling ratio is larger than 0.1 as shown in <ref type="figure">Fig. 6</ref>. When the sampling ratio drops to 0.0017, the verification accuracy obviously decreases, indicating that inter-class interaction during training is not sufficient. To improve the inter-class discrepancy, we train three extra models by enlarging the batch size to 2K, 4K, 8K to embody more within-batch negative classes where the sampling ratio is also increased accordingly. As given in <ref type="figure">Fig. 6</ref>, the performance significantly increases when the batch size is enlarged. Please note that, when PFC does not add extra negative classes outside batches, the training time consumed on the FC layer is negligible compared to the time cost on the backbone. Besides enlarging the batch size, we have also explored inter-class regularization <ref type="bibr" target="#b46">[47]</ref>. By including MICS as the regularization loss, the performance can be also obviously improved in <ref type="figure">Fig. 6</ref>. However, interclass regularization needs non-ignorable computation cost on large-scale training data.</p><p>Besides the analysis on clean data, we also synthesize a WebFace12M-Conflict dataset by randomly splitting 200K identities into another 600K identities, thus WebFace12M-Conflict contains 1M pseudo classes with a high inter-class conflict ratio. As shown in <ref type="figure">Fig. 7a</ref>, FC (r = 1.0) confronts with fluctuation during inter-class optimization but finally over-fits the conflicted dataset <ref type="figure">(Fig. 7b</ref>). By contrast, PFC (r = 0.1) relaxes the inter-class optimization, thus conflicted classes exhibit much higher similarities as given in <ref type="figure">Fig. 7b</ref>. As WebFace12M-Conflict is synthesized, we can use ground-truth labels to separately calculate AMNCS for hard negative classes and conflicted negative classes (i.e. inter-class noises). As shown in <ref type="figure">Fig. 8b</ref>, PFC (r = 0.1) can punish hard negative classes as normal as on the clean dataset ( <ref type="figure">Fig. 7a</ref>) while the conflicted negative classes can still achieve increasing similarities during training. In <ref type="figure">Fig. 8a</ref>, FC (r = 1.0) struggles to decrease the similarities between features and conflicted class centers, lead-ing to over-fitting <ref type="figure">(Fig. 7b)</ref>. As there is a clear margin for PFC to distinguish hard negative classes and conflicted negative classes in <ref type="figure">Fig. 8b</ref>, we can further set an online interclass filtering threshold (i.e. 0.4) in PFC to suppress conflict inter-classes. In this paper, we denote PFC with abnormal inter-class filtering as PFC*.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Datasets. In this paper, we employ the publicly available dataset, WebFace <ref type="bibr" target="#b49">[50]</ref>, to train face recognition models. The cleaned WebFace42M contains 2M identities, while the subsets WebFace12M and WebFace4M include 600K and 200K identities, respectively. We synthesize WebFace12M-Conflict (Tab. 3), WebFace12M-Flip (Tab. 4), WebFace10M-Longtail (Tab. 5) from WebFace to simulate the scenarios of inter-class conflict, label flip, and long-tail distribution.</p><p>For testing, we extensively evaluate the proposed PFC on popular benchmarks, including LFW <ref type="bibr" target="#b15">[16]</ref>, CFP-FP <ref type="bibr" target="#b30">[31]</ref>, AgeDB <ref type="bibr" target="#b28">[29]</ref>, IJB-B <ref type="bibr" target="#b38">[39]</ref> and IJB-C <ref type="bibr" target="#b26">[27]</ref>. As the performance on these celebrity benchmarks tend to be saturated, we conduct the ablation study on MFR <ref type="bibr" target="#b5">[6]</ref>, which contains 1.6M images of 242K identities (non-celebrity) covering four demographic groups: African, Caucasian, South-Asian and East-Asian. On MFR, True Accept Rates (TARs) @ False Positive Rate (FAR) = 1e-6 across different races are reported from the online test server after submitting the models. Besides, we also report the TARs@FAR = 1e-4 on the masked face recognition track of MFR. Experimental settings. All experiments in this paper are implemented using Pytorch, and mixed-precision <ref type="bibr" target="#b27">[28]</ref> is employed to save GPU memory and accelerate training. We follow <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref> to set the hyper-parameters of margin-based softmax loss and adopt flip data augmentation. We use customized ResNet <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> and ViT <ref type="bibr" target="#b9">[10]</ref> as the backbone. On different datasets, CNN models are trained for 20 epochs while ViT models are trained for 40 epochs. For the training of CNN models, the default batch size per GPU is set as 128 unless otherwise indicated. We employ the SGD optimizer with polynomial decay (power=2) and the learning rate is set as 0.1 for the single node training (8 Tesla V100 32GB GPUs). To accelerate the training on WebFace42M, we employ four nodes with 8 ? 4 GPUs and linearly warm up the learning rate from 0 to 0.4 within the first 2 epochs. Afterwards, polynomial decay (power=2) is used for another 18 epochs as shown in <ref type="figure">Fig. 9a</ref>. For the training of ViT models, the default batch size per GPU is set as 384. We use the AdamW <ref type="bibr" target="#b25">[26]</ref> optimizer with a base learning rate of 0.001 and a weight decay of 0.  <ref type="table">Table 3</ref>. Performance analysis of PFC under synthetic inter-class conflict. The WebFace12M-Conflict dataset contains 1M classes split from the WebFace12M dataset. ResNet50 is used here. "+PFC*" denotes additional inter-class filtering to neglect abnormal negative class centers with cosine similarities higher than 0.4.</p><p>the learning rate from 0 to 0.001 within the first 4 epochs. Then, polynomial decay (power=2) is used for another 36 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>PFC across different datasets and sampling ratios.In Tab. 1, we train ResNet50 models on three different datasets with different sampling ratios. Compared to the performance of FC, PFC-0.1 not only accelerates the training but also achieves comparable results across different datasets with identities ranging from 200K to 2M . When the sampling ratio is increased to 0.2 and 0.3, PFC exhibits con-   <ref type="table">Table 7</ref>. Performance comparisons between PFC and recent stateof-the-art methods on various benchmarks. 1:1 verification accuracy (%) is reported on the LFW, CFP-FP and AgeDB datasets. TAR@FAR=1e-4 is reported on the IJB-B and IJB-C datasets. Under the small sampling ratios, only within-batch negative classes are used to construct the softmax loss. Therefore, the batch size of r = 0.04 and r = 0.013 is enlarged to 8K, while the batch size of r = 0.008 is amplified to 16K.</p><p>we synthesize a WebFace12M-Conflict dataset by randomly splitting 200K identities into 600K identities, thus WebFace12M-Conflict contains 1M classes with a high inter-class conflict ratio. The performance of baseline significantly drops from 91.70% to 79.93% on MFR-All, while PFC is less affected by the conflicted inter-classes, demonstrating the robustness of PFC under heavy interclass conflict. By using an online abnormal inter-class filtering ( <ref type="figure" target="#fig_3">Fig. 4 and Fig. 8</ref>), PFC*-0.2/0.3 can achieve almost the same performance as on the cleaned version of Web-Face12M. Even though the baseline (FC) can also use the trick of abnormal inter-class filtering, the margin between hard inter-classes and conflicted inter-classes in FC is less clear than the proposed PFC as shown in <ref type="figure">Fig. 8</ref>. Therefore, FC* only achieves 91.18% even all of the negative classes are checked at higher computation cost in each iteration. In Tab. 4, we randomly change the image labels by adding synthesized label-flip noise into the WebFace12M dataset. As the noise ratios increases from 0% to 40%, the performance of baseline dramatically declines from 91.70% to 43.87%. By contrast, the proposed PFC-0.1 is more robust under label-flip noise, achieving an accuracy of 78.53% under 40% label-flip noise. By using the abnormal inter-class filtering, PFC*-0.1 trained on the noisy WebFace12M (40% label-flips) further improves the verification accuracy to 80.20%, surpassing the FC baseline by 36.33%. Robustness under long-tail distribution. In Tab. 5, we construct a long-tail distributed dataset from WebFace42M. Specifically, 200K identities are directly copied and the rest of 1.8M identities are randomly condensed to contain 2 to 4 face images per identity. As can be seen, PFC-0.2 achieves a verification accuracy of 91.96% on MFR-All, surpassing the FC baseline by 4.52% and DCQ by 2.59%, indicating that PFC is more robust under long-tail distribution. Memory saving and training acceleration. In Tab. 6, we compare PFC-0.1 with other sampling-based methods (e.g. HF-Softmax <ref type="bibr" target="#b41">[42]</ref> and D-Softmax <ref type="bibr" target="#b13">[14]</ref>) on WebFace42M using single computing node. Even though HF-Softmax can significantly reduce memory consumption, the time cost on feature retrieval using CPU can not be ignored. Besides, inter-class conflict still exists in the automatic cleaned Web-Face42M, thus hard mining can even deteriorate the model's performance. D-Softmax separates softmax loss into intraclass and inter-class objectives and reduces the calculation redundancy of the inter-class objective. However, there exists an obvious performance drop for D-Softmax on MFR-All. In addition, the classification layer of D-Softmax uses data parallelism, thus the communication cost of the center weights will decrease the training speed. By contrast, the proposed PFC-0.1 is not only faster but also more accurate, achieving the verification accuracy of 96.19%.</p><p>In Tab. 6, we also test the training speed of PFC-0.1 on synthetic 10M identities by using 8 GPUs. PFC-0.1 can run five times faster than the baseline, consuming less than half GPU memory. As logits occupy a large amount of GPU memory, the batch size of the baseline can be only 16, resulting in a slow training speed. When the GPU number increasing from 8 to 64, we can observe similar phenomena of memory reduction and throughput improvement. In <ref type="figure">Fig. 9</ref>, we compare the loss and performance between the FC baseline and PFC-0.1 during training. The loss of PFC in <ref type="figure">Fig. 9a</ref> is lower than the baseline as the denominator of Eq. 1 is smaller for PFC-0.1. In <ref type="figure">Fig. 9b</ref>, PFC-0.1 achieves better performance than the baseline with half training time, indicating that PFC-0.1 can significantly accelerate model training on the large-scale dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Celebrity Benchmark Results</head><p>To compare with recent state-of-the-art competitors, we train PFC models on different datasets and test on various benchmarks. As reported in Tab. 7, the proposed PFC (r = 0.3) achieves state-of-the-art results compared to the competitors on pose-invariant verification, age-invariant verification, and mixed-media (image and video) face verification. Even though Virtual FC <ref type="bibr" target="#b21">[22]</ref> can reduce the parameters by more than 100 times, there is an obvious performance drop for Virtual FC on the large-scale test set. By contrast, the PFC model (r = 0.04) trained on Web-Face4M significantly outperforms the Virtual FC model by 27.47% and 25.33% on IJB-B and IJB-C, respectively. Both SST <ref type="bibr" target="#b10">[11]</ref> and DCQ <ref type="bibr" target="#b20">[21]</ref> abandon the FC layer and employ a momentum-updated network to produce class weights. However, PFC only needs to train one network instead of a pair of networks. On CFP-FP, PFC models trained on Web-Face4M with sampling ratios of r = 0.04 and r = 0.3 outperform the DCQ model by 0.62% and 0.79%, respectively. When WebFace42M <ref type="bibr" target="#b49">[50]</ref> is employed, the performance on all of these benchmarks tends to be saturated. However, the proposed PFC can still break the record. Specifically, the ResNet200 model trained with PFC-0.3 achieves 99.51% on CFP-FP and 98.70% on AgeDB, while the ViT large model trained with PFC-0.3 obtains 96.71% and 98.00% verification accuracy on IJB-B and IJB-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Discussions</head><p>In this paper, we propose Partial FC (PFC) for training face recognition models on large-scale datasets. In each iteration of PFC, only a small part of class centers are selected to compute the margin-based softmax loss, the probability of inter-class conflict, the frequency of passive update on tail class centers, and the computing requirement can be dramatically reduced. Through extensive experiments, we confirm the effectiveness, robustness and efficiency of the proposed PFC.</p><p>Limitations. Even though the PFC models trained on Web-Face have achieved impressive results on high-quality test sets, it may perform poorly when face resolution is low or faces are captured in low illumination.</p><p>Negative Societal Impact. The PFC models may be used in surveillance and breach privacy rights, thus we will strictly control the license of code and models for academic research use only.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Inter-class conflict and long-tail distribution of Web-Face42M<ref type="bibr" target="#b49">[50]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Memory Consumption and training speed comparisons between model parallel and PFC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Distributed implementation of the proposed PFC. Face features are first gathered from each GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>the batch-size and APCS is an real-time indicator of Intra-class compactness and inter-class discrepancy comparisons under different sampling ratios on the WebFace12M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .Figure 8 .</head><label>678</label><figDesc>(b) MFR-All (TAR@FAR=1e-6) Verification accuracy on IJB-C and MFR-All under different sampling ratios. Inter-class statistics comparisons under different sampling ratios on the WebFace12M and WebFace12M-Conflict datasets. Hard negative and conflicted negative classes analysis under different sampling ratios on the WebFace12M-Conflict datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>which is an real-time indicator of inter-class optimization status on the training data. To evaluate the final inter-class discrepancy, we define the Maximum Inter-class Cosine Similarity (MICS) as MICS i = max j? =i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The proposed PFC has obtained state-ofthe-art performance on different benchmarks, achieving 98.00% on IJB-C and 97.85% on MFR-all.</figDesc><table><row><cell>1</cell></row><row><cell>(sampling ratio) applied to ResNet100 can efficiently</cell></row><row><cell>train 10M identities on a single server with around</cell></row><row><cell>2.5K samples per second, which is five times faster</cell></row><row><cell>than the model parallel solution. Under the ultra-fast</cell></row><row><cell>mode, the sampling ratio of PFC can be decreased to</cell></row><row><cell>an extremely low status (around 0.01) where no ex-</cell></row><row><cell>tra negative class is selected. For PFC-0.008 with</cell></row><row><cell>ResNet100 trained on WebFace42M, the computation</cell></row><row><cell>cost on the FC layer can be almost neglected while the</cell></row><row><cell>verification accuracy on IJB-C reaches 97.51%.</cell></row></table><note>? Robust. PFC is amazingly robust under inter-class conflict, label-flip noise, and real-world long-tailed distribution. Assisted by a simple online abnormal inter-class filtering, PFC can further improve robust- ness under heavy inter-class conflicts.? Accurate.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1 .</head><label>1</label><figDesc>To achieve quick training, we employ eight nodes with 8 ? 8 GPUs and linearly warm up</figDesc><table><row><cell>Datasets</cell><cell></cell><cell></cell><cell>All</cell><cell>Afr</cell><cell cols="2">MFR Cau</cell><cell>S-Asian E-Asian Mask</cell></row><row><cell cols="2">WF4M+FC-1.0</cell><cell></cell><cell>86.25</cell><cell cols="3">83.35 91.11</cell><cell>88.14</cell><cell>65.79</cell><cell>72.05</cell></row><row><cell cols="2">WF4M+PFC-0.04</cell><cell cols="5">74.11 (-12.14) 71.43 81.79</cell><cell>76.25</cell><cell>52.24</cell><cell>54.21</cell></row><row><cell cols="2">WF4M+PFC-0.1</cell><cell cols="5">85.76 (-0.49) 83.82 91.00</cell><cell>87.90</cell><cell>66.04</cell><cell>71.13</cell></row><row><cell cols="2">WF4M+PFC-0.2</cell><cell cols="5">86.36 (+ 0.11) 84.47 91.39</cell><cell>88.45</cell><cell>66.61</cell><cell>71.88</cell></row><row><cell cols="2">WF4M+PFC-0.3</cell><cell cols="5">86.85 (+ 0.60) 84.86 91.57</cell><cell>88.57</cell><cell>67.52</cell><cell>72.28</cell></row><row><cell cols="2">WF4M+PFC-0.4</cell><cell cols="5">86.81 (+ 0.56) 84.75 91.44</cell><cell>88.41</cell><cell>67.17</cell><cell>71.99</cell></row><row><cell cols="2">WF12M+FC-1.0</cell><cell></cell><cell>91.70</cell><cell cols="3">90.72 94.94</cell><cell>93.44</cell><cell>75.10</cell><cell>80.47</cell></row><row><cell cols="7">WF12M+PFC-0.013 87.85 (-3.85) 87.07 92.32</cell><cell>90.70</cell><cell>68.28</cell><cell>72.98</cell></row><row><cell cols="2">WF12M+PFC-0.1</cell><cell cols="5">91.24 (-0.46) 90.80 94.67</cell><cell>93.18</cell><cell>74.97</cell><cell>79.73</cell></row><row><cell cols="2">WF12M+PFC-0.2</cell><cell cols="5">91.78 (+ 0.08) 91.09 95.00</cell><cell>93.53</cell><cell>75.90</cell><cell>79.92</cell></row><row><cell cols="2">WF12M+PFC-0.3</cell><cell cols="5">91.82 (+ 0.12) 91.14 95.00</cell><cell>93.61</cell><cell>75.55</cell><cell>80.08</cell></row><row><cell cols="2">WF12M+PFC-0.4</cell><cell cols="5">91.81 (+ 0.11) 90.97 95.03</cell><cell>93.40</cell><cell>75.55</cell><cell>80.61</cell></row><row><cell cols="2">WF42M+FC-1.0</cell><cell></cell><cell>93.86</cell><cell cols="3">93.33 96.20</cell><cell>95.24</cell><cell>79.46</cell><cell>83.90</cell></row><row><cell cols="7">WF42M+PFC-0.008 91.27 (-2.59) 90.34 95.16</cell><cell>93.04</cell><cell>76.93</cell><cell>81.24</cell></row><row><cell cols="2">WF42M+PFC-0.1</cell><cell cols="5">93.95 (+ 0.09) 93.48 96.37</cell><cell>95.51</cell><cell>80.03</cell><cell>83.79</cell></row><row><cell cols="2">WF42M+PFC-0.2</cell><cell cols="5">94.04 (+ 0.18) 93.67 96.38</cell><cell>95.49</cell><cell>80.07</cell><cell>84.32</cell></row><row><cell cols="2">WF42M+PFC-0.3</cell><cell cols="5">94.03 (+ 0.17) 93.68 96.38</cell><cell>95.52</cell><cell>79.76</cell><cell>84.46</cell></row><row><cell cols="2">WF42M+PFC-0.4</cell><cell cols="5">93.95 (+ 0.09) 93.38 96.35</cell><cell>95.46</cell><cell>79.57</cell><cell>84.42</cell></row><row><cell cols="8">Table 1. Performance comparisons under different sampling ratios</cell></row><row><cell cols="8">on different training datasets. ResNet50 is used here.</cell></row><row><cell>Net</cell><cell>GFlops</cell><cell>IJB-C 1e-5</cell><cell>All</cell><cell>Afr</cell><cell>Cau</cell><cell cols="2">MFR S-Asian E-Asian Mask</cell></row><row><cell>R18</cell><cell>2.62</cell><cell cols="4">93.36 79.13 75.50 86.10</cell><cell></cell><cell>80.55</cell><cell>57.77</cell><cell>63.87</cell></row><row><cell>R50</cell><cell>6.33</cell><cell cols="4">95.94 94.03 93.68 96.38</cell><cell></cell><cell>95.52</cell><cell>79.76</cell><cell>84.31</cell></row><row><cell>R100</cell><cell>12.12</cell><cell cols="4">96.45 96.69 96.68 98.09</cell><cell></cell><cell>97.72</cell><cell>86.14</cell><cell>89.64</cell></row><row><cell>R200</cell><cell>23.47</cell><cell cols="4">96.93 97.70 97.79 98.70</cell><cell></cell><cell>98.54</cell><cell>89.52</cell><cell>91.87</cell></row><row><cell>ViT-T</cell><cell>1.51</cell><cell cols="4">95.97 92.30 91.72 95.20</cell><cell></cell><cell>93.63</cell><cell>77.41</cell><cell>78.46</cell></row><row><cell>ViT-S</cell><cell>5.74</cell><cell cols="4">96.57 95.87 95.74 97.47</cell><cell></cell><cell>96.85</cell><cell>84.87</cell><cell>85.82</cell></row><row><cell>ViT-B</cell><cell>11.42</cell><cell cols="4">97.04 97.42 97.62 98.53</cell><cell></cell><cell>98.20</cell><cell>88.77</cell><cell>89.48</cell></row><row><cell>ViT-L</cell><cell>25.31</cell><cell cols="4">97.23 97.85 98.07 98.81</cell><cell></cell><cell>98.66</cell><cell>89.97</cell><cell>90.88</cell></row><row><cell cols="8">Table 2. Performance analysis of PFC (r=0.3) using different net-</cell></row><row><cell cols="8">work structures (i.e. CNN and ViT). Here, WebFace42M is used</cell></row><row><cell cols="8">as the training data and the gradient check-pointing [5] is used to</cell></row><row><cell cols="2">save memory.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell></cell><cell></cell><cell></cell><cell>All</cell><cell>Afr</cell><cell cols="2">MFR Cau</cell><cell>S-Asian E-Asian</cell></row><row><cell cols="2">WF12M+FC-1.0</cell><cell></cell><cell></cell><cell>91.70</cell><cell cols="3">90.72 94.94</cell><cell>93.44</cell><cell>75.10</cell></row><row><cell cols="3">WF12M-Conflict+FC-1.0</cell><cell></cell><cell>79.93</cell><cell cols="3">79.09 87.56</cell><cell>84.49</cell><cell>55.83</cell></row><row><cell cols="3">WF12M-Conflict+FC*-1.0</cell><cell></cell><cell>91.18</cell><cell cols="3">90.28 94.52</cell><cell>92.74</cell><cell>74.37</cell></row><row><cell cols="3">WF12M-Conflict+PFC-0.1</cell><cell cols="5">91.20 (+ 11.27) 90.65 94.65</cell><cell>93.40</cell><cell>74.99</cell></row><row><cell cols="8">WF12M-Conflict+PFC*-0.1 91.58 (+ 11.65) 91.01 94.81</cell><cell>93.42</cell><cell>75.42</cell></row><row><cell cols="3">WF12M-Conflict+PFC-0.2</cell><cell cols="5">90.55 (+ 10.62) 90.43 94.33</cell><cell>93.13</cell><cell>73.53</cell></row><row><cell cols="8">WF12M-Conflict+PFC*-0.2 91.68 (+ 11.75) 91.19 95.04</cell><cell>93.64</cell><cell>75.52</cell></row><row><cell cols="3">WF12M-Conflict+PFC-0.3</cell><cell cols="5">89.59 (+ 9.66) 89.24 93.67</cell><cell>92.35</cell><cell>71.85</cell></row><row><cell cols="8">WF12M-Conflict+PFC*-0.3 91.68 (+ 11.75) 91.03 94.85</cell><cell>93.60</cell><cell>75.51</cell></row><row><cell cols="3">WF12M-Conflict+PFC-0.4</cell><cell cols="5">87.78 (+ 7.85) 87.51 92.63</cell><cell>91.04</cell><cell>68.59</cell></row><row><cell cols="8">WF12M-Conflict+PFC*-0.4 91.54 (+ 11.61) 91.07 94.63</cell><cell>93.57</cell><cell>75.48</cell></row><row><cell>http://iccv21-mfr.com/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Performance analysis of PFC models trained on the WebFace10M-Longtail dataset. ResNet50 is used here.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell>0.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Loss</cell><cell>10 30 40 20</cell><cell></cell><cell></cell><cell cols="2">Model Parallel Partial FC Learning Rate</cell><cell>0.2 0.3 0.1</cell><cell>0.92 0.77 0.82 0.87 (TAR@FAR=1E-6) 0.67 0.72 MFR-All</cell><cell>Partial FC Model Parallel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.62</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>10 Steps (*10000) 15</cell><cell>20</cell><cell>0 25</cell><cell>0.57</cell><cell>0</cell><cell>8</cell><cell>16 Time (Hour)</cell><cell>24</cell><cell>32</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Loss&amp;LR</cell><cell></cell><cell>(b) Ver. on MFR-All</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Figure 9. Training status comparisons between model parallel and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">PFC on WebFace42M. ResNet100 is employed here. The batch</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">size is 128 ? 8 ? 4.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Method</cell><cell></cell><cell></cell><cell cols="2">Verification Accuracy LFW CFP-FP AgeDB IJB-B IJB-C IJB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">CosFace [36] (CVPR18)</cell><cell cols="2">99.81</cell><cell>98.12</cell><cell>98.11</cell><cell>94.80 96.37</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">ArcFace [8] (CVPR19)</cell><cell cols="2">99.83</cell><cell>98.27</cell><cell>98.28</cell><cell>94.25 96.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">AFRN [18] (ICCV19)</cell><cell cols="2">99.85</cell><cell>95.56</cell><cell>95.35</cell><cell>88.50 93.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MV-Softmax [38] (AAAI20)</cell><cell cols="2">99.80</cell><cell>98.28</cell><cell>97.95</cell><cell>93.60 95.20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">GroupFace [19] (CVPR20)</cell><cell cols="2">99.85</cell><cell>98.63</cell><cell>98.28</cell><cell>94.93 96.26</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">CircleLoss [33] (CVPR20)</cell><cell cols="2">99.73</cell><cell>96.02</cell><cell>-</cell><cell>-</cell><cell>93.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">DUL [4] (CVPR20)</cell><cell cols="2">99.83</cell><cell>98.78</cell><cell>-</cell><cell>-</cell><cell>94.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">CurricularFace [17] (CVPR20) 99.80</cell><cell>98.37</cell><cell>98.32</cell><cell>94.8</cell><cell>96.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">URFace [32] (CVPR20)</cell><cell cols="2">99.78</cell><cell>98.64</cell><cell>-</cell><cell>-</cell><cell>96.60</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">DB [2] (CVPR20)</cell><cell cols="2">99.78</cell><cell>-</cell><cell>97.90</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Sub-center [7] (ECCV20)</cell><cell cols="2">99.80</cell><cell>98.80</cell><cell>98.31</cell><cell>94.94 96.28</cell></row><row><cell>Method</cell><cell>ID</cell><cell cols="5">GPU BS Mem Speed MFR-All</cell><cell cols="5">BroadFace [20] (ECCV20)</cell><cell cols="2">99.85</cell><cell>98.63</cell><cell>98.38</cell><cell>94.97 96.38</cell></row><row><cell cols="2">Model Parallel 2M</cell><cell>8</cell><cell cols="2">128 18.9</cell><cell>2463</cell><cell>95.35</cell><cell cols="5">BioMetricNet [1] (ECCV20)</cell><cell cols="2">99.80</cell><cell>99.35</cell><cell>96.12</cell><cell>-</cell><cell>-</cell></row><row><cell>HF-Softmax</cell><cell>2M</cell><cell>8</cell><cell cols="2">128 10.7</cell><cell>1034</cell><cell>93.21</cell><cell cols="5">SST [11] (ECCV20)</cell><cell cols="2">99.75</cell><cell>95.10</cell><cell>97.20</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">D-Softmax PFC-0.1 Model Parallel 10M 2M 2M</cell><cell>8 8 8</cell><cell cols="2">128 13.8 128 11.8 16 32.0</cell><cell>1840 3552 502</cell><cell>91.69 96.19 -</cell><cell cols="5">VPL [9] (CVPR21) VirFace [23] (CVPR21) DCQ [21] (CVPR21) Virtual FC [22] (CVPR21)</cell><cell cols="2">99.83 99.56 99.80 99.38</cell><cell>99.11 97.15 98.44 95.55</cell><cell>98.60 -98.23 -</cell><cell>95.56 96.76 88.90 90.54 --67.44 71.47</cell></row><row><cell>PFC-0.1</cell><cell>10M</cell><cell>8</cell><cell>64</cell><cell>14.1</cell><cell>2497</cell><cell>-</cell><cell cols="5">WebFace12M [50] (CVPR21)</cell><cell cols="2">99.83</cell><cell>99.38</cell><cell>98.33</cell><cell>-</cell><cell>97.51</cell></row><row><cell cols="2">Model Parallel 2M</cell><cell>64</cell><cell cols="3">240 30.5 15357</cell><cell>95.46</cell><cell cols="5">WebFace42M [50] (CVPR21)</cell><cell cols="2">99.83</cell><cell>99.38</cell><cell>98.53</cell><cell>-</cell><cell>97.76</cell></row><row><cell>PFC-0.1</cell><cell>2M</cell><cell>64</cell><cell cols="3">240 17.2 23396</cell><cell>96.08</cell><cell cols="5">MC-mini-AMC [46] (ICCV21)</cell><cell>-</cell><cell>96.53</cell><cell>97.25</cell><cell>93.13 95.27</cell></row><row><cell cols="2">Model Parallel 10M PFC-0.1 10M</cell><cell>64 64</cell><cell>72 72</cell><cell>27.2 9.4</cell><cell>4840 17819</cell><cell>--</cell><cell cols="5">WF4M, R100, PFC-0.04 WF4M, R100, PFC-0.3 WF12M, R100, PFC-0.013</cell><cell cols="2">99.83 99.85 99.83</cell><cell>99.06 99.23 99.21</cell><cell>97.52 98.01 97.93</cell><cell>94.91 96.80 95.64 97.22 95.84 97.39</cell></row><row><cell cols="7">Table 6. Large-scale training comparison on WebFace42M and synthetic 10M identities. ResNet100 and V100 GPUs are used</cell><cell cols="5">WF12M, R100, PFC-0.3 WF42M, R100, PFC-0.008 WF42M, R100, PFC-0.3</cell><cell cols="2">99.83 99.83 99.85</cell><cell>99.40 99.32 99.40</cell><cell>98.53 98.27 98.60</cell><cell>96.31 97.58 96.02 97.51 96.47 97.82</cell></row><row><cell cols="7">here. "BS" abbreviates the batch size. The memory is GPU storage</cell><cell cols="5">WF42M, R200, PFC-0.3</cell><cell cols="2">99.83</cell><cell>99.51</cell><cell>98.70</cell><cell>96.57 97.97</cell></row><row><cell cols="7">in GB and the speed is throughput in samples/second.</cell><cell cols="5">WF42M, ViT-B, PFC-0.3 WF42M, ViT-L, PFC-0.3</cell><cell cols="2">99.83 99.83</cell><cell>99.40 99.44</cell><cell>98.53 98.67</cell><cell>96.56 97.90 96.71 98.00</cell></row><row><cell cols="7">sistent better performance than the baseline, indicating ran-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">dom sampling on large-scale datasets is beneficial for both</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">training speed and model's robustness. When the sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">ratio is too small (e.g. around 0.01), there is an obvious per-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">formance drop on MFR-All because (1) the inter-class in-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">teraction is insufficient under a low sampling ratio during</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">training, and (2) trillion-level negative pair comparison dur-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ing testing is very challenging.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">PFC across different network structures.In Tab. 2, we</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">train PFC (r=0.3) on the WebFace42M dataset by using</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">CNN or ViT as the backbone. As can be seen, PFC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">achieves impressive performance across different network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">complexities and ViT-based networks can obtain better per-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">formance than CNN-based networks under the similar com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">putation cost. Specifically, the ViT large (ViT-L) model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">obtains 97.23% TAR@FAR =1e-5 on IJB-C and 97.85%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">TAR@FAR =1e-6 on MFR-All.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Robustness under inter-class conflict.</cell><cell>In Tab. 3,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biometricnet: deep unconstrained face verification through learning of metrics regularized onto gaussian distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain balancing: Face recognition on longtailed domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data uncertainty learning in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Masked face recognition challenge: The insightface track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sub-center arcface: Boosting face recognition by large-scale noisy web faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Alexandros Lattas, and Stefanos Zafeiriou. Variational prototype learning for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-siamese training for shallow face learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Uniformface: Learning deep equidistributed representation for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Softmax dissection: Towards understanding intra-and interclass objective for embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noisetolerant paradigm for training face recognition cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruirui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Curricularface: adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentional feature-pair relation networks for accurate face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bong-Nam</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongjin</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daijin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Groupface: Learning latent groups and constructing group-based representations for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Cheol</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongju</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Broadface: Looking at tens of thousands of people at once for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongju</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic class queue for large scale face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Virtual fullyconnected layer: Training a large-scale face recognition dataset with limited computational resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virface: Enhancing face recognition via unlabeled shallow data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptiveface: Adaptive margin and sampling for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Mixed precision training</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Agedb: The first manually collected in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards universal representation learning for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The devil of face recognition is in the noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liren</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Additive margin softmax for face verification. SPL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Co-mining: Deep face recognition with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mis-classified vector guided softmax loss for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jocelyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIFS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Consistent instance false positive improves fairness in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingkun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accelerated training for massive classification via dynamic class selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adacos: Adaptively scaling cosine logits for effectively learning deep face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">P2sgrad: Refined gradients for optimizing deep face models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengya</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Global-local GCN: Large-scale label noise cleansing for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchao</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive label noise cleaning with meta-supervision for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchao</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Regularface: Deep face recognition via exclusive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unequaltraining for deep face recognition with long-tailed noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianteng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Large-scale bisample learning on id versus spot face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Webface260m: A benchmark unveiling the power of million-scale deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
