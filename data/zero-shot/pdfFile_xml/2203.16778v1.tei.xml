<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjun</forename><surname>Cheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory 3</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Sun</surname></persName>
							<email>sunyipeng@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longchao</forename><surname>Wang</surname></persName>
							<email>wanglongchao@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Zhu</surname></persName>
							<email>zhuxiongwei@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yao</surname></persName>
							<email>yaokun01@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
							<email>chenjie@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory 3</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
							<email>hanjunyu@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
							<email>liujingtuo@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<email>wangjingdong@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Equal Contributions. This work is done when Mengjun Cheng is a research intern at Baidu Inc. ? Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual appearance is considered to be the most important cue to understand images for cross-modal retrieval, while sometimes the scene text appearing in images can provide valuable information to understand the visual semantics. Most of existing cross-modal retrieval approaches ignore the usage of scene text information and directly adding this information may lead to performance degradation in scene text free scenarios. To address this issue, we propose a full transformer architecture to unify these cross-modal retrieval scenarios in a single Vision and Scene Text Aggregation framework (ViSTA). Specifically, ViSTA utilizes transformer blocks to directly encode image patches and fuse scene text embedding to learn an aggregated visual representation for cross-modal retrieval. To tackle the modality missing problem of scene text, we propose a novel fusion token based transformer aggregation approach to exchange the necessary scene text information only through the fusion token and concentrate on the most important features in each modality. To further strengthen the visual modality, we develop dual contrastive learning losses to embed both image-text pairs and fusion-text pairs into a common cross-modal space. Compared to existing methods, ViSTA enables to aggregate relevant scene text semantics with visual appearance, and hence improve results under both scene text free and scene text aware scenarios. Experimental results show that ViSTA outperforms other methods by at least 8.4% at Recall@1 for scene text aware retrieval task. Compared with state-of-the-art scene text free retrieval methods, ViSTA can achieve better accuracy on Flicker30K and MSCOCO while running at least three times faster during the inference stage, which validates the effectiveness of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene text embedding Visual embedding</head><p>Image Image</p><formula xml:id="formula_0">! ? " " &lt; ! ! ! " "</formula><p>(a) Conventional cross-modal retrieval (b) Vision and scene text aggregation <ref type="figure">Figure 1</ref>. Given a text query, two images are close in visual semantics for (a) conventional cross-modal retrieval. By considering visual appearance and scene text information, e.g.,"gummy hotdog", into one framework, (b) the proposed Vision and Scene Text Aggregation (ViSTA) approach enables to distinguish the semantic difference between images I1 and I2 (?2 &lt; ?1), and can be also adapted to conventional scene text free scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As one of the most important multi-modal understanding tasks, cross-modal retrieval attracts much attention due to its valuable applications, e.g., news search and product retrieval. Cross-modal text-to-image retrieval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref> aims to return the most relevant candidate based on the relevance between the text content of a query and the visual appearance of an image. The performance of this retrieval task is largely improved by better visual representation and detailed image-text alignment <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>In recent years, following the success of BERT <ref type="bibr" target="#b6">[7]</ref> in natural language modeling, transformer-based single encoder architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref> are adopted to fuse images and text, and image-text pretraining for fine-tuning becomes the mainstream paradigm in modeling visual-language tasks, significantly boosting the cross-modal retrieval performance. However, these approaches with deep interactions between images and text are orders of magnitudes slower and hence impractical for the large-scale cross-modal retrieval task. As dual-encoder architectures, CLIP <ref type="bibr" target="#b36">[37]</ref>, ALIGN <ref type="bibr" target="#b17">[18]</ref> and WenLan <ref type="bibr" target="#b16">[17]</ref> exploit cross-modal contrastive pre-training by encoding images and text separately, which allows that image and text features can be computed in an offline setting to efficiently calculate similarities between large-scale image-text pairs. Even though the performance of the cross-modal retrieval task is greatly improved by the million-scale image-text contrastive pre-training <ref type="bibr" target="#b36">[37]</ref>, it is still difficult and ineffective to learn specific fine-grained visual concepts, e.g., the scene text semantics from images <ref type="bibr" target="#b36">[37]</ref>. More recently, a new cross-modal retrieval task <ref type="bibr" target="#b30">[31]</ref> is proposed to enable the usage of scene text in an image together with its visual appearance. Specifically, an image in this task is paired with the corresponding scene text features to help to determine the similarity between the query's textual content and the image's visual appearance plus scene text. Benefiting from exploiting additional scene text features, this model can improve the cross-modal retrieval accuracy than those exploiting only visual appearance. Nevertheless, in a real-world image corpus, there are only a fraction of images containing scene text instances. The model designed for the scene text aware retrieval task might fail to generate reliable similarities between the query and images without scene text instances, and can not be adapted to the conventional scene text free retrieval task.</p><p>To overcome this issue, we propose an effective Vision and Scene Text Aggregation (ViSTA) framework to tackle both scene text aware and scene text free cross-modal retrieval tasks. Specifically, ViSTA utilizes a full transformer design to directly encode image patches and fuse scene text embedding to learn an aggregated visual representation. To enforce each modality focusing on its most important features, we propose a novel token based aggregation approach by sharing the necessary scene text information only through the fusion token. To tackle the modality missing problem of scene text, we further develop dual contrastive supervisions to strengthen the visual modality, and embed both image-text pairs and fusion-text pairs into a common cross-modal space. Compared to existing fusion methods, ViSTA enables to aggregate relevant scene text semantics with visual appearance, and hence improve results under both scene text free and scene text aware scenarios.</p><p>The contributions of this paper are three-fold. 1) We propose a full transformer architecture to effectively aggregate vision and scene text, which is applicable in both scene text aware and scene text free retrieval scenarios. 2) We propose a fusion token based transformer aggregation design to exchange the relevant information among visual and scene text features, and dual contrastive losses to en-hance visual features.</p><p>3) The proposed cross-modal retrieval framework can remarkably surpass existing methods for the scene text aware retrieval task and achieve better performance than state-of-the-art approaches on scene text free retrieval benchmarks as well.</p><p>To the best of our knowledge, it is the first time to solve scene text free and scene text aware cross-modal retrieval tasks with a vision and scene text aggregated transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Cross-modal retrieval aims to return relevant images or text descriptions given text or an image query. Most approaches learn a joint cross-modal embedding space to produce closer representation for semantically relevant image and text pairs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33]</ref>. Since the deep learning era, the visual representation for cross-modal retrieval has been consistently improved from grid-based CNN (convolution neural network) <ref type="bibr" target="#b9">[10]</ref> to a pre-trained object detector <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>. In the meantime, finer image-text alignment approaches are developed, e.g., attention mechanisms, iterative matching, and graph-based relationship reasoning between image features and text embedding <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. Most of these approaches rely on RoI (region-of-interest) features extracted from a pre-trained Faster-RCNN detector on the Visual Genome (VG) dataset <ref type="bibr" target="#b20">[21]</ref>, which limits the performance on the out-of-domain visual concepts. By contrast, ViSTA directly takes image patches as the input and builds upon the recent contrastive image-text pre-training paradigm, which is capable of achieving better performance by end-to-end training at a much faster inference speed. Vision language pre-training has become a mainstream paradigm in multi-modal understanding, which can remarkably boost the performance on various vision and language tasks, e.g., cross-modal retrieval and visual question answering (VQA), etc. Most of these approaches utilize transformer based architectures, which can be categorized as single-encoder and dual-encoder pre-training. The single encoder architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49</ref>] are adopted to fuse images and text with the multimodal transformer for interactions, performing high accuracy in various downstream tasks. To speed up the inference stage and adapt to more visual categories, grid-based image features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> and newly proposed patch-based image embedding methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref> are utilized for end-toend training, which directly take image pixels or patches and text embedding as the input. However, the computation cost of these approaches is still huge and impractical for the large-scale cross-modal retrieval task. Instead, dualencoder architectures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref> encode images and text separately, making it possible to calculate similarities of image-text pairs in the linear time complexity. Even though the performance of the cross-modal retrieval task is greatly improved by the million-scale image-text contrastive pre- training <ref type="bibr" target="#b36">[37]</ref>, it is still difficult and ineffective to learn specific fine-grained visual concepts, e.g., the scene text semantics from images <ref type="bibr" target="#b36">[37]</ref>. By contrast, ViSTA incorporates vision and scene text into a full transformer based dualencoder architecture, taking image patches, scene text, and text queries as the input for unified cross-modal retrieval. Scene text in vision and language receives much attention as the extension of previous applications, e.g., text-based image caption <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45]</ref> and Text-VQA <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. All these approaches utilize OCR (optical character recognition) results to form scene text embedding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref>, following the typical architecture of single-stream transformer <ref type="bibr" target="#b28">[29]</ref> with RoI region features. Other works <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b42">[43]</ref> for scene text retrieval tasks aim to return images that contain the query word, and a CNN based fusion approach <ref type="bibr" target="#b0">[1]</ref> integrates scene text and visual appearance to improve the performance for fine-grained image classification in specific scenarios. More recently, StacMR <ref type="bibr" target="#b30">[31]</ref> introduces scene text aware cross-modal retrieval (StacMR) considering scene text as an additional modality, which utilizes GCN (graph convolution network) to obtain context representation of images and scene text for final fusion. Different from all these methods, ViSTA utilizes full transformer blocks to encode image patches and scene text with midlevel fusion, which can be adapted to both scene text aware and scene text free scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The overall architecture of our proposed ViSTA framework is developed as a dual-encoder architecture as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, which makes it practical for large-scale crossmodal retrieval. To achieve a strong feature representation for better retrieval accuracy, we adopt a full transformer design to encode images, scene text, and text query by unimodal encoders, respectively, before feeding them for further aggregation and calculating the cross-modal contrastive losses. The whole model including vision, scene text, and text encoders is end-to-end trainable, which allows better generalization beyond RoI features by cross-modal pretraining <ref type="bibr" target="#b15">[16]</ref> [15] [20] <ref type="bibr" target="#b43">[44]</ref>. In order to fuse visual features with relevant scene text semantics, we propose a fusion token based aggregation approach, which shares the relevant information across these two modalities only through the fusion token. As a result, this token can see all the information at each transformer layer and can be used for fusiontext contrastive learning. Since scene text instances do not often appear in images and in some cases the correlation between scene text and images might be weak in visual semantics. Therefore, to enhance the visual representation rather than over-fit to the noisy scene text features, we also utilize image token at the last layer for effective image-text contrastive learning. With such designs, ViSTA can be effectively adapted to both scene text aware and scene text free retrieval scenarios. . If there is no scene text detected in the image, O can be an empty set as ?. In the scene text aware text-to-image re-trieval task <ref type="bibr" target="#b30">[31]</ref>, the model is required to generate a similarity score S(q, I) between a text query q and each image I based on the relevance of the query's textual content and the image's visual features V together with its scene text features O. In the scene text free text-to-image retrieval task, which is the same as the conventional text-to-image retrieval, scene text instances do not appear in images. Therefore, these images are only sorted by the relevance between the visual appearance and the content of text query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vision and Scene Text Encoders</head><p>Following the success of vision transformer <ref type="bibr" target="#b8">[9]</ref>, the vision encoder directly takes image patches as the input. By slicing an image into multiple patches, a patch sequence P = [p 1 , ? ? ? , p Np ] is used to form a simple linear projection of pixels before feeding into transformers. A positional embedding is added to each patch token to encode the position information. Besides, the embedding of the devised special token [IMG] is inserted in P. The vision encoder is built upon a stack of L v standard transformer layers. Let us denote the input sequence of the l-th vision transformer layer by V l . The output sequence of the l-th layer severs as the input sequence of the next layer, calculated as</p><formula xml:id="formula_1">Y l ? MHSA(LN(V l )) + V l V l+1 ? MLP(LN(Y l )) + Y l ,<label>(1)</label></formula><p>where MHSA(?) denotes the multi-head self-attention layer, MLP(?) denotes the multi-layer perception layer, and LN(?) denotes the layer normalization. The input of the first transformer block, V 1 , is just the patch sequence P. Finally, the output of the last vision transformer layer, V Lv , serves as the visual features V = {v j } Nv j=1 . To be specific, the j-th</p><formula xml:id="formula_2">item in V Lv corresponds to the v j as v j = V Lv [:, j].</formula><p>Similar to the vision encoder, the scene text encoder is a stack of L s standard transformer layers. The input scene text embedding is mainly obtained from the OCR results by Google API <ref type="bibr" target="#b12">[13]</ref> and encoded in tokens. The input token from these OCR results is combined with modal type S type and position embedding S token id as</p><formula xml:id="formula_3">S init = Embedding(o word ) + S type + S token id . (2)</formula><p>Following the previous method in Text-VQA <ref type="bibr" target="#b13">[14]</ref>, the scene text embedding encoded by BERT <ref type="bibr" target="#b6">[7]</ref> can be further combined with the 4-dimensional location information of OCR tokens using normalized bounding box coordinates o bbox and can be formulated as</p><formula xml:id="formula_4">S 0 = BERT(S init ) + F linear (o bbox ),<label>(3)</label></formula><p>where F linear linearly projects the normalized coordinates into the two-dimensional position embedding with the same size as the encoded scene text tokens.  <ref type="figure">Figure 3</ref>. The vision scene text aggregation layer. The fusion token shared between two modalities exchanges the relevant information to learn a scene text aggregated visual representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Vision and Scene Text Aggregation</head><p>Since scene text appearing in images may provide valuable information while in most cases images do not contain any scene text information, the semantic relevance between scene text and visual appearance varies case by case, which might be weak in correlations. Therefore, it is challenging to aggregate these two different modalities into a unified visual representation for effective cross-modal retrieval.</p><p>To handle both scene text aware and scene text free cross-modal retrieval tasks, it is necessary for the visual tower to learn corresponding final features of image modality for matching. Therefore we use different tokens, an image token or a fusion token, to get the final features based on whether the OCR recognition result is none or not in the training stage. In the scene text free scenarios, our visual tower degenerates to a pure vision encoder model as in Sec- As shown in the detailed structure of <ref type="figure">Fig. 3</ref>, the vision scene text aggregation layer is composed of a vision transformer layer and a scene text transformer layer from two encoders. To exchange the relevant information of vision and scene text, the two layers are added with a new token, which is a shared special fusion token <ref type="bibr">[FUS]</ref>. We denote the input image token and scene text token of l-th vision encoder and scene text encoder in the aggregation stage by V l and S l . And the input fusion token of l-th vision and scene text aggregation is denoted by F l . The workflow of the vision transformer layer of Eq. 1 in the aggregation stage is updated to</p><formula xml:id="formula_5">Y l ? MHSA(LN([V l ; F l ])) + [V l ; F l ] [V l+1 ; V FUS ] ? MLP(LN(Y l )) + Y l ,<label>(4)</label></formula><p>where V FUS is the output image feature corresponding to the fusion token. Same is the workflow of the scene text transformer layer in the aggregation stage as where S FUS is the output scene text feature corresponding to the fusion token. The input fusion features of the next layer are calculated by their element-wise summation as shown in <ref type="figure">Fig. 3</ref>, defined as F l+1 = V FUS + S FUS . In this way, visual features V and scene text features S are learned through independent transformer layers, respectively. The special fusion token [FUS] plays the role of the bridge of two encoders as it is shared in two encoders. Due to the vision and scene text aggregation layers, the learning of image features and scene text features is affected by each other by the indirect fusion token. A similar bottleneck attention structure for video classification <ref type="bibr" target="#b33">[34]</ref> fuses video patches and sound by averaging the prediction of the two modalities. Instead of updating shared tokens twice, ViSTA directly adds the predicted fused tokens from vision and scene text transformer layers, forming the fusion token during the aggregation process. To further consider the modality missing problem of scene text, we propose additional imagetext contrastive loss to enhance the visual representation together with the fusion-text contrastive loss. Therefore, both image-text pairs and fusion-text pairs contain the information of visual appearance and share only the relevant part of the information within scene text through shared tokens, which aims to benefit scene text aware cross-modal learning. Fusion feature embedding. We consider the fusion token as another modality and therefore add a different modal type embedding to the randomly initialized [FUS] token embedding, which can be calculated as</p><formula xml:id="formula_6">Y l ? MHSA(LN([S l ; F l ])) + [S l ; F l ] [S l+1 ; S FUS ] ? MLP(LN(Y l )) + Y l ,<label>(5)</label></formula><formula xml:id="formula_7">F 0 = F init + F type + F token id ,<label>(6)</label></formula><p>where F 0 is the first input fusion features of vision and scene text aggregation layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Modal Contrastive Learning</head><p>Conventional scene text free and scene text aware imagetext retrieval are two different tasks calling for visual features only and fused visual-semantic features respectively, which correspond with the output features of [IMG] and [FUS] tokens. The final features are constructed into imagetext pairs or fusion-text pairs with text queries. We introduce dual contrastive learning losses to embed both imagetext pairs and fusion-text pairs into a common cross-modal space. The total loss is</p><formula xml:id="formula_8">L total = ?L itc + (1 ? ?)L f tc ,<label>(7)</label></formula><p>where L itc and L f tc are image-text contrastive loss and fusion-text contrastive loss. Note that ? is the parameter to trade off between these losses and is set to 0.9 as default.</p><p>For N image and text pairs as a batch, the fusion-text contrastive loss aims to maximize the similarity between N matched pairs and minimize the similarity between the last N 2 ? N incorrect pairs, formulated as</p><formula xml:id="formula_9">L f tc = 1 2 (L f 2t + L t2f ).<label>(8)</label></formula><p>The fusion-text contrastive learning aims to minimize the symmetric loss between the fused token and text [CLS] as</p><formula xml:id="formula_10">L f 2t = ? 1 N N i=1 log exp(f i t i /?) N j=1 exp(f i t j /?) L t2f = ? 1 N N i=1 log exp(t i f i /?) N j=1 exp(t i f j /?) ,<label>(9)</label></formula><p>where f i and t j are the normalized embedding of fusion features in the i-th pairs and that of text in the j-th pairs, respectively. The temperature parameter ? is a trainable variable and its initial value is set to 0.07 as default <ref type="bibr" target="#b17">[18]</ref>. Same as L f tc , the image-text contrastive loss is formulated as</p><formula xml:id="formula_11">L itc = 1 2 (L i2t + L t2i ),<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">L i2t = ? 1 N N i=1 log exp(v i t i /?) N j=1 exp(v i t j /?) L t2i = ? 1 N N i=1 log exp(t i v i /?) N j=1 exp(t i v j /?) .<label>(11)</label></formula><p>Note that v i is the normalized embedding of the i-th image.</p><p>In the training stage, if the extracted OCR result is None, the L f tc loss would not be added to the total loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on two downstream crossmodal retrieval benchmarks to validate the effectiveness of the proposed approach. The scene text aware cross-modal retrieval task is evaluated on the COCO-Text Captioned (CTC) <ref type="bibr" target="#b30">[31]</ref> dataset, and the conventional cross-modal retrieval experiments are conducted on the Flickr30K <ref type="bibr" target="#b45">[46]</ref> and MSCOCO <ref type="bibr" target="#b18">[19]</ref> benchmarks, including image-to-text and text-to-image retrieval tasks reported in Tab. 3 and Tab. 5. We also analyze the effectiveness of structures of the proposed ViSTA and show some cases in the ablation study.</p><p>Datasets. All the pre-training, fine-tuning, and test settings of different tasks are reported in Tab. 1. The setting of scene text aware cross-modal retrieval task follows <ref type="bibr" target="#b30">[31]</ref>. In conventional scene text free cross-modal retrieval task, four publicly available datasets including Microsoft COCO (MSCOCO) <ref type="bibr" target="#b26">[27]</ref>, Visual Genome (VG) <ref type="bibr" target="#b20">[21]</ref>, SBU Captions (SBU) <ref type="bibr" target="#b34">[35]</ref>, and Google Conceptual Captions (GCC) <ref type="bibr" target="#b37">[38]</ref> datasets are used for pre-training. Since the CTC dataset is also constructed from MSCOCO, all images of CTC-5K test are contained in MSCOCO train set. Therefore, for evaluation purpose on the CTC dataset, we remove the duplicate images from the MSCOCO dataset and denote it as MSCOCO * . For the evaluation metric, all these experiments are evaluated in terms of the percentage of containing a matched pair in the top returns, i.e., R@1, R@5, and R@10, respectively. Implementation details. For a fair comparison, we implement several versions of models with different scales, as shown in Tab. 2. For all experiments, we use the AdamW optimizer with a base learning rate of 1e-4 and augmentation of random horizontal flipping and random augmentation <ref type="bibr" target="#b15">[16]</ref>. We pre-train for 80 epochs on 40 NVIDIA Tesla V100 GPUs and finetune for another 10 epochs on 8 Tesla V100 GPUs. For scene text free cross-modal retrieval task, we pre-train ViSTA-B and ViSTA-L on combined datasets, i.e., SBU, CC, VG and MSCOCO * for fair comparisons with previous approaches. Note that images in the CTC train set are all included in the train set of MSCOCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Scene Text aware Cross-Modal Retrieval</head><p>For fair comparisons in scene text aware retrieval, we evaluate models on CTC-1K and CTC-5K test sets, respectively, strictly following the previous train and test split <ref type="bibr" target="#b30">[31]</ref>. As shown in Tab. 3, Our ViSTA-S model performs a large improvement of 8.4% and 5.4% on R@1 in scene text aware image-text retrieval task on CTC-1k. Compared to STARNet <ref type="bibr" target="#b30">[31]</ref> which uses GCN to get the representation of scene text for fusion, we use BERT to refine it. And the selfattention operators on vision encoders learn the long-range dependence in images and help our ViSTA model to learn the relationship between patches. The vision and scene text aggregation layers learn the joint distribution of modalities of vision and scene text and refine the representation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scene Text Free Cross-Modal Retrieval</head><p>For conventional image-text retrieval, we measure zeroshot and fine-tuned performance on the Karpathy &amp; Fei-Fei split of MS-COCO and Flickr30K <ref type="bibr" target="#b45">[46]</ref> and compare with state-of-the-art methods in Tab. 4 and Tab. 5, respectively. All the settings are the same as the pre-training stage. When fine-tuning on Flickr30K, we use the fine-tuned weight on COCO-5K as the initial weight. Following the efficient framework of dual-tower and patch projection operator, our model has a comparable speed with ViLT <ref type="bibr" target="#b19">[20]</ref> and better performance, as shown in Tab. 5. And our large-scale model ViSTA-L achieves superior results than state-of-the- art methods at a low speed. Our model is not affected in those datasets when the modality of scene text is missing and still performs well on downstream tasks due to the fusion token based vision and scene text aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>To validate the effectiveness of the proposed vision and scene text aggregation layers for the visual tower, we conduct ablation experiments on the CTC dataset. We fix the text tower with BERT-mini and implement different visions of the visual tower. As shown in Tab. 6, only using scene text information encoded by GCN or BERT-mini is insufficient for cross-modal retrieval. Compare with the architecture in STARNet <ref type="bibr" target="#b30">[31]</ref>, incorporating vision transformer in cross-modal retrieval, e.g., ViT-S, can achieve better performance due to the improved visual representation. Compared with results of only using the visual modality, ViSTA with scene text embedding can remarkably improve the performance by 5.5%/2.1% in R@1 on CTC-1K, which is contributed by the effective vision and scene text aggregation. We also conduct several experiments to validate the ef-  fectiveness of the proposed architecture. Tab. 7 shows that the model can improve the results with an increased number of fusion layers. Tab. 8 shows the results between the proposed fusion token based method, multi-modal transformer with global attention <ref type="bibr" target="#b40">[41]</ref> and cross-attention <ref type="bibr" target="#b28">[29]</ref> as well as late fusion strategy for comparisons. As shown in the Tab. 9, our proposed dual contrastive learning performs better than a single fusion based contrastive loss. Two separate contrastive losses for scene text aware scenarios help to maintain effective cross-modal features when the scene text  <ref type="figure">Figure 4</ref>. Examples of the text-to-image retrieval task for comparisons between results with and without scene text. Note that text queries with the corresponding top returned images are shown in (a) to (l). The first three columns show the retrieved results of ViSTA-S without scene text embedding, and the last three columns show the results of ViSTA-S. (best view in colors).</p><p>(a) Top three returned results by ViSTA:</p><p>1) A man eating a Nathans chili cheese dog in front of an ATM.</p><p>2) A man eats a hot dog at a fast food place.</p><p>3) The guy is eating a doughnut at a doughnut shop.</p><p>Top three returned results of ViSTA w/o scene text:</p><p>1) The guy is eating a doughnut at a doughnut shop.</p><p>2) A man eats a hot dog at a fast food place.</p><p>3) A young man biting a hot dog sitting at a table at a fast food court.</p><p>(b) Top three retrieved results by ViSTA:</p><p>1) A STA LUCIA bus is driving down the road.</p><p>2) A bus sits in the parking lot outside of Piccadilly Gardens.</p><p>3) A charter bus with two stories heading to some where.</p><p>Top three returned results of ViSTA w/o scene text:</p><p>1) A bus pull into a small parking lot space.</p><p>2) A charter bus with two stories heading to some where.</p><p>3) A bus sits in the parking lot outside of Piccadilly Gardens.</p><p>(c) Top three returned results by ViSTA:</p><p>1) The arriving passengers on the Ethiopian airliner are deplaning on the runway.</p><p>2) A China Airlines airliner is parked at an airport near another jet.</p><p>3) A large continental jet sitting on a tarmac at an airport.</p><p>Top three returned results of ViSTA w/o scene text: 1) Commercial Lufthansa air plane parked at an airport.</p><p>2) The arriving passengers on the Ethiopian airliner are deplaning on the runway.</p><p>3) An Aegean Airlines airplane on an airport runway. <ref type="figure">Figure 5</ref>. Examples of image-to-text retrieval for comparisons between the top returned results with and without scene text.</p><p>information is noisy or missing. Qualitative comparisons. For visual comparisons, we also report some examples to illustrate the effectiveness of our method. Our model benefits from the scene text information in learning visual features. As shown in <ref type="figure">Fig. 4</ref>, based on the query of "tennis", "1970", and "1971", our ViSTA model matches the correct images while the ViTSA without scene text embedding retrieves a confusing result. And in the second example, the "gummy hotdog" is perfectly retrieved. For the text retrieval task, shown in <ref type="figure">Fig. 5</ref>, the scene text extracted from images has semantic information and is contained in retrieved results with ViSTA while it does not well without scene text embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Discussions</head><p>We have proposed an effective vision and scene text aggregation transformer to learn a scene text enhanced visual representation for cross-modal learning, unifying conven-tional and scene text aware cross-modal retrieval tasks in a single framework. To handle images where scene text does not appear, we propose a fusion token based aggregation approach, sharing relevant information only through the fusion token, and a dual contrastive learning approach to enhance the visual features as well. Experimental results show the superior performance of ViSTA on both scene text aware retrieval and scene text free retrieval methods, which demonstrates the effectiveness of the proposed framework.</p><p>Note that the proposed approach can be also applied in other vision and language tasks when scene text is necessary as an additional modality. The contribution from scene text aggregation also depends on the percentage of images containing relevant scene text semantics and the correlation between visual appearance and scene text in a specific task.</p><p>Broader impacts. Since the proposed approach can be trained with a large amount of image and text pairs collected from the web, further data analysis, balancing and cleaning</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The proposed Vision and Scene Text Aggregation (ViSTA) framework for cross-modal retrieval. With the proposed fusion token based vision scene text aggregation layer, ViSTA learns a common cross-modal space by a dual-encoder transformer architecture, supervised by dual contrastive losses between image-text pairs and fusion-text pairs, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Problem formulation. Given a set of image and text pairs, the vision and scene text encoder aims to encode an image I and recognize the scene text appearing in this image. The scene text instances contain a set of N o detected words and locations by an OCR model as O = {o word j , o bbox j } No j=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>tion 3.1 and outputs the image feature of [IMG] token as final features. In the scene text aware scenarios, we use the scene text encoder to learn semantic features of scene text. And as shown in Fig 2, our visual tower simply adds L f layers of vision and scene text aggregation layer to make mid-level fusion in image modality and outputs fusion features from extra fusion token [FUS] as final features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Text query: A tennis team was featured in a newspaper in 1970 or 1971.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Dataset split for the evaluation of cross-modal retrieval tasks. Note that * indicates that the CTC-5K test samples have been excluded from the MSCOCO train split. 12 layers, 6 heads BERT-mini 224? 224 ViSTA-B 12 layers, 12 heads BERT-Base 384? 384 ViSTA-L 12 layers, 24 heads BERT-Base 384? 384</figDesc><table><row><cell></cell><cell>Task</cell><cell cols="2">Pre-training</cell><cell>Fine-tuning</cell><cell>Test</cell></row><row><cell></cell><cell>Scene text aware</cell><cell></cell><cell>VG</cell><cell>Flickr30K + TC + CTC train</cell><cell>CTC-1K, 5K</cell></row><row><cell cols="4">Conventional scene text free SBU + GCC + VG + MSCOCO  *</cell><cell>Flickr30K train MSCOCO  *  train</cell><cell>Flickr30K test MSCOCO-5K test</cell></row><row><cell cols="3">Table 2. Model settings at various scales.</cell><cell></cell></row><row><cell>Model</cell><cell>Vision encoder</cell><cell>Scene text encoder</cell><cell>Input size</cell></row><row><cell>ViSTA-S</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparisons with the state-of-the-art scene text aware approaches on CTC. Comparisons with other approaches on Flickr30K and MSCOCO in terms of zero-shot retrieval.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">CTC-1K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CTC-5K</cell></row><row><cell>Model</cell><cell cols="3">Image-to-text</cell><cell></cell><cell cols="2">Text-to-image</cell><cell></cell><cell cols="2">Image-to-text</cell><cell></cell><cell>Text-to-image</cell></row><row><cell></cell><cell cols="11">R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>SCAN [22]</cell><cell cols="2">36.3 63.7</cell><cell>75.2</cell><cell cols="2">26.6 53.6</cell><cell>65.3</cell><cell cols="2">22.8 45.6</cell><cell cols="2">54.3</cell><cell>12.3 28.6</cell><cell>39.9</cell></row><row><cell>VSRN [25]</cell><cell cols="2">38.2 67.4</cell><cell>79.1</cell><cell cols="2">26.6 54.2</cell><cell>66.2</cell><cell cols="2">23.7 47.6</cell><cell cols="2">59.1</cell><cell>14.9 34.7</cell><cell>45.5</cell></row><row><cell cols="3">STARNet [31] 44.1 74.8</cell><cell>82.7</cell><cell cols="2">31.5 60.8</cell><cell>72.4</cell><cell cols="2">26.4 51.1</cell><cell cols="2">63.9</cell><cell>17.1 37.4</cell><cell>48.3</cell></row><row><cell>ViSTA-S</cell><cell cols="2">52.5 77.9</cell><cell>87.2</cell><cell cols="2">36.7 66.2</cell><cell>77.8</cell><cell cols="2">31.8 56.6</cell><cell cols="2">67.8</cell><cell>20.0 42.9</cell><cell>54.4</cell></row><row><cell>Model</cell><cell>Time (ms)</cell><cell cols="10">Flickr30k (1K) Image-to-text Text-to-image R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 MS-COCO (5K) Image-to-text Text-to-image</cell></row><row><cell cols="4">ViL-BERT [29]?900 31.9 61.1</cell><cell>72.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Unicoder-VL [23]?925 64.3 85.8</cell><cell>92.3</cell><cell cols="2">48.4 76.0</cell><cell>85.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">ImageBERT [36]?900 70.7 90.2</cell><cell>94.0</cell><cell cols="2">54.3 79.6</cell><cell>87.5</cell><cell cols="2">44.0 71.2</cell><cell>80.4</cell><cell>32.3 59.0</cell><cell>70.2</cell></row><row><cell cols="4">UNITER-B [5]?900 80.7 95.7</cell><cell>98.0</cell><cell cols="2">66.2 88.4</cell><cell>92.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">ViLT-B [20]?15 73.2 93.6</cell><cell>96.5</cell><cell cols="2">55.0 82.5</cell><cell>89.8</cell><cell cols="2">56.5 82.6</cell><cell>89.6</cell><cell>40.4 70.0</cell><cell>81.1</cell></row><row><cell cols="4">ViSTA-B?17 75.3 93.8</cell><cell>97.5</cell><cell cols="2">59.5 84.3</cell><cell>90.3</cell><cell cols="2">60.7 85.8</cell><cell>92.3</cell><cell>44.8 72.8</cell><cell>82.5</cell></row><row><cell cols="4">ViSTA-L?40 79.2 95.4</cell><cell>98.1</cell><cell cols="2">67.0 88.7</cell><cell>93.1</cell><cell cols="2">63.9 87.1</cell><cell>93.0</cell><cell>47.4 75.0</cell><cell>84.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparisons with state-of-the-art approaches on fine-tuning Flicker30K and MSCOCO benchmarks.</figDesc><table><row><cell>Model</cell><cell>Time (ms)</cell><cell cols="12">Flickr30K (1K) Image-to-text Text-to-image R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 MS-COCO (5K) Image-to-text Text-to-image</cell></row><row><cell>SCAN [22]</cell><cell>-</cell><cell cols="2">67.4 90.3</cell><cell>95.8</cell><cell cols="2">48.6 77.7</cell><cell>85.2</cell><cell cols="2">50.4 82.2</cell><cell>90.0</cell><cell cols="2">38.6 69.3</cell><cell>80.4</cell></row><row><cell>VSRN [25]</cell><cell>-</cell><cell cols="2">71.3 90.6</cell><cell>96.0</cell><cell cols="2">54.7 81.8</cell><cell>88.2</cell><cell cols="2">53.0 81.1</cell><cell>89.4</cell><cell cols="2">40.5 70.6</cell><cell>81.1</cell></row><row><cell>IMRAM [3]</cell><cell>-</cell><cell cols="2">74.1 93.0</cell><cell>96.6</cell><cell cols="2">53.9 79.4</cell><cell>87.2</cell><cell cols="2">53.7 83.2</cell><cell>91.0</cell><cell cols="2">39.7 69.1</cell><cell>79.8</cell></row><row><cell>GSMN [28]</cell><cell>-</cell><cell cols="2">76.4 94.3</cell><cell>97.3</cell><cell cols="2">57.4 82.3</cell><cell>89.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SGRAF [8]</cell><cell>-</cell><cell cols="2">77.8 94.1</cell><cell>97.4</cell><cell cols="2">58.5 83.0</cell><cell>88.8</cell><cell>57.8</cell><cell>-</cell><cell>91.6</cell><cell>41.9</cell><cell>-</cell><cell>81.3</cell></row><row><cell cols="4">Vil-BERT [29]?920 58.2 84.9</cell><cell>91.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Unicoder-VL [23]?925 86.2 96.3</cell><cell>99.0</cell><cell cols="2">71.5 91.2</cell><cell>95.2</cell><cell cols="2">62.3 87.1</cell><cell>92.8</cell><cell cols="2">48.4 76.7</cell><cell>85.9</cell></row><row><cell cols="4">UNITER-B [5]?900 85.9 97.1</cell><cell>98.8</cell><cell cols="2">72.5 92.4</cell><cell>96.1</cell><cell cols="2">64.4 87.4</cell><cell>93.1</cell><cell cols="2">50.3 78.5</cell><cell>87.2</cell></row><row><cell cols="4">ERNIE-ViL-B [47]?920 86.7 97.8</cell><cell>99.0</cell><cell cols="2">74.4 92.7</cell><cell>95.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VSEinfty [4]</cell><cell></cell><cell cols="2">88.4 98.3</cell><cell>99.5</cell><cell cols="2">74.2 93.7</cell><cell>96.8</cell><cell cols="2">66.4 89.3</cell><cell>-</cell><cell cols="2">51.6 79.3</cell><cell>-</cell></row><row><cell>PCME [6]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">44.2 73.8</cell><cell>83.6</cell><cell cols="2">31.9 62.1</cell><cell>74.5</cell></row><row><cell>Miech et al [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">72.1 91.5</cell><cell>95.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>12-in-1 [30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">67.9 89.6</cell><cell>94.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Pixel-BERT-X [16]?160 87.0 98.9</cell><cell>99.5</cell><cell cols="2">71.5 92.1</cell><cell>95.8</cell><cell cols="2">63.6 87.5</cell><cell>93.6</cell><cell cols="2">50.1 77.6</cell><cell>86.2</cell></row><row><cell>SOHO [15]</cell><cell>-</cell><cell cols="2">86.5 98.1</cell><cell>99.3</cell><cell cols="2">72.5 92.7</cell><cell>96.1</cell><cell cols="2">66.4 88.2</cell><cell>93.8</cell><cell cols="2">50.6 78.0</cell><cell>86.7</cell></row><row><cell>H Xue et al. [44]</cell><cell>-</cell><cell cols="2">87.0 98.4</cell><cell>99.5</cell><cell cols="2">73.5 93.1</cell><cell>96.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Pixel-BERT-R [16]?60 75.7 94.7</cell><cell>97.1</cell><cell cols="2">53.4 80.4</cell><cell>88.5</cell><cell cols="2">59.8 85.5</cell><cell>91.6</cell><cell cols="2">41.1 69.7</cell><cell>80.5</cell></row><row><cell cols="4">ViLT-B [20]?15 83.5 96.7</cell><cell>98.6</cell><cell cols="2">64.4 88.7</cell><cell>93.8</cell><cell cols="2">61.5 86.3</cell><cell>92.7</cell><cell cols="2">42.7 72.9</cell><cell>83.1</cell></row><row><cell cols="4">ViSTA-B?17 84.8 97.4</cell><cell>99.0</cell><cell cols="2">68.9 91.1</cell><cell>95.1</cell><cell cols="2">63.9 87.8</cell><cell>93.6</cell><cell cols="2">47.8 75.8</cell><cell>84.5</cell></row><row><cell cols="4">ViSTA-L?40 89.5 98.4</cell><cell>99.6</cell><cell cols="2">75.8 94.2</cell><cell>96.9</cell><cell cols="2">68.9 90.1</cell><cell>95.4</cell><cell cols="2">52.6 79.6</cell><cell>87.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on the impact of modality aggregation.</figDesc><table><row><cell>Model</cell><cell>Visual</cell><cell>Scene text</cell><cell>CTC-1K Image-to-text Text-to-image R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell>10.8 20.2 25.4 4.4 11.3 15.6</cell></row><row><cell>BERT-mini</cell><cell></cell><cell></cell><cell>24.3 35.4 40.8 9.6 17.8 22.6</cell></row><row><cell>RoI + GCN [31]</cell><cell></cell><cell></cell><cell>44.1 74.8 82.7 31.5 60.8 72.4</cell></row><row><cell>ViT-S + GCN</cell><cell></cell><cell></cell><cell>47.2 74.2 84.2 33.2 63.6 75.4</cell></row><row><cell>ViSTA-S</cell><cell></cell><cell></cell><cell>47.0 73.8 84.3 34.6 63.4 75.3</cell></row><row><cell>ViSTA-S</cell><cell></cell><cell></cell><cell>52.5 77.9 87.2 36.7 66.2 77.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Ablation study on the number of fusion layers.</figDesc><table><row><cell>The number of fused layers</cell><cell>CTC-1K R@1 R@5 R@10 R@1 R@5 R@10 Image-to-text Text-to-image</cell></row><row><cell>L f = 1</cell><cell>48.2 74.3 85.0 35.6 64.8 76.8</cell></row><row><cell>L f = 2</cell><cell>52.2 77.0 86.3 35.4 64.8 76.2</cell></row><row><cell>L f = 4</cell><cell>52.5 77.9 87.2 36.7 66.2 77.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Ablation study on the impact of various fusion strategies. Ablation study on the impact of loss functions.</figDesc><table><row><cell>Fusion strategies</cell><cell cols="2">CTC-1K R@1 R@5 R@10 R@1 R@5 R@10 Image-to-text Text-to-image</cell></row><row><cell cols="3">Global attention 48.4 75.5 86.5 34.7 64.3 76.2</cell></row><row><cell cols="3">Cross attention 50.5 74.4 84.1 31.1 59.8 72.9</cell></row><row><cell>Fusion token</cell><cell cols="2">52.5 77.9 87.2 36.7 66.2 77.8</cell></row><row><cell>Late fusion</cell><cell cols="2">49.2 73.4 85.8 34.9 65.0 76.7</cell></row><row><cell></cell><cell>CTC-1K</cell><cell></cell></row><row><cell>L f tc L itc</cell><cell>Image-to-text</cell><cell>Text-to-image</cell></row><row><cell></cell><cell cols="2">R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell></cell><cell cols="2">46.6 71.3 82.4 30.3 58.7 71.4</cell></row><row><cell></cell><cell cols="2">52.5 77.9 87.2 36.7 66.2 77.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>should be taken in production to mitigate the negative social impacts caused by distribution bias and mislabeled data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integrating scene text and visual appearance for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="66322" to="66335" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scene text visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rub?n</forename><surname>Ali Furkan Biten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Tito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>Mafla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?mez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Rusi?ol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4290" to="4300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">IMRAM: iterative matching with recurrent attention memory for cross-modal image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning the best pooling strategy for visual semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<idno>2021. 7</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="15789" to="15798" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UNITER: universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (30)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">12375</biblScope>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic embeddings for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Sampaio De Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
		<idno>2021. 7</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="8415" to="8424" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>NAACL-HLT (1)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity reasoning and filtration for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR. OpenReview.net</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VSE++: improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single shot scene text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Mafla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?al</forename><surname>Rusi?ol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (14)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="728" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cloud Vision API</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06-03" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Trevor Darrell, and Marcus Rohrbach. Iterative answer prediction with pointeraugmented multimodal transformers for textvqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="9989" to="9999" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seeing out of the box: End-toend pre-training for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE, 2021. 1, 2, 3</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongzheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Yang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<title level="m">Wenlan: Bridging vision and language by largescale multi</title>
		<imprint/>
	</monogr>
	<note>modal pre-training. CoRR, abs/2103.06561, 2021. 2</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="664" to="676" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vilt: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (4)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="212" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno>abs/2107.07651</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4653" to="4661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (30)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12375</biblScope>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (5)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph structured network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="10918" to="10927" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno>2020. 7</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="10434" to="10443" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacmr: Scene-text aware cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Mafla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Sampaio De Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2219" to="2229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Thinking fast and slow: Efficient text-to-visual retrieval with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>2021. 7</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="9826" to="9836" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop Poster)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Sacheti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Textcaps: A dataset for image captioning with reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12347</biblScope>
			<biblScope unit="page" from="742" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno>2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
	<note>Towards VQA models that can read</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">VL-BERT: pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LXMERT: learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5099" to="5110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene text retrieval via joint text detection and similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenggao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Probing intermodality: Visual parsing with self-attention for visionlanguage pre-training. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">TAP: text-aware pre-training for text-vqa and textcaption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinei</forename><surname>Flor?ncio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="8751" to="8761" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ernie-vil: Knowledge enhanced visionlanguage representations through scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3208" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Beyond OCR + VQA: involving OCR into the flow for robust and accurate textvqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangyan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simple is not easy: A simple strong baseline for textvqa and textcaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="3608" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
