<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Ting</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Center for Biotechnology Information (NCBI)</orgName>
								<orgName type="institution">National Library of Medicine (NLM)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Center for Biotechnology Information (NCBI)</orgName>
								<orgName type="institution">National Library of Medicine (NLM)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>National Institutes of Health (NIH), Bethesda, MD, 20894 USA. *To whom correspondence should be addressed. Contact: zhiyong.lu@nih.gov</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A biomedical relation statement is commonly expressed in multiple sentences and consists of many c oncepts, including gene, disease, chemical, and mutation. To automatically extract information from biomedical literature, existing biomedical text-mining approaches typically formulate the problem as a cross-sentence n-ary relation-extraction task that detects relations among n entities across multiple s entences, and use either a graph neural network (GNN) with long short-term memory (LSTM) or an a ttention mechanism. Recently, Transformer has been shown to outperform LSTM on many natural la nguage processing (NLP) tasks. In this work, we propose a novel architecture that combines Bidirecti onal Encoder Representations from Transformers with Graph Transformer (BERT-GT), through inte grating a neighbor-attention mechanism into the BERT architecture. Unlike the original Transformer architecture, which utilizes the whole sentence(s) to calculate the attention of the current token, the n eighbor-attention mechanism in our method calculates its attention utilizing only its neighbor tokens.</p><p>Thus, each token can pay attention to its neighbor information with little noise. We show that this is c ritically important when the text is very long, as in cross-sentence or abstract-level relation-extraction tasks. Our benchmarking results show improvements of 5.44% and 3.89% in accuracy and F1-measu re over the state-of-the-art on n-ary and chemical-protein relation datasets, suggesting BERT-GT is a robust approach that is applicable to other biomedical relation extraction tasks or datasets .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(T790M). Although, in the second example, the disease "polyneuropathy" and the chemical "ITcytosine arabinoside (ara-C)" do not co-occur in the same sentence, a relation is asserted between the two entities. Extracting such a relation without gathering information across sentences and multiple entities makes it difficult to predict correctly the relation in both cases.</p><p>Over time, the above problem has received the attention of the natural language processing (NLP) field. In 2015, a abstract-level relation extraction dataset was created for an NLP task on chemicaldisease relation (CDR) extraction <ref type="bibr">(Wei, et al., 2016)</ref>. In the dataset, more than 30% of chemicalinduced disease pairs are cross-sentences <ref type="bibr" target="#b25">(Xu, et al., 2016)</ref>. Many cross-sentence methods have been proposed to identify these relations. <ref type="bibr" target="#b22">Verga, et al. (2018)</ref> proposed a Bi-affine Relation Attention Network (BRAN), which is designed to extract relations between entities in the biological text without requiring explicit mention-level annotation. Their neural network (NN) architecture, however, is limited in terms of binary entities and cannot be adapted to n-ary entities. <ref type="bibr" target="#b13">Peng, et al. (2017)</ref> developed a cross-sentence n-ary dataset for detecting drug-gene-mutations across sentences.</p><p>The cross-sentence n-ary relation extraction task is understood as detecting relations among n entities across multiple sentences. <ref type="bibr" target="#b13">Peng, et al. (2017)</ref> further proposed a graph long short-term memory network (graph LSTM) method.</p><p>For a cross-sentence n-ary relation extraction task, previous methods typically utilize dependency information by incorporating long short-term memory (LSTM) or an attention mechanism into a graph neural network (GNN). In recent year, <ref type="bibr" target="#b21">Vaswani, et al. (2017)</ref> propose an encoder-decoder architecture called Transformer, without the use of LSTM or convolutional neural networks (CNN), and they demonstrate that the Transformer outperforms recurrent neural networks (RNNs) with attention to many sequence-to-sequence tasks. Further, <ref type="bibr" target="#b2">Devlin, et al. (2019)</ref> used Transformer architecture to develop BERT and proposed two approaches to generate the non-task-specific pretrained model. They demonstrate that the pre-trained model can be used for different tasks, with transfer-learning approaches, and can outperform state-of-the-art approaches on many NLP tasks.</p><p>Because the self-attention mechanism of the Transformer can efficiently utilize the information from the whole input text, we consider that BERT can be used to classify cross-sentence relations as well. We posit, however, that the cross-sentence also may bring noisy information to BERT and can result in difficulties in focusing on explicit information.</p><p>In this work, we propose a novel model that adds a Graph Transformer (GT) architecture into BERT (BERT-GT). The Graph Transformer uses a neighbor-attention mechanism, which is a modified version of the self-attention mechanism. In the self-attention mechanism, the whole sentence(s) is used to calculate the attention of the current token. In contrast, the neighbor-attention mechanism calculates its attention based on its neighbor tokens only. Thus, the token can pay attention to its neighbor tokens with limited noise, which is especially important when the text is very long, as in multiple sentences. The main contributions are as follows: <ref type="bibr" target="#b28">1)</ref> In this work, we focus on classifying the biomedical relations of different text lengths, in particular relations across multiple sentences, an issue that is common in biomedical text but not well studied. In addition, both binary and n-ary relation classification tasks are considered.</p><p>2) While BERT is a robust and state-of-the-art method, its performance is weakened when processing cross-sentence with unrelated relation keywords. In response, we propose a novel architecture that improves BERT by integrating a neighbor-attention mechanism in a Graph Transformer.</p><p>3) BERT-GT is evaluated on two independent biomedical benchmarks and our experimental results demonstrate a significant improvement over other state-of-the-art methods on both n-ary and CDR datasets. This demonstrates the generalizability and robustness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section introduces recent works on the cross-sentence relation extraction benchmark and then gives a brief introduction of a recent graph transformer architecture. <ref type="bibr" target="#b13">Peng, et al. (2017)</ref>'s dataset is the most commonly used dataset for the cross-sentence nary relation extraction. The BioCreative CDR dataset <ref type="bibr">(Wei, et al., 2016)</ref> is also widely used to evaluate abstract-level relation extraction. In what follows, we review the recent work on these benchmarks. <ref type="bibr" target="#b13">Peng, et al. (2017)</ref> proposed a graph LSTM architecture. They first use the NLTK dependency parser to parse text input into a directed dependency graph, and then the graph is divided into two acyclicdirected graphs (left-to-right and right-to-left). Then, they respectively apply two LSTM layers to the two graphs to generate the output sequences. Finally, the prediction is calculated by aggregating the hidden states of the entities with a softmax function. Their architecture achieves an accuracy of 80.7% and outperforms the feature-based classifier <ref type="bibr">(Quirk and Poon, 2017)</ref> by an accuracy of 3%. <ref type="bibr" target="#b19">Song, et al. (2018)</ref> then proposed a graph state LSTM (GS LSTM) architecture for the task. They consider that there are two limitations in <ref type="bibr" target="#b13">Peng, et al. (2017)</ref>'s architecture. First, <ref type="bibr" target="#b13">Peng, et al. (2017)</ref> can use only the information from a single direction (either left-to-right or right-to-left) in each LSTM layer. Second, <ref type="bibr" target="#b13">Peng, et al. (2017)</ref> do not utilize the dependency edge type. Therefore, <ref type="bibr" target="#b19">Song, et al. (2018)</ref>'s GS LSTM uses a message-passing mechanism where each token can pass the message to itself in the next LSTM layer, and all hidden states of its dependency words can be passed as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cross-sentence n-ary relation extraction</head><p>To address the second problem, they proposed a variation of the LSTM layer, which allows inputs to be weighted according to edge types. GS LSTM achieved an accuracy of 83.2%, which outperforms <ref type="bibr" target="#b13">Peng, et al. (2017)</ref>'s architecture by 2.5%.</p><p>Most recently, <ref type="bibr" target="#b5">Guo, et al. (2019)</ref> introduced an attention-guided graph convolutional networks (AGGCN) model. They consider that the structure of the dependency graph that limits the token can be updated only by its edges. Therefore, they proposed an attention-guided layer to transform the dependency graph, with the word embedding added into the weighted fully-connected matrix, where each cell represents the strength of the edge. They, then proposed a densely connected layer to allow each token to receive a message (hidden state) from all tokens of the previous sub-layer. Finally, <ref type="bibr" target="#b5">Guo, et al. (2019)</ref> aggregate the sentence representation and entity representation to predict the relation type. The AGGCN achieved an accuracy of 87.0% and outperformed <ref type="bibr" target="#b19">Song, et al. (2018)</ref>'s architecture by 3.8%. However, we also found that <ref type="bibr" target="#b19">Song, et al. (2018)</ref>'s open-source implementation has a significant improvement over their original results (see our Experiment section). <ref type="bibr" target="#b25">Xu, et al. (2016)</ref> proposed a Support Vector Machine (SVM)-based approach to classify CDR that involves two feature-based classifiers: sentence-level and abstract-level. Their system achieved an F1-measure of 50.73%. <ref type="bibr" target="#b27">Zhou, et al. (2016)</ref> proposed an ensemble approach for the CDR task. They developed three classifiers: tree-kernel-based, feature-based, and LSTM-based. Their system achieved an F1-measure of 56%. In addition, they proposed some corpus-specific post-processing rules for boosting performance to 61.31% in F1-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Abstract-level chemical-induced disease relation extraction</head><p>In 2018, <ref type="bibr" target="#b9">Li, et al. (2018)</ref> introduced recurrent piecewise convolutional neural networks (RPCNN).</p><p>In their formulation, a candidate means a unique CDR-ID pair, and an instance means a CDR-NE pair. Therefore, one candidate can have multiple instances. They also proposed that the piecewise convolutional neural networks (PCNN) represent the instances of the same candidate as a means to predict the relation type. The recurrent neural network is used to aggregate the hidden states of the instances from the same candidate to predict the relation type. Their architecture achieved an F1measure of 59.1%.</p><p>Recently, <ref type="bibr" target="#b22">Verga, et al. (2018)</ref> proposed Bi-affine Relation Attention Networks (BRANs). They used the Transformer architecture to encode the input text, and the output sequence of the Transformer is passed into two separate multi-layer perceptrons (MLP): head MLP and tail MLP.</p><p>The first entity representation is selected from the output of the head MLP, and the second entity representation is selected from the output of the tail MLP. <ref type="bibr" target="#b22">Verga, et al. (2018)</ref> used a bi-affine function to calculate the correlation between two entity representations and update the hidden sequence. Finally, they aggregate the entity representations of the hidden sequence to calculate the softmax of relation type. Their architecture achieved an F1-measure of 62.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Transformer</head><p>Cai and Lam (2020) proposed a graph transformer architecture for the tasks of the abstract meaning representation and the syntax-based machine translation. They proposed a relation-enhanced global attention mechanism that employed gated recurrent unit (GRU) for learning the relation represent of two nodes, then append the mechanism to the self-attention layer. However, their method does not take generalized pre-trained weights learned with large datasets. Instead, we use the original Transformer and our Graph Transformer (GT) simultaneously. By doing so, the original Transformer part can readily use the pre-trained weights, and only pre-trained word embeddings are needed for the GT part. Therefore, our BERT-GT model is easier to adapt to different languages or text compared with <ref type="bibr">Cai and Lam (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we introduce BERT-GT and its detailed implementation. The BERT-GT architecture is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, and can be seen as having four parts: (1) input representations for two Transformers, (2) Transformer, (3) Graph-Transformer, and (4) layers for aggregating the output states of two Transformers.</p><p>The input representations are a tokenized paragraph and its directed graph. The Transformer is the same as BERT's Transformer, and we take it from BERT, which allows BERT-GT to reuse the pretrained weights from <ref type="bibr" target="#b6">Lee, et al. (2019)</ref>. GT uses an architecture similar to that of the Transformer but has two modifications. First, the input of GT requires the neighbors' positions for each token.</p><p>Second, the self-attention mechanism is replaced with a neighbors-attention mechanism, whereby each token's output value is calculated by only its neighbor tokens. Finally, we aggregate the hidden states of two Transformers and use the softmax function to calculate the probability of each label.</p><p>Here, we describe the problem formulation for the cross-sentence n-ary and the CDR tasks, and then we introduce BERT-GT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem formulation</head><p>In our formulation, we assume the text T and entities E are given. Although an input text can be of any arbitrary length, cross-sentence relations are commonly seen within paragraphs instead of across paragraphs or in multiple sections/chapters. Similar to the previous works, we only evaluated BERT-GT on PubMed abstracts. Accordingly, the length of the input text T is within the maximum of an abstract length. Further, an entity can appear more than once in the text and can have more than one ID. Thus, we expand the entity by its IDs. If an entity with many IDs, we will expand the entity into multiple entities, and each has a unique ID. In other words, if an entity of a training/test instance has two IDs, then we will expand it into two training/test instances. Each entity in an instance has a unique ID after the expansion. We follow the problem definition of <ref type="bibr" target="#b5">Guo, et al. (2019)</ref>. The classification problem is defined as whether a relation R holds for the text and an entity subset ?.</p><p>In the cross-sentence n-ary task, we assume that given a paragraph that contains a variant/mutation , a gene , and a drug . The classification problem checks whether holds for the ( , , ) triple.</p><p>In the abstract-level chemical-induced disease classification task, a chemical and a disease can appear in a abstract multiple times. Each and can have one or more chemical ID ( ) and disease ID ( ). The classification task can be defined as checking whether holds for the ( , ) pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The graph transformer for the BERT</head><p>As noted, BERT-GT is illustrated in <ref type="figure" target="#fig_0">Fig 2.</ref> The Transformer part is adapted from the BERT model <ref type="bibr" target="#b15">(Peng, et al., 2019)</ref> and allows us to use the pre-trained model from a large set of biomedical literature. The input of the Transformer is BERT's preprocessed word embeddings <ref type="bibr" target="#b2">(Devlin, et al., 2019)</ref> of the text , where denotes the -th entity, and the same entity can appear in more than once. For example, in <ref type="figure" target="#fig_0">Fig. 2</ref>, appears twice in the . The input of GT consists of two representations. One is BERT's preprocessed word embeddings, which is the same as the Transformer part. Another is the neighbors of each token. For the n-ary dataset, we use the edges provided by the original dataset, and these edges are generated using the NLTK dependency parser and some heuristic rules. These edges are utilized in many of the previous methods. Because CDR does not provide dependency edges, we use ScispaCy <ref type="bibr" target="#b12">(Neumann, et al., 2019)</ref> to parse the paragraph into dependency trees, and the headword and the adjacent two words of each token are treated as neighbors. Notably, many studies <ref type="bibr" target="#b11">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b14">Peng, et al., 2018;</ref><ref type="bibr" target="#b26">Xu, et al., 2015)</ref> show that the use of the shortest path between entities can improve the relation classification. Therefore, for each entity, we also use the headwords between the entity with the other entities as its neighbors.</p><p>The architectures of the Transformer <ref type="bibr" target="#b21">(Vaswani, et al., 2017)</ref> and GT are illustrated in <ref type="figure">Fig. 3</ref>. GT replaces the self-attention layer of Transformer with the neighbor-attention layer, and and denote the number of gray block layers.</p><p>In the multi-head self-attention mechanism, each input token representation is divided into subrepresentations of the input token, and is the number of heads. Assume that the size of the input token representation is ?, and the size of input token representation for each attention is ? divided by . Here we use ?? to denote the size of input token for each attention in <ref type="figure" target="#fig_1">Fig. 4</ref>, which also illustrates that self-attention is neighbors-attention. Each is transformed into , , and by multiplying the learnable weighted matrices of , , and . The blue vectors of denote the neighbors of 1 . Self-attention calculates 1 by dividing the summation of 1 , multiplying by in the text. Although the text length is longer, 1 may suffer from the noisy message. In contrast, we propose a neighbors-attention mechanism, whereby we use only the neighbors of 1 to calculate the neighbors-attention value 1 . As illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>, the vectors in blue represent the neighbors of . <ref type="figure">Fig. 3</ref>. Transformer and Graph Transformer architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The output layer of the BERT-GT</head><p>For the classification task, we obtain the sentence representation of the Transformer and the entity representation of GT. The same entity in different positions is aggregated by using the mean average, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. We then concatenate the mean averages of different entities to form the entity representation. We also evaluate the effect of using [CLS] for the graph transformer (see our Supplementary Materials). Both sentence representation, for which we choose the output state of the Transformer's first token, and entity representation are passed into the linear transformation layer and follow the dense and softmax layers for the classification. The dimension of the output layer is the same as the number of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>To test the generalizability of our aproach, we evaluate our method on two independent tasks: <ref type="formula">(1)</ref> Peng, et al. (2017)'s n-ary dataset for the cross-sentence n-ary relation extraction task; and <ref type="formula">(2)</ref>  For both tasks, we compare with four state-of-the-art models: (1) BERT model <ref type="bibr" target="#b2">(Devlin, et al., 2019)</ref>; <ref type="formula">(2)</ref> BlueBERT model <ref type="bibr" target="#b15">(Peng, et al., 2019)</ref>; <ref type="formula">(3)</ref> GS LSTM model <ref type="bibr" target="#b19">(Song, et al., 2018)</ref>; and <ref type="formula">(4)</ref> AGGCN model <ref type="bibr" target="#b5">(Guo, et al., 2019)</ref>, through directly using their online source code. Besides, we evaluate the performances of only using graph transformer (GT) architecture. Note that we do not run the state-of-the-art CDR systems on n-ary due to two main constraints. First, most CDR systems <ref type="bibr" target="#b9">(Li, et al., 2018;</ref><ref type="bibr" target="#b25">Xu, et al., 2016;</ref><ref type="bibr" target="#b27">Zhou, et al., 2016)</ref> are not publicly available for retraining on new datasets. Second, the CDR task contains only the relation between two entities. Therefore the features/architectures of these systems cannot be directly used to classify the relation of n entities. In contrast, the n-ary systems can be adapted to classify the relation of two entities in abstract-length text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>The n-ary datasets involve 6,987 drug-gene-mutation (ternary) relation instances and 6,087 drugmutation (binary) relation instances, and each instance is categorized into one of the five classes:</p><p>"resistance," "response," "resistance or nonresponse," "sensitivity," and "none." Following Peng, et al. (2017)'s experiment settings, we also considered the two-class classification task, whereby the class "none" is treated as "no" and the remaining classes are treated as "yes." There are, however, two limitations to the n-ary datasets. First, although the n-ary dataset is cross-sentence, the number of sentences is limited to three per instance. Second, the numbers of instances with "no" and "yes" labels are balanced but are not representative of actual biomedical text. Therefore, we also used the BioCreative CDR <ref type="bibr">(Wei, et al., 2016)</ref> dataset for the evaluation.</p><p>The BioCreative CDR dataset consists of 1500 PubMed abstracts and is annotated with 4409 chemicals, 5818 diseases, and 3116 chemical-disease relations. The annotations include the spans of entities and their corresponding MeSH IDs. The relations between chemicals and diseases were annotated at the abstract-level, which gives only the chemical-disease MeSH ID pair. In the CDR task, 1000 annotated abstracts are released for participants, and 500 annotated abstracts are used as the test set. Previous work <ref type="bibr" target="#b25">(Xu, et al., 2016)</ref> shows that approximately 30% of positive CDR instances are cross-sentence relations. The sizes and comparison of the two datasets are summarized in <ref type="table" target="#tab_3">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation metrics</head><p>Previous works used different evaluation metrics for the n-ary nnd CDR tasks. To compare our method with earlier methods, we follow their evaluation metrics for the two tasks. For the n-ary dataset, we use five-fold cross-validation and report the average test accuracy. The partitions of the five-fold cross-validation are the same as those of prior work <ref type="bibr" target="#b13">(Peng et al., 2017)</ref>. Similar to <ref type="bibr" target="#b19">Song, et al. (2018)</ref> and <ref type="bibr" target="#b5">Guo, et al. (2019)</ref>, we randomly selected 200 instances from the non-test fold as a held-out set for validation and tuning of hyperparameters. For the CDR dataset, we report the precision, recall, and F1-measure on the test set, as the corpus is already divided into the training, <ref type="table" target="#tab_3">Table 1</ref>. Sizes of the -ary and the CDR datasets. "dgm" means "drug-gene-mutation"; "dm" means "drug-mutation" n-ary CDR NE types drug, gene, mutation chemical, disease Normalization None MeSH ID Relation type "resistance or nonresponse", "sensitivity", "response", "resistance," and "none" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results of the cross-sentence n-ary dataset</head><p>The performance of the previous state of the art and all comparing methods is shown in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>"Single" means that we report the accuracy of instances within single sentences, while "All" means the accuracy of all instances. It is of note that the released version of the GS LSTM shows better performance than what was reported in <ref type="bibr" target="#b19">Song, et al. (2018)</ref>. Its performance is also higher than that of AGGCN, especially on the multi-class drug-gene-mutation evaluation metric. Besides, we compare with the pure GT method that removes the Transformer from our BERT-GT. GT scores significantly higher than AGGCN on multi-class, but lower than GS GLSTM. The lower performance might be because GT weights the neighbors by using the neighbor-attention mechanism, and does not utilize the edge type information which is used in GS GLSTM. <ref type="table" target="#tab_4">Table 2</ref> shows that our BERT-GT method outperforms all of the previous methods. Compared with AGGCN, our method outperforms it by accuracies of 5.07% and 11.73% on two-class and multiclass drug-gene-mutation evaluations, respectively. According to our observation of the error cases that are wrong in the AGGCN but are correct in BERT-GT, the relation information of these cases is less explicit. For example, "On the other hand, erlotinibDRUG could not inhibit EGFRGENE phosphorylation in H1975 cells because the T790MMUTATION mutation in EGFR causes a <ref type="table" target="#tab_4">Table 2</ref>. Accuracy of different methods on the n-ary test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drug-Mutation Drug-Gene-Mutation Two-class</head><p>Multiclass Two-class Multiclass Single All All Single All All Logistic regression <ref type="bibr" target="#b13">(Peng, et al., 2017)</ref> 73.90 75.20 -74.70 77.70 -Graph LSTM EMBED <ref type="bibr">(Peng, et al., 2017) 74.30 76.50 -76</ref>.50 80.60 -Graph LSTM FULL <ref type="bibr" target="#b13">(Peng, et al., 2017)</ref> 75.60 76.70 -77.90 80.70 -GS GLSTM <ref type="bibr" target="#b19">(Song, et al., 2018)</ref> 88.15 88.56 86.92 82.86 87.60 85.91 AGGCN <ref type="bibr" target="#b5">(Guo, et al., 2019)</ref> 85  <ref type="bibr" target="#b2">(Devlin, et al., 2019)</ref> 88.59 91.01 90.03 83.75 90.80 90.73 BlueBERT <ref type="bibr" target="#b15">(Peng, et al., 2019)</ref> 89 conformation change at the ATP binding pocket, thus decreasing the affinity between erlotinib and EGFR." The above case was predicted as "None" because both "not" and "inhibit" indicate a negative relationship. If we take both terms into consideration, however, this indicates a positive relationship: ~26% of the cases contain negation words, "not" and "no."</p><p>In addition, most methods show lower performances on multi-class evaluation. GS GLSTM,</p><p>BlueBERT, and our BERT-GT method, however, show that their multi-class evaluation can retain the same level of performance as do their two-class evaluations. In addition, our method outperforms</p><p>BlueBERT on the drug-mutation relation classification by 1.17% and 1.29% on two-class and multiclass, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on the CDR dataset</head><p>The performance of our method and recent methods are shown in <ref type="table">Table 3</ref>. Here, we report only the performance of systems without using domain knowledge in order to compare methods that can be more generalizable. For the same reason, we report their performance that involves no postprocessing or ensemble. <ref type="table">Table 3</ref> shows that BERT-GT outperforms all of the other methods. In fact,</p><p>BERT-GT compares favorably to others even when they use additional post-processing methods or ensemble <ref type="bibr" target="#b4">(Gu, et al., 2017;</ref><ref type="bibr" target="#b22">Verga, et al., 2018;</ref><ref type="bibr" target="#b27">Zhou, et al., 2016)</ref>. These results show that our system has the potential to be adapted to other domains. In addition, our method outperforms <ref type="table">Table 3</ref>. Performance on the CDR test set in comparison with state-of-the-art systens Model P R F CD-REST <ref type="bibr" target="#b25">(Xu, et al., 2016)</ref> 59.60 44.00 50.73 Feature-TreeK-LSTM <ref type="bibr" target="#b27">(Zhou, et al., 2016)</ref> 64.89 49.25 56.00 + post-processing 55.56 68.39 61.31 CNN <ref type="bibr" target="#b4">(Gu, et al., 2017)</ref> 60.90 59.50 60.20 + post-processing 55.70 68.10 61.30 RNN-CNN <ref type="bibr" target="#b9">(Li, et al., 2018)</ref> 55.20 63.60 59.10 BRAN <ref type="bibr" target="#b22">(Verga, et al., 2018)</ref> 55.60 70.8 62.10 + ensemble 63.30 67.10 65.10 GS LSTM <ref type="bibr" target="#b19">(Song, et al., 2018)</ref> 42.31 39.21 40.70 AGGCN <ref type="bibr" target="#b5">(Guo, et al., 2019)</ref> 94.23 19.46 32.26 GT 30.04 74.67 42.84 BERT <ref type="bibr" target="#b2">(Devlin, et al., 2019)</ref> 61.41 58.82 60.09 BlueBERT <ref type="bibr" target="#b15">(Peng, et al., 2019)</ref> 62.80 64.45 63.61 BERT-GT 64.94 67.07 65.99</p><p>BlueBERT by an F1-measure of 2.38%. Considering the size of the CDR test set is small, we implement a statistically significant test to compare BERT-GT and BlueBERT (see our Supplementary Materials).</p><p>GS LSTM and AGGCN, however, show lower recall on the CDR dataset. We observed 25 falsenegative cases that are wrong in AGGCN but correct in our proposed method. In ~65% of these cases, the chemical and disease pairs co-occurred in a sentence at least once. AGGCN, however, cannot identify them correctly. Note that the maximum length of the input text is only three sentences in the n-ary dataset, and each instance in the n-ary dataset consists of only one annotated drug-genemutation. In contrast, the CDR dataset is in abstract-length, a chemical can be mentioned in different locations of the article. A chemical-disease can have a positive relation at one location in the abstract but have no relation at the rest of the locations. Hence, it is sometimes challenging to use a graph to distinguish the positive location from the negative ones. As a result, unlike our approach BERT-GT, AGGCN and GS LSTM are not robust and generalizable to another domain or a new dataset without additional effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The performance improvement as a result of adding the graph transformer</head><p>We randomly sampled 50 error cases that were wrong in BERT's predictions but correct in BERT-GT from the n-ary test set. The most common error cases of BERT are the instances in which "None" labels were assigned to instances with other labels. These cases constitute approximately 56% of the sampled error cases. For example, "Interestingly, cells with these mutations also showed greater sensitivity to gefitinib and erlotinibDRUG than cells with the EGFRGENE mutation (exon 19 deletion), which are associated witsh sensitivity to EGFR inhibitors in NSCLC. Mutations in PI3K (H1047RMUTATION) have been shown to enhance HER2 mediated transformation by amplifying the ligand induced signaling output of the HER family of RTKs." The BERT model predicts this as "resistance or non-response." Almost all of these cases are multiple-sentence instances, and it seems that the long instances with some keywords, such as "sensitivity," "associated," and "enhance,"</p><p>which commonly appear in the positive instances, are not used to express the relationship of these entities and can result in the wrong prediction. In addition, we found that 20% of the sampled cases are intra-sentence instances. For example, "Consistent with this, we found that the introduction of R1275QVARIANT into EML4-ALKGENE had no negative impact on sensitivity to crizotinibDRUG ( IC50 "sensitivity." It seems that, although BERT can use the key information from cross-sentence, it also is relatively easier to make an incorrect prediction by using that information and that training on cross-sentence also makes it easier to misclassify short text. In contrast, BERT-GT suffered from fewer of these problems, though a GT-only model does not perform well in our observation. Taken together, this suggests the strengths of GT in either focusing on the neighbor information or predicting short-text instances, when built with the default BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Error analysis, limitations, and future directions</head><p>Here, we discuss the error cases of BERT-GT and future research directions. According to our observation of the BERT-GT error cases on the CDR dataset, in 61% of the error cases, either chemical or disease mentions occurred only once. For example, in the PMID:25986755, the chemical "caffeine" appears only in the first two sentences of the abstract, and the disease "dysplasia" appears only once in the last sentence of the abstract. There is, however, a positive relationship between "caffeine" and "dysplasia" but no explicit information that mentions this relationship; thus, it was misclassified as no relation. Among these error cases, 75% do not have any single sentence that mentions both the target chemical and disease. Therefore, if the entity appears only once in the entire abstract and does not provide explicit information, BERT-GT is likely to generate the wrong predictions.</p><p>We also observed that there are some limitations to the n-ary dataset. For example, the n-ary dataset is a balanced dataset in which the numbers of positive and negative instances are balanced, which is not common in biomedical domains, such as CDR. Further, the sentence length of their instances is no more than three, but, in the biomedical domain, the sentence length of the text is usually longer. Therefore, we consider it a research direction to develop a more representative benchmark for evaluating n-ary RE tasks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In the biomedical literature, a biomedical relation usually consists of multiple entities and is represented in multiple sentences. The limitations on the architectures/features of previous methods are that they can perform well only on either cross-sentence n-ary relation extraction or CDR extraction. In this work, we propose a BERT-GT method. We show that BERT can be used to classify a cross-sentence relation, such as a PubMed abstract, because the attention mechanism makes it able to utilize the key information from the whole text and thus make the prediction. Our GT allows the BERT to use the graph information, which provides the neighbors of each token. The neighbors can be specified to focus on specific information for each token. The results demonstrate that BERT-GT achieves the highest performance on both problems and demonstrates its potential to be applied to a more generalizable relation classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. BERT-GT on Drug-Drug Interaction and ChemProt</head><p>We conducted new experiments to evaluate the BERT-GT and BlueBERT on the drug-drug interaction (DDI) <ref type="bibr" target="#b28">[1]</ref> and chemical-protein interaction (ChemProt) <ref type="bibr" target="#b29">[2]</ref> datasets. <ref type="table" target="#tab_3">Table 1</ref> shows the performances of BERT-GT, and its performances are slightly higher than the BlueBERT on ChemPort but lower on DDI. Comparing with Peng et.al. <ref type="bibr" target="#b30">[3]</ref> and Zhang et.al. <ref type="bibr" target="#b31">[4]</ref>, the results demonstrate the robustness of our method on traditional tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. t-test on CDR dataset and 5-fold cross-validation on nary dataset</head><p>We conducted a statistical significance t-test on the CDR dataset and compared BERT-GT with</p><p>BlueBERT, because its size is smaller than the size of n-ary dataset. We merged the CDR training, development, and test sets, and then randomly divide it into 10 different train-test partitions where the sizes of the training and test sets are the numbers of 800 and 500 abstracts, respectively. Their performances are shown in <ref type="table" target="#tab_4">Table 2</ref>, and the p-value is 0.044 (&lt;0.05), which indicates that BERT-GT statistically outperformed BlueBERT. <ref type="table">Table 3</ref> reports the values of the standard deviation on nary dataset.  <ref type="table">Table 3</ref>. Accuracy of different methods on the n-ary test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drug-Mutation Drug-Gene-Mutation</head><p>Two-class Multi-class Two-class Multi-class</p><formula xml:id="formula_0">Single ? All ? All ? Single ? All ? All ?</formula><p>Logistic regression <ref type="bibr" target="#b13">(Peng, et al., 2017)</ref> 73 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BERT-GT with [CLS]</head><p>Because previous works of n-ary relation extraction used the entity representation to aggregate the different context information from different entities. Therefore, we followed their formulation to use the entity representation for GT. To address your point, we have added the [CLS] token mechanism to BERT-GT, and used it as the sentence representation of GT. <ref type="table" target="#tab_6">Table 4</ref> shows it result on the CDR dataset. The performances of using <ref type="bibr">[CLS]</ref> for GT are lower than using the entity representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Cost</head><p>We compared the computational costs of the proposed BERT-GT method against BlueBERT. The hyperparameters of the two models are the same with the sequence length of 512 and the number epoch of 5. Because the BERT-GT model requires additional memory to compute the graph transformer's parameters, thus its running time is three times longer than that of BlueBERT, as shown in <ref type="table" target="#tab_7">Table 5</ref>. BERT-GT ~1 hr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effects of Hyper Parameters</head><p>We conducted a new experiment to evaluate the sensitivity of performance on different hyper parameter settings. The results of new experiments are illustrated in the following <ref type="figure" target="#fig_0">Figure 1 and 2</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>BERT-GT architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc><ref type="bibr" target="#b10">Li, et al. (2016)</ref> andWei, et al. (2016)'s CDR dataset for the abstract-level relation classification task. We Two different attention mechanisms. For simplification, we use single cells below to represent the vectors above.also evaluate our BERT-GT on traditional sentence-level biomedical relation extraction tasks (see our Supplementary Materials).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>The effects of different learning rate on performances The effects of maximum number of each token's neighbors (neighbor tokens) on performances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>test sets. We merged training and development sets and randomly selected 200 abstracts for validation and tuning of hyperparameters. Besides, we evaluate the computational costs and effects of different hyper parameters (see our Supplementary Materials).</figDesc><table><row><cell>development, and</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>positive and nega-</cell></row><row><cell></cell><cell></cell><cell>tive (no relation)</cell></row><row><cell cols="2">Number of instances resistance or nonresponse: 1479 dgm</cell><cell>3116 positive pairs;</cell></row><row><cell></cell><cell>and 1259 dm instances;</cell><cell>13477 negative pairs</cell></row><row><cell></cell><cell>sensitivity: 1149 dgm and 1044 dm in-</cell><cell></cell></row><row><cell></cell><cell>stances;</cell><cell></cell></row><row><cell></cell><cell>response: 488 dgm and 501 dm instanc-</cell><cell></cell></row><row><cell></cell><cell>es;</cell><cell></cell></row><row><cell></cell><cell>resistance: 292 dgm and 327 instances;</cell><cell></cell></row><row><cell></cell><cell>none: 3582 dgm and 2956 dm instances;</cell><cell></cell></row><row><cell>Size of dataset set</cell><cell>A total of 6987 dgm and 6087 dm in-</cell><cell>training set: 500 ab-</cell></row><row><cell></cell><cell>stances for the five-fold cross-validation</cell><cell>stracts; development</cell></row><row><cell></cell><cell></cell><cell>set: 500 abstracts;</cell></row><row><cell></cell><cell></cell><cell>test set: 500 ab-</cell></row><row><cell></cell><cell></cell><cell>stracts</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>The performances of BERT-GT and state-of-the-art on the same-sentence tasks.</figDesc><table><row><cell></cell><cell>DDI</cell><cell>ChemProt</cell></row><row><cell></cell><cell>F</cell><cell>F</cell></row><row><cell>Zhang et.al.</cell><cell>72.9</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>The t-test results of BERT-GT and BlueBERT on the CDR dataset.</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>? F</cell></row><row><cell cols="5">BlueBERT 65.65 62.27 63.89 1.47</cell></row><row><cell>BERT-GT</cell><cell cols="4">60.29 70.58 64.92 1.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>The performances of BERT-GT with [CLS] on the CDR dataset.</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>BERT-GT</cell><cell>64.94</cell><cell>67.07</cell><cell>65.99</cell></row><row><cell>BERT-GT with [CLS]</cell><cell>59.64</cell><cell>69.32</cell><cell>64.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>The computational costs of the BERT and BERT-GT on the CDR dataset.</figDesc><table><row><cell></cell><cell>Training time</cell></row><row><cell></cell><cell>per epoch</cell></row><row><cell>BlueBERT</cell><cell>~20m</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to the authors of BERT, BlueBERT, BioBERT, GS LSTM, and AGGCN who make their data and source code publicly available. We would like to thank Dr. Zhijiang Guo for helping us to reproduce the results of AGGCN on the n-ary dataset. We thank Dr. Chih-Hsuan Wei for his assistance on revising the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This research was supported by NIH Intramural Research Program, National Library of Medicine.</p><p>Conflict of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Graph Transformer for Graph-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<idno>AAAI. 2020</idno>
		<imprint>
			<biblScope unit="page" from="7464" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ranking relations between diseases, drugs and genes for a curation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rinaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical semantics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">S3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How user intelligence is improving PubMed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fiorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="937" to="945" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chemical-induced disease relation extraction via convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention Guided Graph Convolutional Networks for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BioBERT: pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinfomatics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recent advances of automated methods for searching and extracting genomic variant information from biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Syntax-aware Multi-task Graph Convolutional Networks for Biomedical Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis</title>
		<meeting>the Tenth International Workshop on Health Text Mining and Information Analysis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="28" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chemical-induced disease extraction via recurrent piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07669</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extracting chemical-protein relations with ensembles of SVM and deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Workshop on Biomedical Natural Language Processing</title>
		<meeting>the 2019 Workshop on Biomedical Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distant Supervision for Relation Extraction beyond the Sentence Boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers. Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation of the therapeutic effect of theta burst stimulation on drug-resistant auditory hallucinations in a schizophrenic patient and its impact on cognitive function and neuronal excitability: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidhoumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical neurophysiology: official journal of the International Federation of Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">802</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">N-ary Relation Extraction using Graph-State LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Assessing the state of the art in biomedical relation extraction: overview of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BioCreative V chemical-disease relation (CDR) task</title>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CD-REST: a system for extracting chemical-induced disease relation in literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting syntactic and semantics information for chemical-disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?a</forename><surname>Herrero-Zazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paloma</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="914" to="920" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Marius Doornenbal, Julen Oyarzabal, Analia Louren?o, Alfonso Valencia. Overview of the BioCreative VI chemicalprotein interaction track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obdulia</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Mart?n P?rez P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gael</forename><forename type="middle">P?rez</forename><surname>Santamar?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Intxaurrondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umesh</forename><surname>Antonio L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioCreative</title>
		<meeting>BioCreative<address><addrLine>Erin Van Buel, Akileshwari Chandrasekhar, Marleen Rodenburg, Astrid Laegreid</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="141" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Extracting chemical-protein relations with ensembles of SVM and deep learning models. Database: the journal of biological databases and curation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
