<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RefineCap: Concept-Aware Refinement for Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yekun</forename><surname>Chai</surname></persName>
							<email>chaiyekun@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RefineCap: Concept-Aware Refinement for Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatically translating images to texts involves image scene understanding and language modeling. In this paper, we propose a novel model, termed RefineCap, that refines the output vocabulary of the language decoder using decoder-guided visual semantics, and implicitly learns the mapping between visual tag words and images. The proposed Visual-Concept Refinement method can allow the generator to attend to semantic details in the image, thereby generating more semantically descriptive captions. Our model achieves superior performance on the MS-COCO dataset in comparison with previous visual-concept based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Holding the promise of bridging the domain gap between computer vision and human language, image-to-text translation, a.k.a image captioning, has lately received great attention in both communities <ref type="bibr" target="#b0">(Johnson et al., 2016;</ref><ref type="bibr">Chen et al., 2017;</ref><ref type="bibr">Anderson et al., 2018;</ref><ref type="bibr">Fan et al., 2019)</ref>. Combining image scene understanding and language generation, it aims to translate descriptive texts given corresponding images.</p><p>Existing work maintained that frequently occurred n-grams of reference captions in the training set are preferred in the caption generation, regardless of image contents <ref type="bibr">(Fan et al., 2019)</ref>. Visual concept (i.e., tag) prediction is proposed to leverage the visual semantics from images for generating relevant words <ref type="bibr">(Wu et al., 2016;</ref><ref type="bibr">You et al., 2016;</ref><ref type="bibr">Gan et al., 2017;</ref><ref type="bibr">Yao et al., 2017;</ref><ref type="bibr">Fan et al., 2019)</ref>. It predicts the probability of each semantic concept that occurs in the corresponding image out of the selected image-grounded vocabulary from reference captions, for the use of following caption generation.</p><p>However, most of them adopted Long Short-Term Memory (LSTM) <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref> as the language decoder, in which sequence-align recurrence inherently precludes parallelization in practice. Initially proposed for neural machine translation (NMT), Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> architecture has made rapid progress in numerous applications, such as <ref type="bibr">NMT (Ott et al., 2018;</ref><ref type="bibr">Gu et al., 2019)</ref>, image generation <ref type="bibr">(Parmar et al., 2018)</ref>, automatic speech recognition <ref type="bibr">(Dong et al., 2018)</ref>, computer games <ref type="bibr">(Vinyals et al., 2019)</ref>, etc. We adopt Transformer blocks rather than RNN to support parallel training in our model.</p><p>Intuitively, when describing an image, people often take much time to "think" of the words to generate coherent functional words, and just "look" at the image regions occasionally for content words. Similarly, for image captioning, only content-related words are actively matched with the image, whereas other functional words can often be automatically inferred using a language model. The adaptive attention model <ref type="bibr">(Lu et al., 2017)</ref> found that not all generated words are actively related to visual contents-some words can be reliably predicted just from the language model, such as "phone" after "taking on a cell". Also, non-visual words like "the" can be generated with the language model inference.</p><p>Grounding on this, we introduce a languagedecoder-guided gate to regulate visual-concept vectors, followed by a scatter connection layer to map each element of the image-grounded vector to the corresponding position in the decoder's vocabulary. This jointly takes into account what the captioner "thinks" via the language model and what it "looks" at with the dynamic visual-semantic concept vector. In this paper, we propose RefineCap, a visualconcept-aware refined encoder-decoder architecture to dynamically modulate the visual-semantic representations and thus enhance the output of language model. Within the proposed model, visual signals are decoder-regulated and the content-based gate removes the independent assumption of visual objects in the attention mechanism.</p><p>Our work illustrates that image-grounded concept detection advances the performance of Transformer-based encoder-decoder captioning architecture by incorporating the visual-semantic representation using reinforcement learning. Our key contributions are as follows:</p><p>? A scatter-connected mechanism to refine the language decoder using extracted visual semantics, which produces more specific descriptive words in caption generation. ? A competing model that outperforms the previous visual-concept based captioning models on the MS-COCO dataset.</p><p>2 Methodology <ref type="figure" target="#fig_0">Fig. 1</ref> shows an overview of the proposed model. We use an object detector to extract visual object features and a relational encoder to capture the pairwise relations among detected objects (Sec. 2.1). Then visual-concept components learn the implicit visual tags in the image. The Transformer language decoder receives the extracted object features (Sec. 2.2), followed by a scatter connection mechanism (Sec. 2.3) for decoder refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer as Relational Encoder</head><p>Following <ref type="bibr">(Anderson et al., 2018)</ref>, we use Faster <ref type="bibr">RCNN (Ren et al., 2015)</ref> with <ref type="bibr">ResNet-101 (He et al., 2016)</ref> as the object detector. We extract the object proposals using Region Proposal Network and mean-pooled convolution to generate the 2048dimensional proposal feature.</p><formula xml:id="formula_0">Let X = [x 1 x 2 ? ? ? x M ] ? R M ?2048 denote</formula><p>the extracted M visual proposal features of each image. We use a fully connected layer to reduce the spatial feature of 2048 into D = 512. Then we apply the standard Transformer encoder with layer number N enc to capture the object-wise relation as in <ref type="bibr">(Zambaldi et al., 2018)</ref>. The output of encoder is represented as</p><formula xml:id="formula_1">F = [f 1 f 2 ? ? ? f M ] ? R M ?D .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer Decoder</head><p>Given the caption sequence with the length of T , we apply a standard Transformer decoder of N dec layers as the caption generator. The input sequence firstly passes into word embedding and sinusoidal positional embedding layers, followed by a masked self-attention sublayer attending to previous histories. Then a cross attention sublayer as in <ref type="bibr">Vaswani et al. (2017)</ref> is applied to capture the multi-modal attention between each word and extracted object features, followed by a position-wise feed-forward layer (FFN). All sublayers are encompassed by a residual connection <ref type="bibr">(He et al., 2016)</ref> and layer normalization (Ba et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual-Concept Refinement</head><p>Visual Concept Layer The visual concept layer is designed to extract the probability of common concept words, such as noun and verbs. We employ the K most frequently occurring words whose POS tags are nouns, verbs, or adjectives as the image-grounded concept vocabulary set V tag . Following (Gan et al., 2017), K is set to 1,000 in our experiment. For each image, the visual concept layer projects its visual object features f m (m = 1, ? ? ? , M ) into a (K/M )-dimensional vector, and then concatenates M different visual outputs to get the K-dimensional concept representation. Then we apply an activation function to get the probability of each concept word in V tag , which is a multi-label binary prediction</p><p>.</p><formula xml:id="formula_2">v = f 1 W 0 f 2 W 0 ? ? ? f M W 0 (1) v = ?(v)<label>(2)</label></formula><p>where W 0 ? R D?K/M denotes trainable weights, denotes the concatenation along the last axis, ?(x) = 1/(1 + exp(?x)), f m ? R D (m = 1, 2, ? ? ? , M ) is the m-th object feature out of M visual proposals.v ? R K represents the visual concept vector, in which each element represents the confidence of image-grounded concept words.</p><p>Denoting the decoder output at t-th time step (t = 1, 2, ? ? ? , T ) by h t ? R D , we compute the context vector c t by considering the interaction between the t-th decoder output and all encoded features F in one image, followed by a non-linearity g (we use sigmoid function here) to produce the decoder-guided gate for concept-aware modulation.</p><formula xml:id="formula_3">c t = u tanh(W 1 h t + W 2 F) (3) g t = g(W 3 c t ) (4) where {W 1 , W 2 } ? R D ?D , u ? R D , W 3 ? R K?M are parameters, D represents the interme- diate hidden dimension.</formula><p>At t-th time step, the visual-concept vectorv is modulated by the decoder-guided gate g t to render the final refined representation o t :</p><formula xml:id="formula_4">o t = g t v<label>(5)</label></formula><p>where indicates the element-wise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scatter-Connected Mapping</head><p>Since the selected image-grounded vocabulary V tag is the subset of caption vocabulary V cap , i.e., V tag ? V cap , we apply scatter-connected mapping by adding the corresponding element of V tag onto V cap to enhance the confidence of concept word prediction:</p><formula xml:id="formula_5">h t [j] = h t [j] + o t [k] if V cap (j) = V tag (k) h t [j] otherwise,<label>(6)</label></formula><p>where V cap (j) and V tag (k) indicate the corresponding concept in the j-th position of caption vocabulary set and k-th word of concept vocabulary set.</p><p>[.] is tensor indexing operation. Then a softmax function is applied afterward to get the probability over all caption vocabularies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training with Policy gradient</head><p>Captioning as Reinforcement Learning The image captioning task is cast to the RL problem: the policy network RefineCap, defined as ? ? parameterized by ?, takes an action a t at t-th time step for each observation (i.e., image) to predict the next word w t until reaching the rollout end. The return G for each rollout is defined as CIDEr-D scores <ref type="bibr">(Vedantam et al., 2015)</ref> between hypothesis and ground-truth captions. We leverage the REINFORCE with baseline algorithm to reduce the gradient variation. The parameter ? t+1 is updated as follows:</p><formula xml:id="formula_6">? t+1 = ? t + ?(G t ? b)? ln ?(?)<label>(7)</label></formula><p>where b takes the average of batch returns, ? is the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Dataset and Evaluation We experiment on the MS-COCO dataset <ref type="bibr">(Lin et al., 2014)</ref> and report the performance on Karpathy offline splits, consisting of 113,287/5,000/5,000 images for training/val/test sets, in which each image is paired with 5 human annotations. We empploy BLEU <ref type="bibr">(Papineni et al., 2002)</ref>, ROUGE-L (Lin, 2004), ME-TEOR (Denkowski and Lavie, 2014), <ref type="bibr">CIDEr-D (Vedantam et al., 2015)</ref>, and SPICE <ref type="bibr" target="#b0">(Anderson et al., 2016)</ref> as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We set the embedding dimension D to 512, the layer numbers of both encoder and decoder as 3 (for fast training), batch size of 50, head number h as 8, the hidden dimension of FFN layer as 2,048, the maximum number of extracted features as 50. Word embeddings are randomly initialized. We employ Adam optimizer (Kingma and Ba, 2014) with ? 1 = 0.9, ? 2 = 0.98 as in <ref type="bibr">(Li et al., 2019)</ref>. To avoid over-fitting, we set the dropout rate as 0.1, and early stopping patience as 5 during training. The beam width is set as 5 during beam search decoding. To initialize the model weights for RL training, we pretrain the model with supervised learning using crossentropy loss with the same setting. All experiment are trained on a single NVIDIA Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Table 1 exhibits the performance of visual-concept based image-to-text translation models on MS-COCO dataset, in which the proposed model outperforms baseline models by a clear margin.</p><p>Ablation Test <ref type="table" target="#tab_3">Table 2</ref> shows that the proposed method boosts the CIDEr scores on standard Transformer for image caption generation, indicating the proposed visual-semantic module and scatteredconnection mechanism promote the CIDEr-D score in contrast with other baseline models.</p><p>Qualitative Analysis <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the generated caption samples, where the detected visual concepts advance the quality of predicted captions. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> bottom left, discovered visual tags enhance the adequacy of generated captions, such as "tie" with the confidence of 1, which is not mentioned in ground truth. Our model adds "near the ocean" as the adverbial modifier in comparison with baseline in the bottom right. Visual concept: blue(1), yellow(1), giraffe(1), young(1), standing(1), brown(1), green(1), big(1), food(1), grass(0.991), walking(0.980), field(0.983) Baseline: three giraffes are standing in a field of tall grass. Ours: two giraffes and other animals standing in a field. Reference: A giraffe grazing on a tree in the wilderness with other wildlife. Several giraffes eating leaves from the ground and tree. Two giraffes and another animal in a field. Several giraffe eating the leaves from neighboring trees. Giraffes are feeding on the trees and grass.</p><p>Visual concept:</p><p>black <ref type="formula">(1)</ref>, young(1), woman(1), sitting(1), person(1), red(1), wooden(0.999), hand(0.997)</p><p>Baseline: a woman sitting in the woods talking on a cell phone. Ours: a woman sitting on a suitcase in the woods. Reference: A woman sitting on a piece of luggage in a field. A woman sits on a brief case in the woods. A girl with a lot of tattoos sitting on a piece of luggage. A woman with lots of tattoos sits on a suitcase in a forest. Lady with arm full of tattoos sitting on her suitcase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>A man sitting next to a woman while wearing a suit. Two people are posing for a photograph together A black and white photo of a couple. A black and white photo of a man and woman A man and a woman are posing for a photograph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual concept:</head><p>man <ref type="formula">(1)</ref>, white(1),picture(1), person <ref type="formula">(1)</ref>, young(1), black(1), tie(1), wearing <ref type="formula">(1)</ref>    Better Accuracy Transformer baseline without the proposed method sometimes generates mismatched words, which can be reliably rectified by the proposed method. For example, in upper left of <ref type="figure" target="#fig_1">Fig. 2</ref>, our model correctly predicts the presence of "two giraffes and another animal" but baseline identifies them as "three giraffes" by mistake.</p><p>Better Adequacy Our model can capture more specific details and meaningful contents in the image background that might be ignored by the baseline or even omitted in the ground truth. For example, as shown in <ref type="figure" target="#fig_1">Fig. 2 (bottom left)</ref>, the proposed model predicts the occurrence of "tie" which is overlooked by both the baseline and ground truth.</p><p>Implicit Visual Concept Modeling We found that the detected concepts can exactly match semantic objects in the image. Notably, such visual-concept detection can be treated as the side effect of the scatter-connection mappings since the proposed method implicitly learns the visual mapping from the visual objects to visual-concept vocabulary (as aforementioned in Sec. 2.3), instead of using an explicit training objective for learning such mapping.</p><p>To investigate the necessity of explicit visualconcept learning, we design further experiments by pre-training the visual-concept detector for multilabel binary classification before training the caption generator. We empirically find it unnecessary to pretrain the visual-concept detector before the caption generation, as shown in Sec. A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose a new visual-concept based image captioning framework to generate meaningful descriptive sentences. Leveraging visual concepts and scatter mapping, RefineCap demonstrates its effectiveness on the MS-COCO dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>A.1 Case Study of Generated Captions <ref type="figure" target="#fig_1">Fig. 2, 3</ref>, 4 manifest the comparison between the baseline and proposed models, with the presence of visual concepts and their corresponding confidence (presented in brackets) for each image.</p><p>Visual-Concept Indication The proposed method implicitly learns the connection between visual concepts and image regions without explicit training on the visual concept detector. Further analysis can be found at Appendix 2.</p><p>Better Adequacy It can be seen that our model can capture more specific details and meaningful contents in the image background that might be ignored by the baseline or even omitted in the ground truth. For example, as shown in the bottom left in <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed model predicts the occurrence of "tie" which is overlooked by both the baseline and ground truth. Other examples like "ocean" at the bottom right of <ref type="figure" target="#fig_1">Fig. 2, "</ref>store" at bottom left example of <ref type="figure" target="#fig_3">Fig. 3a, "</ref>ocean" at upper left in <ref type="figure">Fig. 4a</ref>, "a ground of people ... around" at the upper right <ref type="figure">Fig. 4a, "</ref>kitchen" at the bottom left of <ref type="figure">Fig. 4a, "</ref>clock" at upper left in <ref type="figure">Fig. 4b, "</ref>store" at upper right in <ref type="figure">Fig. 4b, "</ref>street sign" at bottom left in <ref type="figure">Fig. 4b, "</ref>basket" at bottom right in <ref type="figure">Fig. 4b</ref>, etc.</p><p>We found that captions generated by the proposed model could achieve better scores in terms of the adequacy.</p><p>Better Accuracy Meanwhile, captions decoded by the proposed models equip better accuracy. For instance, in upper left of <ref type="figure" target="#fig_1">Fig. 2</ref>, our model predicts the presence of "two giraffes and another animal" but baseline identifies them as "three giraffes" by mistake. Other proofs, such as "suitcase" in the top right of <ref type="figure" target="#fig_1">Fig. 2</ref>, "living room" in the top left of <ref type="figure" target="#fig_3">Fig. 3a</ref>, " a body of water" in the top right of <ref type="figure" target="#fig_3">Fig. 3a,  "</ref>flags" in the bottom right of <ref type="figure" target="#fig_3">Fig. 3a</ref>, "suitcase" and "floor" in the top left of <ref type="figure" target="#fig_3">Fig. 3b, "</ref>floor" in the bottom left of <ref type="figure" target="#fig_3">Fig. 3b</ref>, etc. Wrongly predicted words are marked in red in the examples. It can be concluded that the proposed model can produce image descriptions in a more accurate manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Additional Details for Experiments</head><p>We use SpaCy ? as the tokenizer and pos tagger to process the reference captions, with lowercase and punctuation removal. Tokens whose occurrence ? https://spacy.io/ less than 5 are treated as unknown words in our vocabulary. The reference vocabulary size is 10,201, whereas the most frequent 1,000 words from reference sentences are used as the visual-concept vocabulary. We set the maximum length of sentences as 20, in which all input sequences are right padded with 0s or cut on the right hand side. For hyperparameter tuning, we test the decoder layer number N dec ? {3, 6} and found that model with 6 layers slows down the training speed but achieves the similar performance. RL training takes the majority of time costs due to the reward computation. Specifically, it took around 4/2/1.5 hours to train one epoch for RL/MLE/tag training in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Ablation Study Curves</head><p>To investigate the necessity of explicit training on visual tags, we add additional pretraining stage on the visual concept detection as a multi-label classification task. <ref type="figure" target="#fig_5">Fig.5</ref> compares the performance of proposed model with pretraining for 10 epochs using a binary cross-entropy loss (referred as +tag pretraining), and the counterpart without the pretraining, i.e., RefineCap, on the validation set. As shown in <ref type="figure" target="#fig_5">Fig.5</ref>, RefineCap conducts supervised training using cross-entropy loss as the weight initialization over the period of the beginning 25 epochs, followed by reinforcement learning process. Model with tag pretraining has three different stages: tag pretraining from epoch 0-10, supervised training from epoch 10-35, and finally reinforcement learning after 35-th epoch.</p><p>It can be seen that the overall trend of CIDEr-D scores, BLEU-1, BLEU-4 curves for models with tag pretraining witnesses the degeneration of performance, with no obvious change but a slight decrease, and the extra time and computing costs in the first 10 epochs. Besides, over the period of reinforcement learning, the rewards and reward baseline values of +tag pretraining models have decreased with 0.05 shift due to the different starting points but maintain a similar increase slope as RefineCap. Thus we extrapolate that explicit visualconcept pretraining could impede the weight initialization for the following training process in our model. We also observed a clear performance gain with REINFORCE algorithm in the final stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>A fluffy white chair that faces away from a television. A pillow covered reading chair in the corner of the living room. The living room is empty with the television on. White ornate seat in nicely decorated room with television. A white chair, books and shelves and a tv on in this room.</p><p>Visual concept: blue(1), white(1),room(1), brown(1), old(0.997), television(0.980), living(0.970) Baseline: a bedroom with a white bed and a tv.</p><p>Ours: a living room with a white bed and a television. Ours: a flock of birds flying over a body of water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>A cow standing near a curb in front of a store. There is a cow on the sidewalk standing in front of a door. A cow on the sidewalk on a corner in front of a store. Cow standing on sidewalk in city area near shops. A cow on a city sidewalk in front of a business.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual concept:</head><p>white <ref type="formula">(1)</ref>, blue(1), red(1), grass(0.991), green(0.990), bird(0.972), small(0.964), water(0.950), wooden(0.843), field(0.733)</p><p>Baseline: a boat with an american flag on the back of it.</p><p>Ours: a boat with flags on a boat in the water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>A swan is floating down the river by the boat. A boat with flags and tents is docked next to a grassy bank. A boat that is decorated with flags on the water. A parked boat with some items inside of it. A large red boat sitting next to a lush green shore.</p><p>Visual concept: blue(1), white(1), picture(1), looking(1), standing(1), red(1), black(0.999), street(0.998), walking(0.917), brown(0.824) Baseline: a cow standing on the side of a city street Ours: a cow walking down a sidewalk in front of a store .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>Visual concept: young(1), little(1), sitting(1), brown(1), small(1), black(1), green(0.998), red(0.991), playing(1), room(0.942), looking(0.902), luggage(0.881) Baseline: a baby sitting on a chair with a laptop. Ours: a baby sitting in a suitcase on the floor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>Young boy sitting on top of a briefcase A young baby sits on top of a briefcase. A little boy sitting on a suitcase on the floor. A small child sitting on top of a brief case. A toddler boy is sitting on a brief case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual concept:</head><p>looking(1), red(1), large(1), airplane(1), car(0.981), old(0.972), field(0.970) Baseline: airplanes parked at an airport with a large window. Ours: airplanes are parked at an airport on the tarmac. Reference: An airport filled with planes sitting on tarmacs. The view of runway from behind the windows of airport. A truck driving towards some planes parked on the runway Planes on a wet tarmac unloading at arrival gates. Window view from the inside of airplanes, baggage carrier and tarmac.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>A brown and white dog wearing a neck tie. A brown and white dog wearing a tie on carpet. A dog wearing a tie poses for the camera. A dog is sitting with a neck tie on. A close up of a dog wearing a tie.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual concept:</head><p>large <ref type="formula">(1)</ref>   Baseline: two kites flying in the sky on a beach.</p><p>Ours: two kites flying in the sky over the ocean. Reference: A kite flying through a blue sky with a long tail. A colorful kite is flying over the ocean. A kite flying through the sky on a clear day. A very pretty kite is flying high in the sky. There are different kites flying in the sky.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>A group of people observing two planes at an air show. A family walks on a runway near huge planes. People walking towards aircrafts on display outside during the day. People walking around two different Air Force airplanes. Large, air force airplanes sit on a runway while tourists look at them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>A woman standing over a pan filled with food in a kitchen. A woman smiling while she prepares a plate of food. A smiling woman standing next to a plate of food she made A woman in a bright pink summer shirt smiles and displays a party platter she has made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual concept:</head><p>woman <ref type="formula">(1)</ref>, black(1), person(1), picture(1),  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual concept:</head><p>looking(1), red(1), large(1), airplane(1), car(0.981), old(0.972), field(0.970) Baseline: a woman riding a skateboard on a sidewalk. Ours: a woman riding a skateboard in front of a store. Reference: A girl is skateboarding down the Hollywood walk of fame. A woman who is skateboarding down the street. A woman with glasses and a scarf skateboards along Hollywood's Walk of Fame.</p><p>Lady in front of a store standing on a pink skateboard A woman is riding her skate board down the sidewalk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>A stop sign and street sign encased in snow and ice. A couple of street signs sticking out the side of a ski slope. A stop sign and street sign encased in a wall of snow. A stop sign is buried in a large pile of snow. A large pile of snow with a stop sign and a street sign poking out.</p><p>Visual concept: red(1), white(1), big(1), snow(0.998), street(0.997), street(0.997), stop(0.980), large(0.965), sign(0.951), snowy(0.893), skis(0.903), ski(0.882), field(0.767) Baseline: a stop sign is covered in snow in a Ours: a stop sign and a street sign in the snow .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference:</head><p>Two giraffes stand and eat food out of a basket. Two giraffes looking for food in an empty feeding basket A couple of giraffes reach for a basket Two giraffes that are eating from a basket. Two giraffes eating from a basket on a pole.</p><p>Visual concept: yellow(1), young(1), food(1), brown(1), white(1), giraffe(1), standing(1), looking(1), large(0.997), walking(0.867), couple(0.856) Baseline: two giraffes standing next to each other in a zoo. Ours: two giraffes are eating from a basket in a zoo. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic illustration of proposed models, where "text" indicates previous words of the captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Detected tags and generated captions using baseline (Transformer) and proposed models on MS-COCO, where red and green backgrounds indicate wrong and correct predictions respectively. The value in brackets means the confidence (i.e., probability) of corresponding tags in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>small birds flying in the sky over the water. Several birds that are flying together over a body of water. A flock of birds flying over a field. A black and white image showing birds flying over a body of water. A group of birds flying over the water. Visual concept: black(1), white(1), bird(1), looking(1), picture(1), air(0.998), water(0.995), flying(0.979) Baseline: a flock of birds flying over a beach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Supplemental examples of the generated captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>people sit in an open court yard. A small group of people standing around a ball patio. A group of people walking around a parking lot. A group of people in front of a white building. Many people on a courtyard under a clock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>blue(1), large(0.998), standing(0.997), people(0.988), man(0.976), building(0.962), outside(0.882), woman(0.832), group(0.822) Baseline: a group of people standing in front of a building. Ours: a group of people standing outside of a building with a clock. The performance curves of RefineCap w/ and w/o tag pretraining (in blue / green separately) on evaluation set, including CIDEr-D/BLEU-1/BLEU-4 metrics, validation loss, rewards and baseline reward values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall performance of the proposed model and visual-concept based models.</figDesc><table><row><cell>SemAttn (You et al., 2016)</cell><cell>0.709</cell><cell>0.537</cell><cell>0.402</cell><cell>0.304</cell><cell>0.243</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Att-CNN+LSTM (Wu et al., 2016) 0.74</cell><cell>0.56</cell><cell>0.42</cell><cell>0.31</cell><cell>0.26</cell><cell>-</cell><cell>0.94</cell><cell>-</cell></row><row><cell>LSTM-C (Yao et al., 2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.230</cell><cell>-</cell><cell>-</cell></row><row><cell>Skeleton Key (Wang et al., 2017)</cell><cell>0.673</cell><cell>0.489</cell><cell>0.355</cell><cell>0.259</cell><cell>0.247</cell><cell>0.489</cell><cell>0.966</cell><cell>0.196</cell></row><row><cell>SCN-LSTM (Gan et al., 2017)</cell><cell>0.728</cell><cell>0.566</cell><cell>0.433</cell><cell>0.330</cell><cell>0.257</cell><cell>-</cell><cell>1.041</cell><cell>-</cell></row><row><cell>Bridging (Fan et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.330</cell><cell>0.264</cell><cell>0.586</cell><cell>1.066</cell><cell>-</cell></row><row><cell>Ours</cell><cell>0.802</cell><cell>0.645</cell><cell>0.499</cell><cell>0.378</cell><cell>0.283</cell><cell>0.580</cell><cell>1.272</cell><cell>0.225</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>buffalo grazing on grass next to the ocean. A herd of black sheep grazing near the shore A field full of wild animals next to a beach. A group of animals grazing next to a beach and ocean. A herd of large wholly sheep walk near a beach.</figDesc><table><row><cell>, Baseline: a man and a woman posing for a couple(0.953), suit(0.910) picture. Ours: a man in a suit and tie standing next to a woman.</cell><cell>Reference: A herd of Visual concept: blue(1), food(0.996), white(0.990), grass(0.990), large(0.967), field(0.955), sheep(0.895), group(0.869), young(0.840), eating(0.587) Baseline: a herd of sheep grazing on the beach. Ours: a group of sheep grazing on a beach near the ocean.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with standard Transformer.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
