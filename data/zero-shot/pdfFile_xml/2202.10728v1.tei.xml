<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilled Neural Networks for Efficient Learning to Rank</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Franco</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosimo</forename><surname>Nardini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Rulli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rossano</forename><surname>Trani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venturini</surname></persName>
						</author>
						<title level="a" type="main">Distilled Neural Networks for Efficient Learning to Rank</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Web search</term>
					<term>learning-to-rank</term>
					<term>neural networks</term>
					<term>efficiency</term>
					<term>distillation</term>
					<term>pruning</term>
					<term>matrix multiplication</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies in Learning to Rank have shown the possibility to effectively distill a neural network from an ensemble of regression trees. This result leads neural networks to become a natural competitor of tree-based ensembles on the ranking task. Nevertheless, ensembles of regression trees outperform neural models both in terms of efficiency and effectiveness, particularly when scoring on CPU. In this paper, we propose an approach for speeding up neural scoring time by applying a combination of Distillation, Pruning and Fast Matrix multiplication. We employ knowledge distillation to learn shallow neural networks from an ensemble of regression trees. Then, we exploit an efficiency-oriented pruning technique that performs a sparsification of the most computationally-intensive layers of the neural network that is then scored with optimized sparse matrix multiplication. Moreover, by studying both dense and sparse high performance matrix multiplication, we develop a scoring time prediction model which helps in devising neural network architectures that match the desired efficiency requirements. Comprehensive experiments on two public learning-to-rank datasets show that neural networks produced with our novel approach are competitive at any point of the effectiveness-efficiency trade-off when compared with tree-based ensembles, providing up to 4x scoring time speed-up without affecting the ranking quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE estimation of relevance is a task of paramount importance in Web search. In fact, search engines provide the users with a list of relevant results answering a information need formulated as a textual query. In the last years, Learning to Rank (LtR) techniques have been successfully applied to solve this task. LtR is the field of machine learning devoted to the development of supervised techniques addressing the ranking problem. LtR techniques have been proficiently used in Web search, a scenario characterized by tight latency bounds for query processing <ref type="bibr" target="#b10">[11]</ref>. For this reason, the investigation of new LtR techniques targets both effectiveness and efficiency to provide accurate solutions that can be used in modern query processors. State-of-theart approaches in learning to rank are ensembles of regression trees. Specifically, LambdaMART <ref type="bibr" target="#b8">[9]</ref> is an effective state-of-the-art LtR algorithm that builds ensembles of regression trees by optimizing a loss function that depends on a listwise information retrieval metric, e.g., NDCG <ref type="bibr" target="#b29">[30]</ref>. The counterpart of the retrieval accuracy guaranteed by treebased models is the computational effort needed to traverse hundreds or even thousands of trees. This computational effort hinders the application of this kind of models on low-latency query processors. Furthermore, each tree in an ensemble work by testing a sequence of boolean conditions on the input. The natural translation of this structure in ifthen-else code conflicts with modern CPU architectures that heavily rely on branch prediction and caching. A recent line of research investigates techniques for efficient traversal of ensembles of regression trees. The state-of-the-art algorithm for traversing tree-based models is QuickScorer <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, which implements an interleaved feature-wise traversal of the ensemble that maximizes the efficiency of branch predictor and cache of modern CPUs. Motivated by the success of neural solutions in other fields such as Natural Language Processing and Computer Vision, several attempts have been made to bring Neural Networks (NNs) in the LtR field. Despite that, treebased solutions still provide state-of-the-art performances on different benchmarks, especially when dealing with handcrafted features <ref type="bibr" target="#b47">[48]</ref>. Recently, Qin et al <ref type="bibr" target="#b47">[48]</ref> identify the reasons for the superiority of tree-based solutions in i) the sensitiveness of neural network to input features scale and transformations, ii) the lack of expressiveness in mostly adopted neural models in LtR, iii) the limited size of available LtR datasets w.r.t. to Natural Language Processing or Computer Vision. Cohen et al. <ref type="bibr" target="#b11">[12]</ref> develop an approach that permit to overcome these limitations on standard LtR datasets by training classic multi-layer perceptrons using simple data normalization (Z-normalization) and by leveraging a data augumentation technique (Section 3). Cohen et al. <ref type="bibr" target="#b11">[12]</ref> propose to train neural networks to mimic the outputs of a pre-trained ensemble of regression trees. They do so by employing a knowledge distillation approach <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[23]</ref> that treats the ensemble of regression trees as a black box generating accurate document scores. Given that neural models are universal approximators <ref type="bibr" target="#b23">[24]</ref>, the network can reproduce the predictions of the ensemble of regression trees. In practice, this is done by using the Mean Square Error between the scores and the network predictions as training loss. The performance of a neural network trained by scores approximation are bounded by the performance arXiv:2202.10728v1 <ref type="bibr">[cs.</ref>LG] 22 Feb 2022 of the tree-based model used to generate the scores: even in a perfect approximation scenario, the neural model will introduce no improvement in terms of effectiveness. In general, instead, the approximation will cause a degradation in the ranking precision. However, the reason to move to a neural document scoring engine is to exploit fast inference mechanisms available for NNs. In this direction, Cohen et al. <ref type="bibr" target="#b11">[12]</ref> compare the efficiency of a neural solution for ranking (on GPU and CPU) with QuickScorer <ref type="bibr" target="#b40">[41]</ref> (on CPU). In the original work, the authors claim that neural models are as accurate as ensembles of regression trees in terms of Mean Average Precision (MAP), and largely outperform them in terms of execution time (?s/doc). We observe that their comparison presents some weaknesses. They compare a single-thread CPU version of QuickScorer against a multithread GPU version of the neural forward pass. Due to the differences between the computational engines, this does not permit to actually state which one of the two solutions is the more efficient. Even when comparing on CPU, the comparison is done using: i) a single-threaded C++ implementation of QuickScorer for ensembles of regression trees and ii) a multi-threaded Python neural inference running with an unspecified number of threads. The use of Python APIs may also entail some latency in calling the underlying optimized matrix multiplication routine on which these frameworks usually rely. <ref type="bibr" target="#b0">1</ref> Moreover, the two sets of experiments are conducted on different CPUs. These aspects hamper a direct comparison of the performance achieved.</p><p>In this article, we propose a solid, fair and comprehensive comparison of the efficiency of ensemble of regression trees and neural models. We compare QuickScorer <ref type="bibr" target="#b12">[13]</ref> against a novel and optimized implementation of neural network inference written in C++. We perform the evaluation on the same hardware by executing the two solutions using a single thread. Moreover, both solutions exploit instruction-level parallelism (AVX2 instruction set). Since CPU and GPU are two different processing units and each of them requires specific optimization techniques, in this work we focus on providing an accurate study of the efficiency of the two approaches on CPU, while we plan to extend it to the GPU in the future. Regarding the training phase, we adopt the same neural architectures of Cohen et al. <ref type="bibr" target="#b11">[12]</ref> and we re-implement their methodology with our own code in Pytorch <ref type="bibr" target="#b44">[45]</ref>. However, differently from the original work, in our experiments we train the ensemble of regression trees with the LightGBM library <ref type="bibr" target="#b30">[31]</ref>, since it is the state-of-the-art library for learning ensemble models on ranking tasks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p><p>The results of our comprehensive experimentation on the MSN30K dataset show that, in contrast with the results reported by Cohen et al. <ref type="bibr" target="#b11">[12]</ref>, ensembles of regression trees are both faster and more accurate than neural models. In <ref type="table" target="#tab_2">Table 1</ref>, we report the Mean Average Precision (MAP), the Normalized Discounted Cumulative Gain (NDCG, with cutoff at 10 and without cutoff), and the scoring time per document. Symbols evidence statistically significant improvement w.r.t. to Mid Forest , and Small Forest ? , according to the Fisher's randomization test, p &lt; 0.05. We run different <ref type="bibr" target="#b0">1</ref>  tests for each metrics, but we use shared symbols to ease the notation. <ref type="table" target="#tab_2">Table 1</ref> shows that ensemble of regression trees deliver the same performance of neural models while being largely faster, with a speedup ranging from 2.8x (Small Net vs Small Forest) to 16.2x (Large Net vs Mid Forest). Also, the Large Forest is the best performing model with a large margin, while being 3x faster than the Large Net. These evidences highlight how tree-based solutions are currently faster than neural networks on CPU. We bridge the large gap between tree-based models and neural networks by proposing a novel framework to efficiently design and train effective and efficient feed-forward networks for ranking on CPU.</p><p>The novel contributions of this article are:</p><p>? we present a combination of state-of-the-art approaches to improve the performance of neural networks on Learning to Rank tasks. By leveraging efficiencyoriented pruning techniques and high-performance Dense and Sparse Matrix Multiplication techniques, we build neural models that outperform ensembles of regression trees. An extensive experimental evaluation on two well-established public benchmarks, i.e., the MSN30K <ref type="bibr" target="#b46">[47]</ref> and the Tiscali Istella-S <ref type="bibr" target="#b12">[13]</ref> datasets, shows the effectiveness of our method. Experimental results confirm that on the MSN30K dataset it is possible to obtain up to 4.4x faster scoring time with no loss of accuracy. ? we provide a novel way to estimate the execution time of neural network forward pass, by mean of dense and sparse time predictors, respectively for Dense-Dense and Sparse-Dense Matrix Multiplication (DMM &amp; SDMM). To the best of our knowledge, this is the first work that dives into the technicality of matrix multiplication to precisely predict the execution time of neural models. These predictors are derived from a broad study of the implementation of the relative operations on modern CPUs. In explaining how predictors are developed, we also provide a clear and concise explanation of these two fundamental operations with plenty of scientific applications. ? we develop an efficient and effective approach to design neural models, using the aforementioned time predictors, which allow to estimate the execution time of a feed-forward network a priori, by providing the architecture -i.e., the number of layers and the neu-rons per layer -and the sparsity level of each layer. This design methodology tackles the costly problem of model architectures search <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b51">[52]</ref>, since it allows to train exclusively the models respecting the latency requirements, tearing down the costs, in terms of time and energy consumption, of the experimental phase. The rest of the paper is organized as follows: Section 2 discusses the related work in the field. Section 3 details the process of distilling ensemble of regression trees into neural networks as proposed by Cohen et al. <ref type="bibr" target="#b11">[12]</ref>. Section 4 introduces the implementation of dense-dense matrix multiplication and sparse-dense matrix multiplication on modern CPUs, together with our time predictors. Section 5 describes our novel method for designing efficient neural models for ranking. Moreover, Section 6 presents a comprehensive experimental evaluation of our proposed technique on public data. Finally, Section 7 concludes the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we introduce Learning to Rank (LtR) and its use in Information Retrieval (IR). Then, we describe QuickScorer <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b40">[41]</ref> an efficient algorithm for scoring ensemble of regression trees. Finally, we discuss the field of model compression, a branch of machine learning that aims to compress Deep Neural Networks without affecting their accuracy. Here, we focus our attention in particular on pruning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning to Rank</head><p>Learning to Rank (LtR) consists in applying machine learning techniques to the problem of ranking documents with respect to a query. RankNet <ref type="bibr" target="#b6">[7]</ref> leverages a probabilistic ranking framework based on a pairwise approach to train a neural network. The difference between the predicted scores of two different documents is mapped to a probability by means of the sigmoid function. Hence, using the crossentropy loss this probability is compared with the ground truth labels, and Stochastic Gradient Descent (SGD) is used to minimize this loss. FRank <ref type="bibr" target="#b53">[54]</ref> exploits a generative additive model and substitutes the cross-entropy loss with the fidelity loss, a distance metric adopted in physics, superior to cross-entropy when applied on top of the aforementioned probabilistic framework since 1) has minimum in zero, 2) is bounded in [0, 1]. Neither RankNet nor FRank directly optimize a ranking metric (e.g., NDCG), and this discrepancy weakens the power of the model. Since ranking metrics are flat and discontinuous, coding them into the loss function is troublesome. To overcome this issue, LambdaRank <ref type="bibr" target="#b7">[8]</ref> heuristically corrects the RankNet gradients, exploiting the rank position of the document in the overall sorting: it multiplies the RankNet gradient with a term that measures the increase in terms of NDCG when switching the terms, generating the so-called ?-gradients. McRank <ref type="bibr" target="#b37">[38]</ref> casts the problem of ranking as MultiClass classification task, using a boosting tree algorithm to learn the class probabilities and then converting them into relevances with the expected relevance, outperforming LambdaRank. This work also highlights that modeling the ranking problem as a classification task works better than modeling it as a regression one. Lam-daMART <ref type="bibr" target="#b8">[9]</ref> combines the successful training methodology provided by ?-gradients with Multiple Additive Regression Trees (MART) -as McRank <ref type="bibr" target="#b37">[38]</ref>, and it has been establishing as the state-of-the-art in LtR. Currently, ensembles of regression trees are the most effective solution among LtR techniques when dealing with handcrafted features. In the next section, we describe state-of-the-art approaches for efficient traversal of these trees, in order to employ them in latency-bound scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Traversal of Tree-based Models</head><p>QuicksScorer <ref type="bibr" target="#b40">[41]</ref> is a state-of-the-art algorithm that allows to speedup the traversal of an ensemble of regression trees. As detailed in the previous section, ensemble of regression trees is the model exploited by several state-ofthe-art learning-to-rank solutions, e.g., LambdaMART <ref type="bibr" target="#b8">[9]</ref>. QuickScorer codes each tree of the ensemble as a bitvector of length n, where n is the number of leaves, which is used to select the exit leaf in the tree. Furthermore, each decision node in each tree is associated with a bitvector of the same length called mask. If the corresponding test is evaluated to false, the bits corresponding to the unreachable leaves are set to zero. By performing the logical AND among all the masks, we obtain another bitvector, named leafidx, in which the first one entry corresponds to the exit leaf. To efficiently compute the exit leaf, QuickScorer process all the nodes in a feature by feature fashion. For each feature f , the associated thresholds among all the nodes in the forest are sorted in ascending order. Let us a consider a threshold ? associated with a node g: when x f &gt; ?, the corresponding leafidx is updated performing the AND operation with the mask relative to g. Since the thresholds are sorted, as soon as x f ? ?, the evaluation of the current feature is interrupted, since the following instances will evaluate true as well. To further improve the efficiency of the algorithm, two variations of the original algorithm are introduced: 1) Block-Wise QuickScorer (BWQS), in which the forest is partitioned into blocks of trees fitting the L3 cache, reducing the cache-miss ration and 2) Vectorized QuickScorer (vQS) <ref type="bibr" target="#b41">[42]</ref>, in which scoring is vectorized using AVX2 instructions and 256-bit registers, allowing to process up to 8 document at time. Lettich et al. <ref type="bibr" target="#b34">[35]</ref> propose a GPU version of QuickScorer, to exploit the massive parallelism of this computational engine. By properly managing the GPU memory hierarchy and furnishing an adequate degree of parallelism in the document scoring process, this version results up to 100x faster than the corresponding CPU version, when dealing with very large forests (20,000 trees).</p><p>The cost of traversing an ensemble of regression trees with QuickScorer depends on the number of false nodes, rather than on the length of the root-to-leaf paths. Since machine-learnt trees are imbalanced, the authors experimentally show that this reduces the percentage of nodes to evaluate from 80% of classical traversal to the 30% of QuickScorer <ref type="bibr" target="#b40">[41]</ref>. Moreover, QuickScorer is implemented carefully taking into account cache and CPU issues. For example, QuickScorer structures are accessed sequentially thus favoring pre-fetching and avoiding branch mispredictions. However, when the number of leaves is larger than 64, scoring a model with QuickScorer can be inefficient. Recently, RapidScorer tackles the problem of forest with a larger number of leaves <ref type="bibr" target="#b58">[59]</ref>. In fact, when |leaves| &gt; 64, the logical AND between the bitvectors cannot be carried out in just one CPU instruction, hampering efficiency. For this reason, RapidScorer introduces a tree-size insensitive encoding, named epitome. Moreover, it leverages a node merging strategy that evaluates just once nodes sharing the same threshold on the same feature. By doing so, RapidScorer outperforms QuickScorer when dealing with a large number of leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Compression</head><p>The effectiveness of Deep Neural Networks (DNNs) comes at the cost of a high computational complexity <ref type="bibr" target="#b1">[2]</ref>, hindering the deployment and the usage of DNNs, especially for resource-constrained devices. An inherent feature of DNNs is over-parameterization, i.e., the redundancy of networks parameters: it has been proven that the same performance can be obtained with just a portion of the original parameters <ref type="bibr" target="#b13">[14]</ref>. Model Compression (MC) is a recent research field investigating effective techniques for reducing the memory impact of DNNs, their inference time, and energy consumption without affecting their accuracy, exploiting over-parameterization. In MC techniques, we observe the presence of several lines of research: pruning <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b59">[60]</ref>, quantization <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref> design of efficient architectures <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b60">[61]</ref>, knowledge distillation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>Recently, pruning has shown to be extremely effective <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Pruning techniques delete useless connections in a pre-trained model, producing sparse weight tensors that are lighter to store and allow for faster inference time. Performing a retraining after pruning avoids accuracy loss, even in the case of high compression factors <ref type="bibr" target="#b16">[17]</ref>. The canonical classification of pruning techniques divide them into two families: 1) element-wise pruning, which sets to zero individual weights, generating sparse weight tensors and 2) structured pruning, which prunes entire groups of weights, i.e., columns, filters, or even entire layers. In the latter case, the resulting network's weights still belong to the dense domain. In this paper, we focus on element wise-pruning techniques. These methods employ heuristics to determine what are the relevant weights of the network. In particular, magnitude-based heuristics work by removing low absolute-value weights and are proved to be effective <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In their na?ve version, magnitude based approaches remove a fixed percentage of weights from the original model (level pruning). Han et al. show that the gradual increase of the target sparsity, interleaved with a number of steps of re-training, can improve the accuracy of the final model <ref type="bibr" target="#b18">[19]</ref>. Furthermore, they propose a layer-wise threshold-based method to determine whether a parameter shall be kept or not. For each layer, its threshold t i is computed as as t i = ? i * s i , with ? i the standard deviation of weights distribution and s i a sensitivity parameter to be chosen. By assuming that parameters follow a Normal distribution N (0, ? 2 ), setting s i = 1 would approximately prune away about the 68% of the weights. The pruning step is followed by a number of re-training epochs on the surviving weights. The procedure can be then iterated by gradually increasing s i thus inducing higher sparsity. The Distiller Framework <ref type="bibr" target="#b63">[64]</ref> version that we adopt, keeps this threshold fixed, relying on the fact that as the tensor is pruned, more elements are pulled towards the center of the distribution and then pruned. Pruning techniques have shown to be able to sparsify state-of-the-art neural architectures up to 90%, thus strongly reducing their memory burden and easing the transmission and deployment on resource-constrained devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRAINING BY SCORES APPROXIMATION</head><p>In this section, we detail the methodology proposed by Cohen et al. <ref type="bibr" target="#b11">[12]</ref> to train neural models approximating ensembles of regression trees. Their technique can be considered as a special case of Knowledge Distillation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Knowledge distillation is a training technique in which a small student model is trained to mimic the outputs of a large and expressive teacher model. In the case of Cohen et al., the ensemble of regression trees plays the role of the teacher, while the neural network is the student model. The core idea of their approach is to treat the tree-based model as a black box producing accurate scores. Formally, let us consider a Learning to Rank dataset D = (X, Y ), X ? R f ?|D| , where f is the number of extracted features per document, |D| is the cardinality of the dataset, and Y ? N |D| is the set of ground-truth relevances of a document w.r.t. a query. Let F : R f ? R be the underlying function learned by an ensemble of regression trees during the training that maps a single document x ? X into a relevance score. If the neural model can reproduce the function F , it achieves the same ranking quality as the original model. The effectiveness of this approach relies on theoretical results showing that NNs can approximate continuous <ref type="bibr" target="#b23">[24]</ref> and piecewise continuous functions <ref type="bibr" target="#b38">[39]</ref>. In practice, the approximation is implemented by using the Mean Squared Error as loss function computed between the network prediction and the ensemble prediction. Furthermore, the training procedure is enriched with a data augmentation step which enforces the approximation capabilities of the neural network. Consider the set of f features in the dataset. For each feature, Cohen et al. <ref type="bibr" target="#b11">[12]</ref> build a list composed of the split points corresponding to that feature in the ensemble of regression trees, and, in the same list, they also put the maximum and the minimum for that feature in the training set. This way they obtain a set of f lists, where f is the number of the features in the dataset. Each of these lists is then sorted, and replaced with its ordered midpoints, e.g., each adjacent pair {x i , x i+1 } is replaced with its midpoint, . At each training step, half of the training data is built by randomly sampling from this feature-wise set of lists to have a better coverage of the whole feature space. Before feeding them to the network, all the training data are normalized by subtracting the mean and by dividing by the variance (Z-normalization). This approach is more proficient than directly learning the ground-truth relevance <ref type="bibr" target="#b11">[12]</ref>. As detailed in Section 1, the approximation error introduced is small but statistically significant in terms of ranking quality. In Section 5, we show how to mitigate this effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODELING MATRIX MULTIPLICATION</head><p>In this section, we detail the optimization of matrix multiplication on modern CPUs. We start with the implementation of dense-dense matrix multiplication (DMM) and then we move to the sparse-dense (SDMM) matrix case. Matrix multiplication has a prominent role in a wide spectrum of scientific applications (linear algebra, physics, economics, engineering), and it also represents the structural operation in neural network forward and backward pass. We believe that, when dealing with the efficiency-effectiveness tradeoff, a comprehensive analysis of the underlying multiplication mechanisms is essential. We develop time predictors for matrix multiplication both in the dense and in the sparse domain, and we then jointly apply them to develop an analytical model that estimates the scoring time of a neural network given the matrix shapes and the sparsity percentage of each layer of the Feed Forward Network (FFN). Our predictors are analytic, i.e., not learned, and they are based on 1) the knowledge gained from the implementation of DMM and SDMM on modern CPU architectures, 2) empirical measurements showing the performance of CPU on these operations under different conditions. We observe that, by exploiting the predictors we are proposing, we are allowed to train only the architectures that match the desired efficiency constraints. In a latency-bound application, the efficiency constraints are specified in the requirements. In an effectiveness-oriented context, they can be inferred by observing the execution time of the competitor, i.e., ensembles of tree-based models. As a consequence, the use of our predictors allows to significantly reduce the search space of the optimal architecture. Furthermore, our predictors are task-agnostic, hence they can be applied in any Feed Forward Network (FFN) application field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dense Matrix Multiplication</head><p>In this section, we investigate how Dense Matrix Multiplication (DMM) is optimized on modern CPUs. DMM has countless applications, hence lots of effort has been spent to attain fast implementations. The current state-of-the-art algorithm for DMM is the the well-known Goto Algorithm <ref type="bibr" target="#b15">[16]</ref>, on which are based several open (GotoBLAS <ref type="bibr" target="#b15">[16]</ref>, OpenBLAS <ref type="bibr" target="#b57">[58]</ref>, BLIS <ref type="bibr" target="#b25">[26]</ref>) or commercial (Intel MKL <ref type="bibr" target="#b56">[57]</ref>) implementations.</p><p>The multiplication of two n ? n dense matrices involves O(n 3 ) floating-point operations with O(n 2 ) data, as can be easily evicted from Equation 1. In modern processors, the interaction with memory is more time-consuming than the computation itself (memory bandwidth bottleneck), but a wise memory management allows to amortize the data movement over a large number of computations. The mathematical definition of matrix multiplication is the following: given A ? R m?k , B ? R k?n , the matrix multiplication binary operator computes C = A * B with C ? R m?n , where every element of C is given by</p><formula xml:id="formula_0">C i,j = k p=1 A i,p B p,j i = 1, . . . , m j = 1, . . . , n (1)</formula><p>The Goto Algorithm consists of iteratively decomposing the overall DMM into a series of smaller matrix operations in a cache-aware fashion, until matrices fit the CPU registers. Then matrices are multiplied by means of a highly engineered micro-kernel. We now provide a breakdown of the Goto Algorithm as implemented in the BLIS library <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b54">[55]</ref>, which assumes the CPU to be equipped with 3 levels of cache and vectorized instructions. The first three steps of the blocked matrix multiplication algorithm are depicted in <ref type="figure" target="#fig_5">Figure 1</ref>.     The blocked matrix multiplication algorithm begins by partitioning along the columns of C and B into blocks of size n c , obtaining sub-matrices of C of shape m ? n c and sub-matrices of B of shape k ? n c . Each C sub-matrix is obtained by multiplying the complete A matrix with the corresponding sub-matrix of B. Then, the procedure partitions the columns of A and the rows of B into blocks of size k c , to obtain A p , i.e., vertical panels of size m ? k c , and B i , i.e., horizontal panels of size k c ? n. The B i panels are packed into the L3 cache reordering data according to a specific pattern which allows to access data contiguously even after the subsequent partitions. We adopt the notatio? X to indicate that the sub-matrix X respects this pattern. Observe that, after the blocking on the k axis, the original multiplication is boiled down into a series of rank-k updates so that C = C + A p B p . A further partition is performed along rows of A, with size m c , generating C i and A i . A i is, as was B i previously, packed into? i in the L2 cache. Macro-Kernel. The macro-kernel, or inner kernel as in the original algorithm by Goto et al. <ref type="bibr" target="#b15">[16]</ref>, is responsible for orchestrating the memory movement between the RAM memory and the caches. Let us consider the operation</p><formula xml:id="formula_1">9 S g Y J L P K v 3 E 8 p i y C R 3 x X k Y 1 V d z 6 6 f z W G T n L l C E J I 5 O V R j J X f 0 + k V F k 7 V U H W q S i O 7 b K X i / 9 5 v Q T D a z 8 V O k 6 Q a 7 Z Y F C a S Y E T y x 8 l Q G M 5 Q T j N C m R H Z r Y S N q a E M</formula><formula xml:id="formula_2">O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; C i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x X q w z H X V 7 L + b h d y 4 e 8 q 1 H n z C c M A = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X Y C t 4 K o k U 1 F u x F 4 8 V 7 A e 0 s W y 2 k 3 b p Z h N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W a e H w u u 0 X G + r Z X V t f W N z d x W f n t n d 2 + / c H D Y 1 F G i G D R Y J C L V 9 q k G w S U 0 k K O A d q y A h r 6 A l j + q T f 3 W A</formula><formula xml:id="formula_3">= " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X 3 F 9 R Q U b m 8 E g W I V d E d Q u x M Y y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 S C 0 0 8 c O F w z r 3 c e 0 + Y c K a N 5 3 0 6 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p C 6 0 R y q V o h 1 p Q z Q e u G G U 5 b i a I 4 D j l t h s O r i d + 8 o 0 o z K W 7 M K K F B j P u C R Y x g Y 6 X G s E t c h L q F o l f y p k C L x P 8 h x f J B 7 Y u 9 V d 6 r 3 c J H p y d J G l N h C M d a t 3 0 v M U G G l W G E 0 7 H b S T V N M B n i P m 1 b K n B M d Z B N r x 2 j Y 6 v 0 U C S V L W H Q V P 0 9 k</formula><formula xml:id="formula_4">V C I U j K K V H l S f 9 c s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S i C t k k h r T 9 d w E / Y x q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b P 5 u f O i V n V h m Q M N a 2 F J K 5 + n s i o 5 E x k y i w n R H F k V n 2 Z u J / X j f F 8 M r P h E p S 5 I o t F o W p J B i T 2 d 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 2 R z o v z 7 n w s W g t O P n M M f + B 8 / g B M H I 3 U &lt; / l a t e x i t &gt; n c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K j P k C o F 0 Q E q V B P 1 b h 4 W z F i o e + 9 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E S c J 9 y M 6 V C I U j K K V H l S f 9 c s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S i C t k k h r T 9 d w E / Y x q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b P 5 u f O i V n V h m Q M N a 2 F J K 5 + n s i o 5 E x k y i w n R H F k V n 2 Z u J / X j f F 8 M r P h E p S 5 I o t F o W p J B i T 2 d 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 2 R z o v z 7 n w s W g t O P n M M f + B 8 / g B M H I 3 U &lt; / l a t e x i t &gt; n c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K j P k C o F 0 Q E q V B P 1 b h 4 W z F i o e + 9 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E S c J 9 y M 6 V C I U j K K V H l S f 9 c s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S i C t k k h r T 9 d w E / Y x q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b P 5 u f O i V n V h m Q M N a 2 F J K 5 + n s i o 5 E x k y i w n R H F k V n 2 Z u J / X j f F 8 M r P h E p S 5 I o t F o W p J B i T 2 d 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 n Z E P w</formula><formula xml:id="formula_5">C i ? C i +? i * B p , with C i of size m c ? n,? i of size m c ?k c andB p of size k c ?n.</formula><p>The macro kernel decomposes this operation into a series of block-panel multiplications, as shown in <ref type="figure" target="#fig_15">Figure 2</ref>. As aforementioned, both? i andB p are packed with a special pattern, indicated by the arrows in           In particular,? i is organized into sub-matrices? j of size m r ?k c , with elements stored in column-major order, whileB p is organized in panels of size k c ?n r , stored in rowmajor order, namedB j . This data access pattern reflects the order in which the micro-kernel accesses data.</p><formula xml:id="formula_6">V N 0 s S h K B b Y x n v 2 N B 1 w z a s X E E U I 1 d 7 d i O i K a U O v S K b k Q / O W X V 0 n r o u r X q t f 3 t U r 9 J o + j C C d w C u f g w y X U 4 Q 4 a 0 A Q K Q 3 i G V 3 h D A r 2 g d / S x a C 2 g f O Y Y / g B 9 / g B h U o 3 i &lt; / l a t e x i t &gt; k c &lt; l a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t e x i t s h a 1 _ b a s e 6 4 = " v o m h u u x 2 x A S z i B u q R 1 R / S x 6 J G n 8 = " &gt; A A</head><formula xml:id="formula_7">A B 7 X i c b V C 7 S g N B F L 0 b X 3 F 9 R Q U b m 8 E g W I V d E d Q u x M Y y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 S C 0 0 8 c O F w z r 3 c e 0 + Y c K a N 5 3 0 6 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p C 6 0 R y q V o h 1 p Q z Q e u G G U 5 b i a I 4 D j l t h s O r i d + 8 o 0 o z K W 7 M K K F B j P u C R Y x g Y 6 X G s E t c h L q F o l f y p k C L x P 8 h x f J B 7 Y u 9 V d 6 r 3 c J H p y d J G l N h C M d a t 3 0 v M U G G l W G E 0 7 H b S T V N M B n i P m 1 b K n B M d Z B N r x 2 j Y 6 v 0 U C S V L W H Q V P 0 9 k e F Y 6 1 E c 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>s 4 Y m 4 G e 9 y b i f 1 4 7 N d F F k D G R p I Y K M l s U p R w Z i S a v o x 5 T l B g + s g Q T x e y t i A y w w s T Y g F w b g j / / 8 i J p n J b 8 s 9 J l z a Z R g R n y c A h H c A I + n E M Z r q E K d S B w C / f w C E + O d B 6 c Z + d l 1 p p z f m b 2 4 Q + c 1 2 8 I 4 p H W &lt; / l a t e x i t &gt;</head><formula xml:id="formula_8">= " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X 3 F 9 R Q U b m 8 E g W I V d E d Q u x M Y y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 S C 0 0 8 c O F w z r 3 c e 0 + Y c K a N 5 3 0 6 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p C 6 0 R y q V o h 1 p Q z Q e u G G U 5 b i a I 4 D j l t h s O r i d + 8 o 0 o z K W 7 M K K F B j P u C R Y x g Y 6 X G s E t c h L q F o l f y p k C L x P 8 h x f J B 7 Y u 9 V d 6 r 3 c J H p y d J G l N h C M d a t 3 0 v M U G G l W G E 0 7 H b S T V N M B n i P m 1 b K n B M d Z B N r x 2 j Y 6 v 0 U C S V L W H Q V P 0 9 k</formula><formula xml:id="formula_9">V N 0 s S h K B b Y x n v 2 N B 1 w z a s X E E U I 1 d 7 d i O i K a U O v S K b k Q / O W X V 0 n r o u r X q t f 3 t U r 9 J o + j C C d w C u f g w y X U 4 Q 4 a 0 A Q K Q 3 i G V 3 h D A r 2 g d / S x a C 2 g f O Y Y / g B 9 / g B h U o 3 i &lt; / l a t e x i t &gt;</formula><formula xml:id="formula_10">= " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X 3 F 9 R Q U b m 8 E g W I V d E d Q u x M Y y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 S C 0 0 8 c O F w z r 3 c e 0 + Y c K a N 5 3 0 6 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p C 6 0 R y q V o h 1 p Q z Q e u G G U 5 b i a I 4 D j l t h s O r i d + 8 o 0 o z K W 7 M K K F B j P u C R Y x g Y 6 X G s E t c h L q F o l f y p k C L x P 8 h x f J B 7 Y u 9 V d 6 r 3 c J H p y d J G l N h C M d a t 3 0 v M U G G l W G E 0 7 H b S T V N M B n i P m 1 b K n B M d Z B N r x 2 j Y 6 v 0 U C S V L W H Q V P 0 9 k e F Y 6 1 E c 2 s 4 Y m 4 G e 9 y b i f 1 4 7 N d F F k D G R p I Y K M l s U p R w Z i S a v</formula><formula xml:id="formula_11">V N 0 s S h K B b Y x n v 2 N B 1 w z a s X E E U I 1 d 7 d i O i K a U O v S K b k Q / O W X V 0 n r o u r X q t f 3 t U r 9 J o + j C C d w C u f g w y X U 4 Q 4 a 0 A Q K Q 3 i G V 3 h D A r 2 g d / S x a C 2 g f O Y Y / g B 9 / g B h U o 3 i &lt; / l a t e x i t &gt; k c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v o m h u u x 2 x A S z i B u q R 1 R / S x 6 J G n 8 = " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X 3 F 9 R Q U b m 8 E g W I V d E d Q u x M Y y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 S C 0 0 8 c O F w z r 3 c e 0 + Y c K a N 5 3 0 6 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p C 6 0 R y q V o h 1 p Q z Q e u G G U 5 b i a I 4 D j l t h s O r i d + 8 o 0 o z K W 7 M K K F B j P u C R Y x g Y 6 X G s E t c h L q F o l f y p k C L x P 8 h x f J B 7 Y u 9 V d 6 r 3 c J H p y d J G l N h C M d a t 3 0 v M U G G l W G E 0 7 H b S T V N M B n i P m 1 b K n B M d Z B N r x 2 j Y 6 v 0 U C S V L W H Q V P 0 9 k e F Y 6 1 E c 2 s 4 Y m 4 G e 9 y b i f 1 4 7 N d F F k D G R p I Y K M l s U p R w Z i S a v</formula><formula xml:id="formula_12">9 S g Y J L P K v 3 E 8 p i y C R 3 x X k Y 1 V d z 6 6 f z W G T n L l C E J I 5 O V R j J X f 0 + k V F k 7 V U H W q S i O 7 b K X i / 9 5 v Q T D a z 8 V O k 6 Q a 7 Z Y F C a S Y E T y x 8 l Q G M 5 Q T j N C m R H Z r Y S N q a E M s 3 j y E L z l l 1 d J + 6 L u X d Z v H i 5 r j d s i j j K c w C m c g w d X 0 I B 7 a E I L G I z h G V 7 h z V H O i / P u f C x a S 0 4 x c w x / 4 H z + A H / V j e c = &lt; / l a t e x i t &gt; A i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V O e E l T S G E T M u Q q n z w z J v i a l l Q E E = " &gt; A A A B / 3 i c b V D J S g N B E O 2 J W 4 x b V P D i Z T A R P I U Z C a i 3 q B e P E c w C m S H 0 d G q S J j 0 L 3 T V i G O f g r 3 j x o I h X f 8 O b f 2 N n O W j i g 4 L H e 1 V U 1 f N i w R V a 1 r e R W 1 p e W V 3 L r x c 2 N r e 2 d 4 q 7 e 0 0 V J Z J B g 0 U i k m 2 P K h A 8 h A Z y F N C O J d D A E 9 D y h t d j v 3 U P U v E o v M N R D G 5 A + y H 3 O a O o p W 7 x w E F 4 Q M 9 P y w 5 y 0 Y P 0 s s u z c t Y t l q y K N Y G 5 S O w Z K Z E Z 6 t 3 i l 9 O L W B J A i E x Q p T q 2 F a O b U o m c C c g K T q I g p m x I + 9 D R N K Q B K D e d 3 J + Z x 1 r p m X 4 k d Y V o T t T f E y k N l B o F n u 4 M K A 7 U v D c W / / M 6 C f r n b s r D O E E I 2 X S R n w g T I 3 M c h t n j E h i K k S a U S a 5 v N d m A S s p Q R 1 b Q I d j z L y + S 5 m n F r l Y u b q u l 2 t U s j j w 5 J E f k h N j k j N T I D a m T B m H k k T y T V / J m P B k v x r v x M W 3 N G b O Z f f I H x u c P G H a W K w = = &lt; / l a t e x i t &gt; C j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q o i T 0 + Q T X B Q i 6 6 z I r 5 z K T r a d 3 C U = " &gt; A A A B 9 X i c b V D J S g N B E O 2 J W 4 x b 1 K O X x k T w F G Y k o N 6 C u X i M Y B Z I x t D T q U n a 9 C x 0 1 6 h h y H 9 4 8 a C I V / / F m 3 9 j Z z l o 4 o O C x 3 t V V N X z Y i k 0 2 v a 3 l V l Z X V v f y G 7 m t r Z 3 d v f y + w c N H S W K Q 5 1 H M l I t j 2 m Q I o Q 6 C p T Q i h W w w J P Q 9 I b V i d 9 8 A K V F F N 7 i K A Y 3 Y P 1 Q + I I z N N J d B + E J P T 8 t V r v 3 x X E 3 X 7 B L 9 h R 0 m T h z U i B z 1 L r 5 r 0 4 v 4 k k A I X L J t G 4 7 d o x u y h Q K L m G c 6 y Q a Y s a H r A 9 t Q 0 M W g H b T 6 d V j e m K U H v U j Z S p E O l V / T 6 Q s 0 H o U e K Y z</formula><formula xml:id="formula_13">v z O H F v T 1 z U G s E 8 r H 0 3 o k y G v a Z 0 = " &gt; A A A B / 3 i c b V D J S g N B E O 2 J W 4 z b q O D F y 2 A i e A o z E l B v I V 4 8 R j A L J M P Q 0 6 l J W n s W u m v E M O b g r 3 j x o I h X f 8 O b f 2 N n O W j i g 4 L H e 1 V U 1 f M T w R X</formula><formula xml:id="formula_14">U i Q z 1 D 3 z q 9 u L W R p C h E x Q p T q O n a C b U Y m c C R g V u q m C h L I 7 2 o e O p h E N Q b n Z 5 P 6 R d a y V n h X E U l e E 1 k T 9 P Z H R U K l h 6 O v O k O J A z X t j 8 T + v k 2 J w 7 m Y 8 S l K E i E 0 X B a m w M L b G Y V g 9 L o G h G G p C m e T 6 V o s N q K Q M d W Q</formula><formula xml:id="formula_15">= " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 C b a C B y m J F N R b 0 Y v H C v Y D 2 h A 2 2 0 2 7 d r M J u x N p C f k r X j w o 4 t U / 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m n h 9 z p s C 2 v 4 3 C 2 v r G 5 l Z x u 7 S z u 7 d / Y B 6 W 2 y p K J K E t E v F I d n 2 s K G e C t o A B p 9 1 Y U h z 6 n H b 8 8 e 3 M 7 z x R q V g k H m A a U z f E Q 8 E C R j B o y T P L f a A T 8 I O 0 S r x U n j 9 m 1 c w z K 3 b N n s N a J U 5 O K i h H 0 z O / + o O I J C E V Q D h W q u f Y M b g p l s A I p 1 m p n y g a Y z L G Q 9 r T V O C Q K j e d 3 5 5 Z p 1 o Z W E E k d Q m w 5 u r v i R S H S k 1 D X 3 e G G E Z q 2 Z u J / 3 m 9 B I I r N 2 U i T o A K s l g U J N y C y J o F Y Q 2 Y p A T 4 V B N M J N O 3 W m S E J S a g 4 y r p E J z l l 1 d J + 6 L m 1 G v X 9 / V K 4 y a P o 4 i O 0 Q k 6 Q w 6 6 R A 1 0 h 5 q o h Q i a o G f 0 i t 6 M z H g x 3 o 2 P R W v B y G e O 0 B 8 Y n z / a D p R W &lt; / l a t e x i t &gt; L 2 cache &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l f Q W F A d U z n S q 1 W C x Y V v o 4 N j w H Q s = " &gt; A A A B + n i c b V D L S g N B E J z 1 G e N r o 0 c v g 0 H w F H Z D Q L 0 F v X j w E M E 8 I F n C 7 K S T D J l 9 M N O r h j W f 4 s W D I l 7 9 E m / + j Z N k D 5 p Y 0 F B U d d P d 5 c d S a H S c b 2 t l d W 1 9 Y z O 3 l d / e 2 d 3 b t w s H D R 0 l i k O d R z J S L Z 9 p k C K E O g q U 0 I o V s M C X 0 P R H V 1 O / e Q</formula><p>Goto et al. <ref type="bibr" target="#b15">[16]</ref> observed the advantages of packing? i into the L2 cache. The ratio between FLOPs and memory operations, regardless if the original data rely in L3 cache or in main memory, can be modeled as</p><formula xml:id="formula_16">2m c k c (2m c + k c )</formula><p>if k c &lt;&lt; n. Hence, the higher is the m c k c product, the smaller is the overhead of memory transfer on the overall computation. Knowing that L2 cache is larger than L1, we can afford larger m c and k c values 2 . Micro-Kernel. The micro-kernel is the core operation of blocked matrix multiplication and the speed of the whole routine largely depends on the speed of this kernel. For this reason, in high-performance libraries, the micro-kernel is often written in assembly language, to exploit vectorized instructions and hand-tuned data pre-fetching <ref type="bibr" target="#b54">[55]</ref>. The micro kernel computes c r,j = c r,j +? jBj , where? j is an horizontal micro-panel of? i andB j is a vertical micropanel ofB p , residing, respectively, in L2 and L1 cache, as reported in <ref type="figure">Figure 3</ref>. The operation is performed as k c rank-1 updates, by computing the outer product between a column of? j and a row ofB j and by accumulating the results into the m r ? n r c r,j submatrix. In this way, c r,j can be kept in CPU registers until the loop over k c is , allowing to move data from the registers to the memory just once. This means that 2m c n c k r FLOPs can be performed with just m r n r memory operations. Furthermore, this data reading pattern benefits from the data packing performed in the previous loops. In fact, columns and rows of? j andB j respectively will be accessed contiguously, which is generally known to be faster than accessing non-in-stride memory 2. In the original work, Goto et al. <ref type="bibr" target="#b15">[16]</ref> point out that C i ? C i +? iBp should be computed at the peak rate of CPU. This condition is true if all three matrices reside in L1 cache, but it can be considered true even if? i is in L2.  locations <ref type="bibr" target="#b39">[40]</ref>. In conclusion, pre-fetching instructions that load successive entries of? j andB j are interleaved with instructions performing the rank-1 update. This allows to mask the latency of the caches with the computation time of the CPU. Choosing the kernel parameters. Blocked matrix multiplication requires to determine a number of parameters n c , m c , k c , n r , m r , controlling how the matrices are gradually decomposed. These parameters can differ from one processor to another, since they are influenced by hardware features such as the cache size or the number of SIMD registers. Choosing the optimal parameters for a given CPU architecture is a research problem, tackled for example by Low et al. <ref type="bibr" target="#b39">[40]</ref>, which goes beyond the scope of this paper. Here, we want to list some general rules governing good choices for parameters. The micro-kernel is characterized by m r , n r , k c . The values of m r and n r should be large enough so that the computation masks the latency of the caches. However, it should also allow to leave space in the registers for the next entries of? j andB j . k c should be as large as possible, but must take into account the following constraints: 1) k c n r entries fromB j should fit the L1 cache 2) m c k c entries from? i reside in the L2 cache. Moreover, cache replacement policies should also be taken into account. These policies control which data are kept and which are discarded from the levels of cache and may impact on the optimal macro-kernel values. A general solution is provided by Goto et al., who suggest choosing k c so thatB j takes less than the half of the L1 cache <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_17">b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m l 6 / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x N e + R m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S</formula><p>Concerning the macro-kernel, we already discussed that the m c k c product should be as large as possible. One of the key insights of the Goto algorithm is to consider the role of the Translation Look-Aside Buffer (TLB) in choosing the macro-kernel parameters. To hide the limits of randomaccess memories capacity (RAM), modern computing architectures use virtual memory. With this mechanism, the memory (RAM and hard disk) is partitioned into pages and a table, called page table, keeps track whether a page is in memory or on disk. Scanning the page table entails additional overhead to check whether the requested page is on memory or disk. Hence, the TLB, which is smaller than the overall page table, keeps track of the most recently used pages: in case of a TLB hit, the translation is fast. On the other side, in case of a TLB miss, the complete page table is checked and the new entry is moved to the TLB. Actually, the TLB has the same role as the cache and the hit/miss dichotomy involves the same consequences. Thus, besides ensuring that m c k c entries from? i fit the L2 cache, it is crucial that? i , n r columns from C k , and n r columns ofB j are simultaneously addressable by the TLB, to avoid TLB misses during the block-panel multiplications of the macrokernel. The only limit to the n c parameter is that k c n c have to fit the L3 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dense Neural Forward Pass Time Predictor</head><p>In the previous section, we detail how Dense Matrix Multiplication is implemented on modern CPU architectures. We now show how the insights deriving from a deep understanding of matrix multiplication can be used to develop a time predictor for a Feed Forward Network (FFN) forward pass. We empirically demonstrate that even the highly engineered Goto algorithm suffers when dealing with edge matrix dimensions. Hence, we leverage this intuition to build a hybrid analytical-empirical model for predicting dense matrix multiplication. A FFN is composed of a stack of fully connected layers, where each neuron of layer i is connected to all neurons of layer i + 1. Each layer is composed of a weight matrix W i , a bias vector b i and a non-linear activation function ? i (?). Let x i be the input to the i-th layer, the forward pass of layer i is described by:</p><formula xml:id="formula_18">x i+1 = ? i (W t i x i + b i )<label>(2)</label></formula><p>where x i+1 represents the output of the i-th layer. Hence, forwarding through a FFN layer consists of: 1) multiplying the input with the weight matrix, 2) summing the bias, 3) applying a non-linear activation function, usually ReLU or its variants. The overall forward pass on a FFN of d layers has a cost, in terms of execution time, given by:</p><formula xml:id="formula_19">T = t m ? (f ? l 1 + d i=2 l i l i?1 + l d ) + t a ? d i=1 l i + t r ? d i=1 l i t m ? (f ? l 1 + d i=2 l i ? l i?1 + l d )<label>(3)</label></formula><p>where t m is the normalized time per multiplication, t a is the time for addition, t r is the time to perform the ReLU operation on a single neuron. As reported in Equation 3, the time to perform matrix multiplication dominates the overall execution time, both in terms of number of operations and in terms of the complexity of the operation itself. We observe that t m can be inferred as:</p><formula xml:id="formula_20">t m = 1 GFLOPS<label>(4)</label></formula><p>The theoretical peak of GLOPs can be derived form the hardware specifications of the processor 3 . However, real performance can be significantly different from the theoretical ones, especially when facing limit cases, such as narrow or wide matrices. To include these cases into our evaluation, we develop a prediction model to measure the performance of a specific neural networks architecture. Among the different instantiations of the BLAS library, we choose oneDNN 4 , a C++ high-performance framework <ref type="bibr" target="#b2">3</ref>. https://software.intel.com/en-us/articles/a-simple-example-tomeasure-the-performance-of-an-intel-mkl-function 4. https://github.com/oneapi-src/oneDNN for deep learning primitives developed by Intel, used as backbone inference system by Pytorch <ref type="bibr" target="#b44">[45]</ref>, Tensorflow <ref type="bibr" target="#b0">[1]</ref>. With respect to the Math Kernel Library (MKL) <ref type="bibr" target="#b56">[57]</ref> by Intel, oneDNN guarantees the same performances while being open source. The oneDNN library adopts the following parameters for CPUs with AVX2 ISA enabled: m c = 10000, n c = 384, k c = 192, while for the micro-kernel we have m r = 24, n r = 4. The macro-kernel parameters m c , n c , k c are selected to deal with very large matrices; for the sequential case, the library contains a mechanism to tailor smaller shapes. Let us call m c , n c , k c the parameters that the macro and micro kernels actually use. m c is chosen as:</p><formula xml:id="formula_21">m c = rnd_up(min(max(m, m r ), m c ), m r )</formula><p>where rnd_up(a, b) is a function which approximates a as a = n * b, with n * = min{n | nb ? a}, i.e., to the subsequent multiple of b. This way, it is ensured that m c is larger than the micro-kernel parameter m r and that the default m c is not involved if m ? m c . By means of the rnd_up function, we ensure that m c mod m r = 0 to avoid undersized horizontal? j panels in the micro-kernel. Similar refinements are adopted to chose n c and k c . Moreover, oneDNN triggers when the cost of packing the matrices into contiguous arrays surpasses the cost of multiplication. In this case, besides changing the macro-kernel parameters, it also performs a different routine that skips copying the matrices in cache-aware buffers. In Section 4.1 we have described the optimization techniques beyond Dense Matrix Multiplication on modern CPUs. We also detail the tailored refinements implemented by the oneDNN framework to deal with matrices where at least one dimension is small. We now show the performance of the oneDNN framework with differently shaped matrices, aiming at identifying a reliable t m for Equation <ref type="bibr" target="#b3">4</ref>.</p><p>In these experiments, we multiply random matrices with different shapes to empirically analyze how the oneDNN library adapts to different matrix dimensions. We propose two different cases: 1) m = k, 2) mk = c, with c as a constant integer. We run our tests on a i9-9900K processor, with AVX2 instructions, 3.6 GHz, max frequency 5.0 GHz. Each core has a 32 KiB L1 cache for data, 32 KiB L2 cache for instructions, both 8-way set associative, 256 KiB L2 cache 4-way set associative, and 2 MiB L3 cache, 16-way set associative. We report the results for single-thread execution. In our first experiment, we vary m and k in a fixed range and report the corresponding GFLOPs, with different values of n. Observe that A, of shape m ? k, represents the weight matrix W , B, of shape k ? n represents the input matrix x, obtained by stacking n input vector. We vary m, k and n to model real use-case scenarios: m and k correspond to the sizes of Feed Forward Network layers, while n, the batch size, is the number of documents we give in input to the neural network at a time. Results are reported in <ref type="figure" target="#fig_20">Figure 4</ref>, which shows that GFLOPS grow as the size of the matrices even with the aforementioned techniques tailored to edge cases. In <ref type="figure" target="#fig_21">Figure 5</ref>   values of m cause serious performance degradation. The variation of the GFLOPS with the matrix shapes suggests that a unique and size-independent t m is not reliable. As aforementioned, this evidence some limitations of the Goto algorithm when dealing with edge combinations of input dimensions. A correct analysis expresses t m as a function of the m, n, k parameters, or in the case of the Feed Forward Network , as a function of the dimensions of the layers, i.e., t m = t m (l 1 , . . . , l d ). Given the variability of the performance with input shapes, we shall empirically measure them. We can use <ref type="figure" target="#fig_20">Figure 4</ref> and 5 to derive a lookup table that maps the matrix shapes to the corresponding GFLOPs. The previous graphs are synthesized in <ref type="figure" target="#fig_22">Figure 6</ref>, which shows an heatmap of the GLFOPs with different values of m and k and n = 1000.</p><p>We observe three performance zones, defined by horizontal stripes induced by partitioning the k axis.</p><p>? K ? 512 : high-performance (130 GFLOPs) ? 128 ? K ? 512: Medium performance (110 GFLOPS) ? K ? 128: Low performance (90 GFLOPs) For a network of size {1000, 500, 500, 100}, we can assume to be always in the high-performance region, except for the last layer. Observe that the last layer has a negligible impact on the overall forward time and we can ignore it.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sparse-Dense Matrix Multiplication</head><p>In this section, we study Sparse-Dense Matrix Multiplication (SDMM), a special case of matrix multiplication where the first matrix is sparse: we recall that sparsity is defined as the percentage of zero entries in a data structure, in this case, a matrix. First, we describe a common format to store sparse matrix, Compressed Sparse Row (CR). Then, we detail how SDMM is implemented on modern CPU processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSR Format.</head><p>A sparse matrix is completely identified by its non-zero values and their positions since all the others entries are zeros. This motivates the use of a different representation for sparse matrices w.r.t. to dense ones. The different representation aims at saving storage space and improving the performance of matrix multiplication. For this purpose, several formats have been developed: the most common are Compressed Sparse Row (CSR), Compressed Sparse Column (CSC), Coordinate List (COO). Among them, we analyze CSR, since it is usually supported by off-the-shelf libraries, both for storing and for matrix operators, such as multiplication and it naturally fits to Sparse-Dense Matrix Multiplication, as we will detail.    <ref type="bibr" target="#b52">[53]</ref>, non-negative matrix factorization <ref type="bibr" target="#b31">[32]</ref>, economic modeling, seismic simulations <ref type="bibr" target="#b4">[5]</ref>, and machine learning <ref type="bibr" target="#b50">[51]</ref>. Pruning a neural network pre-trained model naturally induces the usage of SDMM in the forward pass of a Multi-Layer Perceptron, since it converts dense weights into sparse ones. Let us consider Equation 2: in the most general case W represents the dense weight matrix. After pruning, W is transformed into a sparse matrix? , thus converting? T x into a Sparse Dense Matrix Multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>s G G 4 E t h O F N A o E t o L R 3 c x v P a H S P J Y P Z p y g H 9 G B 5 C F n 1 F i p f t k r l t y y O w d Z J V 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q D K c C Z w W u q n G h L I R H W D H U k k j 1 P 5 k f u i U n F m l T 8 J Y 2 Z K G z N X f E x M a a T 2 O A t s Z U T P U y 9 5 M / M / r p C a 8 8 S d c J q l B y R a L w l Q Q E 5 P Z 1 6 T P F T I j x p Z Q p r i 9 l b A h V Z Q Z m 0 3 B h u A t v 7 x K m h d l r 1 K + q l d K 1 d s s j j y c w C m c g w f X U I V 7 q E E D G C</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D d t s W f u z Z v b L 9 / t A B w 5 b 9 K u A Z U M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y V o 0 e i F 4 + Q y C O B D Z k d e m F k d n Y z M 2 t C C F / g x Y P G e P W T v P k D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R c x v P a H S P J Y P Z p y g H 9 G B 5 C F n 1 F i p f t k r l t y y O w d Z J V 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q D K c C Z w W u q n G h L I R H W D H U k k j 1 P 5 k f u i U n F m l T 8 J Y 2 Z K G z N X f E x M a a T 2 O A t s Z U T P U y 9 5 M / M / r p C a 8 8 S d c J q l B y R a L w l Q Q E 5 P Z 1 6 T P F T I j x p Z Q p r i 9 l b A h V Z Q Z m 0 B h u A t v 7 x K m h d l r 1 K + q l d K 1 d s s j j y c w C m c g w f X U I V 7 q E E D G C A 8 w y u 8 O Y / O i / P u f C x a c 0 4 2 c w x / 4 H z + A I A 9 j L 8 = &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D d t s W f u z Z v b L 9 / t A B w 5 b 9 K u A Z U M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y V o 0 e i F 4 + Q y C O B D Z k d e m F k d n Y z M 2 t C C F / g x Y P G e P W T v P k D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R c x v P a H S P J Y P Z p y g H 9 G B 5 C F n 1 F i p f t k r l t y y O w d Z J V 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q D K c C Z w W u q n G h L I R H W D H U k k j 1 P 5 k f u i U n F m l T 8 J Y 2 Z K G z N X f E x M a a T 2 O A t s Z U T P U y 9 5 M / M / r p C a 8 8 S d c J q l B y R a L w l Q Q E 5 P Z 1 6 T P F T I j x p Z Q p r i 9 l b A h V Z Q Z m 0 B h u A t v 7 x K m h d l r 1 K + q l d K 1 d s s j j y c w C m c g w f X U I V 7 q E E D G C A 8 w y u 8 O Y / O i / P u f C x a c 0 4 2 c w x / 4 H z + A I A 9 j L 8 = &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D d t s W f u z Z v b L 9 / t A B w 5 b 9 K u A Z U M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y V o 0 e i F 4 + Q y C O B D Z k d e m F k d n Y z M 2 t C C F / g x Y P G e P W T v P k D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R c x v P a H S P J Y P Z p y g H 9 G B 5 C F n 1 F i p f t k r l t y y O w d Z J V 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q D K c C Z w W u q n G h L I R H W D H U k k j 1 P 5 k f u i U n F m l T 8 J Y 2 Z K G z N X f E x M a a T 2 O A t s Z U T P U y 9 5 M / M / r p C a 8 8 S d c J q l B y R a L w l Q Q E 5 P Z 1 6 T P F T I j x p Z Q p r i 9 l b A h V Z Q Z m 0 B h u A t v 7 x K m h d l r 1 K + q l d K 1 d s s j j y c w C m c g w f X U I V 7 q E E D G C</head><p>Consider the operation C = AB, where A ? R m?k is a sparse matrix in the CSR representation with nnz non-zero values, and B ? R k?n , C ? R m?n are dense matrices. The mundane algorithm induced by A being in CSR Format is reported in Algorithm 1. This format is suitable for row-wise access, allowing to consider exclusively the non-zero entries of the left-side matrix. The total number of floating-point operations is reduced from 2mnk to 2nnzN w.r.t. dense case, but the irregular access pattern induced by sparsity hinders the efficiency of the algorithm. To overcome this problem, a twofold strategy, as for the dense case, is applied: 1) proficient data access pattern, 2) optimization of the core operation (micro-kernel).</p><p>The most used library for sparse matrix multiplication is the Math Kernel Library (MKL) <ref type="bibr" target="#b56">[57]</ref>, which implements the sparse versions of third level BLAS routines. Since the library is closed and there are no details on how the multiplication is implemented 5 , we refer to the implementation of the LIBXSMM <ref type="bibr" target="#b21">[22]</ref>, which is open-source. Later on in this section, we show that LIBXSMM actually outperforms MKL in the spectrum of shapes involved by our neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse-Dense matrix multiplication with LIBXSMM.</head><p>LIBXSMM <ref type="bibr" target="#b21">[22]</ref> is a high-performance library specifically tailored for Intel architectures, specialized in small dense matrix multiplication, sparse matrix multiplication, and deep learning primitives in general. It is based on "Just in Time" (JIT) code specialization, which intends to exploit the runtime information about its operands. The sparsedense routine was originally developed to solve seismic equations <ref type="bibr" target="#b4">[5]</ref>.</p><p>We now detail the sparse-dense matrix multiplication as implemented in the LIBXSMM library, with A in CSR format. The dense matrix B is converted into a three dimensional tensor of shape k ?N b ?n b , as reported in <ref type="figure">Figure 8</ref>, so that N = N b ? n b . This means to factorize the N dimension in two sub-dimension, in which one (n b ) is induced by the underlying hardware. The ideal value of n b in fact, coincides with the SIMD length of the processor, i.e., the number of different numbers that a SIMD vector can store. Using floating-point variables (32 bit) on a machine with AVX2 ISA (256 bit), the SIMD length is 8. This packing allows to multiply each non-zero element of A with nb values of B at time, using just one vectorized instruction.</p><p>The problem of irregular accesses is tackled by hardwiring the loading of the elements of A and B, so that only relevant elements are loaded. The data access pattern provides for multiplying each non-zero element of A (a i,j ) with the j-th rows of B (B j ) and accumulate the results into the i-th row C (C i ). The computation is carried on one row of A at time. <ref type="figure">Figure 9</ref> shows the sub-routine performed for each row i. Let us call the first non-zero element of the current row x, in position (i, j); we assume to have at least one non-zero entry, otherwise the row is skipped. C i is loaded into N b SIMD registers, each containing n b values.</p><p>x is broadcasted to a SIMD CPU register, i.e., n b consecutive copies of the x vector are loaded into the register. We refer to this vector as x. B j is loaded as well and C i is updated as</p><formula xml:id="formula_22">C i ? C i +xB j ; the update involves N b Fused Multiply Add (FMA) instructions C i,k ? C i,k + xB j,k .</formula><p>Then, the routine moves to the next non-zero elements in the i-th row of A.</p><p>Once all the non-zero elements have been multiplied, C i is stored in memory and the algorithm moves on to the next row of A. LIBXSMM is equipped with a mechanism that interrupts the code generation if the number of instructions is too elevated. This can happen if the number of nonzero elements in A or the N dimension are too large. Since the N dimension corresponds to the batch size in the neural forward, we are free to reduce it to overcome this limit. When necessary, we also split the m ? k A matrix along the M dimension to generate a set of sub-matrices A S = {A 1 , . . . , A s | A i of size M/s ? k}. Each A i will have fewer non-zero entries, preventing code generation failure. The C matrix is trivially obtained by multiplying each A i ? A S with B separately and by stacking the results along the M (vertical) axis:</p><formula xml:id="formula_23">C = ? ? ? ? ? ? A 1 B A 2 B . . . A s B ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIBXSMM vs MKL.</head><p>As aforementioned, MKL is known to provide the fastest routine for sparse-dense matrix multiplication. We now show that LIBXSMM outperforms MKL on small, very sparse, and asymmetric matrices, which is the typology of matrices we employ in our MLPs for document scoring. In <ref type="table" target="#tab_6">Table 3</ref>, we report the execution time of C = AB, with A sparse in the CSR format and B dense, both for MKL and LIBXSMM; on the x-axis is reported the shape (m?k) A and its sparsity. B has shape k ?n, where n is the batch size, set to 64. The matrices correspond to the first layer of real models trained on the MSN30K dataset <ref type="bibr" target="#b46">[47]</ref>, which provides 136 handcrafted features. The <ref type="table">Table shows</ref> that LIBXSMM is always faster than MKL on these shapes, with a speedup factor often larger than 2x. This consideration, together with the availability of the code, has led us to pick the LIBXSMM library as the reference implementation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sparse Time Predictor</head><p>In this section, we illustrate the development of a Sparse-Dense matrix multiplication time predictor, specularly for what we have done for the dense case. As detailed in Section 4.3, the algorithm provides for iterating over the rows of A with at least one non-zero entry. We start by analyzing the time cost of multiplying the i-th row of A with B, which is given by the sum of the cost of the following operations. 1) Loading N b vectorized elements from C (each one of size n b ). 2) Loading each non-zero element in A i . Since the nonzero values of A are stored contiguously in A.values, this operation benefits from cache memory. 3) Loading N b vectorized elements of B (of size n b ) for each non zero element of A. 4) Updating C j ? C j + x * B j , for each x = 0 in A i . Each update consists in N b FMA instructions. 5) Storing N b vectorized elements of C (each one of size n b ). Let us define a c as the set of active columns in A, namely the set of columns containing at least one non-zero element, and a r the set of active rows in A. Let us also define L c as the cost to load and store N b elements of C, L a the cost of loading one element of A and updating C j with N b FMA instructions, and L b the cost of loading N b elements of B. When generalizing the previous costs to the entire matrices, we have to take into account the effects of caching. While A and C are loaded just once, B elements can be loaded multiple times; whether they benefit or not from the caching mechanism depends on the access pattern induced by the non-zero entries of A. For example, if x in position (i, j) is a non-zero element, B j needs to be loaded into the registers from the main memory and the cache will also retain a copy of B j . Assume that in a successive row of A exists an element x = 0 on the same column of x, i.e. in position (g, j), with arbitrary g. When performing C g ? C g + x B j , B j already resides in the cache: since loading elements from the cache is way much faster than loading them from memory, the cost of re-loading B j can be considered negligible. Assuming that once a row of B is loaded into the cache it remains there until the end of the operation, we pay the cost of loading a row B j just the first time that this row is loaded. At the same time, if there are any inactive rows, they are never used in the multiplication routine. Since the number of active rows in B is equal to the number of active columns of A, the cost of loading B can be approximated with L b |a c |, with |a c | representing the number of active columns in A.</p><p>The overall cost of SPMM with the LIBXSMM is given by:</p><formula xml:id="formula_24">T = |a r | * L c + nnz * L a + |a c | * L b<label>(5)</label></formula><p>With an accurate estimation of L a , L b , and L c , we can predict the execution time of a sparse-dense matrix multiplication just from the structure of the sparse matrix. Note that this structure is known a priori, being the sparse matrices the pruned weights of the neural model. We begin by observing that L b and L c both describe memory operations, with the difference that L c measures both data reading and writing, while L b refers to data reading. We empirically verify that both the operations have the same time cost, i.e., L c = 2L b .</p><p>We now infer the coefficients L a , L b , L c , starting from L b . We cannot measure with a timer the cost of the elementary operations we have divided the LIBXSMM SPMM routine in, but we can empirically compute them by difference.</p><p>Let us consider two different sparse matrices A c and A rd with the same shape m ? k and the same number of nonzero entries (nnz). A c has the non-zero values disposed on the same column j * , i.e., is a matrix where a i,j = 0 if j = j * . A rd is a sparse matrix which has single non-zero entry for each row and each column, i.e., i=m?1 i=0 a i,j = 1, ?j = 0, . . . , m ? 1 and i=k?1 i=0 a i,j = 1, ?i = 0, . . . , k ? 1. The cost of multiplying A c and A rd with a dense matrix is given by:</p><formula xml:id="formula_25">T (A c ) = m * L c + nnz * L a + 1 * L b T (A rd ) = m * L c + nnz * L a + k * L b so, T (A rd ) ? T (A c ) = (k ? 1) * L b</formula><p>We can experimentally measure T (A rd ) and T (A c ) and use them to compute L b , since k in known.</p><p>To derive L a , we use the same A c as before and a second matrix A 2c , having 2 * nnz non-zero entries, organized along two columns. The cost for multiplying A 2c with a dense matrix is given by</p><formula xml:id="formula_26">T (A 2c ) = m * L c + 2 * nnz * L a + 2 * L b</formula><p>Since L b can be derived using the previous expression, we can subtract T (A 2c ) and A c and obtain L a as:</p><formula xml:id="formula_27">L a = (T (A 2c ) ? T (A c ))/nnz ? L b</formula><p>Our aim is to compute size-agnostic L a , L b and L c . We set M = K and vary them in {200, 300, 400, 500} and we experiment N ? {16, 32, 64}. L b and L c depends on N (and so do L c , which is computed doubling L b ), so we normalize diving by N . We observe that when N ? 128, the value obtained for L a and L b diverge w.r.t. to smaller batch size. In fact, larger N values (N ? 128) break the hypothesis of B residing inside the cache during the whole multiplication, which is a fundamental assumption of our time predictor. The definitive time predictor parameters are computed as an average of their value obtained with different shape configurations. We demonstrate the validity of our sparse predictor in <ref type="table" target="#tab_8">Table 4</ref>, where we report the predicted and the real execution time needed to multiply the weights of the first layer of several neural models with a random input. We restrain our experiments to the sparsity range obtained with pruning on our neural architectures. As we will detail later, at these sparsity levels the time required for SDMM is negligible w.r.t. to its dense counterpart. We also evidence that our time predictor is specific to matrix multiplication, hence can be essentially applied to fully-connected layers. Convolution or attention-based architectures have different properties that require a specific investigation. We leave these analyses for future works.</p><p>As we can see, the predictor is capable of correctly estimating the execution time of different models at high levels of sparsity, with a small error. Specifically, the predictor can fruitfully distinguish between matrix with the same shape but with different sparsity percentages; two examples are the 200 ? 136 and the 100 ? 136 instances in <ref type="table" target="#tab_8">Table 4</ref>. First, we observe that with sparsity percentage in the order of 1%, SDMM execution times can vary up to 30%. Second, the time predictor correctly reflects this peculiarity, thanks to the deep understanding of the routine details that stands behind its development.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NEURAL ENGINEERING</head><p>In Section 1, we claim that ensembles of regression trees consistently outperform, both in terms of effectiveness and efficiency, NNs trained with the method proposed by Cohen et al. <ref type="bibr" target="#b11">[12]</ref>, when documents are scored on CPU. In this section, we break down a methodology used to create efficient neural models for ranking that can compete with ensembles of tree-based ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Approximation of an Ensembles of Trees</head><p>We employ the methodology proposed by Cohen et al. <ref type="bibr" target="#b11">[12]</ref> to train neural models that approximate the scores of an ensemble of regression trees. This approach is effective since we use a powerful model, i.e., an ensemble of regression trees, and a profitable learning strategy, i.e., a listwise approach, to extrinsic the structure of the actual underlying probability distribution. This facilitates the learning process of a simpler model, i.e., a shallow neural network. The idea is inherited from a deep learning compression technique named Knowledge Distillation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b22">[23]</ref> in which a small, production-oriented, network (student) is trained to mimic the output of a large and effective network (teacher).</p><p>To fully leverage the benefits of this technique, we train an ensemble of regression trees with the best performance on a validation set without taking into account its efficiency. Then, we use its scores as ground truth in a distillation process that trains our neural models. In <ref type="table" target="#tab_10">Table 5</ref>, we report the validity of this approach using the MSN30K dataset <ref type="bibr" target="#b46">[47]</ref>, a widely adopted LtR dataset composed of more than 30,000 queries, with about 120 documents per query, where each document is a vector of 136 features <ref type="bibr" target="#b5">6</ref> . We adopt the NDCG@10 as quality metric. First, we observe the difference in terms of ranking precision between: 1) a model trained with a fixed number of leaves, i.e., 64, 2) the best model we could obtain on the MSN30K dataset. The latter one, which results to have 256 leaves per tree, consistently outperforms the 64-leaves model. Increasing the number of leaves in tree-based models allows for a remarkable gain in terms of NDCG@10. Indeed, when scoring a tree-based model with QuickScorer, the execution time scales at least linearly with the number of trees and leaves <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Hence, a 256-leaves model is more than 4x slower than a 64-leaves one with the same number of trees. In fact, given that the scoring time per document of 64-leaves models is 8.2 ?s, a 256-leaves one takes at least 33?s to be traversed with QuickScorer. This means that, when pursuing a trade-off between effectiveness and efficiency, the best solution is the ensemble of 878 trees with 64 leaves, due to the linear dependency of the scoring time with respect to the number of leaves.</p><p>Furthermore, we report the results when using two treebased models as teachers for two different neural networks. Our experiments clearly show the positive effects of approximating a more effective teacher <ref type="table" target="#tab_10">(Table 5</ref>). In fact, thanks to the teacher upgrade, the 1000 ? 500 ? 500 ? 100 can provide the same ranking precision as the 64-leaves treebased model. Observe that the student is teacher-agnostic: the architecture of the network is independent w.r.t. the tree-based model which is approximating, and so is the time to perform the forward pass. In conclusion, distilling from a more effective teacher bridges the gap between neural models and ensemble of regression trees in terms of effectiveness. Nevertheless, a margin still exists between the two families of models in terms of efficiency. In the following sections, we will show how to tackle this aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Design of a Neural Model</head><p>In this section, we present our novel methodology to design efficient neural models for ranking. We leverage the insights gained in studying dense and sparse matrix multiplication to show how to make correct architectural choices, thus training a very limited set of candidate models. We provide an empirical evaluation to show the correctness of our assumptions. Experiments are conducted on the MSN30K dataset <ref type="bibr" target="#b46">[47]</ref>, as in Section 5.1. We first show how to develop <ref type="bibr" target="#b5">6</ref>  dense models matching some given time requirements. Then, we employ pruning techniques to sparsify these models and outperform ensembles of regression trees. Architecture design. Our approach begins by choosing the dense architectures matching some given time constraints.</p><p>For the sake of simplicity, we will assume to have two tree-based models to compete with, a 300-trees ensemble and a 500-trees ensemble, each one with 64 leaves per tree. Their NDCG@10 and their scoring time (?s) are reported in <ref type="table" target="#tab_12">Table 6</ref>. By using the time predictor developed in Section 4.2, the identification of the architectures matching the time requirements is now an easier task. We build a heatmap as in <ref type="figure" target="#fig_22">Figure 6</ref> and then use it to predict the execution time of the architecture, without the need of testing its performance on real hardware. This allows to discard models that do not match the desired latency constraints. As reported in <ref type="table" target="#tab_12">Table 6</ref>, there can be several models fitting the time budget. In our case, we propose 2, 3, 4 layers NNs. We train the chosen models and compare their NDCG@10.</p><p>Deep networks (more layers) afford better performance w.r.t. wide ones (more neurons per layer), coherently with the evolution of neural models witnessed in the last decade. The reason is that deep networks are generally capable of extracting higher levels features thus creating more complex representation of the input. The higher representations are built on simpler ones, generating a nested hierarchy of concepts which allows to improve the understanding and the learning from the data <ref type="bibr" target="#b14">[15]</ref>. We empirically verify that 5-layers models matching the time constraints do not offer advantages with respect to 4-layers ones, showing that 4layer networks are expressive enough for the ranking task. Dense networks offer performance close to the tree-based model but do not really guarantee advantages neither in terms of effectiveness or efficiency, as shown in <ref type="table" target="#tab_12">Table 6</ref>.  pick a 400?200?200?100 network: its performance are reported in <ref type="table" target="#tab_15">Table 8</ref>. As detailed in Section 2.3, magnitude-based pruning methods deliver high compression rates without accuracy loss. We adopt this family of pruning techniques to sparsify the parameters of our model. Recall that magnitude pruning technique zero-out a given amount of low absolute value weights. The amount of zeroed-out values determines the aggressiveness of the sparsification: the way this aggressiveness is controlled distinguishes between level pruning and threshold-based pruning. In the case of level pruning, we can explicitly set the sparsity target, e.g., 70%.</p><p>In the threshold-based magnitude pruning by Han et al. <ref type="bibr" target="#b18">[19]</ref>, instead, we need to chose a statistical based threshold, as detailed in Section 2. The choice is generally based on the sensitivity of each layer, namely the property that describes a layer's resistance to sparsification. We perform two kind of sensitivity analysis: static and dynamic. Both procedures prune a growing percentage of weights in each layer, one layer at a time, and evaluate the behavior of the partiallypruned model on the validation set. In the static version, there is no re-training <ref type="bibr" target="#b18">[19]</ref> of the weights that survived the pruning in the chosen layer and the weights in the other layers, while in the dynamic version re-training is applied. Static analysis is reported on the left side of <ref type="figure" target="#fig_5">Figure 10</ref>. The sensitivity of each layer appears to decrease as we go deeper into the network, meaning that the first layers suffer the most from sparsification. Dynamic analysis (right side of <ref type="figure" target="#fig_5">Figure 10)</ref> shows an inverse trend and highlights a peculiar behavior of the first layer: high levels of sparsity in this layer allow the pruned model to outperform the dense one in terms of NDCG@10. This is an example of a model compression technique acting as regularizer <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b61">[62]</ref>. TThis effect is especially evident in the first layer as it presents the weights with the largest absolute values among all the other network layers. Observe that the effect of matrix multiplication is dominated by large absolute value entries, and the larger are the values, the larger is their impact. So a reduced number of high absolute weights can well approximate the overall result of matrix multiplication. From a learning point of view, since the network is working on handcrafted features, the sparsification selects just the essential combinations of input features. Pruning techniques were originally developed to reduce the size of pre-trained models <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Despite, in our context we aim at speeding up the forward pass without incurring in performance degradation. This induces us to consider each layer's relative impact on the inference step before applying a pruning technique. In <ref type="table" target="#tab_14">Table 7</ref>, we report a breakdown of the execution times among different layers in different architectures. Observe that the most timeconsuming layer is always the first one, even if the largest matrix is the one storing the second layer weights, as for the 400?200?200?100 network. Applying bias and ReLU6, in fact, causes the output matrix of the first layer to be brought into the cache, where it resides there during the computation of the second layer. Observe also that it is sufficient to reduce the execution time of one of the first two layers to match the time budget of 3?s. By using our sparse time predictor we can infer the required sparsity to obtain a given speedup. In <ref type="figure" target="#fig_5">Figure 11</ref>, we draw the sparsity-speedup curve for some matrices, representing the first layers of different architectures. Even if the dense first layer usually has a major impact on the overall execution time, the quadratic growth of the sparse speedup in the selected range annihilates its contribution after the sparsification. For example, in the 400?200?200?100 architecture, the impact of the first layer in the dense version is about 35%, while at 95% of sparsity, the estimated speedup using sparse multiplication is 10x, meaning that the first layer after pruning becomes the second less time-consuming layer after fc5.</p><p>Outperforming tree-based models. By jointly harvesting 1) the prominent impact of the first layer on the total execution time, and 2) the regularization effect of pruning the first   layer, we develop our early-layers efficiency-oriented pruning. We apply the threshold-based magnitude pruning using the Distiller framework <ref type="bibr" target="#b63">[64]</ref>, a deep learning compression framework developed by Intel. This pruning technique generally offers more flexibility and better performance with respect to level pruning. We prune only the first layer in an aggressive fashion and we fine-tune its surviving entries and all the weights of the other layers. In our final model, the first layer is 98.7% sparse, meaning that there are about 700 surviving non-zero weights in the first layer, out of 54400 (400 ? 136) in the dense matrix. We use our sparse time predictor to compute the execution time. The speedup obtained with this sparsity ratio on the multiplication of the first layer is about 25x. This means that the impact of the first layer, which previously amounted to about the 35%, is negligible. In <ref type="table" target="#tab_15">Table 8</ref> we report the comparison between tree-based models and neural models. While the dense model did not offer any advantages with respect to the tree-based models, the hybrid model -first layer sparse, other layers dense -is both the fastest and the most accurate model. For example, at the same NDCG@10 value, it is 3.2x faster than the 878-trees model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we provide an extensive evaluation of our methodology to design, train and sparsify neural models for the document scoring task. In particular, we compare them against tree-based models at different points of the efficiency-effectiveness trade-off. Throughout this article, we have used the MSN30K dataset as use case. We now complement our evaluation with the Istella-S dataset <ref type="bibr" target="#b12">[13]</ref>. First, we present the experimental setup. Then, we report our experimental results and we show that neural models obtained with our technique can outperform ensembles of trees. To ease the reproducibility of the results presented in this article, code and trained models have been made publicly available 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We perform our experiments on two datasets: Istella-S and MSN30K. The Istella-S dataset <ref type="bibr" target="#b12">[13]</ref> consists of a collection of 33,018 queries with an average of 103 documents per query. Each document-query pair is represented by 220 features. The MSN30K (Fold 1) dataset, which we already introduced, is composed by more than 30,000 queries, with about 120 documents per query and 136 features per document-query pair. In both the dataset, documentquery pairs are labeled with 5-graded relevance judgments ranging from 0 (irrelevant) to 4 (perfectly relevant). Both datasets are split in train-validation-test according to a 60%-20%-20% criterion.</p><p>LambdaMART models. We employ the LightGBM framework <ref type="bibr" target="#b30">[31]</ref> to train ensembles of regression trees using the LambdaMART algorithm. For each training, we perform hyper-parameter tuning using the HyperOpt library <ref type="bibr" target="#b3">[4]</ref>. In particular, we determine the optimal combination of the following set of hyper-parameters: learning rate, max depth, min_sum_hessian_in_leaf, min_data_in_leaf. To avoid overfitting, we apply an early stopping criterion on the validation loss every 100 trees. We train 64-leaves model as target model to compare against neural networks and 256-leaves models to use as teachers. The latter models offer higher retrieval performance while being 4x slower, which is not suitable for the use in latency-bounded applications. We score the LambdaMART models using a C++ implementation of QuickScorer that exploits instruction-level parallelism by using AVX2 instructions <ref type="bibr" target="#b34">[35]</ref>.</p><p>Neural Networks. We train neural models (students) to approximate the scores of top-performing regression forest (teacher), accordingly to the knowledge distillation <ref type="bibr" target="#b2">[3]</ref> paradigm, detailed in Section 5.1. Models are trained using Pytorch <ref type="bibr" target="#b44">[45]</ref>, adopting the same strategy for randomly generating training data of Cohen et al. <ref type="bibr" target="#b11">[12]</ref>. We employ RELU6 as activation function after every linear layer, except for the last one, where RELU6(x) = min(max(x, 0), 6). We use the Distiller <ref type="bibr" target="#b63">[64]</ref> framework to prune the neural networks. Both in training and pruning, we employ Adam <ref type="bibr" target="#b32">[33]</ref> as optimizer, with learning rate 0.001 and no weight decay. <ref type="table" target="#tab_17">Table 9</ref> summarizes the other training and pruning hyperparameters, which are dataset-dependent. E t represents the 7. https://github.com/hpclab/efficient nn for ltr number of training epochs. The pruning phase is composed of E p epochs of pruning/fine-tuning and of E f t epochs of only fine-tuning, as done by Han et al. <ref type="bibr" target="#b18">[19]</ref>. Both for training and pruning, we scale the learning rate by multiplying it by ? at the epochs specified by ? step . Dropout, if employed (see <ref type="table" target="#tab_17">Table 9</ref>), is applied only after the first layer. When training and pruning the neural models, we always distill from the most effective ensemble of regression trees for the current dataset. On MSN30K, it is a model with 600 trees and 256 leaves per tree, reaching 0.5291 of NDCG@10, while on Istella-S it is a forest with 2500 trees with 256 leaves per tree, reaching 0.7821 of NDCG@10. The neural forward pass is implemented in C++. We use the dnnl sgemm routine from the OneDNN framework for dense matrix multiplication and the LIBXSMM <ref type="bibr" target="#b21">[22]</ref> C++ library for sparse-dense matrix multiplication (after pruning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Et  Experimental Methodology. We experimentally evaluate the performance of neural networks and ensemble of regression trees on two different experimental scenarios:</p><p>? High-Quality Retrieval: this scenario covers use cases where high-precision retrieval is required, even at the price of a larger scoring time. We impose a constraint on the retrieval quality to our models, specified by a threshold on the ranking metric. As threshold, we choose the 99% of the retrieval quality of the top performing tree-based competitor on each dataset. ? Low-Latency Retrieval: this scenario is orthogonal to the previous one as it focus on the efficiency of the retrieval process. We specify a maximum per-document scoring time and we select only the models that can match it. For both datasets, we set the maximum per-document scoring time to be 0.5?s. We perform the comparison between neural models and ensemble of regression trees by considering one scenario at a time. For each dataset, we consider the Pareto frontier of ensembles of tree-based models respecting the constraint of the considered scenario (green lines in <ref type="figure" target="#fig_5">Figures 12, 13</ref>). By doing so, we train several tree-based competitors at different efficiency-effectiveness trade-offs. We then apply our technique and we show that neural networks can outperform ensembles of regression trees. We employ our time predictors to train and prune only neural network models that fit the time budget constrained by the ensembles of treebased models considered. We recall that our methodology allows to train a neural model and to prune its first layer. In fact, in Section 5, we demonstrate that the first layer has a prominent impact on the overall execution time. By zeroing out at least 95% of the parameters, its impact becomes negligible ( <ref type="figure" target="#fig_5">Figure 11</ref>). Furthermore, the sparsification of the first layer has a positive effect on the generalization capabilities of the model as it act as a regularizer. Then, we forecast the overall execution time by subtracting the contribution of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSN30K</head><p>Istella-S <ref type="figure" target="#fig_5">Fig. 12</ref>: Comparison between neural networks and ensemble of regression tree on the high-quality retrieval scenario.</p><p>the dense first layer from the overall execution time. Both times can be estimated with our dense predictor with no computational effort. Experimental Platform. All the inference algorithms are compiled with GCC 9.2.1 with the -O3 compiler option. Scoring times are measured on a Intel i9-9900K CPU clocked at 3.5 GHz, with AVX2 instructions, with a L1-cache of 256KiB, a L2-cache of 2 MiB, and a L3-cache of 16MiB. All the scoring experiments have been performed in singlethread execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>High-Quality Retrieval. The first scenario of our comparison involves models delivering high-quality ranking. As previously detailed, we consider a model (both neural and tree-based) to be in the high-quality ranking region if its NDCG@10 is at least the 99% of the top-quality tree model with 64 leaves. By following the experimental methodology described above, we first construct the Pareto frontier for the ensemble of regression trees (green line in Figure12). We then move to the design of the neural network models. We can estimate the execution time of a neural model whose first layer is sparse with our time predictors. In particular, in <ref type="table" target="#tab_2">Table 10</ref> we report the estimated execution time for the dense architecture, the relative impact of the first layer on the overall execution time, and the predicted execution time after pruning the first layer. We always assume the sparsity of the first layer in the final model to be above 95%, so that its impact on the overall execution time is negligible. Our experiments show that this level of sparsity does not hamper the ranking capability of the model. Observe that our time predictors permit to locate a neural model on the y-axis of the effectiveness-efficiency plot without any computational effort, analytically computing it given the architectures of network. Once we have designed our models to compete with the tree-based ones, we train and prune them, according to the methodology listed in Section 6.1. <ref type="figure" target="#fig_5">Figure 12</ref> illustrates the comparison on a effectivenessefficiency plot between neural models and ensemble of regression trees scored with QuickScorer. On the x-axis we report the NDCG@10 on the test set, and on the y-axis the scoring time per document in ?s. First, we observe that the predicted times reported in <ref type="table" target="#tab_2">Table 10</ref> coincide with real scoring time, confirming the precision of our theoretical approach. Hence, our methodology allows to train exclusively the required architectures. Secondly, neural models can outperform tree-based models in scoring documents, both in terms of effectiveness and efficiency. The neural Pareto-optimality, reported in blue in <ref type="figure" target="#fig_5">Figure 12</ref>, lays below the tree-based one (in green), either on the MSN30K dataset and on Istella-S. On the MSN30K dataset, for example, the 300?200?100 architecture is 4.4x faster than the 878trees model and it also provides a higher retrieval quality. Furthermore, the 200?50?50?25 architecture is the fastest model respecting the quality constraint on this dataset. The same consideration holds for Istella-S, where the fastest model respecting the imposed quality constraint is a neural network (400?200?200?100). On this dataset, neural models still outperform ensemble of regression trees on a large portion of the selected effectiveness-efficiency space, even if tree-based model deliver a slightly superior performance in the top performing region. This leaves space for research work to further improve the quality of this approximation.</p><p>Low-Latency Retrieval. We now compare neural models and ensemble of regression trees on a low-latency retrieval setting, i.e., a scenario requiring the scoring time to be lower than 0.5?s per document. The Pareto Optimality curve for the ensemble of regression trees is drawn in green in <ref type="figure" target="#fig_5">Figure 13</ref>. We use this plot to identify the latency constraints for our neural networks. Our proposed methodology permits to precisely estimate the execution time of a model, before carrying out the costly training-pruning phase. In <ref type="table" target="#tab_2">Table 11</ref>, we demonstrate the usage of our methodology. As we did for the high-quality retrieval use case <ref type="table" target="#tab_2">(Table 10)</ref>, we report the predicted execution time for the dense architecture, the relative impact of the first layer and the predicted time after sparsification. We still consider the impact of the first layer to be negligible. <ref type="figure" target="#fig_5">Figure 13</ref> illustrates the comparison between neural model and ensemble of regression trees when dealing with low-latency constraints. Even in this case, our methodology permits to create neural networks that outperform ensembles of regression trees. On the MSN30K dataset, neural models dominate over tree-based models, as happened for the high-quality use case. In fact, the Pareto frontier of neural models (in blue) always lies below the tree-based one,   confirming the superiority of our technique on this dataset (left side of <ref type="figure" target="#fig_5">Figure 13</ref>). In particular, the 200?50?50?25 architecture is 3x faster than the regression forest with 300 trees and 32 leaves, while being also more precise in terms of NDCG@10. On the Istella-S dataset (right side of <ref type="figure" target="#fig_5">Figure 13</ref>, the performance of our neural models can be considered on pair with tree-based models. In fact, the two Pareto frontiers intersect in this portion of the efficiencyeffectiveness trade-off. Despite that, neural networks still provide the most effective model respecting the time requirement (200?75?75?25). This dataset confirms to be troublesome for neural models, as witnessed in the highquality retrieval scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we presented an effective and efficient methodology to design neural networks for document scoring in a modern information retrieval system. The neural models we take into account are trained to approximate the scores of an ensemble of regression trees. By leveraging a combination of high-performance dense-dense, sparsedense matrix multiplication, and element-wise pruning, the neural models can compete with the original models.  our methodology is effective. By developing time predictors based on an accurate study of how these operations are implemented on modern processors, we are capable to precisely estimate the execution time of a given architecture by knowing the shape and the sparsity level of each layer. This allows to train only a limited number of models, the ones matching the time requirements given by the specific context. Our methodology is thus efficient. Besides presenting our method, throughout the paper emerges a comparison between ensembles of regression trees and NNs on the document scoring task, tested on the MSN30K and Istella-S datasets. In our experiments, neural networks are not capable of reaching the accuracy of their teacher, hence treebased methods are superior in top-quality retrieval scenarios. At any other level of the efficiency-effectiveness tradeoff, neural models designed and trained with our approach can always outscore or at least compete with ensembles of regression trees.</p><p>As future work, we intend to apply different compression methods such as quantization or early exiting to further improve the efficiency of our neural models. Moreover, we plan to extend our comparison between neural networks and ensemble of regression trees to other computational engines, such as General-Purpose Graphic Processing Unit (GPU) or Field Programmable Gate Array (FPGA). We also aim at improving the training by distillation procedure of neural models, in order to bridge the effectiveness gap with ensembles of regression trees.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>m c &lt; l a t e x i t s h a 1 _ b a s e 6 4 =</head><label>14</label><figDesc>" b X d Z A u v H x z A u B 5 c N X O 4 P E d J k J R 8 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r 2 A 9 o Q 9 l s N + 3 S 3 U 3 Y n Q g l 9 C 9 4 8 a C I V / + Q N / + N S Z u D t j 4 Y e L w 3 w 8 y 8 I J b C o u t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U t l F i G G + x S E a m G 1 D L p d C 8 h Q I l 7 8 a G U x V I 3 g k m d 7 n f e e L G i k g / 4 j T m v q I j L U L B K O a S G r D K o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f R i x R X C O T 1 N q e 5 8 b o p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s 3 j y E L z l l 1 d J + 6 L u X d Z v H i 5 r j d s i j j K c w C m c g w d X 0 I B 7 a E I L G I z h G V 7 h z V H O i / P u f C x a S 0 4 x c w x / 4 H z + A H / V j e c = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>y j N I 3 m H 4 x i 8 k A 4 k D z i j a K T 7 L s I T + k F a q v V 4 a d I r F J 2 y M 4 O 9 T N y M F E m G e q / w 1 e 1 H L A l B I h N U 6 4 7 r x O i l V C F n A i b 5 b q I h p m x E B 9 A x V N I Q t J f O r p 7 Y p 0 b p 2 0 G k T E m 0 Z + r v i Z S G W o 9 D 3 3 S G F I d 6 0 Z u K / 3 m d B I N L L + U y T h A k m y 8 K E m F j Z E 8 j s P t c A U M x N o Q y x c 2 t N h t S R R m a o P I m B H f x 5 W X S P C + 7 l f L V b a V Y v c 7 i y J F j c k L O i E s u S J X c k D p p E E Y U e S a v 5 M 1 6 t F 6 s d + t j 3 r p i Z T N H 5 A + s z x 8 b l Z J G &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 cO j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p vz q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l h U G X b v v S E m V Q z X c y K Z X s d 4 i V Z 8 = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q L 0 F c / E Y w T w w C W F 2 0 p s M m Z 1 d Z n r F s O Q v v H h Q x K t / 4 8 2 / c Z L s Q R M L G o q q b r q 7 / F g K g 6 7 7 7 e T W 1 j c 2 t / L b h Z 3 d v f 2 D 4 u F R 0 0 S J 5 t D g k Y x 0 2 2 c G p F D Q Q I E S 2 r E G F v o S W v 6 4 N v N b j 6 C N i N Q 9 T m L o h W y o R C A 4 Q y s 9 d B G e 0 A / S 2 r R f L L l l d w 6 6 S r y M l E i G e r / 4 1 R 1 E P A l B I Z f M m I 7 n x t h L m U b B J U w L 3 c R A z P i Y D a F j q W I h m F 4 6 v 3 h K z 6 w y o E G k b S m k c / X 3 R M p C Y y a h b z t D h i O z 7 M 3 E / 7 x O g s F V L x U q T h A U X y w K E k k x o r P 3 6 U B o 4 C g n l j C u h b 2 V 8 h H T j K M N q W B D 8 J Z f X i X N i 7 J X K V / f V U r V m y y O P D k h p + S c e O S S V M k t q Z M G 4 U S R Z / J K 3 h z j v D j v z s e i N e d k M 8 f k D 5 z P H 9 o k k Q 4 = &lt; / l a t e x i t &gt; += k c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v o m h u u x 2 x A S z i B u q R 1 R / S x 6 J G n 8 = " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X 3 F 9 R Q U b m 8 E g W I V d E d Q u x M Y y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 S C 0 0 8 c O F w z r 3 c e 0 + Y c K a N 5 3 0 6 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p C 6 0 R y q V o h 1 p Q z Q e u G G U 5 b i a I 4 D j l t h s O r i d + 8 o 0 o z K W 7 M K K F B j P u C R Y x g Y 6 X G s E t c h L q F o l f y p k C L x P 8 h x f J B 7 Y u 9 V d 6 r 3 c J H p y d J G l N h C M d a t 3 0 v M U G G l W G E 0 7 H b S T V N M B n i P m 1 b K n B M d Z B N r x 2 j Y 6 v 0 U C S V L W H Q V P 0 9 k e F Y 6 1 E c 2 s 4 Y m 4 G e 9 y b i f 1 4 7 N d F F k D G R p I Y K M l s U p R w Z i S a v o x 5 T l B g + s g Q T x e y t i A y w w s T Y g F w b g j / / 8 i J p n J b 8 s 9 J l z a Z R g R n y c A h H c A I + n E M Z r q E K d S B w C / f w C E + O d B 6 c Z + d l 1 p p z f m b 2 4 Q + c 1 2 8 I 4 p H W &lt; / l a t e x i t &gt; k c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v o m h u u x 2 x A S z i B u q R 1 R / S x 6 J G n 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>e F Y 6 1 E c 2 s 4 Y m 4 G e 9 y b i f 1 4 7 N d F F k D G R p I Y K M l s U p R w Z i S a v o x 5 T l B g + s g Q T x e y t i A y w w s T Y g F w b g j / / 8 i J p n J b 8 s 9 J l z a Z R g R n y c A h H c A I + n E M Z r q E K d S B w C / f w C E + O d B 6 c Z + d l 1 p p z f m b 2 4 Q + c 1 2 8 I 4 p H W &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 9 k w + W + l o Y s W M y K E w + v s v F n W u J I = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q L 1 F v X i M Y B 6 Y h D A 7 6 U 2 G z M 4 u M 7 1 i W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d f i y F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y a J E c 6 j z S E a 6 5 T M D U i i o o 0 A J r V g D C 3 0 J T X 9 0 M / W b j 6 C N i N Q 9 j m P o h m y g R C A 4 Q y s 9 d B C e 0 A / S q 0 m v W H L L 7 g x 0 m X g Z K Z E M t V 7 x q 9 O P e B K C Q i 6 Z M W 3 P j b G b M o 2 C S 5 g U O o m B m P E R G 0 D b U s V C M N 1 0 d v G E n l i l T 4 N I 2 1 J I Z + r v i Z S F x o x D 3 3 a G D I d m 0 Z u K / 3 n t B I O L b i p U n C A o P l 8 U J J J i R K f v 0 7 7 Q w F G O L W F c C 3 s r 5 U O m G U c b U s G G 4 C 2 + v E w a Z 2 W v U r 6 8 q 5 S q 1 1 k c e X J E j s k p 8 c g 5 q Z J b U i N 1 w o k i z + S V v D n G e X H e n Y 9 5 a 8 7 J Z g 7 J H z i f P 9 c a k Q w = &lt; / l a t e x i t &gt; B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 3 o 2 y f 2 k b 6 U Y m s k 5 z + j o F Z L K I 3 4 = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q L 2 F e P E Y w T w w C W F 2 0 p s M m Z 1 d Z n r F s O Q v v H h Q x K t / 4 8 2 / c Z L s Q R M L G o q q b r q 7 / F g K g 6 7 7 7 e T W 1 j c 2 t / L b h Z 3 d v f 2 D 4 u F R 0 0 S J 5 t D g k Y x 0 2 2 c G p F D Q Q I E S 2 r E G F v o S W v 7 4 Z u a 3 H k E b E a l 7 n M T Q C 9 l Q i U B w h l Z 6 6 C I 8 o R + k t W m / W H L L 7 h x 0 l X g Z K Z E M 9 X 7 x q z u I e B K C Q i 6 Z M R 3 P j b G X M o 2 C S 5 g W u o m B m P E x G 0 L H U s V C M L 1 0 f v G U n l l l Q I N I 2 1 J I 5 + r v i Z S F x k x C 3 3 a G D E d m 2 Z u J / 3 m d B I O r X i p U n C A o v l g U J J J i R G f v 0 4 H Q w F F O L G F c C 3 s r 5 S O m G U c b U s G G 4 C 2 / v E q a F 2 W v U r 6 + q 5 S q t S y O P D k h p + S c e O S S V M k t q Z M G 4 U S R Z / J K 3 h z j v D j v z s e i N e d k M 8 f k D 5 z P H 9 i f k Q 0 = &lt; / l a t e x i t &gt; A p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G G X 6 T a 3 4 u O k a a 9 c 0 n 1 c 0 u x K D + 8 M = " &gt; A A A B 9 X i c b V D L T g J B E O z 1 i f h C P X q Z C C a e y K 4 h U W + o F 4 + Y y C O B l c w O s z B h 9 p G Z X p V s + A 8 v H j T G q / / i z b 9 x g D 0 o W E k n l a r u d H d 5 s R Q a b f v b W l p e W V 1 b z 2 3 k N 7 e 2 d 3 Y L e / s N H S W K 8 T q L Z K R a H t V c i p D X U a D k r V h x G n i S N 7 3 h 9 c R v P n C l R R T e 4 S j m b k D 7 o f A F o 2 i k + w 7 y J / T 8 t H T Z j U v j b q F o l + 0 p y C J x M l K E D L V u 4 a v T i 1 g S 8 B C Z p F q 3 H T t G N 6 U K B Z N 8 n O 8 k m s e U D W m f t w 0 N a c C 1 m 0 6 v H p N j o / S I H y l T I Z K p + n s i p Y H W o 8 A z n Q H F g Z 7 3 J u J / X j t B / 9 x N R R g n y E M 2 W + Q n k m B E J h G Q n l C c o R w Z Q p k S 5 l b C B l R R h i a o v A n B m X 9 5 k T R O y 0 6 l f H F b K V a v s j h y c A h H c A I O n E E V b q A G d W C g 4 B l e 4 c 1 6 t F 6 s d + t j 1 r p k Z T M H 8 A f W 5 w 8 j L 5 J L &lt; / l a t e x i t &gt; B p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R U / o z rG + t 2 l t + 6 i Y t G O c 6 9 y F b x k = " &gt; A A A B 9 X i c b V D L T g J B E O z 1 i f h C P X q Z C C a e y K 4 h U W 8 E L x 4 x k U c C K 5 k d Z m H C 7 C M z v S r Z 8 B 9 e P G i M V / / F m 3 / j A H t Q s J J O K l X d 6 e 7 y Y i k 0 2 v a 3 t b K 6 t r 6 x m d v K b + / s 7 u 0 X D g 6 b O k o U 4 w 0 W y U i 1 P a q 5 F C F v o E D J 2 7 H i N P A k b 3 m j 6 6 n f e u B K i y i 8 w 3 H M 3 Y A O Q u E L R t F I 9 1 3 k T + j 5 a a n W i 0 u T X q F o l + 0 Z y D J x M l K E D P V e 4 a v b j 1 g S 8 B C Z p F p 3 H D t G N 6 U K B Z N 8 k u 8 m m s e U j e i A d w w N a c C 1 m 8 6 u n p B T o / S J H y l T I Z K Z + n s i p Y H W 4 8 A z n Q H F o V 7 0 p u J / X i d B / 9 J N R R g n y E M 2 X + Q n k m B E p h G Q v l C c o R w b Q p k S 5 l b C h l R R h i a o v A n B W X x 5 m T T P y 0 6 l f H V b K V Z r W R w 5 O I Y T O A M H L q A K N 1 C H B j B Q 8 A y v 8 G Y 9 W i / W u / U x b 1 2 x s p k j + A P r 8 w c k t 5 J M &lt; / l a t e x i t &gt; += m c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b X d Z A u v H x z A u B 5 c N X O 4 P E d J k J R 8 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r 2 A 9 o Q 9 l s N + 3 S 3 U 3 Y n Q g l 9 C 9 4 8 a C I V / + Q N / + N S Z u D t j 4 Y e L w 3 w 8 y 8 I J b C o u t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U t l F i G G + x S E a m G 1 D L p d C 8 h Q I l 7 8 a G U x V I 3 g k m d 7 n f e e L G i k g / 4 j T m v q I j L U L B K O a S G r D K o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f R i x R X C O T 1 N q e 5 8 b o p 9 S g Y J L P K v 3 E 8 p i y C R 3 x X k Y 1 V d z 6 6 f z W G T n L l C E J I 5 O V R j J X f 0 + k V F k 7 V U H W q S i O 7 b K X i / 9 5 v Q T D a z 8 V O k 6 Q a 7 Z Y F C a S Y E T y x 8 l Q G M 5 Q T j N C m R H Z r Y S N q a E M s 3 j y E L z l l 1 d J + 6 L u X d Z v H i 5 r j d s i j j K c w C m c g w d X 0 I B 7 a E I L G I z h G V 7 h z V H O i / P u f C x a S 0 4 x c w x / 4 H z + A H / V j e c = &lt; / l a t e x i t &gt; k c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v o m h u u x 2 x A S z i B u q R 1 R / S x 6 J G n 8 = " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X 3 F 9 R Q U b m 8 E g W I V d E d Q u x M Y y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 S C 0 0 8 c O F w z r 3 c e 0 + Y c K a N 5 3 0 6 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p C 6 0 R y q V o h 1 p Q z Q e u G G U 5 b i a I 4 D j l t h s O r i d + 8 o 0 o z K W 7 M K K F B j P u C R Y x g Y 6 X G s E t c h L q F o l f y p k C L x P 8 h x f J B 7 Y u 9 V d 6 r 3 c J H p y d J G l N h C M d a t 3 0 v M U G G l W G E 0 7 H b S T V N M B n i P m 1 b K n B M d Z B N r x 2 j Y 6 v 0 U C S V L W H Q V P 0 9 k e F Y 6 1 E c 2 s 4 Y m 4 G e 9 y b i f 1 4 7 N d F F k D G R p I Y K M l s U p R w Z i S a v o x 5 T l B g + s g Q T x e y t i A y w w s T Y g F w b g j / / 8 i J p n J b 8 s 9 J l z a Z R g R n y c A h H c A I + n E M Z r q E K d S B w C / f w C E + O d B 6 c Z + d l 1 p p z f m b 2 4 Q + c 1 2 8 I 4 p H W &lt; / l a t e x i t &gt; k c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v o m h u u x 2 x A S z i B u q R 1 R / S x 6 J G n 8 = " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X 3 F 9 R Q U b m 8 E g W I V d E d Q u x M Y y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 S C 0 0 8 c O F w z r 3 c e 0 + Y c K a N 5 3 0 6 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p C 6 0 R y q V o h 1 p Q z Q e u G G U 5 b i a I 4 D j l t h s O r i d + 8 o 0 o z K W 7 M K K F B j P u C R Y x g Y 6 X G s E t c h L q F o l f y p k C L x P 8 h x f J B 7 Y u 9 V d 6 r 3 c J H p y d J G l N h C M d a t 3 0 v M U G G l W G E 0 7 H b S T V N M B n i P m 1 b K n B M d Z B N r x 2 j Y 6 v 0 U C S V L W H Q V P 0 9 k e F Y 6 1 E c 2 s 4 Y m 4 G e 9 y b i f 1 4 7 N d F F k D G R p I Y K M l s U p R w Z i S a v o x 5 T l B g + s g Q T x e y t i A y w w s T Y g F w b g j / / 8 i J p n J b 8 s 9 J l z a Z R g R n y c A h H c A I + n E M Z r q E K d S B w C / f w C E + O d B 6 c Z + dl 1 p p z f m b 2 4 Q + c 1 2 8 I 4 p H W &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S g 4 J r Q E 5 6 9 J x Q 2 V f 1 z i b o c A 6 f d k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s R / Q h r L Z T t q l m 0 3 Y 3 Q i l 9 B 9 4 8 a C I V / + R N / + N m z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 8 x v P 6 H S P J a P Z p K g H 9 G h 5 C F n 1 F j p Q Z b 6 5 Y p b d e c g q 8 T L S Q V y N P r l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 K V W G M 4 G z U i / V m F A 2 p k P s W i p p h N q f z i + d k T O r D E g Y K 1 v S k L n 6 e 2 J K I 6 0 n U W A 7 I 2 p G e t n L x P + 8 b m r C K 3 / K Z Z I a l G y x K E w F M T H J 3 i Y D r p A Z M b G E M s X t r Y S N q K L M 2 H C y E L z l l 1 d J 6 6 L q 1 a r X 9 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A E B i E 8 w y u 8 O W P n x X l 3 P h a t B S e f O Y Y / c D 5 / A A 9 6 j R I = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; A i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 9 Y i V M 3 H D L m d S W V y Y C 4 p p R c v D O U = " &gt; A A A B 9 X i c b V D L T g J B E J z 1 i f h C P X q Z C C a e y K 4 h U W + o F 4 + Y y C O B l c w O v T B h 9 p G Z X p V s + A 8 v H j T G q / / i z b 9 x g D 0 o W E k n l a r u d H d 5 s R Q a b f v b W l p e W V 1 b z 2 3 k N 7 e 2 d 3 Y L e / s N H S W K Q 5 1 H M l I t j 2 m Q I o Q 6 C p T Q i h W w w J P Q 9 I b X E 7 / 5 A E q L K L z D U Q x u w P q h 8 A V n a K T 7 D s I T e n 5 a u u y K 0 r h b K N p l e w q 6 S J y M F E m G W r f w 1 e l F P A k g R C 6 Z 1 m 3 H j t F N m U L B J Y z z n U R D z P i Q 9 a F t a M g C 0 G 4 6 v X p M j 4 3 S o 3 6 k T I V I p + r v i Z Q F W o 8 C z 3 Q G D A d 6 3 p u I / 3 n t B P 1 z N x V h n C C E f L b I T y T F i E 4 i o D 2 h g K M c G c K 4 E u Z W y g d M M Y 4 m q L w J w Z l / e Z E 0 T s t O p X x x W y l W r 7 I 4 c u S Q H J E T 4 p A z U i U 3 p E b q h B N F n s k r e b M e r R f r 3 f q Y t S 5 Z 2 c w B + Q P r 8 w c Y h Z J E &lt; / l a t e x i t &gt; B p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R U / o z r G + t 2 l t + 6 i Y t G O c 6 9 y F b x k = " &gt; A A A B 9 X i c b V D L T g J B E O z 1 i f h C P X q Z C C a e y K 4 h U W 8 E L x 4 x k U c C K 5 k d Z m H C 7 C M z v S r Z 8 B 9 e P G i M V / / F m 3 / j A H t Q s J J O K l X d 6 e 7 y Y i k 0 2 v a 3 t b K 6 t r 6 x m d v K b + / s 7 u 0 X D g 6 b O k o U 4 w 0 W y U i 1 P a q 5 F C F v o E D J 2 7 H i N P A k b 3 m j 6 6 n f e u B K i y i 8 w 3 H M 3 Y A O Q u E L R t F I 9 1 3 k T + j 5 a a n W i 0 u T X q F o l + 0 Z y D J x M l K E D P V e 4 a v b j 1 g S 8 B C Z p F p 3 H D t G N 6 U K B Z N 8 k u 8 m m s e U j e i A d w w N a c C 1 m 8 6 u n p B T o / S J H y l T I Z K Z + n s i p Y H W 4 8 A z n Q H F o V 7 0 p u J / X i d B / 9 J N R R g n y E M 2 X + Q n k m B E p h G Q v l C c o R w b Q p k S 5 l b C h l R R h i a o v A n B W X x 5 m T T P y 0 6 l f H V b K V Z r W R w 5 O I Y T O A M H L q A K N 1 C H B j B Q 8 A y v 8 G Y 9 W i / W u / U x b 1 2 x s p k j + A P r 8 w c k t 5 J M &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S g 4 J r Q E 5 6 9 J x Q 2 V f 1 z i b o c A 6 f d k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s R / Q h r L Z T t q l m 0 3 Y 3 Q i l 9 B 9 4 8 a C I V / + R N / + N m z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 8 x v P 6 H S P J a P Z p K g H 9 G h 5 C F n 1 F j p Q Z b 6 5 Y p b d e c g q 8 T L S Q V y N P r l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 K V W G M 4 G z U i / V m F A 2 p k P s W i p p h N q f z i + d k T O r D E g Y K 1 v S k L n 6 e 2 J K I 6 0 n U W A 7 I 2 p G e t n L x P + 8 b m r C K 3 / K Z Z I a l G y x K E w F M T H J 3 i Y D r p A Z M b G E M s X t r Y S N q K L M 2 H C y E L z l l 1 d J 6 6 L q 1 a r X 9 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A E B i E 8 w y u 8 O W P n x X l 3 P h a t B S e f O Y Y / c D 5 / A A 9 6 j R I = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; += k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S g 4 J r Q E 5 6 9 J x Q 2 V f 1 z i b o c A 6 f d k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s R / Q h r L Z T t q l m 0 3 Y 3 Q i l 9 B 9 4 8 a C I V / + R N / + N m z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 8 x v P 6 H S P J a P Z p K g H 9 G h 5 C F n 1 F j p Q Z b 6 5 Y p b d e c g q 8 T L S Q V y N P r l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 K V W G M 4 G z U i / V m F A 2 p k P s W i p p h N q f z i + d k T O r D E g Y K 1 v S k L n 6 e 2 J K I 6 0 n U W A 7 I 2 p G e t n L x P + 8 b m r C K 3 / K Z Z I a l G y x K E w F M T H J 3 i Y D r p A Z M b G E M s X t r Y S N q K L M 2 H C y E L z l l 1 d J 6 6 L q 1 a r X 9 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A E B i E 8 w y u 8 O W P n x X l 3 P h a t B S e f O Y Y / c D 5 / A A 9 6 j R I = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; n c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K j P k C o F 0 Q E q V B P 1 b h 4 W z F i o e + 9 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E S c J 9 y M 6 V C I U j K K V H l S f 9 c s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S i C t k k h r T 9 d w E / Y x q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b P 5 u f O i V n V h m Q M N a 2 F J K 5 + n s i o 5 E x k y i w n R H F k V n 2 Z u J / X j f F 8 M r P h E p S 5 I o t F o W p J B i T 2 d 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 2 R z o v z 7 n w s W g t O P n M M f + B 8 / g B M H I 3 U &lt; / l a t e x i t &gt; n c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K j P k C o F 0 Q E q V B P 1 b h 4 W z F i o e + 9 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E S c J 9 y M 6 V C I U j K K V H l S f 9 c s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S i C t k k h r T 9 d w E / Y x q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b P 5 u f O i V n V h m Q M N a 2 F J K 5 + n s i o 5 E x k y i w n R H F k V n 2 Z u J / X j f F 8 M r P h E p S 5 I o t F o W p J B i T 2 d 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 2 R z o v z 7 n w s W g t O P n M M f + B 8 / g B M H I 3 U &lt; / l a t e x i t &gt; n c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K j P k C o F 0 Q E q V B P 1 b h 4 W z F i o e + 9 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E S c J 9 y M 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 :</head><label>1</label><figDesc>l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 2 R z o v z 7 n w s W g t O P n M M f + B 8 / g B M H I 3 U &lt; / l a t e x i t &gt; First three steps of the Goto algorithm for Dense Matrix multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " w B 3 w Y 7 1 p w f e L E D O p z Z s b T Y x h N n E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z S R B P 6 J D y U P O q L H S g + y r f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P I z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B i 2 I 3 j &lt; / l a t e x i t &gt; m c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b X d Z A u v H x z A u B 5 c N X O 4 P E d J k J R 8 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r 2 A 9 o Q 9 l s N + 3 S 3 U 3 Y n Q g l 9 C 9 4 8 a C I V / + Q N / + N S Z u D t j 4 Y e L w 3 w 8 y 8 I J b C o u t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U t l F i G G + x S E a m G 1 D L p d C 8 h Q I l 7 8 a G U x V I 3 g k m d 7 n f e e L G i k g / 4 j T m v q I j L U L B K O a S G r D K o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f R i x R X C O T 1 N q e 5 8 b o p 9 S g Y J L P K v 3 E 8 p i y C R 3 x X k Y 1 V d z 6 6 f z W G T n L l C E J I 5 O V R j J X f 0 + k V F k 7 V U H W q S i O 7 b K X i / 9 5 v Q T D a z 8 V O k 6 Q a 7 Z Y F C a S Y E T y x 8 l Q G M 5 Q T j N C m R H Z r Y S N q a E M s 3 j y E L z l l 1 d J + 6 L u X d Z v H i 5 r j d s i j j K c w C m c g w d X 0 I B 7 a E I L G I z h G V 7 h z V H O i / P u f C x a S 0 4 x c w x / 4 H z + A H / V j e c = &lt; / l a t e x i t &gt; C i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x X q w z H X V 7 L + b h d y 4 e 8 q 1 H n z C c M A = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X Y C t 4 K o k U 1 F u x F 4 8 V 7 A e 0 s W y 2 k 3 b p Z h N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W a e H w u u 0 X G + r Z X V t f W N z d x W f n t n d 2 + / c H D Y 1 F G i G D R Y J C L V 9 q k G w S U 0 k K O A d q y A h r 6 A l j + q T f 3 W A y j N I 3 m H 4 x i 8 k A 4 k D z i j a K T 7 L s I T + k F a q v V 4 a d I r F J 2 y M 4 O 9 T N y M F E m G e q / w 1 e 1 H L A l B I h N U 6 4 7 r x O i l V C F n A i b 5 b q I h p m x E B 9 A x V N I Q t J f O r p 7 Y p 0 b p 2 0 G k T E m 0 Z + r v i Z S G W o 9 D 3 3 S G F I d 6 0 Z u K / 3 m d B I N L L + U y T h A k m y 8 K E m F j Z E 8 j s P t c A U M x N o Q y x c 2 t N h t S R R m a o P I m B H f x 5 W X S P C + 7 l f L V b a V Y v c 7 i y J F j c k L O i E s u S J X c k D p p E E Y U e S a v 5 M 1 6 t F 6 s d + t j 3 r p i Z T N H 5 A + s z x 8 b l Z J G &lt; / l a t e x i t &gt; m r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q u 3 M i L W 0 k w 3 C k 2 f l 7 m p t 9 E D D Z D 8 = " &gt; A A A B 6 n i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e x K Q b 0 V v X i s a D + g X U o 2 z b a h S X Z J s k J Z + h O 8 e F D E q 7 / I m / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 M B H c W M / 7 R o W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U M n G q K W v S W M S 6 E x L D B F e s a b k V r J N o R m Q o W D s c 3 8 7 8 9 h P T h s f q 0 U 4 S F k g y V D z i l F g n P c i + 7 p c r X t W b A 6 8 S P y c V y N H o l 7 9 6 g 5 i m k i l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 6 j i k h m g m x + 6 h S f O W W A o 1 i 7 U h b P 1 d 8 T G Z H G T G T o O i W x I 7 P s z c T / v G 5 q o 6 s g 4 y p J L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " b X d Z A u v H x z A u B 5 c N X O 4 P E d J k J R 8 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r 2 A 9 o Q 9 l s N + 3 S 3 U 3 Y n Q g l 9 C 9 4 8 a C I V / + Q N / + N S Z u D t j 4 Y e L w 3 w 8 y 8 I J b C o u t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U t l F i G G + x S E a m G 1 D L p d C 8 h Q I l 7 8 a G U x V I 3 g k m d 7 n f e e L G i k g / 4 j T m v q I j L U L B K O a S G r D K o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f R i x R X C O T 1 N q e 5 8 b o p 9 S g Y J L P K v 3 E 8 p i y C R 3 x X k Y 1 V d z 6 6 f z W G T n L l C E J I 5 O V R j J X f 0 + k V F k 7 V U H W q S i O 7 b K X i / 9 5 v Q T D a z 8 V O k 6 Q a 7 Z Y F C a S Y E T y x 8 l Q G M 5 Q T j N C m R H Z r Y S N q a E M s 3 j y E L z l l 1 d J + 6 L u X d Z v H i 5 r j d s i j j K c w C m c g w d X 0 I B 7 a E I L G I z h G V 7 h z V H O i / P u f C x a S 0 4 x c w x / 4 H z + A H / V j e c = &lt; / l a t e x i t &gt; A i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V O e E l T S G E T M u Q q n z w z J v i a l l Q E E = " &gt; A A A B / 3 i c b V D J S g N B E O 2 J W 4 x b V P D i Z T A R P I U Z C a i 3 q B e P E c w C m S H 0 d G q S J j 0 L 3 T V i G O f g r 3 j x o I h X f 8 O b f 2 N n O W j i g 4 L H e 1 V U 1 f N i w R V a 1 r e R W 1 p e W V 3 L r x c 2 N r e 2 d 4 q 7 e 0 0 V J Z J B g 0 U i k m 2 P K h A 8 h A Z y F N C O J d D A E 9 D y h t d j v 3 U P U v E o v M N R D G 5 A + y H 3 O a O o p W 7 x w E F 4 Q M 9 P y w 5 y 0 Y P 0 s s u z c t Y t l q y K N Y G 5 S O w Z K Z E Z 6 t 3 i l 9 O L W B J A i E x Q p T q 2 F a O b U o m c C c g K T q I g p m x I + 9 D R N K Q B K D e d 3 J + Z x 1 r p m X 4 k d Y V o T t T f E y k N l B o F n u 4 M K A 7 U v D c W / / M 6 C f r n b s r D O E E I 2 X S R n w g T I 3 M c h t n j E h i K k S a U S a 5 v N d m A S s p Q R 1 b Q I d j z L y + S 5 m n F r l Y u b q u l 2 t U s j j w 5 J E f k h N j k j N T I D a m T B m H k k T y T V / J m P B k v x r v x M W 3 N G b O Z f f I H x u c P G H a W K w = = &lt; / l a t e x i t &gt; n r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w B 3 w Y 7 1 p w f e L E D O p z Z s b T Y x h N n E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z S R B P 6 J D y U P O q L H S g + y r f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P I z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B i 2 I 3 j &lt; / l a t e x i t &gt; k c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v o m h u u x 2 x A S z i B u q R 1 R / S x 6 J G n 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>e F Y 6 1 E c 2 s 4 Y m 4 G e 9 y b i f 1 4 7 N d F F k D G R p I Y K M l s U p R w Z i S a v o x 5 T l B g + s g Q T x e y t i A y w w s T Y g F w b g j / / 8 i J p n J b 8 s 9 J l z a Z R g R n y c A h H c A I + n E M Z r q E K d S B w C / f w C E + O d B 6 c Z + d l 1 p p z f m b 2 4 Q + c 1 2 8 I 4 p H W &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S g 4 J r Q E 5 6 9 J x Q 2 V f 1 z i b o c A 6 f d k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s R / Q h r L Z T t q l m 0 3 Y 3 Q i l 9 B 9 4 8 a C I V / + R N / + N m z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 8 x v P 6 H S P J a P Z p K g H 9 G h 5 C F n 1 F j p Q Z b 6 5 Y p b d e c g q 8 T L S Q V y N P r l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 K V W G M 4 G z U i / V m F A 2 p k P s W i p p h N q f z i + d k T O r D E g Y K 1 v S k L n 6 e 2 J K I 6 0 n U W A 7 I 2 p G e t n L x P + 8 b m r C K 3 / K Z Z I a l G y x K E w F M T H J 3 i Y D r p A Z M b G E M s X t r Y S N q K L M 2 H C y E L z l l 1 d J 6 6 L q 1 a r X 9 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A E B i E 8 w y u 8 O W P n x X l 3 P h a t B S e f O Y Y / c D 5 / A A 9 6 j R I = &lt; / l a t e x i t &gt;B p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r C 2 X j 3 o 8 A + A e w K 1 4 A A J r g P j j W E E = " &gt; A A A B / 3 i c b V D J S g N B E O 2 J W 4 z b q O D F y 2 A i e A o z E l B v I V 4 8 R j A L Z I b Q 0 6 l J m v Q s d N e I Y c z B X / H i Q R G v / o Y 3 / 8 b O c t D E B w W P 9 6 q o q u c n g i u 0 7 W 8 j t 7 K 6 t r 6 R 3 y x s b e / s 7 p n 7 B 0 0 V p 5 J B g 8 U i l m 2 f K h A 8 g g Z y F N B O J N D Q F 9 D y h 9 c T v 3 U P U v E 4 u s N R A l 5 I + x E P O K O o p a 5 5 5 C I 8 o B 9 k J R e 5 6 E F W 6 y b j 0 r h r F u 2 y P Y W 1 T J w 5 K Z I 5 6 l 3 z y + 3 F L A 0 h Q i a o U h 3 H T t D L q E T O B I w L b q o g o W x I + 9 D R N K I h K C + b 3 j + 2 T r X S s 4 J Y 6 o r Q m q q / J z I a K j U K f d 0 Z U h y o R W 8 i / u d 1 U g w u v Y x H S Y o Q s d m i I B U W x t Y k D K v H J T A U I 0 0 o k 1 z f a r E B l Z S h j q y g Q 3 A W X 1 4 m z f O y U y l f 3 V a K 1 d o 8 j j w 5 J i f k j D j k g l T J D a m T B m H k k T y T V / J m P B k v x r v x M W v N G f O Z Q / I H x u c P J L C W M w = = &lt; / l a t e x i t &gt; += m r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q u 3 M i L W 0 k w 3 C k 2 f l 7 m p t 9 E D D Z D 8 = " &gt; A A A B 6 n i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e x K Q b 0 V v X i s a D + g X U o 2 z b a h S X Z J s k J Z + h O 8 e F D E q 7 / I m / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 M B H c W M / 7 R o W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U M n G q K W v S W M S 6 E x L D B F e s a b k V r J N o R m Q o W D s c 3 8 7 8 9 h P T h s f q 0 U 4 S F k g y V D z i l F g n P c i + 7 p c r X t W b A 6 8 S P y c V y N H o l 7 9 6 g 5 i m k i l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 6 j i k h m g m x + 6 h S f O W W A o 1 i 7 U h b P 1 d 8 T G Z H G T G T o O i W x I 7 P s z c T / v G 5 q o 6 s g 4 y p J L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " w B 3 w Y 7 1 p w f e L E D O p z Z s b T Y x h N n E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z S R B P 6 J D y U P O q L H S g + y r f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P I z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B i 2 I 3 j &lt; / l a t e x i t &gt; k c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v o m h u u x 2 x A S z i B u q R 1 R / S x 6 J G n 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>o x 5 T l B g + s g Q T x e y t i A y w w s T Y g F w b g j / / 8 i J p n J b 8 s 9 J l z a Z R g R n y c A h H c A I + n E M Z r q E K d S B w C / f w C E + O d B 6 c Z + d l 1 p p z f m b 2 4 Q + c 1 2 8 I 4 p H W &lt; / l a t e x i t &gt; n r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w B 3 w Y 7 1 p w f e L E D O p z Z s b T Y x h N n E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z S R B P 6 J D y U P O q L H S g + y r f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P I z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B i 2 I 3 j &lt; / l a t e x i t &gt; m r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q u 3 M i L W 0 k w 3 C k 2 f l 7 m p t 9 E D D Z D 8 = " &gt; A A A B 6 n i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e x K Q b 0 V v X i s a D + g X U o 2 z b a h S X Z J s k J Z + h O 8 e F D E q 7 / I m / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 M B H c W M / 7 R o W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U M n G q K W v S W M S 6 E x L D B F e s a b k V r J N o R m Q o W D s c 3 8 7 8 9 h P T h s f q 0 U 4 S F k g y V D z i l F g n P c i + 7 p c r X t W b A 6 8 S P y c V y N H o l 7 9 6 g 5 i m k i l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 6 j i k h m g m x + 6 h S f O W W A o 1 i 7 U h b P 1 d 8 T G Z H G T G T o O i W x I 7 P s z c T / v G 5 q o 6 s g 4 y p J L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>o x 5 T l B g + s g Q T x e y t i A y w w s T Y g F w b g j / / 8 i J p n J b 8 s 9 J l z a Z R g R n y c A h H c A I + n E M Z r q E K d S B w C / f w C E + O d B 6 c Z + d l 1 p p z f m b 2 4 Q + c 1 2 8 I 4 p H W &lt; / l a t e x i t &gt; m c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b X d Z A u v H x z A u B 5 c N X O 4 P E d J k J R 8 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r 2 A 9 o Q 9 l s N + 3 S 3 U 3 Y n Q g l 9 C 9 4 8 a C I V / + Q N / + N S Z u D t j 4 Y e L w 3 w 8 y 8 I J b C o u t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U t l F i G G + x S E a m G 1 D L p d C 8 h Q I l 7 8 a G U x V I 3 g k m d 7 n f e e L G i k g / 4 j T m v q I j L U L B K O a S G r D K o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f R i x R X C O T 1 N q e 5 8 b o p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Y D j Q i 9 5 E / M 9 r J + h f u K k I 4 w Q h 5 L N F f i I p R n Q S A e 0 J B R z l y B D G l T C 3 U j 5 g i n E 0 Q e V M C M 7 i y 8 u k c V Z y y q X L m 3 K h c j W P I 0 u O y D E 5 J Q 4 5 J x V y T W q k T j h R 5 J m 8 k j f r 0 X q x 3 q 2 P W W v G m s 8 c k j + w P n 8 A H R u S R w = = &lt; / l a t e x i t &gt; Cj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q o i T 0 + Q T X B Q i 6 6 z I r 5 z K T r a d 3 C U = " &gt; A A A B 9 X i c b V D J S g N B E O 2 J W 4 x b 1 K O X x k T w F G Y k o N 6 C u X i M Y B Z I x t D T q U n a 9 C x 0 1 6 h h y H 9 4 8 a C I V / / F m 3 9 j Z z l o 4 o O C x 3 t V V N X z Y i k 0 2 v a 3 l V l Z X V v f y G 7 m t r Z 3 d v f y + w c N H S W K Q 5 1 H M l I t j 2 m Q I o Q 6 C p T Q i h W w w J P Q 9 I b V i d 9 8 A K V F F N 7 i K A Y 3 Y P 1 Q + I I z N N J d B + E J P T 8 t V r v 3 x X E 3 X 7 B L 9 h R 0 m T h z U i B z 1 L r 5 r 0 4 v 4 k k A I X L J t G 4 7 d o x u y h Q K L m G c 6 y Q a Y s a H r A 9 t Q 0 M W g H b T 6 d V j e m K U H v U j Z S p E O l V / T 6 Q s 0 H o U e K Y z Y D j Q i 9 5 E / M 9 r J + h f u K k I 4 w Q h 5 L N F f i I p R n Q S A e 0 J B R z l y B D G l T C 3 U j 5 g i n E 0 Q e V M C M 7 i y 8 u k c V Z y y q X L m 3 K h c j W P I 0 u O y D E 5 J Q 4 5 J x V y T W q k T j h R 5 J m 8 k j f r 0 X q x 3 q 2 P W W v G m s 8 c k j + w P n 8 A H R u S R w = = &lt; / l a t e x i t &gt;B j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>a 9 r e R W 1 p e W V 3 L r x c 2 N r e 2 d 8 z d v a a K U 8 m g w W I R y 7 Z P F Q g e Q Q M 5 C m g n E m j o C 2 j 5 d 5 d j v 3 U P U v E 4 u s F h A m 5 I + x E P O K O o J c 8 8 6 C I 8 o B 9 k p S 5 y 0 Y O s N v J u S y P P L N p l e w J r k T g z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>F H Y I z / / I i a Z 6 W n U r 5 4 r p S r N Z m c e T J I T k i J 8 Q h Z 6 R K r k i d N A g j j + S Z v J I 3 4 8 l 4 M d 6 N j 2 l r z p j N 7 J M / M D 5 / A B u 3 l i 0 = &lt; / l a t e x i t &gt; L1 cache &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e C F f Z 5 O u z F s 9 d p w P u h L S r c d f L w w = " &gt; A A A B + n i c b V D L S g N B E J y N r x h f G z 1 6 G Q y C p 7 A b A u o t 4 M W D h w j m A c k S Z i e d Z M j s g 5 l e N a z 5 F C 8 e F P H q l 3 j z b 5 w k e 9 D E g o a i q p v u L j + W Q q P j f F u 5 t f W N z a 3 8 d m F n d 2 / / w C 4 e N n W U K A 4 N H s l I t X 2 m Q Y o Q G i h Q Q j t W w A J f Q s s f X 8 3 8 1 j 0 o L a L w D i c x e A E b h m I g O E M j 9 e z i T c + l X Y R H T C l n f A T T n l 1 y y s 4 c d J W 4 G S m R D P W e / d X t R z w J I E Q u m d Y d 1 4 n R S 5 l C w S V M C 9 1 E Q 8 z 4 m A 2 h Y 2 j I A t B e O j 9 9 S k + N 0 q e D S J k K k c 7 V 3 x M p C 7 S e B L 7 p D B i O 9 L I 3 E / / z O g k O L r x U h H G C E P L F o k E i K U Z 0 l g P t C w U c 5 c Q Q x p U w t 1 I + Y o p x N G k V T A j u 8 s u r p F k p u 9 X y 5 W 2 1 V K t k c e T J M T k h Z 8 Q l 5 6 R G r k m d N A g n D + S Z v J I 3 6 8 l 6 s d 6 t j 0 V r z s p m j s g f W J 8 / e w e T e g = = &lt; / l a t e x i t &gt; c r,j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V t T Y l u 2 j I N g k l 4 h I E 2 / M r L r W H 8 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 2 :</head><label>2</label><figDesc>9 K i y i 8 w 3 E M X s A G o e g L z t B I X b t w 0 y 3 T D s I j p p Q z P o R J 1 y 4 6 J W c G u k z c j B R J h l r X / u r 0 I p 4 E E C K X T O u 2 6 8 T o p U y h 4 B I m + U 6 i I W Z 8 x A b Q N j R k A W g v n Z 0 + o S d G 6 d F + p E y F S G f q 7 4 m U B V q P A 9 9 0 B g y H e t G b i v 9 5 7 Q T 7 5 1 4 q w j h B C P l 8 U T + R F C M 6 z Y H 2 h A K O c m w I 4 0 q Y W y k f M s U 4 m r T y J g R 3 8 e V l 0 i i X 3 E r p 4 r Z S r F 5 m c e T I E T k m p 8 Q l Z 6 R K r k m N 1 A k n D + S Z v J I 3 6 8 l 6 s d 6 t j 3 n r i p X N H J I / s D 5 / A I F p k 4 s = &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S g 4 J r Q E 5 6 9 J x Q 2 V f 1 z i b o c A 6 f d k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s R / Q h r L Z T t q l m 0 3 Y 3 Q i l 9 B 9 4 8 a C I V / + R N / + N m z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 8 x v P 6 H S P J a P Z p K g H 9 G h 5 C F n 1 F j p Q Z b 6 5 Y p b d e c g q 8 T L S Q V y N P r l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 K V W G M 4 G z U i / V m F A 2 p k P s W i p p h N q f z i + d k T O r D E g Y K 1 v S k L n 6 e 2 J K I 6 0 n U W A 7 I 2 p G e t n L x P + 8 b m r C K 3 / K Z Z I a l G y x K E w F M T H J 3 i Y D r p A Z M b G E M s X t r Y S N q K L M 2 H C y E L z l l 1 d J 6 6 L q 1 a r X 9 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A E B i E 8 w y u 8 O W P n x X l 3 P h a t B S e f O Y Y / c D 5 / A A 9 6 j R I = &lt; / l a t e x i t &gt; Macro-Kernel in the Goto algorithm for Dense Matrix Multiplication (DMM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. In particular,? i is organized into sub-matrices? j of size m r ?k c , with elements stored in column-major order, whileB p is organized in panels of size k c ?n r , stored in rowmajor order, namedB j . This data access pattern reflects the order in which the micro-kernel accesses data. Goto et al. [16] observed the advantages of packing? i into the L2 cache. The ratio between FLOPs and memory operations, regardless if the original data rely in L3 cache or in main memory, can be modeled as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>+= n r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w B 3 w Y 7 1 p w f e L E D O p z Z s b T Y x h N n E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z S R B P 6 J D y U P O q L H S g + y r f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P I z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B i 2 I 3 j &lt; / l a t e x i t &gt; m r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q u 3 M i L W 0 k w 3 C k 2 f l 7 m p t 9 E D D Z D 8 = " &gt; A A A B 6 n i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e x K Q b 0 V v X i s a D + g X U o 2 z b a h S X Z J s k J Z + h O 8 e F D E q 7 / I m / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 M B H c W M / 7 R o W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U M n G q K W v S W M S 6 E x L D B F e s a b k V r J N o R m Q o W D s c 3 8 7 8 9 h P T h s f q 0 U 4 S F k g y V D z i l F g n P c i + 7 p c r X t W b A 6 8 S P y c V y N H o l 7 9 6 g 5 i m k i l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 6 j i k h m g m x + 6 h S f O W W A o 1 i 7 U h b P 1 d 8 T G Z H G T G T o O i W x I 7 P s z c T / v G 5 q o 6 s g 4 y p J L V N 0 s S h K B b Y x n v 2 N B 1 w z a s X E E U I 1 d 7 d i O i K a U O v S K b k Q / O W X V 0 n r o u r X q t f 3 t U r 9 J o + j C C d w C u f g w y X U 4 Q 4 a 0 A Q K Q 3 i G V 3 h D A r 2 g d / S x a C 2 g f O Y Y / g B 9 / g B h U o 3 i &lt; / l a t e x i t &gt; m r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q u 3 M i L W 0 k w 3 C k 2 f l 7 m p t 9 E D D Z D 8 = " &gt; A A A B 6 n i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e x K Q b 0 V v X i s a D + g X U o 2 z b a h S X Z J s k J Z + h O 8 e F D E q 7 / I m / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 M B H c W M / 7 R o W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U M n G q K W v S W M S 6 E x L D B F e s a b k V r J N o R m Q o W D s c 3 8 7 8 9 h P T h s f q 0 U 4 S F k g y V D z i l F g n P c i + 7 p c r X t W b A 6 8 S P y c V y N H o l 7 9 6 g 5 i m k i l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 6 j i k h m g m x + 6 h S f O W W A o 1 i 7 U h b P 1 d 8 T G Z H G T G T o O i W x I 7 P s z c T / v G 5 q o 6 s g 4 y p J L V N 0 s S h K B b Y x n v 2 N B 1 w z a s X E E U I 1 d 7 d i O i K a U O v S K b k Q / O W X V 0 n r o u r X q t f 3 t U r 9 J o + j C C d w C u f g w y X U 4 Q 4 a 0 A Q K Q 3 i G V 3 h D A r 2 g d / S x a C 2 g f O Y Y / g B 9 / g B h U o 3 i &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w Q + 2 f o I Z 8 y m A b t o + K 5 q O 6 s 2 m J B k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>1 &lt;Fig. 3 :</head><label>13</label><figDesc>a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / f n 2 M w Q = = &lt; / l a t e x i t &gt; c r,j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V t T Y l u 2 j I N g k l 4 h I E 2 / M r L r W H 8 8 = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 C b a C B y m J F N R b 0 Y v H C v Y D 2 h A 2 2 0 2 7 d r M J u x N p C f k r X j w o 4 t U / 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m n h 9 z p s C 2 v 4 3 C 2 v r G 5 l Z x u 7 S z u 7 d / Y B 6 W 2 y p K J K E t E v F I d n 2 s K G e C t o A B p 9 1 Y U h z 6 n H b 8 8 e 3 M 7 z x R q V g k H m A a U z f E Q 8 E C R j B o y T P L f a A T 8 I O 0 S r x U n j 9 m 1 c w z K 3 b N n s N a J U 5 O K i h H 0 z O / + o O I J C E V Q D h W q u f Y M b g p l s A I p 1 m p n y g a Y z L G Q 9 r T V O C Q K j e d 3 5 5 Z p 1 o Z W E E k d Q m w 5 u r v i R S H S k 1 D X 3 e G G E Z q 2 Z u J / 3 m 9 B I I r N 2 U i T o A K s l g U J N y C y J o F Y Q 2 Y p A T 4 V B N M J N O 3 W m S E J S a g 4 y r p E J z l l 1 d J + 6 L m 1 G v X 9 / V K 4 y a P o 4 i O 0 Q k 6 Q w 6 6 R A 1 0 h 5 q o h Q i a o G f 0 i t 6 M z H g x 3 o 2 P R W v B y G e O 0 B 8 Y n z / a D p R W &lt; / l a t e x i t &gt;? j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j 6 C d 5 E 6 8 G a 7 c o x k z C j B E 2 8 i a 6 E 8 = " &gt; A A A C A H i c b V D J S g N B E O 2 J W 4 x b 1 I M H L 4 2 J 4 C n M S E C 9 R b 1 4 j G A W S E L o 6 d Q k b X o W u m v E M M z F X / H i Q R G v f o Y 3 / 8 b O c t D E B w W P 9 6 q o q u d G U m i 0 7 W 8 r s 7 S 8 s r q W X c 9 t b G 5 t 7 + R 3 9 + o 6 j B W H G g 9 l q J o u 0 y B F A D U U K K E Z K W C + K 6 H h D q / H f u M B l B Z h c I e j C D o + 6 w f C E 5 y h k b r 5 g z b C I 7 p e U m y j k D 1 I L t P u P S 2 m 3 X z B L t k T 0 E X i z E i B z F D t 5 r / a v Z D H P g T I J d O 6 5 d g R d h K m U H A J a a 4 d a 4 g Y H 7 I + t A w N m A + 6 k 0 w e S O m x U X r U C 5 W p A O l E / T 2 R M F / r k e + a T p / h Q M 9 7 Y / E / r x W j d 9 5 J R B D F C A G f L v J i S T G k 4 z R o T y j g K E e G M K 6 E u Z X y A V O M o 8 k s Z 0 J w 5 l 9 e J P X T k l M u X d y W C 5 W r W R x Z c k i O y A l x y B m p k B t S J T X C S U q e y S t 5 s 5 6 s F + v d + p i 2 Z q z Z z D 7 5 A + v z B 3 g T l l Y = &lt; / l a t e x i t &gt; n r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w B 3 w Y 7 1 p w f e L E D O p z Z s b T Y x h N n E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W t B / Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z S R B P 6 J D y U P O q L H S g + y r f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P I z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q l e r X t / X K v W b P I 4 i n M A p n I M H l 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B i 2 I 3 j &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " w Q + 2 f o I Z 8 y m A b t o + K 5 q O 6 s 2 m J B k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m l 6 / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x N e + R m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / f n 2 M w Q = = &lt; / l a t e x i t &gt; B j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s A i T X n X x h 6 H d F Y M r z 0 6 8 f W C P g w A = " &gt; A A A C A H i c b V D J S g N B E O 2 J W 4 x b 1 I M H L 4 2 J 4 C n M S E C 9 h X j x G M E s k I T Q 0 6 l J 2 v Q s d N e I Y Z i L v + L F g y J e / Q x v / o 2 d 5 a C J D w o e 7 1 V R V c + N p N B o 2 9 9 W Z m V 1 b X 0 j u 5 n b 2 t 7 Z 3 c v v H z R 0 G C s O d R 7 K U L V c p k G K A O o o U E I r U s B 8 V 0 L T H V 1 P / O Y D K C 3 C 4 A 7 H E X R 9 N g i E J z h D I / X y R x 2 E R 3 S 9 p N h B I f u Q V N P e P S 2 m v X z B L t l T 0 G X i z E m B z F H r 5 b 8 6 / Z D H P g T I J d O 6 7 d g R d h O m U H A J a a 4 T a 4 g Y H 7 E B t A 0 N m A + 6 m 0 w f S O m p U f r U C 5 W p A O l U / T 2 R M F / r s e + a T p / h U C 9 6 E / E / r x 2 j d 9 l N R B D F C A G f L f J i S T G k k z R o X y j g K M e G M K 6 E u Z X y I V O M o 8 k s Z 0 J w F l 9 e J o 3 z k l M u X d 2 W C 5 X q P I 4 s O S Y n 5 I w 4 5 I J U y A 2 p k T r h J C X P 5 J W 8 W U / W i / V u f c x a M 9 Z 8 5 p D 8 g f X 5 A 3 m d l l c = &lt; / l a t e x i t &gt; Micro-Kernel in the Goto algorithm for Dense Matrix Multiplication (DMM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>, we show the results of the reverse experiment: instead of gradually increasing both m and k, we keep the size of A constant (the mk product is constant). The figure shows that small values of m with large values of k still afford high-performance (left side of the graph). On the other hand, small values of k paired with larger</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 4 :</head><label>4</label><figDesc>Matrix Multiplication with oneDNN, as m and k grow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 5 :</head><label>5</label><figDesc>Matrix Multiplication with oneDNN, with the product mk constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 6 :</head><label>6</label><figDesc>Matrix Multiplication HeatMap with n = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>8 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 2 &lt; 3 &lt;</head><label>81423</label><figDesc>" e E 1 P + I p h o k / J k d T f t x l e 5 Y T p l W E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s c e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 N v P b T 6 g 0 j + W j m S T o R 3 Q o e c g Z N V Z 6 q J F S v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M P 6 X K c C Z w V u q l G h P K x n S I X U s l j V D 7 0 / m p M 3 J m l Q E J Y 2 V L G j J X f 0 9 M a a T 1 J A p s Z 0 T N S C 9 7 m f i f 1 0 1 N W P O n X C a p Q c k W i 8 J U E B O T 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 0 n C 8 F b f n m V t C 6 q 3 m X 1 6 v 6 y U r / J 4 y j C C Z z C O X h w D X W 4 g w Y 0 g c E Q n u E V 3 h z h v D j v z s e i t e D k M 8 f w B 8 7 n D x H i j Q I = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " K A B t T u E 2 b H d 1 X L i 8 D f Y Y + 6 P g z g s = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y J R o 9 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S o 9 I v l t y y u w B Z J 1 5 G S p C h 3 i 9 + 9 Q Y x S y O U h g m q d d d z E + N P q T K c C Z w V e q n G h L I x H W L X U k k j 1 P 5 0 c e i M X F h l Q M J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P S q 9 5 c / M / r p i a 8 8 a d c J q l B y Z a L w l Q Q E 5 P 5 1 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 B h u C t v r x O W p W y V y 1 f N a q l 2 m 0 W R x 7 O 4 B w u w Y N r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H 6 5 j L 4 = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " D d t s W f u z Z v b L 9 / t A B w 5 b 9 K u A Z U M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y V o 0 e i F 4 + Q y C O B D Z k d e m F k d n Y z M 2 t C C F / g x Y P G e P W T v P k 3 D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3 v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>A 8 w y u 8 O 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 9 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 9 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 2 &lt; l a t e x i t s h a 1 _ 9 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 3 &lt; l a t e x i t s h a 1 _ 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 8 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 9 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 2 &lt; l a t e x i t s h a 1 _</head><label>8514914514914219143151481451491421</label><figDesc>Y / O i / P u f C x a c 0 4 2 c w x / 4 H z + A I A 9 j L 8 = &lt; / l a t e x i t &gt; " p F q I S X l c K f S o E w e V g t x r L S Z z 4 i k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 n E o s e i F 4 9 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q d e P C j i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l h x r p l y t u 1 Z 2 D r B I v J x X I 0 e i X v 3 q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T U i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 0 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z t 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 0 7 J h u A t v 7 x K W h d V 7 7 J a u 7 + s 1 G / y O I p w A q d w D h 5 c Q R 3 u o A F N Y B D C M 7 z C m z N 2 X p x 3 5 2 P R W n D y m W P 4 A + f z B 9 j e j O s = &lt; / l a t e x i t &gt; " q V 6 S S a w W X + r i V t 9 a z c o m 4 Z F d I f c = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K R L 0 F v X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G d z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + k 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V 6 U v U r 5 s l 4 p V W + z O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 4 l V j M U = &lt; / l a t e x i t &gt; " p F q I S X l c K f S o E w e V g t x r L S Z z 4 i k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 n E o s e i F 4 9 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q d e P C j i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l h x r p l y t u 1 Z 2 D r B I v J x X I 0 e i X v 3 q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T U i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 0 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z t 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 0 7 J h u A t v 7 x K W h d V 7 7 J a u 7 + s 1 G / y O I p w A q d w D h 5 c Q R 3 u o A F N Y B D C M 7 z C m z N 2 X p x 3 5 2 P R W n D y m W P 4 A + f z B 9 j e j O s = &lt; / l a t e x i t &gt; " q V 6 S S a w W X + r i V t 9 a z c o m 4 Z F d I f c = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K R L 0 F v X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G d z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + k 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V 6 U v U r 5 s l 4 p V W + z O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 4 l V j M U = &lt; / l a t e x i t &gt; b a s e 6 4 = " K A B t T u E 2 b H d 1 X L i 8 D f Y Y + 6 P g z g s = " &gt; A A A B 6 H i c b V D L T g J B E Oz F F + I L 9 e h l I j H x R H Y J R o 9 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S o 9 I v l t y y u w B Z J 1 5 G S p C h 3 i 9 + 9 Q Y x S y O U h g m q d d d z E + N P q T K c C Z w V e q n G h L I x H W L X U k k j 1 P 5 0 c e i M X F h l Q M J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P S q 9 5 c / M / r p i a 8 8 a d c J q l B y Z a L w l Q Q E 5 P 5 1 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 B h u C t v r x O W p W y V y 1 f N a q l 2 m 0 W R x 7 O 4 B w u w Y N r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H 6 5 j L 4 = &lt; / l a t e x i t &gt; " q V 6 S S a w W X + r i V t 9 a z c o m 4 Z F d I f c= " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K R L 0 F v X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G d z O /9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + k 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V 6 U v U r 5 s l 4 p V W + z O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 4 l V j M U = &lt; / l a t e x i t &gt; b a s e 6 4 = " D d t s W f u z Z v b L 9 / t A B w 5 b 9 K u A Z U M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y V o 0 e i F 4 + Q y C O B D Z k d e m F k d n Y z M 2 t C C F / g x Y P G e P W T v P k 3 D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3 v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R 3 c x v P a H S P J Y P Z p y g H 9 G B 5 C F n 1 F i p f t k r l t y y O w d Z J V 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q D K c C Z w W u q n G h L I R H W D H U k k j 1 P 5 k f u i U n F m l T 8 J Y 2 Z K G z N X f E x M a a T 2 O A t s Z U T P U y 9 5 M / M / r p C a 8 8 S d c J q l B y R a L w l Q Q E 5 P Z 1 6 T P F T I j x p Z Q p r i 9 l b A h V Z Q Z m 0 3 B h u A t v 7 x K m h d l r 1 K + q l d K 1 d s s j j y c w C m c g w f X U I V 7 q E E D G C A 8 w y u 8 O Y / O i / P u f C x a c 0 4 2 c w x / 4 H z + A I A 9 j L 8 = &lt; / l a t e x i t &gt; " p F q I S X l c K f S o E w e V g t x r L S Z z 4 i k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 n E o s e i F 4 9 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q d e P C j i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l h x r p l y t u 1 Z 2 D r B I v J x X I 0 e i X v 3 q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T U i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 0 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z t 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 0 7 J h u A t v 7 x K W h d V 7 7 J a u 7 + s 1 G / y O I p w A q d w D h 5 c Q R 3 u o A F N Y B D C M 7 z C m z N 2 X p x 3 5 2 P R W n D y m W P 4 A + f z B 9 j e j O s = &lt; / l a t e x i t &gt; " e E 1 P + I p h o k / J k d T f t x l e 5 Y T p l W E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s c e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 N v P b T 6 g 0 j + W j m S T o R 3 Q o e c g Z N V Z 6 q J F S v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M P 6 X K c C Z w V u q l G h P K x n S I X U s l j V D 7 0 / m p M 3 J m l Q E J Y 2 V L G j J X f 0 9 M a a T 1 J A p s Z 0 T N S C 9 7 m f i f 1 0 1 N W P O n X C a p Q c k W i 8 J U E B O T 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 0 n C 8 F b f n m V t C 6 q 3 m X 1 6 v 6 y U r / J 4 y j C C Z z C O X h w D X W 4 g w Y 0 g c E Q n u E V 3 h z h v D j v z s e i t e D k M 8 f w B 8 7 n D x H i j Q I = &lt; / l a t e x i t &gt; " p F q I S X l c K f S o E w e V g t x r L S Z z 4 i k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 n E o s e i F 4 9 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q d e P C j i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l h x r p l y t u 1 Z 2 D r B I v J x X I 0 e i X v 3 q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T U i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 0 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z t 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 0 7 J h u A t v 7 x K W h d V 7 7 J a u 7 + s 1 G / y O I p w A q d w D h 5 c Q R 3 u o A F N Y B D C M 7 z C m z N 2 X p x 3 5 2 P R W n D y m W P 4 A + f z B 9 j e j O s = &lt; / l a t e x i t &gt; " q V 6 S S a w W X + r i V t 9 a z c o m 4 Z F d I f c = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K R L 0 F v X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G d z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + k 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V 6 U v U r 5 s l 4 p V W + z O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 4 l V j M U = &lt; / l a t e x i t &gt; 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 5 T J 2 4 Z t G v v O J U a p K e I o 4 H i 0 i / I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j 2 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l p t s v V 9 y q O w d Z J V 5 O K p C j 0 S 9 / 9 Q Y x S y O U h g m q d d d z E + N n V B n O B E 5 L v V R j Q t m Y D r F r q a Q R a j + b H z o l Z 1 Y Z k D B W t q Q h c / X 3 R E Y j r S d R Y D s j a k Z 6 2 Z u J / 3 n d 1 I T X f s Z l k h q U b L E o T A U x M Z l 9 T Q Z c I T N i Y g l l i t t b C R t R R Z m x 2 Z R s C N 7 y y 6 u k f V H 1 a t X L Z q 1 S v 8 n j K M I J n M I 5 e H A F d b i D B r S A A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P e 7 G M v A = = &lt; / l a t e x i t &gt; b a s e 6 4 = " K A B t T u E 2 b H d 1 X L i 8 D f Y Y + 6 P g z g s = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y J R o 9 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S o 9 I v l t y y u w B Z J 1 5 G S p C h 3 i 9 + 9 Q Y x S y O U h g m q d d d z E + N P q T K c C Z w V e q n G h L I x H W L X U k k j 1 P 5 0 c e i M X F h l Q M J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P S q 9 5 c / M / r p i a 8 8 a d c J q l B y Z a L w l Q Q E 5 P 5 1 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 B h u C t v r x O W p W y V y 1 f N a q l 2 m 0 W R x 7 O 4 B w u w Y N r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H 6 5 j L 4 = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>A 8 w y u 8 O 6 &lt; l a t e x i t s h a 1 _ 6 &lt; l a t e x i t s h a 1 _ 7 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = 2 &lt; 2 &lt; 2 &lt; 3 &lt; 4 &lt; 3 &lt;Fig. 7 :</head><label>861617142223437</label><figDesc>Y / O i / P u f C x a c 0 4 2 c w x / 4 H z + A I A 9 j L 8 = &lt; / l a t e x i t &gt; b a s e 6 4 = " D O D A i o N 1 a V 4 7 M l p L Y 6 0 K + x z X Z H M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y N P o 5 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e T W 1 j c 2 t / L b h Z 3 d v f 2 D 4 u F R U 8 e p Y t h g s Y h V O 6 A a B Z f Y M N w I b C c K a R Q I b A W j u 5 n f e k K l e S w f z D h B P 6 I D y U P O q L F S / a p X L L l l d w 6 y S r y M l C B D r V f 8 6 v Z j l k Y o D R N U 6 4 7 n J s a f U G U 4 E z g t d F O N C W U j O s C O p Z J G q P 3 J / N A p O b N K n 4 S x s i U N m a u / J y Y 0 0 n o c B b Y z o m a o l 7 2 Z + J / X S U 1 4 4 0 + 4 T F K D k i 0 W h a k g J i a z r 0 m f K 2 R G j C 2 h T H F 7 K 2 F D q i g z N p u C D c F b f n m V N C / K X q V 8 W a + U q r d Z H H k 4 g V M 4 B w + u o Q r 3 U I M G M E B 4 h l d 4 c x 6 d F + f d + V i 0 5 p x s 5 h j + w P n 8 A Y T J j M I = &lt; / l a t e x i t &gt; b a s e 6 4 = " D O D A i o N 1 a V 4 7 M l p L Y 6 0 K + x z X Z H M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y N P o 5 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e T W 1 j c 2 t / L b h Z 3 d v f 2 D 4 u F R U 8 e p Y t h g s Y h V O 6 A a B Z f Y M N w I b C c K a R Q I b A W j u 5 n f e k K l e S w f z D h B P 6 I D y U P O q L F S / a p X L L l l d w 6 y S r y M l C B D r V f 8 6 v Z j l k Y o D R N U 6 4 7 n J s a f U G U 4 E z g t d F O N C W U j O s C O p Z J G q P 3 J / N A p O b N K n 4 S x s i U N m a u / J y Y 0 0 n o c B b Y z o m a o l 7 2 Z + J / X S U 1 4 4 0 + 4 T F K D k i 0 W h a k g J i a z r 0 m f K 2 R G j C 2 h T H F 7 K 2 F D q i g z N p u C D c F b f n m V N C / K X q V 8 W a + U q r d Z H H k 4 g V M 4 B w + u o Q r 3 U I M G M E B 4 h l d 4 c x 6 d F + f d + V i 0 5 p x s 5 h j + w P n 8 A Y T J j M I = &lt; / l a t e x i t &gt; " g i F 3 L Q y U w W 7 g W t J S K X q N D 7 u 4 S E w = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y N B o 9 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S o 9 o v l t y y u w B Z J 1 5 G S p C h 3 i 9 + 9 Q Y x S y O U h g m q d d d z E + N P q T K c C Z w V e q n G h L I x H W L X U k k j 1 P 5 0 c e i M X F h l Q M J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P S q 9 5 c / M / r p i a 8 8 a d c J q l B y Z a L w l Q Q E 5 P 5 1 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 B h u C t v r x O W l d l r 1 K + b l R K t d s s j j y c w T l c g g d V q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A I Z N j M M = &lt; / l a t e x i t &gt; 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 5 T J 2 4 Z t G v v O J U a p K e I o 4 H i 0 i / I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j 2 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l p t sv V 9 y q O w d Z J V 5 O K p C j 0 S 9 / 9 Q Y x S y O U h g m q d d d z E + N n V B n O B E 5 L v V R j Q t m Y D r F r q a Q R a j + b H z o l Z 1 Y Z k D B W t q Q h c / X 3 R E Y j r S d R Y D s j a k Z 6 2 Z u J / 3 n d 1 I T X f s Z l k h q U b L E o T A U x M Z l 9 T Q Z c I T N i Y g l l i t t b C R t R R Zm x 2 Z R s C N 7 y y 6 u k f V H 1 a t X L Z q 1 S v 8 n j K M I J n M I 5 e H A F d b i D B r S A A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P e 7 G M v A = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " K A B t T u E 2 b H d 1 X L i 8 D f Y Y + 6 P g z g s = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y J R o 9 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S o 9 I v l t y y u w B Z J 1 5 G S p C h 3 i 9 + 9 Q Y x S y O U h g m q d d d z E + N P q T K c C Z w V e q n G h L I x H W L X U k k j 1 P 5 0 c e i M X F h l Q M J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P S q 9 5 c / M / r p i a 8 8 a d c J q l B y Z a L w l Q Q E 5 P 5 1 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 B h u C t v r x O W p W y V y 1 f N a q l 2 m 0 W R x 7 O 4 B w u w Y N r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H 6 5 j L 4 = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " K A B t T u E 2 b H d 1 X L i 8 D f Y Y + 6 P g z g s = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y J R o 9 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S o 9 I v l t y y u w B Z J 1 5 G S p C h 3 i 9 + 9 Q Y x S y O U h g m q d d d z E + N P q T K c C Z w V e q n G h L I x H W L X U k k j 1 P 5 0 c e i M X F h l Q M J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P S q 9 5 c / M / r p i a 8 8 a d c J q l B y Z a L w l Q Q E 5 P 5 1 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 B h u C t v r x O W p W y V y 1 f N a q l 2 m 0 W R x 7 O 4 B w u w Y N r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H 6 5 j L 4 = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " K A B t T u E 2 b H d 1 X L i 8 D f Y Y + 6 P g z g s = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y J R o 9 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S o 9 I v l t y y u w B Z J 1 5 G S p C h 3 i 9 + 9 Q Y x S y O U h g m q d d d z E + N P q T K c C Z w V e q n G h L I x H W L X U k k j 1 P 5 0 c e i M X F h l Q M J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P S q 9 5 c / M / r p i a 8 8 a d c J q l B y Z a L w l Q Q E 5 P 5 1 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 B h u C t v r x O W p W y V y 1 f N a q l 2 m 0 W R x 7 O 4 B w u w Y N r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H 6 5 j L 4 = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " D d t s W f u z Z v b L 9 / t A B w 5 b 9 K u A Z U M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y V o 0 e i F 4 + Q y C O B D Z k d e m F k d n Y z M 2 t C C F / g x Y P G e P W T v P k 3 D r A H B S v p p F LV n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3 v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R 3 c x v P a H S P J Y P Z p y g H 9 G B 5 C F n 1 F i p f t k r l t y y O w d Z J V 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q D K c C Z w W u q n G h L I R H W D H U k k j 1 P 5 k f u i U n F m l T 8 J Y 2 Z K G z N X f E x M a a T 2 O A t s Z U T P U y 9 5 M / M / r p C a 8 8 S d c J q l B y R a L w l Q Q E 5 P Z 1 6 T P F T I j x p Z Q p r i 9 l b A h V Z Q Z m 0 3 B h u A t v 7 x K m h d l r 1 K + q l d K 1 d s s j j y c w C m c g w f X U I V 7 q E E D G C A 8 w y u 8 O Y / O i / P u f C x a c 0 4 2 c w x / 4 H z + A I A 9 j L 8 = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " R y t y 9 5 3 u O y 2 U r I t p r N 8 6 h c g S d y o = " &gt; A A A B 5 H i c b V B N S 8 N A E J 3 U r x q / q l c v i 0 X w V B K p 6 L H o x W M F + w F t K J v t p F 2 7 2 Y T d j V B C f 4 E X D 4 p X f 5 M 3 / 4 3 b N g d t f T D w e G + G m X l h K r g 2 n v f t l D Y 2 t 7 Z 3 y r v u 3 v 7 B 4 V H F P W 7 r J F M M W y w R i e q G V K P g E l u G G 4 H d V C G N Q 4 G d c H I 3 9 z v P q D R P 5 K O Z p h j E d C R 5 x B k 1 V n q o D y p V r + Y t Q N a J X 5 A q F G g O K l / 9 Y c K y G K V h g m r d 8 7 3 U B D l V h j O B M 7 e f a U w p m 9 A R 9 i y V N E Y d 5 I t D Z + T c K k M S J c q W N G S h / p 7 I a a z 1 N A 5 t Z 0 z N W K 9 6 c / E / r 5 e Z 6 C b I u U w z g 5 I t F 0 W Z I C Y h 8 6 / J k C t k R k w t o U x x e y t h Y 6 o o M z Y b 1 4 b g r 7 6 8 T t q X N b 9 e u 6 o 2 b o s w y n A K Z 3 A B P l x D A + 6 h C S 1 g g P A C b / D u P D m v z s e y s e Q U E y f w B 8 7 n D x d L i 5 Y = &lt; / l a t e x i t &gt; Columns RowIndex Values l a t e x i t s h a 1 _ b a s e 6 4 = " D d t s W f u z Z v b L 9 / t A B w 5 b 9 K u A Z U M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y V o 0 e i F 4 + Q y C O B D Z k d e m F k d n Y z M 2 t C C F / g x Y P G e P W T v P k 3 D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3 v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R 3 c x v P a H S P J Y P Z p y g H 9 G B 5 C F n 1 F i p f t k r l t y y O w d Z J V 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q D K c C Z w W u q n G h L I R H W D H U k k j 1 P 5 k f u i U n F m l T 8 J Y 2 Z K G z N X f E x M a a T 2 O A t s Z U T P U y 9 5 M / M / r p C a 8 8 S d c J q l B y R a L w l Q Q E 5 P Z 1 6 T P F T I j x p Z Q p r i 9 l b A h V Z Q Z m 0 3 B h u A t v 7 x K m h d l r 1 K + q l d K 1 d s s j j y c w C m c g w f X U I V 7 q E E D G C A 8 w y u 8 O Y / O i / P u f C x a c 0 4 2 c w x / 4 H z + A I A 9 j L 8 = &lt; / l a t e x i t &gt; CSR Format for Sparse Matrices. Let us consider a matrix M ? R m?n with nnz non-zero elements. An example of the CSR representation is reported in Figure 7. It consists of three vectors: values ? R nnz , columnIndex ? R nnz , rows ? R m+1 . The values array stores the non-zero entries, and columnIndex stores their column index in the original matrix, meaning that columnIndex[i] stores the columns index of values[i]. The rows array is built so that rows[i+1]?rows[i] is the number of non-zero entries for row i. Sparse Dense Matrix Multiplication. Sparse Dense Matrix Multiplication or sparse Multi-vector multiplication (SDMM) has a large range of applications: fluid dynamics, graph analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>5 .Fig. 8 : 1 :</head><label>581</label><figDesc>https://community.intel.com/t5/Intel-oneAPI-Math-Kernel-Library/Sparse-Dense-Matrix-Multiplication/m-p/1173953 C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l h U G X b v v S E m V Q z X c y K Z X s d 4 i V Z 8 = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q L 0 F c / E Y w T w w C W F 2 0 p s M m Z 1 d Z n r F s O Q v v H h Q x K t / 4 8 2 / c Z L s Q R M L G o q q b r q 7 / F g K g 6 7 7 7 e T W 1 j c 2 t / L b h Z 3 d v f 2 D 4 u F R 0 0 S J 5 t D g k Y x 0 2 2 c G p F D Q Q I E S 2 r E G F v o S W v 6 4 N v N b j 6 C N i N Q 9 T m L o h W y o R C A 4 Q y s 9 d B G e 0 A / S 2 r R f L L l l d w 6 6 S r y M l E i G e r / 4 1 R 1 E P A l B I Z f M m I 7 n x t h L m U b B J U w L 3 c R A z P i Y D a F j q W I h m F 4 6 v 3 h K z 6 w y o E G k b S m k c / X 3 R M p C Y y a h b z t D h i O z 7 M 3 E / 7 x O g s F V L x U q T h A U X y w K E k k x o r P 3 6 U B o 4 C g n l j C u h b 2 V 8 h H T j K M N q W B D 8 J Z f X i X N i 7 J X K V / f V U r V m y y O P D k h p + S c e O S S V M k t q Z M G 4 U S R Z / J K 3 h z j v D j v z s e i N e d k M 8 f k D 5 z P H 9 o k k Q 4 = &lt; / l a t e x i t &gt; A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 9 k w + W + l o Y s W M y K E w + v s v F n W u J I = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q L 1 F v X i M Y B 6 Y h D A 7 6 U 2 G z M 4 u M 7 1 i W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d f i y F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y a J E c 6 j z S E a 6 5 T M D U i i o o 0 A J r V g D C 3 0 J T X 9 0 M / W b j 6 C N i N Q 9 j m P o h m y g R C A 4 Q y s 9 d B C e 0 A / S q 0 m v W H L L 7 g x 0 m X g Z K Z E M t V 7 x q 9 O P e B K C Q i 6 Z M W 3 P j b G b M o 2 C S 5 g U O o m B m P E R G 0 D b U s V C M N 1 0 d v G E n l i l T 4 N I 2 1 J I Z + r v i Z S F x o x D 3 3 a G D I d m 0 Z u K / 3 n t B I O L b i p U n C A o P l 8 U J J J i R K f v 0 7 7 Q w F G O L W F c C 3 s r 5 U O m G U c b U s G G 4 C 2 + v E w a Z 2 W v U r 6 8 q 5 S q 1 1 k c e X J E j s k p 8 c g 5 q Z J b U i N 1 w o k i z + S V v D n G e X H e n Y 9 5 a 8 7 J Z g 7 J H z i f P 9 c a k Q w = &lt; / l a t e x i t &gt; B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 3 o 2 y f 2 k b 6 U Y m s k 5 z + j o F Z L K I 3 4 = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q L 2 F e P E Y w T w w C W F 2 0 p s M m Z 1 d Z n r F s O Q v v H h Q x K t / 4 8 2 / c Z L s Q R M L G o q q b r q 7 / F g K g 6 7 7 7 e T W 1 j c 2 t / L b h Z 3 d v f 2 D 4 u F R 0 0 S J 5 t D g k Y x 0 2 2 c G p F D Q Q I E S 2 r E G F v o S W v 7 4 Z u a 3 H k E b E a l 7 n M T Q C 9 l Q i U B w h l Z 6 6 C I 8 o R + k t W m / W H L L 7 h x 0 l X g Z K Z E M 9 X 7 x q z u I e B K C Q i 6 Z M R 3 P j b G X M o 2 C S 5 g W u o m B m P E x G 0 L H U s V C M L 1 0 f v G U n l l l Q I N I 2 1 J I 5 + r v i Z S F x k x C 3 3 a G D E d m 2 Z u J / 3 m d B I O r X i p U n C A o v l g U J J J i R G f v 0 4 H Q w F F O L G F c C 3 s r 5 S O m G U c b U s G G 4 C 2 / v E q a F 2 W v U r 6 + q 5 S q t S y O P D k h p + S c e O S S V M k t q Z M G 4 U S R Z / J K 3 h z j v D j v z s e i N e d k M 8 f k D 5 z P H 9 i f k Q 0 = &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S g 4 J r Q E 5 6 9 J x Q 2 V f 1 z i b o c A 6 f d k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s R / Q h r L Z T t q l m 0 3 Y 3 Q i l 9 B 9 4 8 a C I V / + R N / + N m z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 8 x v P 6 H S P J a P Z p K g H 9 G h 5 C F n 1 F j p Q Z b 6 5 Y p b d e c g q 8 T L S Q V y N P r l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 K V W G M 4 G z U i / V m F A 2 p k P s W i p p h N q f z i + d k T O r D E g Y K 1 v S k L n 6 e 2 J K I 6 0 n U W A 7 I 2 p G e t n L x P + 8 b m r C K 3 / K Z Z I a l G y x K E w F M T H J 3 i Y D r p A Z M b G E M s X t r Y S N q K L M 2 H C y E L z l l 1 d J 6 6 L q 1 a r X 9 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A E B i E 8 w y u 8 O W P n x X l 3 P h a t B S e f O Y Y / c D 5 / A A 9 6 j R I = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; = k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S g 4 J r Q E 5 6 9 J x Q 2 V f 1 z i b o c A 6 f d k = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s R / Q h r L Z T t q l m 0 3 Y 3 Q i l 9 B 9 4 8 a C I V / + R N / + N m z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 8 x v P 6 H S P J a P Z p K g H 9 G h 5 C F n 1 F j p Q Z b 6 5 Y p b d e c g q 8 T L S Q V y N P r l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 K V W G M 4G z U i / V m F A 2 p k P s W i p p h N q f z i + d k T O r D E g Y K 1 v S k L n 6 e 2 J K I 6 0 n U W A 7 I 2 p G e t n L x P + 8 b m r C K 3 / K Z Z I a l G y x K E w F M T H J 3 i Y D r p A Z M b G E M s X t r Y S N q K L M 2 H C y E L z l l 1 d J 6 6 L q 1 a r X 9 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A E B i E 8 w y u 8 O W P n x X l 3 P h a t B S e f O Y Y / c D 5 / AA 9 6 j R I = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; NB &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 e 4 o y h L A A p 7 b q v Y U n z w A W N P M Z J E = " &gt; A A A B 6 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K R I 8 h X j x J F P O A Z A m z k 9 5 k y O z s M j M r h J A / 8 O J B E a / + k T f / x k m y B 0 0 s a C i q u u n u C h L B t X H d b y e 3 t r 6 x u Z X f L u z s 7 u 0 f F A + P m j p O F c M G i 0 W s 2 g H V K L j E h u F G Y D t R S K N A Y C s Y 3 c z 8 1 h M q z W P 5 a M Y J + h E d S B 5 y R o 2 V H u 5 q v W L J L b t z k F X i Z a Q E G e q 9 4 l e 3 H 7 M 0 Q m m Y o F p 3 P D c x / o Q q w 5 n A a a G b a k w o G 9 E B d i y V N E L t T + a X T s m Z V f o k j J U t a c h c / T 0 x o Z H W 4 y i w n R E 1 Q 7 3 s z c T / v E 5 q w m t / w m W S G p R s s S h M B T E x m b 1 N + l w h M 2 J s C W W K 2 1 s J G 1 J F m b H h F G w I 3 v L L q 6 R 5 U f Y q 5 c v 7 S q l a y + L I w w m c w j l 4 c A V V u I U 6 N I B B C M / w C m / O y H l x 3 p 2 P R W v O y W a O 4 Q + c z x 8 y c o 0 m &lt; / l a t e x i t &gt; nb &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 5 K a D O Y / d p a o o 2 / g T N B K v S 9 + h y w = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 9 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q d e P C j i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l B x X 0 y x W 3 6 s 5 B V o m X k w r k a P T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t V T R i B s / m 1 8 6 J W d W G Z A w 1 r Y U k r n 6 e y K j k T G T K L C d E c W R W f Z m 4 n 9 e N 8 X w 2 s + E S l L k i i 0 W h a k k G J P Z 2 2 Q g N G c o J 5 Z Q p o W 9 l b A R 1 Z S h D a d k Q / C W X 1 4 l r Y u q V 6 t e 3 t c q 9 Z s 8 j i K c w C m c g w d X U I c 7 a E A T G I T w D K / w 5 o y d F + f d + V i 0 F p x 8 5 h j + w P n 8 A Z O S j W Y = &lt; / l a t e x i t &gt; = NB &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 e 4 o y h L A A p 7 b q v Y U n z w A W N P M Z J E = " &gt; A A A B 6 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K R I 8 h X j x J F P O A Z A m z k 9 5 k y O z s M j M r h J A / 8 O J B E a / + k T f / x k m y B 0 0 s a C i q u u n u C h L B t X H d b y e 3 t r 6 x u Z X f L u z s 7 u 0 f F A + P m j p O F c M G i 0 W s 2 g H V K L j E h u F G Y D t R S K N A Y C s Y 3 c z 8 1 h M q z W P 5 a M Y J + h E d S B 5 y R o 2 V H u 5 q v W L J L b t z k F X i Z a Q E G e q 9 4 l e 3 H 7 M 0 Q m m Y o F p 3 P D c x / o Q q w 5 n A a a G b a k w o G 9 E B d i y V N E L t T + a X T s m Z V f o k j J U t a c h c / T 0 x o Z H W 4 y i w n R E 1 Q 7 3 s z c T / v E 5 q w m t / w m W S G p R s s S h M B T E x m b 1 N + l w h M 2 J s C W W K 2 1 s J G 1 J F m b H h F G w I 3 v L L q 6 R 5 U f Y q 5 c v 7 S q l a y + L I w w m c w j l 4 c A V V u I U 6 N I B B C M / w C m / O y H l x 3 p 2 P R W v O y W a O 4 Q + c z x 8 y c o 0 m &lt; / l a t e x i t &gt; nb &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 5 K a D O Y / d p a o o 2 / g T N B K v S 9 + h y w = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 9 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q d e P C j i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l B x X 0 y x W 3 6 s 5 B V o m X k w r k a P T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t V T R i B s / m 1 8 6 J W d W G Z A w 1 r Y U k r n 6 e y K j k T G T K L C d E c W R W f Z m 4 n 9 e N 8 X w 2 s + E S l L k i i 0 W h a k k G J P Z 2 2 Q g N G c o J 5 Z Q p o W 9 l b A R 1 Z S h D a d k Q / C W X 1 4 l r Y u q V 6 t e 3 t c q 9 Z s 8 j i K c w C m c g w d X U I c 7 a E A T G I T w D K / w 5 o y d F + f d + V i 0 F p x 8 5 h j + w P n 8 A Z O S j W Y = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E S q f k p o O z Q T K x I V + z Y 2 0 h F 3 G U b w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 f + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m u N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l 7 Y u q V 6 t e N 2 u V + k 0 e R x F O 4 B T O w Y N L q M M d N K A F D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A N Z l j P s = &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m J Q O V U g 0 B x X z r M q A u Y Y D 8 P d N o M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j y 2 Y G u h D W W z n b R r d 5 O w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C h 5 h y 3 A j s J M o p D I Q + B C M b 2 f + w x M q z e P o 3 k w S 9 C U d R j z k j B o r N W W / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L J U Y G S a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p R G V q P 1 s f u i U n F l l Q M J Y 2 Y o M m a u / J z I q t Z 7 I w H Z K a k Z 6 2 Z u J / 3 n d 1 I R X f s a j J D U Y s c W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 9 k X V q 1 W v m 7 V K / S a P o w g n c A r n 4 M E l 1 O E O G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / 2 W 2 M / Q = = &lt; / l a t e x i t &gt; LIBXSMM Sparse-Dense Matrix Multiplication (SPMM). Data: CSR A ? R m?k , B ? R k?n Result: C ? R m?n C[i, k] = 0; for i = 0 to M ? 1 do for j = A.rows[i] to A.rows[i + 1] ? 1 do for k = 0 to N-1 do idx = A.cols[j]; C[i, k] ? C[i, k] + A.val[j] * I[idx, k]; end end end Algorithm Sparse-Dense Matrix Multiplication algorithm with CSR format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>i 1 &lt; 2 &lt; 3 &lt; 4 &lt; 5 &lt; 6 &lt; 7 &lt; 8 &lt;Fig. 9 :</head><label>123456789</label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q r u E Y t K 4 p M s o 8 R f I S T / n K P 8 P X d Q = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j 2 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l J u + X K 2 7 V n Y O s E i 8 n F c j R 6 J e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S v q h 6 t e p l s 1 a p 3 + R x F O E E T u E c P L i C O t x B A 1 r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H 0 h W M 9 Q = = &lt; / l a t e x i t &gt; = y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 A z N 9 p S c e E k V y Y O 5 Y Y 8 D + 4 u g x y 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j 2 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l 5 q R f r r h V d w 6 y S r y c V C B H o 1 / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X U k k j 1 H 4 2 P 3 R K z q w y I G G s b E l D 5 u r v i Y x G W k + i w H Z G 1 I z 0 s j c T / / O 6 q Q m v / Y z L J D U o 2 W J R m A p i Y j L 7 m g y 4 Q m b E x B L K F L e 3 E j a i i j J j s y n Z E L z l l 1 d J + 6 L q 1 a q X z V q l f p P H U Y Q T O I V z 8 O A K 6 n A H D W g B A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f 6 l W N B Q = = &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U p R G l r T i 7 k k s L P o d G L + v v 7 E / i 4 I = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 8 E q e C q J K H q z 4 M V j C / Y D 2 l A 2 2 0 m 7 d r M J u x u x h P 4 C L x 4 U q U d v / h 1 v / h s 3 r Q d t f W H h 4 X 1 n 2 J n x Y 8 6 U d p w v K 7 e 0 v L K 6 l l 8 v b G x u b e 8 U d / c a K k o k x T q N e C R b P l H I m c C 6 Z p p j K 5 Z I Q p 9 j 0 x 9 e Z 3 n z H q V i k b j V o x i 9 k P Q F C x g l 2 l i 1 h 2 6 x 5 J S d q e x F c H + g d P U x y f R W 7 R Y / O 7 2 I J i E K T T l R q u 0 6 s f Z S I j W j H M e F T q I w J n R I + t g 2 K E i I y k u n g 4 7 t Y + P 0 7 C C S 5 g l t T 9 3 f H S k J l R q F v q k M i R 6 o + S w z / 8 v a i Q 4 u v Z S J O N E o 6 O y j I O G 2 j u x s a 7 v H J F L N R w Y I l c z M a t M B k Y R q c 5 u C O Y I 7 v / I i N E 7 L 7 l n 5 v O a W K k c w U x 4 O 4 B B O w I U L q M A N V K E O F B A e 4 R l e r D v r y X q 1 J r P S n P X T s w 9 / Z L 1 / A + s G k W I = &lt; / l a t e x i t &gt; j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C a O X C F j 7 M 1 e F U 5 r i G H C r A S r K D p A = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I p p 4 I r s G o 0 c S L x 4 h k U c C G z I 7 9 M L A 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J U 8 e p Y t h g s Y h V O 6 A a B Z f Y M N w I b C c K a R Q I b A X j + 7 n f e k K l e S w f z S R B P 6 I D y U P O q L F S f d Q r l t y y u w B Z J 1 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q T K c C Z w V u q n G h L I x H W D H U k k j 1 P 5 0 c e i M X F q l T 8 J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P U q 9 5 c / M / r p C a 8 8 6 d c J q l B y Z a L w l Q Q E 5 P 5 1 6 T P F T I j J p Z Q p r i 9 l b A h V Z Q Z m 0 3 B h u C t v r x O m t d l r 1 K + q V d K 1 Y s s j j y c w T l c g Q e 3 U I U H q E E D G C A 8 w y u 8 O S P n x X l 3 P p a t O S e b O Y U / c D 5 / A M q T j N g = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w S P e w W m d 8 u q y C s d 1 t o 8 e z n S p O K E = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S p 4 K o l U 9 F j w 4 r E F + w F t K J v t p F 2 7 2 Y T d j V B C f 4 E X D 4 p 4 9 S d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X J I J r 4 7 r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u 6 T h V D J s s F r H q B F S j 4 B K b h h u B n U Q h j Q K B 7 W B 8 N / P b T 6 g 0 j + W D m S T o R 3 Q o e c g Z N V Z q J P 1 S 2 a 2 4 c 5 B V 4 u W k D D n q / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 o V N y Y Z U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j r Z 1 w m q U H J F o v C V B A T k 9 n X Z M A V M i M m l l C m u L 2 V s B F V l B m b T d G G 4 C 2 / v E p a V x W v W r l u V M u 1 8 z y O A p z C G V y C B z d Q g 3 u o Q x M Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F r X n H z m B P 7 A + f w B 0 6 u M 3 g = = &lt; / l a t e x i t &gt; i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q r u E Y t K 4 p M s o 8 R f I S T / n K P 8 P X d Q = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j 2 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l J u + X K 2 7 V n Y O s E i 8 n F c j R 6 J e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S v q h 6 t e p l s 1 a p 3 + R x F O E E T u E c P L i C O t x B A 1 r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H 0 h W M 9 Q = = &lt; / l a t e x i t &gt; Ci &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I Z n y n Q h k P 9 A 6 M / s c E X 1 L y g W K Q z o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L F b B U 0 m k o s d C L x 4 r 2 g 9 o Q 9 l s J + 3 S z S b s b o Q S / A l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 G k M f M 7 j 6 g 0 j + W D m S b o R 3 Q k e c g Z N V a 6 b w z 4 o F x x q + 4 c Z J V 4 O a l A j u a g / N U f x i y N U B o m q N Y 9 z 0 2 M n 1 F l O B P 4 V O q n G h P K J n S E P U s l j V D 7 2 f z U J 3 J u l S E J Y 2 V L G j J X f 0 9 k N N J 6 G g W 2 M 6 J m r J e 9 m f i f 1 0 t N e O N n X C a p Q c k W i 8 J U E B O T 2 d 9 k y B U y I 6 a W U K a 4 v Z W w M V W U G Z t O y Y b g L b + 8 S t q X V a 9 W v b q r V e p n e R x F O I F T u A A P r q E O t 9 C E F j A Y w T O 8 w p s j n B f n 3 f l Y t B a c f O Y Y / s D 5 / A E J Z I 2 N &lt; / l a t e x i t &gt; j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U N y J v u M f L M x p U r X bg Q I Y L P h / 6 I E = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o l U 9 F j w 4 r E F + w F t K J v t p N 1 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H k f u 6 3 n 1 B p H s t H M 0 3 Q j + h Q 8 p A z a q z U GP d L Z b f i L k D W i Z e T M u S o 9 0 t f v U H M 0 g i l Y Y J q 3 f X c x P g Z V Y Y z g b N i L 9 W Y U D a h Q + x a K mm E 2 s 8 W h 8 7 I p V U G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m p F e 9 e b i f 1 4 3 N e G d n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k w F X y I y Y W k K Z 4 v Z W w k Z U U W Z s N k U b g r f 6 8 j p p X V e 8 a u W m U S 3 X q n k c B T i H C 7 g C D 2 6 h B g 9 Q h y Y w Q H i G V 3 h z x s 6 L 8 + 5 8 L F s 3 n H z m D P 7 A + f w B z 2 O M 6 A = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I X N v K F Z + J u H 8 N I X + r 1 O e D f p N 8 F M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W yx W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p m Q z K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 W 9 W v W 6 W a v U a 3 k c R T i D c 7 g E D 2 6 g D v f Q g B Y w Q H i G V 3 hz H p 0 X 5 9 3 5 W L Y W n H z m F P 7 A + f w B 2 H u M 7 g = = &lt; / l a t e x i t &gt; Bj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p M N 7 5 y r f K 8 M e p i Y 6 b i / D 0 P y B B l Y = " &gt; A A A B 6 n i c b Z C 7 T s M w F I Z P y q W l 3 A q M L B Y F i a l K E A j G C h h g o h X 0 I r V R 5 b h O a + o 4 k e 0 g V V E f g Y U B h F g Y e C I 2 d h 4 E N + 0 A L b 9 k 6 d P / n y O f c 7 y I M 6 V t + 8 v K L C w u L W d z K / n V t f W N z c L W d l 2 F s S S 0 R k I e y q a H F e V M 0 J p m m t N m J C k O P E 4 b 3 u B i n D c e q F Q s F H d 6 G F E 3 w D 3 B f E a w N t b t e e e + U y j a J T s V m g d n C s V y 9 v r 7 5 r 1 6 W e k U P t v d k M Q B F Z p w r F T L s S P t J l h q R j g d 5 d u x o h E m A 9 y j L Y M C B 1 S 5 S T r q C B 0 Y p 4 v 8 U J o n N E r d 3 x 0 J D p Q a B p 6 p D L D u q 9 l s b P 6 X t W L t n 7 k J E 1 G s q S C T j / y Y I x 2 i 8 d 6 o y y Q l m g 8 N Y C K Z m R W R P p a Y a H O d v D m C M 7 v y P N S P S s 5 x 6 a T q F M v 7 M F E O d m E P D s G B U y j D F V S g B g R 6 8 A j P 8 G J x 6 8 l 6 t d 4 m p R l r 2 r M D f 2 R 9 / A A G a 5 C C &lt; / l a t e x i t &gt; Bp &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t Z y i 8 a 5 d t m 0 x Z F b e A U i 9 F K 5 Q f T M = " &gt; A A A B 6 n i c b Z D L S g M x F I Z P v L X W W 9 W l m 2 A V X J U Z U X R Z 1 I W u b N F e o B 1 K J s 2 0 o Z n M k G S E M v Q R 3 L h Q x I 0 L n 8 i d e x / E 9 L L Q 1 h 8 C H / 9 / D j n n + L H g 2 j j O F 1 p Y X F p e y W R X c 2 v r G 5 t b + e 2 d m o 4 S R V m V R i J S D Z 9 o J r h k V c O N Y I 1 Y M R L 6 g t X 9 / u U o r z 8 w p X k k 7 8 0 g Z l 5 I u p I H n B J j r b u L d t z O F 5 y i M x a e B 3 c K h V L m 5 v v 2 v X J V b u c / W 5 2 I J i G T h g q i d d N 1 Y u O l R B l O B R v m W o l m M a F 9 0 m V N i 5 K E T H v p e N Q h P r R O B w e R s k 8 a P H Z / d 6 Q k 1 H o Q + r Y y J K a n Z 7 O R + V / W T E x w 7 q V c x o l h k k 4 + C h K B T Y R H e + M O V 4 w a M b B A q O J 2 V k x 7 R B F q 7 H V y 9 g j u 7 M r z U D s u u i f F 0 4 p b K B 3 A R F n Y g 3 0 4 A h f O o A T X U I Y q U O j C I z z D C x L o C b 2 i t 0 n p A p r 2 7 M I f o Y 8 f D 4 O Q i A = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 X / p O K h a H 1 k w + r t 1 b N M g D j C M n O k = " &gt; A A A C C 3 i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w V R K p 6 L L Y j c s K 9 g F p K J P J p B 0 6 m Y S Z G 6 G E f o J L t / o R 7 s S t H + E 3 + B N O 2 i x s 6 4 G B w z n 3 N c d P B N d g 2 9 9 W a W N z a 3 u n v F v Z 2 z 8 4 P K o e n 3 R 1 n C r K O j Q W s e r 7 R D P B J e s A B 8 H 6 i W I k 8 g X r + Z N W 7 v e e m N I 8 l o 8 w T Z g X k Z H k I a c E j O Q O W l x R w Y L M m Q 2 r N b t u z 4 H X i V O Q G i r Q H l Z / B k F M 0 4 h J o I J o 7 T p 2 A l 5 G F H A z c l Y Z p J o l h E 7 I i L m G S h I x 7 W X z k 2 f 4 w i g B D m N l n g Q 8 V / 9 2 Z C T S e h r 5 p j I i M N a r X i 7 + 5 7 k p h L d e x m W S A p N 0 s S h M B Y Y Y 5 / / H A V e M g p g a Q q j i 5 l Z M x 0 Q R C i a l p S 1 0 E Y 0 G l u g 8 H W c 1 i 3 X S v a o 7 j f r 1 Q 6 P W v C t y K q M z d I 4 u k Y N u U B P d o z b q I I p i 9 I J e 0 Z v 1 b L 1 b H 9 b n o r R k F T 2 n a A n W 1 y 9 7 H J w z &lt; / l a t e x i t &gt; Load Ci in registers &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x G n N d V J N a d / l n M D S 5 N 0 6 8 L K T M O 4 = " &gt; A A A C G X i c b V C 7 T s M w F H V 4 l v I K M D C w W L R I T F V S g W C s 1 I W B o U j 0 I b V R 5 D h u a 9 W x I 9 t B q q J 8 C S M r f A Q b Y m X i G / g J n D Y D b T m S p a N z 7 t X x P U H M q N K O 8 2 2 t r W 9 s b m 2 X d s q 7 e / s H h / b R c U e J R G L S x o I J 2 Q u Q I o x y 0 t Z U M 9 K L J U F R w E g 3 m D R z v / t E p K K C P + p p T L w I j T g d U o y 0 k X z 7 9 F 6 g E F a b P q 1 C y q E k I 5 N p 5 n 2 7 4 t S c G e A q c Q t S A Q V a v v 0 z C A V O I s I 1 Z k i p v u v E 2 k u R 1 B Q z k p U H i S I x w h M 0 I n 1 D O Y q I 8 t L Z A R m 8 M E o I h 0 K a x z W c q X 8 3 U h Q p N Y 0 C M x k h P V b L X i 7 + 5 / U T P b z 1 U s r j R B O O 5 0 H D h E E t Y N 4 G D K k k W L O p I Q h L a v 4 K 8 R h J h P M O F l I w l e a O 0 J Q T q 8 y 0 4 y 5 3 s U o 6 9 Z p 7 V b t + q F c a 1 a K n E j g D 5 + A S u O A G N M A d a I E 2 w C A D L + A V v F n P 1 r v 1 Y X 3 O R 9 e s Y u c E L M D 6 + g X J 5 q B j &lt; / l a t e x i t &gt; Load x in registers &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z R y U t y C K 6 L 8 I D o P u M Q m 2 m c U E 5 s A = " &gt; A A A C F 3 i c b V D L S s N A F J 3 U V 6 2 v q L h y M 9 g K r k p S F F 0 W 3 L h w U c H W Q h v K Z H L T D p 1 M w s x E L K E f 4 t K t f o Q 7 c e v S b / A n n L R d 2 N Y D A 4 d z 7 u X c O X 7 C m d K O 8 2 0 V V l b X 1 j e K m 6 W t 7 Z 3 d P X v / o K X i V F J o 0 p j H s u 0 T B Z w J a G q m O b Q T C S T y O T z 4 w + v c f 3 g E q V g s 7 v U o A S 8 i f c F C R o k 2 U s 8 + u o 1 J g C t P F c w E l t A 3 i W a 6 Z 5 e d q j M B X i b u j J T R D I 2 e / d M N Y p p G I D T l R K m O 6 y T a y 4 j U j H I Y l 7 q p g o T Q I e l D x 1 B B I l B e N j l / j E + N E u A w l u Y J j S f q 3 4 2 M R E q N I t 9 M R k Q P 1 K K X i / 9 5 n V S H V 1 7 G R J J q E H Q a F K Y c 6 x j n X e C A S a C a j w w h V D J z K 6 Y D I g n N O 5 h L o U y a f w S m n E S N T T v u Y h f L p F W r u u f V i 7 t a u V 6 Z 9 V R E x + g E n S E X X a I 6 u k E N 1 E Q U Z e g F v a I 3 6 9 l 6 t z 6 s z + l o w Z r t H K I 5 W F + / h a + f v A = = &lt; / l a t e x i t &gt; Load Bj in registers &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p z E Q E D 7 V k r K R X Z x I E g Z G K i a T J H o = " &gt; A A A C G X i c b V C 7 T s M w F H X K q 5 R X g I G B x a J F Y q q S C g R j B Q s D Q 5 H o Q 2 q j y H G c 1 t R x I t t B q q J 8 C S M r f A Q b Y m X i G / g J n D Y D b T m S p a N z 7 t X x P V 7 M q F S W 9 W 2 U V l b X 1 j f K m 5 W t 7 Z 3 d P X P / o C O j R G D S x h G L R M 9 D k j D K S V t R x U g v F g S F H i N d b 3 y T + 9 0 n I i S N + I O a x M Q J 0 Z D T g G K k t O S a R 3 c R 8 m H t 2 n 2 s Q c q h I E O d q e d d s 2 r V r S n g M r E L U g U F W q 7 5 M / A j n I S E K 8 y Q l H 3 b i p W T I q E o Z i S r D B J J Y o T H a E j 6 m n I U E u m k 0 w M y e K o V H w a R 0 I 8 r O F X / b q Q o l H I S e n o y R G o k F 7 1 c / M / r J y q 4 c l L K 4 0 Q R j m d B Q c K g i m D e B v S p I F i x i S Y I C 6 r / C v E I C Y T z D u Z S M B X 6 D l + X E 8 t M t 2 M v d r F M O o 2 6 f V 6 / u G 9 U m 7 W i p z I 4 B i f g D N j g E j T B L W i B N s A g A y / g F b w Z z 8 a 7 8 W F 8 z k Z L R r F z C O Z g f P 0 C y e S g Y w = = &lt; / l a t e x i t &gt; Ci Ci + xBj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p X B b D z 8 Z D T z k T i G o j 9 2 T f l s C T t 4 = " &gt; A A A C H H i c b V D J S g N B E O 1 x j X G L e t N L Y x Q E I c x I R I / B X D x G M A s k Y e j p 1 C R t e h a 6 a 9 Q Q A n 6 J R 6 / 6 E d 7 E q + A 3 + B N 2 l o N J f N D w 6 r 0 q q u t 5 s R Q a b f v b W l h c W l 5 Z T a 2 l 1 z c 2 t 7 Y z O 7 s V H S W K Q 5 l H M l I 1 j 2 m Q I o Q y C p R Q i x W w w J N Q 9 b r F o V + 9 B 6 V F F N 5 i L 4 Z m w N q h 8 A V n a C Q 3 s 1 9 0 B W 1 I 8 J E p F T 3 Q Y X l K H + m V e + d m s n b O H o H O E 2 d C s m S C k p v 5 a b Q i n g Q Q I p d M 6 7 p j x 9 j s M 4 W C S x i k G 4 m G m P E u a 0 P d 0 J A F o J v 9 0 Q 0 D e m y U F v U j Z V 6 I d K T + n e i z Q O t e 4 J n O g G F H z 3 p D 8 T + v n q B / 2 e y L M E 4 Q Q j 5 e 5 C e S Y k S H g d C W U M B R 9 g x h X A n z V 8 o 7 T D G O J r a p L V w o c 0 d L I 8 R 6 Y N J x Z r O Y J 5 W z n J P P n d / k s 4 W j S U 4 p c k A O y Q l x y A U p k G t S I m X C y R N 5 I a / k z X q 2 3 q 0 P 6 3 P c u m B N Z v b I F K y v X 3 4 f o T c = &lt; / l a t e x i t &gt; Ci Ci + yBp &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + i c x Y 1 d + j 8 G I n h 1 x q u N O R l J D n 6 o = " &gt; A A A C H H i c b V D J S g N B E O 2 J W 4 z b q D e 9 N E Z B E M K M R P Q Y z M V j B L N A E o a e T k 3 S p G e h u 0 Y J I e C X e P S q H + F N v A p + g z 9 h Z z m Y 6 I O G V + 9 V U V 3 P T 6 T Q 6 D h f V m Z p e W V 1 L b u e 2 9 j c 2 t 6 x d / d q O k 4 V h y q P Z a w a P t M g R Q R V F C i h k S h g o S + h 7 v f L Y 7 9 + D 0 q L O L r D Q Q L t k H U j E Q j O 0 E i e f V D 2 B G 1 J C J A p F T / Q c X l G B / T a S z w 7 7 x S c C e h f 4 s 5 I n s x Q 8 e z v V i f m a Q g R c s m 0 b r p O g u 0 h U y i 4 h F G u l W p I G O + z L j Q N j V g I u j 2 c 3 D C i J 0 b p 0 C B W 5 k V I J + r v i S E L t R 6 E v u k M G f b 0 o j c W / / O a K Q Z X 7 a G I k h Q h 4 t N F Q S o p x n Q c C O 0 I B R z l w B D G l T B / p b z H F O N o Y p v b w o U y d 3 Q 0 Q q J H J h 1 3 M Y u / p H Z e c I u F i 9 t i v n Q 8 y y l L D s k R O S U u u S Q l c k M q p E o 4 e S T P 5 I W 8 W k / W m / V u f U x b M 9 Z s Z p / M w f r 8 A Y l u o T 4 = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " t h i X m R + Y w Q 6 q o + y 3 p 0 H R 6 Y a T I A Y = " &gt; A A A C C 3 i c b V D L S s N A F J 3 4 r P V V d e l m s A q u S l I q u i x 0 4 7 K C f U A a y m R y 0 w 6 d Z M L M R C i h n + D S r X 6 E O 3 H r R / g N / o S T N g v b e m D g c M 5 9 z f E T z p S 2 7 W 9 r Y 3 N r e 2 e 3 t F f e P z g 8 O q 6 c n H a V S C W F D h V c y L 5 P F H A W Q 0 c z z a G f S C C R z 6 H n T 1 q 5 3 3 s C q Z i I H / U 0 A S 8 i o 5 i F j B J t J H f Q Y p J y C L L 6 b F i p 2 j V 7 D r x O n I J U U Y H 2 s P I z C A R N I 4 g 1 5 U Q p 1 7 E T 7 W V E a m Z G z s q D V E F C 6 I S M w D U 0 J h E o L 5 u f P M N X R g l w K K R 5 s c Z z 9 W 9 H R i K l p p F v K i O i x 2 r V y 8 X / P D f V 4 Z 2 X s T h J N c R 0 s S h M O d Y C 5 / / H A Z N A N Z 8 a Q q h k 5 l Z M x 0 Q S q k 1 K S 1 v o I h q l I V F 5 O s 5 q F u u k W 6 8 5 j d r N Q 6 P a v C x y K q F z d I G u k Y N u U R P d o z b q I I o E e k G v 6 M 1 6 t t 6 t D + t z U b p h F T 1 n a A n W 1 y 9 z t J w W &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " r h i O R h h q d 2 t p Z L a y L Y v R J x I B 3 M U = " &gt; A A A C C 3 i c b V D L T s J A F J 3 i C / G F u n Q z E U 1 c k V Y x u i R h 4 x I T Q Z L S k O n 0 F i Z M p 8 3 M 1 I Q 0 f I J L t / o R 7 o x b P 8 J v 8 C e c Q h c C n m S S k 3 P u a 4 6 f c K a 0 b X 9 b p b X 1 j c 2 t 8 n Z l Z 3 d v / 6 B 6 e N R V c S o p d G j M Y 9 n z i Q L O B H Q 0 0 x x 6 i Q Q S + R w e / X E r 9 x + f Q C o W i w c 9 S c C L y F C w k F G i j e T 2 W 0 x S D k F 2 N R 1 U a 3 b d n g G v E q c g N V S g P a j + 9 I O Y p h E I T T l R y n X s R H s Z k Z q Z k d N K P 1 W Q E D o m Q 3 A N F S Q C 5 W W z k 6 f 4 3 C g B D m N p n t B 4 p v 7 t y E i k 1 C T y T W V E 9 E g t e 7 n 4 n + e m O r z 1 M i a S V I O g 8 0 V h y r G O c f 5 / H D A J V P O J I Y R K Z m 7 F d E Q k o d q k t L C F z q N R G h K V p + M s Z 7 F K u p d 1 p 1 G / v m / U m m d F T m V 0 g k 7 R B X L Q D W q i O 9 R G H U R R j F 7 Q K 3 q z n q 1 3 6 8 P 6 n J e W r K L n G C 3 A + v o F d V K c F w = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " / i y k 9 M C e o G t p n W L B V O k k Z R K J H c w = " &gt; A A A C C 3 i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 1 V w V R K p 6 L L Q j c s K 9 g F p K J P J T T t 0 M g k z E 6 G E f o J L t / o R 7 s S t H + E 3 + B N O 2 i x s 6 4 G B w z n 3 N c d P O F P a t r + t 0 s b m 1 v Z O e b e y t 3 9 w e F Q 9 P u m q O J U U O j T m s e z 7 R A F n A j q a a Q 7 9 R A K J f A 4 9 f 9 L K / d 4 T S M V i 8 a i n C X g R G Q k W M k q 0 k d x B i 0 n K I c g a s 2 G 1 Z t f t O f A 6 c Q p S Q w X a w + r P I I h p G o H Q l B O l X M d O t J c R q Z k Z O a s M U g U J o R M y A t d Q Q S J Q X j Y / e Y Y v j R L g M J b m C Y 3 n 6 t + O j E R K T S P f V E Z E j 9 W q l 4 v / e W 6 q w z s v Y y J J N Q i 6 W B S m H O s Y 5 / / H A Z N A N Z 8 a Q q h k 5 l Z M x 0 Q S q k 1 K S 1 v o I h q l I V F 5 O s 5 q F u u k e 1 1 3 G v W b h 0 a t e V H k V E Z n 6 B x d I Q f d o i a 6 R 2 3 U Q R T F 6 A W 9 o j f r 2 X q 3 P q z P R W n J K n p O 0 R K s r 1 9 2 8 J w Y &lt; / l a t e x i t &gt; Load y in registers &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N x Q l D k 7 r x z q m O 0 Q 0 f 0 2 t T V B 9 5 M 8 = " &gt; A A A C F 3 i c b V C 7 T s M w F H X K q 5 R X A D G x W L R I T F V S g W C s x M L A U C T 6 k N q o c h y n t e r Y k e 0 g R V E / h J E V P o I N s T L y D f w E T p u B t h z J 0 t E 5 9 + p c H z 9 m V G n H + b Z K a + s b m 1 v l 7 c r O 7 t 7 + g X 1 4 1 F E i k Z i 0 s W B C 9 n y k C K O c t D X V j P R i S V D k M 9 L 1 J 7 e 5 3 3 0 i U l H B H 3 U a E y 9 C I 0 5 D i p E 2 0 t A + u R c o g L W 0 B i m H k o x M o p k e 2 l W n 7 s w A V 4 l b k C o o 0 B r a P 4 N A 4 C Q i X G O G l O q 7 T q y 9 D E l N M S P T y i B R J E Z 4 g k a k b y h H E V F e N j t / C s + N E s B Q S P O 4 h j P 1 7 0 a G I q X S y D e T E d J j t e z l 4 n 9 e P 9 H h j Z d R H i e a c D w P C h M G t Y B 5 F z C g k m D N U k M Q l t T c C v E Y S Y T z D h Z S M J X m H 4 E p J 1 Z T 0 4 6 7 3 M U q 6 T T q 7 m X 9 6 q F R b d a K n s r g F J y B C + C C a 9 A E d 6 A F 2 g C D D L y A V / B m P V v v 1 o f 1 O R 8 t W c X O M V i A 9 f U L h 1 q f v Q = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " F i U + f n S B 3 T i I H P m v l G a X a H 4 m r d Q = " &gt; A A A C C 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B V c l U R a d F n o x m U F + 4 A 0 l M l k 0 g 6 d T M L M R C i h n + D S r X 6 E O 3 H r R / g N / o S T N g v b e m D g c M 5 9 z f E T z p S 2 7 W 9 r Y 3 N r e 2 e 3 t F f e P z g 8 O q 6 c n H Z V n E p C O y T m s e z 7 W F H O B O 1 o p j n t J 5 L i y O e 0 5 0 9 a u d 9 7 o l K x W D z q a U K 9 C I 8 E C x n B 2 k j u o M U k 4 T T I G r N h p W r X 7 D n Q O n E K U o U C 7 W H l Z x D E J I 2 o 0 I R j p V z H T r S X Y a m Z G T k r D 1 J F E 0 w m e E R d Q w W O q P K y + c k z d G W U A I W x N E 9 o N F f / d m Q 4 U m o a + a Y y w n q s V r 1 c / M 9 z U x 3 e e R k T S a q p I I t F Y c q R j l H + f x Q w S Y n m U 0 M w k c z c i s g Y S 0 y 0 S W l p C 1 l E o z R N V J 6 O s 5 r F O u n e 1 J x 6 r f F Q r z Y v i 5 x K c A 4 X c A 0 O 3 E I T 7 q E N H S A Q w w u 8 w p v 1 b L 1 b H 9 b n o n T D K n r O Y A n W 1 y 9 4 j p w Z &lt; / l a t e x i t &gt; Load Bp in registers &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t c O a + 7 9 4 9 A Z / y o b E 7 B x Q Y M 0 n g f I = " &gt; A A A C G X i c b V C 9 T s M w G H T K X y l / A Q Y G F o s W i a l K K h C M F S w M D E W i t F I b R Y 7 j t F Y d O 7 I d p K r K k z C y w k O w I V Y m n o G X w G k z 0 J a T L J 3 u v k / n 7 4 K E U a U d 5 9 s q r a y u r W + U N y t b 2 z u 7 e / b + w a M S q c S k j Q U T s h s g R R j l p K 2 p Z q S b S I L i g J F O M L r J / c 4 T k Y o K / q D H C f F i N O A 0 o h h p I / n 2 0 Z 1 A I a x d + 0 k N U g 4 l G Z h M M + / b V a f u T A G X i V u Q K i j Q 8 u 2 f f i h w G h O u M U N K 9 V w n 0 d 4 E S U 0 x I 1 m l n y q S I D x C A 9 I z l K O Y K G 8 y P S C D p 0 Y J Y S S k e V z D q f p 3 Y 4 J i p c Z x Y C Z j p I d q 0 c v F / 7 x e q q M r b 0 J 5 k m r C 8 S w o S h n U A u Z t w J B K g j U b G 4 K w p O a v E A + R R D j v Y C 4 F U 2 n u C E 0 5 i c p M O + 5 i F 8 v k s V F 3 z + s X 9 4 1 q s 1 b 0 V A b H 4 A S c A R d c g i a 4 B S 3 Q B h h k 4 A W 8 g j f r 2 X q 3 P q z P 2 W j J K n Y O w R y s r 1 / T 5 q B p &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 6 Z + R 4 d e 4 h i X 2 X V L H + a 0 1 R S a g m k = " &gt; A A A C C 3 i c b V D L T s J A F J 3 i C / G F u n Q z E U 1 c k d b g Y 0 n C x i U m g i S l I d P p F C Z M p 8 3 M r Q l p + A S X b v U j 3 B m 3 f o T f 4 E 8 4 h S 4 E P M k k J + f c 1 x w / E V y D b X 9 b p b X 1 j c 2 t 8 n Z l Z 3 d v / 6 B 6 e N T V c a o o 6 9 B Y x K r n E 8 0 E l 6 w D H A T r J Y q R y B f s 0 R + 3 c v / x i S n N Y / k A k 4 R 5 E R l K H n J K w E h u v 8 U V F S z I r q e D a s 2 u 2 z P g V e I U p I Y K t A f V n 3 4 Q 0 z R i E q g g W r u O n Y C X E Q X c j J x W + q l m C a F j M m S u o Z J E T H v Z 7 O Q p P j d K g M N Y m S c B z 9 S / H R m J t J 5 E v q m M C I z 0 s p e L / 3 l u C u G t l 3 G Z p M A k n S 8 K U 4 E h x v n / c c A V o y A m h h C q u L k V 0 x F R h I J J a W E L n U e j g S U 6 T 8 d Z z m K V d C / r T q N + d d + o N c + K n M r o B J 2 i C + S g G 9 R E d 6 i N O o i i G L 2 g V / R m P V v v 1 o f 1 O S 8 t W U X P M V q A 9 f U L e i y c G g = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " x D o c D m i i x V i k M 8 V L 0 b Y R F h 1 A s S 0 = " &gt; A A A C C 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B V c l U Q q d V n o x m U F + 4 A 0 l M l k 0 g 6 d T M L M R C i h n + D S r X 6 E O 3 H r R / g N / o S T N g v b e m D g c M 5 9 z f E T z p S 2 7 W 9 r Y 3 N r e 2 e 3 t F f e P z g 8 O q 6 c n H Z V n E p C O y T m s e z 7 W F H O B O 1 o p j n t J 5 L i y O e 0 5 0 9 a u d 9 7 o l K x W D z q a U K 9 C I 8 E C x n B 2 k j u o M U k 4 T T I G r N h p W r X 7 D n Q O n E K U o U C 7 W H l Z x D E J I 2 o 0 I R j p V z H T r S X Y a m Z G T k r D 1 J F E 0 w m e E R d Q w W O q P K y + c k z d G W U A I W x N E 9 o N F f / d m Q 4 U m o a + a Y y w n q s V r 1 c / M 9 z U x 3 e e R k T S a q p I I t F Y c q R j l H + f x Q w S Y n m U 0 M w k c z c i s g Y S 0 y 0 S W l p C 1 l E o z R N V J 6 O s 5 r F O u n e 1 J x 6 7 f a h X m 1 e F j m V 4 B w u 4 B o c a E A T 7 q E N H S A Q w w u 8 w p v 1 b L 1 b H 9 b n o n T D K n r O Y A n W 1 y 9 7 y p w b &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " g M K 7 B 4 e z 9 F z j S u + G U e 6 u X W b v J m U = " &gt; A A A C C 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B V c l U Q q d l n o x m U F + 4 A 0 l M l k 0 g 6 d T M L M R C i h n + D S r X 6 E O 3 H r R / g N / o S T N g v b e m D g c M 5 9 z f E T z p S 2 7 W 9 r Y 3 N r e 2 e 3 t F f e P z g 8 O q 6 c n H Z V n E p C O y T m s e z 7 W F H O B O 1 o p j n t J 5 L i y O e 0 5 0 9 a u d 9 7 o l K x W D z q a U K 9 C I 8 E C x n B 2 k j u o M U k 4 T T I G r N h p W r X 7 D n Q O n E K U o U C 7 W H l Z x D E J I 2 o 0 I R j p V z H T r S X Y a m Z G T k r D 1 J F E 0 w m e E R d Q w W O q P K y + c k z d G W U A I W x N E 9 o N F f / d m Q 4 U m o a + a Y y w n q s V r 1 c / M 9 z U x 0 2 v I y J J N V U k M W i M O V I x y j / P w q Y p E T z q S G Y S G Z u R W S M J S b a p L S 0 h S y i U Z o m K k / H W c 1 i n X R v a k 6 9 d v t Q r z Y v i 5 x K c A 4 X c A 0 O 3 E E T 7 q E N H S A Q w w u 8 w p v 1 b L 1 b H 9 b n o n T D K n r O Y A n W 1 y 9 9 a J w c &lt; / l a t e x i t &gt; Store Ci &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / K O Y g u M v d / S O 3 7 3 X D G f t B y u / K 3 E = " &gt; A A A B 8 n i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B F v B U 9 k t i h 4 L v X i s a D 9 g u 5 R s m m 1 D s 8 m S z A p l 6 c / w 4 k E R r / 4 a b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z w k R w A 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J x 6 h U U 9 a m S i j d C 4 l h g k v W B g 6 C 9 R L N S B w K 1 g 0 n z b n f f W L a c C U f Y Z q w I C Y j y S N O C V j J f w C l G a 4 2 B 7 w 6 K F f c m r s A X i d e T i o o R 2 t Q / u o P F U 1 j J o E K Y o z v u Q k E G d H A q W C z U j 8 1 L C F 0 Q k b M t 1 S S m J k g W 5 w 8 w x d W G e J I a V s S 8 E L 9 P Z G R 2 J h p H N r O m M D Y r H p z 8 T / P T y G 6 D T I u k x S Y p M t F U S o w K D z / H w + 5 Z h T E 1 B J C N b e 3 Y j o m m l C w K Z V s C N 7 q y + u k U 6 9 5 V 7 X r + 3 q l U c 3 j K K I z d I 4 u k Y d u U A P d o R Z q I 4 o U e k a v 6 M 0 B 5 8 V 5 d z 6 W r Q U n n z l F f + B 8 / g D 8 T Z B Q &lt; / l a t e x i t &gt; Micro Kernel of LIBXSMM Sparse-Dense Matrix Multiplication (SPMM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. 10 :</head><label>10</label><figDesc>Static and Dynamic Sensitivity Analysis for a 400?200?200?100 network on the MSN30K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Fig. 13 :</head><label>13</label><figDesc>Comparison between neural networks and ensemble of regression tree on the low-latency retrieval scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Cosimo Rulli and Rossano Venturini are with the University of Pisa, Italy. E-mail: cosimo.rulli@phd.unipi.it, rossano.venturini@di.unipi.it.</figDesc><table /><note>?? Franco Maria Nardini, Cosimo Rulli, Salvatore Trani and Rossano Ven- turini are with the ISTI-CNR, Pisa, Italy. E-mail: {cosimo.rulli, f.nardini, s.trani, rossano.venturini}@isti.cnr.it.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. See for example https://scipy-cookbook.readthedocs.io/items/ ParallelProgramming.html#Use-parallel-primitives</figDesc><table><row><cell>Model</cell><cell cols="2">NDCG@10 NDCG</cell><cell>MAP</cell><cell>Scoring Time (?s/ doc)</cell></row><row><cell cols="2">Large Forest 0.5246  ?</cell><cell>0.7473  ?</cell><cell>0.6604  ?</cell><cell>8.2</cell></row><row><cell>Mid Forest</cell><cell>0.5206  ?</cell><cell>0.7454  ?</cell><cell>0.6582  ?</cell><cell>1.5</cell></row><row><cell>Small Forest</cell><cell>0.5181</cell><cell>0.7438</cell><cell>0.6578</cell><cell>0.8</cell></row><row><cell>Large Net</cell><cell>0.5198  ?</cell><cell>0.7445  ?</cell><cell>0.6582  ?</cell><cell>24.4</cell></row><row><cell>Small Net</cell><cell>0.5171</cell><cell>0.7432</cell><cell>0.6575</cell><cell>2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>A comparison between QuickScorer and Neural Networks on the MSN30K dataset. Symbols evidence statistically significant improvement w.r.t. to Mid Forest ( ), and Small Forest ( ? ), according to the Fisher's randomization test, p &lt; 0.05.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>illustrates how the prediction model can substitute the experimental procedure of training and testing a model, turning out to be essential to reduce the architecture search space.</figDesc><table><row><cell>Model</cell><cell cols="2">Scoring Time (?s/doc)</cell></row><row><cell></cell><cell>Real</cell><cell>Predicted</cell></row><row><cell>1000?500?500?100</cell><cell>14.4</cell><cell>14.5</cell></row><row><cell>200?100?100?50</cell><cell>1.3</cell><cell>1.3</cell></row><row><cell>300?150?150?30</cell><cell>2.0</cell><cell>2.2</cell></row><row><cell>500?100</cell><cell>2.1</cell><cell>2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance of our dense prediction model. Real execution times measured with batch size = 1000.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Comparison between MKL and LIBXSMM for Sparse Dense Matrix Multiplication (SDMM) . Shapes and sparsities represent the first layer of FFNs trained on MSN30K. Batch size is set to 64.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc>Some examples of our sparse time predictor with different values of N .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>. The list of features is available at https://www.microsoft.com/enus/research/project/mslr/</figDesc><table><row><cell>Model</cell><cell>Teacher</cell><cell>NDCG@10</cell></row><row><cell>878 trees, 64 leaves</cell><cell>/</cell><cell>0.5246</cell></row><row><cell cols="2">600 trees, 256 leaves /</cell><cell>? 0.5291</cell></row><row><cell>500?100</cell><cell>878 trees, 64 leaves 600 trees, 256 leaves</cell><cell>0.5180 ? 0.5198</cell></row><row><cell>100?500?500?100</cell><cell>878 trees, 64 leaves 600 trees, 256 leaves</cell><cell>0.5208 ? 0.5243</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 :</head><label>5</label><figDesc>Comparison in terms of NDCG@10 among Neural Networks on MSN30K, when trained to approximate different teachers. ? indicates statistically significant improvement (Fisher's randomization test, p &lt; 0.05).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>In our experiments, dense models do not reach the performance of ensembles of regression trees scored with QuickScorer. We now address the problem by leveraging the advantages brought by model compression, in particular by network pruning<ref type="bibr" target="#b16">[17]</ref>,<ref type="bibr" target="#b18">[19]</ref>, a technique that deeply sparsifies a neural model without incurring in performance degradation. Let us consider the time budget of 3?s: we devise a model which exceeds the time budget but affords an NDCG@10 close the 300 trees model. By mean of pruning, we can move to the sparse domain and benefit of fast Sparse Dense Matrix Multiplication routines (SDMM). As example model, we</figDesc><table><row><cell>Model</cell><cell cols="2">Scoring Time (?s/doc) NDCG@10</cell></row><row><cell>QuickScorer 300, 64</cell><cell>3.0</cell><cell>0.5230</cell></row><row><cell>500?100</cell><cell>2.2</cell><cell>0.5196</cell></row><row><cell>300?200?100</cell><cell>2.4</cell><cell>0.5209</cell></row><row><cell>300?150?150?30</cell><cell>2.2</cell><cell>0.5207</cell></row><row><cell>QuickScorer 500, 64</cell><cell>4.9</cell><cell>0.5240</cell></row><row><cell>1000?200</cell><cell>5.5</cell><cell>0.5150</cell></row><row><cell>600?300?100</cell><cell>5.6</cell><cell>0.5203</cell></row><row><cell>500?250?250?100</cell><cell>5.4</cell><cell>0.5218</cell></row></table><note>Sensitivity analysis and pruning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 6 :</head><label>6</label><figDesc>Comparison in terms of Scoring Time between QuickScorer and Neural Networks on MSN30K. The notation "QuickScorer x, y" indicates that x is the number of trees, and y the number of leaves per tree.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 7 :</head><label>7</label><figDesc>Breakdown of the relative execution time among different layers for different neural models.Fig. 11: Matrix multiplication speedup at various levels of sparsity estimated with our sparse time predictor. We assume the number of active columns/rows to be equal to the total number of columns/rows (worst-case scenario).</figDesc><table><row><cell>Model</cell><cell cols="3">Description NDCG@10 Sc. Time (?s/doc)</cell></row><row><cell></cell><cell>878 trees</cell><cell>? 0.5246</cell><cell>8.2</cell></row><row><cell>QuickScorer</cell><cell>500 trees</cell><cell>0.5240</cell><cell>4.9</cell></row><row><cell></cell><cell>300 trees</cell><cell>0.5230</cell><cell>3.0</cell></row><row><cell>Neural</cell><cell>Dense Sparse</cell><cell>0.5222 ? 0.5246</cell><cell>3.8 2.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 8</head><label>8</label><figDesc>QuickScorer in terms of NDCG@10 and Scoring Time (Sc. Time). ? indicates statistically significant improvement w.r.t. models of the same family (Fisher's randomization test, p &lt; 0.05).</figDesc><table><row><cell>:</cell><cell>Dense</cell><cell>and</cell><cell>sparse</cell><cell>neural</cell><cell>models</cell></row><row><cell cols="2">(400?200?200?100) vs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 9 :</head><label>9</label><figDesc>Training and pruning parameters employed for neural networks on MSN30K and Istella-S.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 10 :</head><label>10</label><figDesc></figDesc><table /><note>Prediction of model scoring time (Sc. Time) when pruning the first layer, in High Quality Retrieval.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 11 :</head><label>11</label><figDesc></figDesc><table /><note>Prediction of model scoring time (Sc. Time) when pruning the first layer, in Low-Latency Retrieval.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This paper is partially supported by the "Algorithms, Data Structures and Combinatorics for Machine Learning" (MIUR-PRIN 2017) and the OK-INSAID (MIUR-PON 2018, grant agreement ARS01 00917) projects.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rossano Venturini is an associate professor at the Computer Science Department, University of Pisa. His research interests include the design and the analysis of algorithms and data structures with focus on indexing and searching large textual collections. He received two Best Paper Awards at ACM SI-GIR in 2014 and 2015. For more information: http://pages.di.unipi.it/rossano.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Memory consumption and FLOP count estimates for convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<ptr target="https://github.com/albanie/convnet-burden" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Petaflop seismic simulations in the public cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heinecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing (HiPC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine learning (ICML)</title>
		<meeting>the International Conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">From RankNet to LambdaRank to LambdaMart: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Learning</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning with low precision by half-wave gaussian quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalability challenges in web search engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Cambazoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced topics in information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal approximation functions for fast learning to rank: Replacing expensive regression forests with simple feed-forward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACM International Conference on Research &amp; Development in Information Retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast ranking with additive ensembles of oblivious and non-oblivious regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venturini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TOIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anatomy of high-performance matrix multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A V D</forename><surname>Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic network surgery for efficient dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AMC: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LIBXSMM: accelerating small matrix multiplications by runtime code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pabst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">BLISlab: A sandbox for optimizing GEMM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Van De Geijn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00076</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data-driven sparse structure selection for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>J?rvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kek?l?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<publisher>TOIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">LightGBM: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast nonnegative matrix factorization: An active-set-like method and comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Basic linear algebra subprograms for Fortran usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Krogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multicore/manycore parallel traversal of large forests of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lettich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venturini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing Simulation (HPCS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ternary weight networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1605.04711</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno>abs/1608.08710</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">McRank: Learning to rank using multiple classification and gradient boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Constructive approximation of discontinuous functions by neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Llanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lantar?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?inz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Analytical modeling is enough for high-performance BLIS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Igual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Quintana-Orti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">QuickScorer: A fast algorithm to rank documents with additive ensembles of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venturini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>ACM International Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploiting CPU SIMD extensions to speed-up document scoring with tree ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venturini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the ACM International Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ThiNet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-M</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1306.2597</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Introducing LETOR 4.0 datasets</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Are neural rankers still outperformed by gradient boosted decision trees?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Pasumarthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">XNOR-Net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference con Computer Vision (ECCV)</title>
		<meeting>the European Conference con Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>MobileNetV2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sparse inverse covariance selection via alternating linearization methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">All-pairs shortest paths computation in the BSP model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tiskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">FRank: a ranking method with fidelity loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the ACM International Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">BLIS: A framework for rapidly instantiating BLAS functionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Van Zee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to prune: Exploring the frontier of fast and accurate parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Intel math kernel library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computing on the Intel? Xeon Phi?</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xianyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chothia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenBLAS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">RapidScorer: fast tree ensemble evaluation by maximizing compactness in data level parallelization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</title>
		<meeting>the ACM International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scalpel: Customizing dnn pruning to the underlying hardware parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukefahr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Palframan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1702.03044</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Trained ternary quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>abs/1612.01064</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Neural network distiller: A python package for dnn compression research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zmora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zlotnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elharar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Novik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
