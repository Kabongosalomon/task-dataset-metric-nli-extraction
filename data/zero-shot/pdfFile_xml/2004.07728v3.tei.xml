<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Image Quality Assessment: Unifying Structure and Texture Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyan</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Kede</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Depart-ment of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Image Quality Assessment: Unifying Structure and Texture Similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image quality assessment</term>
					<term>structure similarity</term>
					<term>texture similarity</term>
					<term>perceptual optimization !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objective measures of image quality generally operate by comparing pixels of a "degraded" image to those of the original. Relative to human observers, these measures are overly sensitive to resampling of texture regions (e.g., replacing one patch of grass with another). Here, we develop the first full-reference image quality model with explicit tolerance to texture resampling. Using a convolutional neural network, we construct an injective and differentiable function that transforms images to multi-scale overcomplete representations. We demonstrate empirically that the spatial averages of the feature maps in this representation capture texture appearance, in that they provide a set of sufficient statistical constraints to synthesize a wide variety of texture patterns. We then describe an image quality method that combines correlations of these spatial averages ("texture similarity") with correlations of the feature maps ("structure similarity"). The parameters of the proposed measure are jointly optimized to match human ratings of image quality, while minimizing the reported distances between subimages cropped from the same texture images. Experiments show that the optimized method explains human perceptual scores, both on conventional image quality databases, as well as on texture databases. The measure also offers competitive performance on related tasks such as texture classification and retrieval. Finally, we show that our method is relatively insensitive to geometric transformations (e.g., translation and dilation), without use of any specialized training or data augmentation. Code is available at https://github.com/dingkeyan93/DISTS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>! I MAGE quality assessment (IQA) -the quantification of human perception of image quality -is a fundamental problem in both human and computational vision, and is of paramount importance in a variety of real-world applications, such as image restoration, compression, and rendering. For more than 50 years, the mean squared error (MSE) was the standard full-reference method for assessing signal fidelity and quality, and it continues to play a fundamental role in the development of signal and image processing algorithms, despite its poor correlation with human perception <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>A variety of proposed full-reference IQA methods provide a better account of human perception than MSE <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b7">[8]</ref>, and the Structural Similarity (SSIM) index <ref type="bibr" target="#b2">[3]</ref> has become a de facto standard in the field of image processing. But these methods rely on alignment of the images being compared, and are thus highly sensitive to differences between images of the same texture (e.g., two different cropped regions of the same bed of pebbles). Two samples of the same texture differ substantially in the precise arrangement of their features, while appearing nearly the same to a human observer (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Since textured surfaces are ubiquitous in photographic images, it is important to develop objective IQA metrics that are consistent with this aspect of perceptual similarity. Such a metric would allow the development of a new generation of image processing solutions -for example, a compression engine that statistically synthesizes texture regions rather than trying to exactly re-create the pixels of the original image <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We present the first full-reference IQA method that is insensitive to resampling of visual textures. Our method is constructed by first nonlinearly transforming images to a multi-scale overcomplete representation, using a variant of the VGG convolutional neural network (CNN) <ref type="bibr" target="#b13">[14]</ref>. We show that the spatial averages of the feature maps provide a compact set of statistical constraints that is sufficient to capture the visual appearance of textures <ref type="bibr" target="#b14">[15]</ref>. Specifically, we use the test originally proposed by Julesz <ref type="bibr" target="#b15">[16]</ref>, and demonstrate that synthesizing a new image by forcing it to match the channel averages computed from a given texture image results in an image of similar visual appearance. Although the number of statistics in the set is substantially smaller than that of pixels in the image, we find that the result holds for a wide variety of textures, regardless of the initialization, thus revealing the robustness of this model to adversarial examples <ref type="bibr" target="#b16">[17]</ref>.</p><p>After transforming the original and corrupted images, we construct our measure by combining two terms over all feature maps: one that compares the spatial averages (and thus, the texture properties) of the two images, and a second that compares the structural details. The final distortion score is computed as a weighted sum of these two terms, with the weights adjusted to match human perception of image quality and invariance to resampled texture patches. The first is achieved by comparing the responses of the model with a database of human image quality ratings. The second is achieved by minimizing the distance between pairs of patches sampled from the same texture images. We  <ref type="bibr" target="#b2">[3]</ref>, FSIM <ref type="bibr" target="#b10">[11]</ref>, VIF <ref type="bibr" target="#b3">[4]</ref>, GMSD <ref type="bibr" target="#b11">[12]</ref>, DeepIQA <ref type="bibr" target="#b12">[13]</ref>, PieAPP <ref type="bibr" target="#b7">[8]</ref>, and LPIPS <ref type="bibr" target="#b6">[7]</ref>, predict that image (b) has a better perceived quality than image (c), which is in disagreement with human rating. In contrast, the proposed DISTS model makes the correct prediction. (Zoom in to improve visibility of details).</p><p>show that the resulting Deep Image Structure and Texture Similarity (DISTS) index can be transformed into a proper metric in the mathematical sense. Moreover, DISTS correlates well with human quality judgments in several independent datasets, and achieves a high degree of invariance to texture substitution. We also demonstrate competitive performance of DISTS on tasks of texture classification and retrieval. Last, we show that DISTS is insensitive to mild local and global geometric distortions <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which may be imperceptible to the human visual system (HVS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">BACKGROUND</head><p>Pioneering work on perceptual full-reference IQA dated back to the 1970s, when Mannos and Sakrison <ref type="bibr" target="#b19">[20]</ref> investigated a class of visual fidelity measures in the context of rate-distortion optimization. A number of alternative models were subsequently proposed <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, each mimicking certain functionalities of the HVS and penalizing the errors between the reference and distorted images "perceptually". However, the HVS is a complex and highly nonlinear system <ref type="bibr" target="#b22">[23]</ref>, and most IQA measures within the error visibility framework rely on strong assumptions and simplifications (e.g., linear or quasi-linear models for early vision characterized by restricted visual stimuli), and exhibit shortcomings regarding the definition of visual quality, quantification of suprathreshold distortions, and generalization to natural images <ref type="bibr" target="#b23">[24]</ref>. The SSIM index <ref type="bibr" target="#b2">[3]</ref> introduced the concept of comparing structure similarity (instead of measuring error visibility), opening the door to a new class of full-reference IQA measures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Other design methodologies for knowledge-driven IQA include informationtheoretic criterion <ref type="bibr" target="#b3">[4]</ref> and perception-based pooling <ref type="bibr" target="#b25">[26]</ref>.</p><p>Recently, there has been a surge of interest in leveraging advances in large-scale optimization to develop data-driven IQA measures <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, databases of human quality scores are often insufficiently rich to constrain the large number of model parameters. As a result, these learned methods are at risk of over-fitting <ref type="bibr" target="#b26">[27]</ref>. Nearly all knowledge-driven full-reference IQA models base their quality measurements on point-by-point comparisons between pixels or convolution responses (e.g., wavelets). As such, they are not capable of handling "visual textures", which are loosely defined as spatially ho-mogeneous regions with repeated elements, often subject to some randomization in their location, size, color, and orientation <ref type="bibr" target="#b14">[15]</ref>. Different images of the same texture can look nearly the same to the human eye, while differing substantially at the level of pixel intensities. Research on visual texture has a long history, and can be partitioned into four problems: texture classification, texture segmentation, texture synthesis, and shape from texture. At the core of texture analysis is an efficient description (i.e., representation) that matches human perception of visual textures. In this paper, we aim to measure perceptual texture similarity, a goal first elucidated and explored in <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p><p>The response amplitudes and variances of computational texture features (e.g., Gabor basis functions <ref type="bibr" target="#b29">[30]</ref>, local binary patterns <ref type="bibr" target="#b30">[31]</ref>) have achieved good performance for texture classification, but are not well correlated with human perceptual ratings of texture similarity <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Texture representations that incorporate more sophisticated statistical features, such as correlations of complex wavelet coefficients <ref type="bibr" target="#b14">[15]</ref>, have shown significantly more power for texture synthesis, suggesting that they may provide a good substrate for similarity measures. In recent years, the use of such statistics extracted from CNN-based representations <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> has led to even richer texture description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE DISTS INDEX</head><p>Our goal is to develop a new full-reference IQA model that combines sensitivity to structural distortions (e.g., artifacts due to noise, blur, or compression) with a tolerance of texture resampling (exchanging the content of a texture region with a new sample of the same texture). As is common in many IQA methods, we first transform the reference and distorted images to a new representation, using a CNN. Within this representation, we develop a set of measurements that are sufficient to capture the appearance of a variety of different visual textures. Finally, we combine these texture parameters with global structural measurements to form an IQA measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Initial Transformation</head><p>Our model is built on an initial transformation, f : R n ? R r , that maps the reference and distorted images (x and y, respectively) to "perceptual" representations (x and?, respectively). The primary motivation is that perceptual distances are non-uniform in the pixel space <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, and this is the main reason that MSE is inadequate as a perceptual IQA model. The purpose of function f is to transform the pixel representation to a space that is more perceptually uniform. Previous IQA methods have used filter banks to capture the frequency-dependence of error visibility <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Others have used transformations that mimic the early visual system <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. More recently, deep CNNs have shown surprising power in representing perceptual image distortions <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>. In particular, Zhang et al. <ref type="bibr" target="#b6">[7]</ref> have demonstrated that pre-trained deep features from VGG can be used as a substrate for quantifying perceptual quality.</p><p>As such, we also chose to base our model on the VGG16 CNN <ref type="bibr" target="#b13">[14]</ref>, pre-trained for object recognition <ref type="bibr" target="#b39">[40]</ref> on the ImageNet database <ref type="bibr" target="#b40">[41]</ref>. The VGG transformation is constructed by a feedforward cascade of layers, each including spatial convolution, halfwave rectification, and downsampling. All operations are continuous and differentiable, both advantageous for an IQA method that is to be used in optimizing image processing systems. We modified the VGG architecture to achieve two additional desired properties. First, in order to provide a good substrate for the invariances needed for texture resampling, we wanted the initial transformation to be aliasing-free. The "max pooling" operation of the original VGG architecture has been shown to introduce visible aliasing artifacts when used to interpolate between images with geodesic sequences <ref type="bibr" target="#b41">[42]</ref>. To avoid aliasing when subsampling by a factor of two, the Nyquist theorem requires blurring with a filter whose cutoff frequency is below ? 2 radians/sample <ref type="bibr" target="#b42">[43]</ref>. Following this principle, we replaced all max pooling layers in VGG with weighted 2 pooling <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_0">P (x) = g * (x x),<label>(1)</label></formula><p>where denotes pointwise product, and the blurring kernel g(?) was implemented by a Hanning window that approximately enforces the Nyquist criterion with a stride of 2.</p><p>As additional motivation, we note that 2 pooling has been used to describe the behavior of complex cells in primary visual cortex <ref type="bibr" target="#b43">[44]</ref>, and is also closely related to the complex modulus used in the scattering transform <ref type="bibr" target="#b44">[45]</ref>. A second desired property for our transformation is that it should be injective: distinct inputs should map to distinct outputs. This is necessary to ensure that the final quality measure is a proper metric (in the mathematical sense) -if the representation of an image is non-unique, then equality of the output representations will not imply equality of the input images. This property has proven useful in perceptual optimization, although it is not present in many recent methods. For example, the mapping function in GMSD <ref type="bibr" target="#b11">[12]</ref> extracts image gradients, discarding local luminance information that is essential to human perception of image quality. Similarly, GTI-CNN <ref type="bibr" target="#b18">[19]</ref>, makes deliberate use of a surjective transformation, in an attempt to achieve invariance to mild geometric transformations, but throws away a substantial amount of structural information that is perceptually important.</p><p>Considerable effort has been made in developing invertible CNN-based transformations in the context of density modeling <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b48">[49]</ref>. These methods place strict constraints on either network architectures <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref> or network parameters <ref type="bibr" target="#b48">[49]</ref>, which limit the expressiveness in learning qualityrelevant representations. Ma et al. <ref type="bibr" target="#b49">[50]</ref> proved that under Gaussian-distributed random weights and ReLU nonlinearity, a two-layer CNN is injective provided that it is sufficiently expansive (i.e., the output dimension of each layer should increase by at least a logarithmic factor). Although mathematically appealing, this result does not constrain parameter settings of CNNs of more than two layers. In addition, a Gaussian-weighted CNN is less likely to be perceptually relevant <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Like most CNNs, VGG discards information at each stage of transformation. To ensure an injective mapping, we simply included the input image as an additional feature map (the "zeroth" layer of the network). The representation then consists of the input image x, concatenated with the convolution responses of five VGG layers (labelled conv1 2, conv2 2, conv3 3, conv4 3, and conv5 3):</p><formula xml:id="formula_1">f (x) = {x (i) j ; i = 0, . . . , m; j = 1, . . . , n i },<label>(2)</label></formula><p>where m = 5 denotes the number of convolution layers chosen to construct f , n i is the number of feature maps in the i-th convolution layer, andx (0) = x. Similarly, we also computed the representation of the distorted image:</p><formula xml:id="formula_2">f (y) = {? (i) j ; i = 0, . . . , m; j = 1, . . . , n i }.<label>(3)</label></formula><p>We used a na?ve task -reference image recoveryto visually demonstrate the necessity of injective feature transformations. Specifically, given an original image x and an initial image y 0 , we aim to recover x by numerically optimizing y = arg min y D(x, y), where D denotes a full-reference IQA measure with a lower score indicating higher predicted quality, and y is the recovered image. For example, if D is the MSE, the (trivial) analytical solution is y = x, indicating full recoverability. For the majority of existing IQA models, which are continuous and differentiable, solutions must be sought numerically, using gradientbased iterative solvers. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the recovery results of our method from a JPEG-corrupted copy of the original image and a white Gaussian noise image, respectively, in comparison to three state-of-the-art models: GTI-CNN <ref type="bibr" target="#b18">[19]</ref>, GMSD <ref type="bibr" target="#b11">[12]</ref>, and LPIPS <ref type="bibr" target="#b6">[7]</ref>. The first two, which are based on surjective mappings, fail dramatically on this simple task when initialized with purely white Gaussian noise. LPIPS, which is built on VGG but with no enforcement of the injective property, recovers most structures and details, but leaves some visible artifacts in the converged image ( <ref type="figure" target="#fig_1">Fig.  2 (j)</ref>). In contrast, DISTS successfully recovers the reference image from any initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Texture Representation</head><p>The visual appearance of textures is often characterized in terms of sets of local statistics <ref type="bibr" target="#b15">[16]</ref> that are presumably measured by the HVS. Models consisting of various sets of features <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> have been tested using synthesis: one generates an image with statistics that match those of a texture photograph. If the set of statistical measurements is (a) Reference (b) Initial (c) GTI-CNN <ref type="bibr" target="#b18">[19]</ref> (d) GMSD <ref type="bibr" target="#b11">[12]</ref> (e) LPIPS <ref type="bibr" target="#b6">[7]</ref> (f) DISTS (ours) (g) Initial (h) GTI-CNN <ref type="bibr" target="#b18">[19]</ref> (i) GMSD <ref type="bibr" target="#b11">[12]</ref> (j) LPIPS <ref type="bibr" target="#b6">[7]</ref> (k) DISTS (ours) a complete description of the appearance of the texture, then the synthesized image should be perceptually indistinguishable from the original <ref type="bibr" target="#b15">[16]</ref>, at least based on preattentive judgments <ref type="bibr" target="#b52">[53]</ref>. Portilla and Simoncelli <ref type="bibr" target="#b14">[15]</ref> found that the local correlations (and other pairwise statistics) of complex wavelet responses were sufficient to capture the visual appearance of a wide variety of textures, while at the same time being of low enough dimensionality (? 700 dimensions). Gatys et al. <ref type="bibr" target="#b31">[32]</ref> used correlations across channels of many layers in a VGG network, and were able to synthesize consistently better textures, albeit with a much larger set of statistics (? 306K parameters). Since the number of statistics is typically larger than that of pixels in the input image, it is likely that this image was unique in matching these statistics. In this case, diversity in the synthesis results reflects local optima of the optimization procedure, rather than the entropy of the implicitly represented probability distribution. Ustyuzhaninov et al. <ref type="bibr" target="#b53">[54]</ref> provided more direct evidence of this hypothesis: If the number of the statistical measurements is sufficiently large (on the order of millions), a single-layer CNN with random filters can always produce textures that are visually indiscernible to the human eye. Subsequent results suggest that a reduced set of statistics, containing only the mean and variance of CNN channels, is sufficient for texture classification or style transfer <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref>.</p><p>In our experiments, we found that an even more reduced set, containing only the spatial means of the feature maps (a total of 1, 475 statistics), provides an effective parametric model for visual textures. Specifically, we used this model to synthesize textures <ref type="bibr" target="#b14">[15]</ref> by solving y = arg min y D(x, y) = arg min</p><formula xml:id="formula_3">y i,j ? (i) xj ? ? (i) yj 2 ,<label>(4)</label></formula><p>where x is the target texture image, and y is the synthesized texture image, obtained by gradient descent optimization from a random initialization. ? j , respectively. <ref type="figure">Fig. 3</ref> shows the synthesis results of our texture model using statistical constraints from individual and combined convolution layers of the pre-trained VGG. Similar to observations in Gatys et al. <ref type="bibr" target="#b31">[32]</ref>, we found that measurements from early layers appear to capture basic intensity and color information, and those from later layers summarize the shape and structure information. When matching statistics up to layer conv5 3, the synthesized texture appears visually similar to the reference. <ref type="figure">Fig. 4</ref> shows three synthesis results of our 1475parameter texture model in comparison with the 710parameter texture model of Portilla &amp; Simoncelli <ref type="bibr" target="#b14">[15]</ref> and the ? 306k-parameter model of Gatys et al. <ref type="bibr" target="#b31">[32]</ref>. As one might expect, the visual quality of samples synthesized by our model lies between the other two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Perceptual Distance Measure</head><p>Next, we specified quality measurements based on f (x) and f (y). <ref type="figure" target="#fig_5">Fig. 5</ref> visualizes some feature maps of the six stages of the reference image "Buildings". As can been seen, spatial structures are present at all stages, indicating strong statistical dependencies between neighbouring coefficients. Therefore, use of an p -norm, that assumes statistical independence of errors at different locations, is not appropriate. Inspired by the form of SSIM <ref type="bibr" target="#b2">[3]</ref>, we defined separate quality measurements for the texture (using the global means) and the structure (using the global correlations) of each pair of corresponding feature maps: where ?</p><formula xml:id="formula_4">l(x (i) j ,? (i) j ) = 2? (i) xj ? (i) yj + c 1 ? (i) xj 2 + ? (i) yj 2 + c 1 ,<label>(5)</label></formula><formula xml:id="formula_5">s(x (i) j ,? (i) j ) = 2? (i) xj?j + c 2 ? (i) xj 2 + ? (i) yj 2 + c 2 ,<label>(6)</label></formula><formula xml:id="formula_6">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k)</formula><formula xml:id="formula_7">(i) xj , ? (i) yj , (? (i) xj ) 2 , (? (i) yj ) 2 , and ? (i)</formula><p>xj?j represent the global means and variances ofx Finally, the proposed DISTS model combines the quality measurements from different convolution layers using a weighted sum: where {? ij , ? ij } are positive learnable weights, satisfying m i=0 ni j=1 (? ij + ? ij ) = 1. Note that the convolution kernels are fixed throughout the development of the method. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the full computation diagram of our quality assessment system.</p><formula xml:id="formula_8">D(x, y; ?, ?) = 1 ? m i=0 ni j=1 ? ij l(x (i) j ,? (i) j ) + ? ij s(x (i) j ,? (i) j ) ,<label>(7)</label></formula><formula xml:id="formula_9">(a) (b) (c) (d) (e) (f)</formula><formula xml:id="formula_10">Lemma 1. For ?x (i) j ,? (i) j ? R n + (</formula><p>as is the case for responses after ReLU nonlinearity), it can be shown that</p><formula xml:id="formula_11">d(x, y) = D(x, y)<label>(8)</label></formula><p>is a proper metric, satisfying of SSIM-motivated quality measurements. To verify the triangle inequality, we first rewrite d(x, y) as</p><formula xml:id="formula_12">d(x, y) = m i=0 ni j=1 d 2 ij (x, y),<label>(9)</label></formula><p>where</p><formula xml:id="formula_13">d ij (x, y) = ? ij (1 ? l(x (i) j ,? (i) j )) + ? ij (1 ? s(x (i) j ,? (i) j )).<label>(10)</label></formula><p>Brunet et al. <ref type="bibr" target="#b57">[58]</ref> have proved that d ij (x, y) is a metric for ? ij ? 0 and ? ij ? 0. Then,</p><formula xml:id="formula_14">d(x, y) ? i,j (d ij (x, z) + d ij (z, y)) 2 (11) ? i,j d 2 ij (x, z) + i,j d 2 ij (y, z) (12) = d(x, z) + d(z, y),<label>(13)</label></formula><p>where Eq. (12) follows from the Cauchy-Schwarz inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model Training</head><p>The perceptual weights {?, ?} in Eq. (7) were jointly optimized for human perception of image quality and texture invariance. Specifically, for image quality, we minimized the absolute error between model predictions and human ratings:</p><formula xml:id="formula_15">E 1 (x, y; ?, ?) = |D(x, y; ?, ?) ? q(y))|,<label>(14)</label></formula><p>where q(y) denotes the normalized ground-truth quality score of y collected from psychophysical experiments. We chose the large-scale IQA dataset KADID-10k <ref type="bibr" target="#b58">[59]</ref> as the training set, which contains 81 reference images, each of which is distorted by 25 distortion types at 5 distortion levels. In addition, we explicitly enforced the model to be invariant to texture substitution in a data-driven fashion. We minimized the distance (measured by Eq. <ref type="formula" target="#formula_8">(7)</ref>) between two patches (z 1 , z 2 ) sampled from the same texture image z:</p><formula xml:id="formula_16">E 2 (z; ?, ?) = D(z 1 , z 2 ; ?, ?).<label>(15)</label></formula><p>We selected texture images from the describable textures dataset (DTD) <ref type="bibr" target="#b59">[60]</ref>, consisting of 5, 640 images (47 categories and 120 images for each category). In practice, we randomly sampled two minibatches Q and T from KADID-10k and DTD, respectively, and used a variant of stochastic gradient descent to adjust the parameters {?, ?}:</p><formula xml:id="formula_17">E(Q, T ; ?, ?) = 1 |Q| x,y?Q E 1 (x, y; ?, ?) + ? 1 |T | z?T E 2 (z; ?, ?)<label>(16)</label></formula><p>where ? governs the trade-off between the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Connections to Other Full-Reference IQA Methods</head><p>The proposed DISTS model has a close relationship to a number of existing IQA methods.</p><p>? SSIM and its variants <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b63">[63]</ref>: The multi-scale extension of SSIM <ref type="bibr" target="#b63">[63]</ref> incorporates the variations of viewing conditions in IQA, and calibrates the cross-scale parameters via subjective testing on artificially synthesized images. Our model follows a similar approach, building on a multi-scale hierarchical representation and directly calibrating crossscale parameters (i.e., ?, ?) using subject-rated natural images with various distortions. The extension of SSIM into the complex wavelet domain <ref type="bibr" target="#b24">[25]</ref> gains invariance to small geometric transformations by measuring relative phase patterns of the wavelet coefficients. As we show in Section 3.5, by optimizing for texture invariance, DISTS inherits insensitivity to mild geometric transformations. It is worth noting that unlike SSIM and its variants, DISTS is based on global spatial statistics, and thus does not provide a spatial map of quality.</p><p>? The adaptive linear system framework <ref type="bibr" target="#b17">[18]</ref> decomposes the distortion between two images into a linear combination of components that are adapted to local image structures, separating structural and nonstructural distortions. It generalizes many IQA models, including MSE, space/frequency weighting <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b65">[65]</ref>, transform domain masking <ref type="bibr" target="#b21">[22]</ref>, and the tangent distance <ref type="bibr" target="#b66">[66]</ref>. DISTS can be seen as an adaptive nonlinear system, where structure comparison captures structural distortions, and texture comparison measures non-structural distortions, with basis functions adapted to global image content.</p><p>? Style and content separation <ref type="bibr" target="#b54">[55]</ref> based on the pretrained VGG network has reignited the field of style transfer. Specifically, the style loss is built upon the correlations between convolution responses at the same stages (i.e., the Gram matrix) while the content loss is defined by the MSE between the two representations. These two components are redundant, and the combined loss does not have the desired property of unique minima we seek.</p><p>? Image restoration losses <ref type="bibr" target="#b67">[67]</ref> in the era of deep learning are typically defined as a weighted sum of p -norm distances computed on the raw pixels and several stages of VGG feature maps, where the weights are manually tuned for tasks at hand. Later stages of the VGG representation are often preferred so as to incorporate image semantics into low-level vision, encouraging perceptually meaningful details that are not necessarily aligned with the underlying image. This type of loss does not achieve the level of texture invariance we are looking for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we present the implementation details of the proposed DISTS. We then compare our method with a wide range of image similarity models in terms of quality prediction, texture similarity, texture classification/retrieval, and invariance of geometric transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>We fixed the filter kernels of the pre-trained VGG, and learned the perceptual weights {?, ?}. The training was carried out by optimizing the objective function in Eq. <ref type="formula" target="#formula_0">(16)</ref>, assuming a value of ? = 1, using Adam <ref type="bibr" target="#b68">[68]</ref> with a batch size of 32 and an initial learning rate of 1?10 ?4 . After every 1K iterations, we reduced the learning rate by a factor of 2.</p><p>We trained DISTS for 5K iterations, which takes approximately one hour on an NVIDIA GTX 2080 GPU. To ensure a unique minimum of our model, we projected the weights of the zeroth stage onto the interval [0.02, 1] after each gradient step. We chose a 5 ? 5 Hanning window to reduce subsampling-induced aliasing in the VGG representation. Both c 1 in Eq. (5) and c 2 in Eq. (6) were set to 10 ?6 . During training and testing, we followed the suggestions in <ref type="bibr" target="#b2">[3]</ref>, and re-scaled the input images such that the smaller dimension has 256 pixels. The size of texture patches as input to Eq. (15) was 256 ? 256 ? 3, cropped from the same texture images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance on Quality Prediction</head><p>After training on the entire KADID dataset <ref type="bibr" target="#b58">[59]</ref>, DISTS was tested on the other three standard IQA databases LIVE <ref type="bibr" target="#b60">[61]</ref>, CSIQ <ref type="bibr" target="#b4">[5]</ref> and TID2013 <ref type="bibr" target="#b62">[62]</ref>. We used the Pearson linear correlation coefficient (PLCC), the Spearman rank correlation coefficient (SRCC), and the Kendall rank correlation coefficient (KRCC) as evaluation criteria. Before computing PLCC, we fitted a four-parameter function to allow and compensate for a smooth nonlinear relationship:</p><formula xml:id="formula_18">D = (? 1 ? ? 2 ) / (1 + exp (? (D ? ? 3 ) / |? 4 |)) + ? 2 ,<label>(17)</label></formula><p>where {? i } 4 i=1 are parameters. We compared DISTS against a set of full-reference IQA methods, including nine knowledge-driven models and three data-driven CNNbased models. The implementations of all methods were obtained from the respective authors, except for DeepIQA <ref type="bibr" target="#b12">[13]</ref>, which was retrained on KADID for fair comparison. As LPIPS <ref type="bibr" target="#b6">[7]</ref> has different configurations, we chose the default one (known as LPIPS-VGG-lin).</p><p>Results, reported in <ref type="table" target="#tab_1">Table 1</ref>, demonstrate that DISTS performs favorably in comparison to both classic methods (e.g., PSNR and SSIM <ref type="bibr" target="#b2">[3]</ref>) and CNN-based models (e.g., DeepIQA <ref type="bibr" target="#b12">[13]</ref> and LPIPS <ref type="bibr" target="#b6">[7]</ref>). Overall, the best performances across all three databases and all comparison metrics are obtained with MAD <ref type="bibr" target="#b4">[5]</ref>, FSIM c <ref type="bibr" target="#b10">[11]</ref> and GMSD <ref type="bibr" target="#b11">[12]</ref>. It is worth noting that these three databases have been re-used for many years throughout the algorithm design processes, and recent full-reference IQA methods may be unintentionally over-adapting via extensive computational module selection, raising the risk of over-fitting (see <ref type="figure" target="#fig_1">Fig. 2</ref>). <ref type="figure" target="#fig_7">Fig. 7</ref> shows scatter plots of raw model predictions of representative IQA methods versus subjective mean opinion scores (MOSs) on the TID2013 database. From the fitted functions (Eq. (17)), one can observe that DISTS is nearly linear in MOS.</p><p>We also tested DISTS on BAPPS <ref type="bibr" target="#b6">[7]</ref>, a large-scale and highly-varied patch similarity dataset. BAPPS contains traditional synthetic distortions, such as geometric and photometric manipulation, noise contamination, blurring and compression, CNN-based distortions (e.g., from denoising autoencoders and image restoration tasks), and distortions generated by real-world image processing systems. The human judgments are obtained from a two-alternative forced   <ref type="table" target="#tab_3">Table 2</ref>, showing that DISTS (which was not trained on BAPPS, or any similar database) achieves a comparable performance to LPIPS <ref type="bibr" target="#b6">[7]</ref> (which was trained on BAPPS). We conclude that DISTS predicts image quality well, and generalizes well to challenging unseen distortions, such as those caused by real-world algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance on Texture Similarity</head><p>We also tested the performance of DISTS on texture quality assessment. Since most knowledge-driven full-reference IQA models are not good at measuring texture similarity (see <ref type="figure" target="#fig_0">Fig 1)</ref>, we only included a subset for reference. To these we added CW-SSIM <ref type="bibr" target="#b24">[25]</ref> and three computational models specifically designed for texture similarity -STSIM <ref type="bibr" target="#b28">[29]</ref>, NPTSM <ref type="bibr" target="#b69">[69]</ref> and IGSTQA <ref type="bibr" target="#b70">[70]</ref>. STSIM is available in several configurations, and we chose local STSIM-2 that is publicly available 1 . We used a synthesized texture quality assessment database SynTEX <ref type="bibr" target="#b71">[71]</ref>, consisting of 21 reference textures with 105 synthesized versions generated by five texture synthesis algorithms. <ref type="table" target="#tab_4">Table 3</ref> shows the results of correlation coefficients, where we can see that texture similarity models 1. https://github.com/andreydung/Steerable-filter generally perform better than IQA models. Focusing on texture similarity, IGSTQA <ref type="bibr" target="#b70">[70]</ref> achieves a relatively high performance, but is still inferior to DISTS. This indicates that the VGG-based global measurements of DISTS capture the essential features and attributes of visual textures.</p><p>To further test the capabilities of DISTS in quantifying texture distortions, we constructed a texture quality database (TQD), based on 10 texture images selected from Pixabay 2 . Each texture image was corrupted with seven traditional synthetic distortions: additive white Gaussian noise, Gaussian blur, JPEG compression, JPEG2000 compression, pink noise, chromatic aberration, and image color quantization. For each distortion type, we randomly selected one distortion level from a set of three levels, and applied it to each texture image. We then created four copies of each texture using different texture synthesis algorithms, including two classical ones (a parametric model <ref type="bibr" target="#b14">[15]</ref> and a non-parametric model <ref type="bibr" target="#b72">[72]</ref>) and two CNN-based algorithms <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b73">[73]</ref>. Last, to produce "high-quality" images, we randomly cropped four subimages from each of the original textures. In total, TQD has 10 ? 15 images. We gathered human data from 10 subjects, who had general knowledge of image processing but were unaware of the detailed purpose of the study. The viewing distance was fixed to enforce a visual resolution 32 pixels per degree of visual angle. Each subject was shown all ten sets of images, one set at a time, starting with the reference image, and was asked to rank the images according to their perceptual similarity to the reference. Rather than simply averaging 2. https://pixabay.com/images/search/texture  the human opinions, we used reciprocal rank fusion <ref type="bibr" target="#b74">[74]</ref> to obtain the final ranking</p><formula xml:id="formula_19">r(x) = K k=1 1 ? + r k (x) ,<label>(18)</label></formula><p>where r k (x) is the rank of x given by the k-th subject and ? is an additive constant that helps to mitigate the impact of outliers <ref type="bibr" target="#b74">[74]</ref>. <ref type="table" target="#tab_4">Table 3</ref> lists the results, where we computed the correlations within each texture pattern and averaged them across textures. We found that nearly all existing models perform poorly on the new database, including those tailored for texture similarity. In contrast, DISTS significantly outperforms these methods by a large margin. <ref type="figure" target="#fig_8">Fig. 8</ref> shows a set of texture examples, where we noticed that DISTS gives high rankings to resampled images and low rankings to images suffering from visible distortions. This demonstrates that DISTS is in close agreement with human perception of texture quality, and suggests potential uses in other texture analysis problems, such as high-quality texture retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Applications to Texture Classification and Retrieval</head><p>We also applied DISTS to texture classification and retrieval. We used the grayscale and color Brodatz texture databases <ref type="bibr" target="#b75">[75]</ref> (denoted by GBT and CBT, respectively), each of which contains 112 different texture images. We resampled nine non-overlapping 256 ? 256 ? 3 patches from each texture pattern. <ref type="figure" target="#fig_9">Fig. 9</ref> shows a representative texture image from CBT, partitioned into nine patches. The texture classification problem consists of assigning an unknown sample image to one of the known texture classes. For each texture, we randomly chose five patches for training, two for validation, and the remaining two for testing. A simple k-nearest neighbors (k-NN) classification algorithm was implemented, which allowed us to incorporate and compare different similarity models as distance measures. The predicted label of a test image was determined by a majority vote over its k nearest neighbors in the training set, where the value of k was chosen using the validation set. We implemented a baseline model -the bagof-words of SIFT features <ref type="bibr" target="#b76">[76]</ref> with k-NN. The classification accuracy results are listed in <ref type="table" target="#tab_5">Table 4</ref>, where we can see that this baseline model beats most image similarity-based k-NN classifiers, except LPIPS (on CBT) and DISTS. This shows that our model is effective at discriminating and classifying textures that are visually different to the human eye.</p><p>The content-based texture retrieval problem consists of searching for images from a large database that are visually similar. In our experiment, for each texture, we set three patches as the queries, and aimed to retrieve the remaining six patches. Specifically, the distances between each query and the remaining images in the dataset were computed and ranked so as to retrieve the images with minimal distances. To evaluate the retrieval performance, we used mean average precision (mAP), which is defined by</p><formula xml:id="formula_20">mAP = 1 Q Q q=1 1 K K k=1 P (k) ? rel(k) ,<label>(19)</label></formula><p>where Q is the number of queries, K is the number of similar images in the database, P (k) is the precision at cutoff k in the ranked list, and rel(k) is an indicator function equal to one if the item at rank k is a similar image and zero otherwise. As seen in <ref type="table" target="#tab_5">Table 4</ref>, DISTS achieves the best performance on both CBT and GBT datasets. The classification/retrieval errors are primarily due to textures with noticeable inhomogeneities (e.g., middle patch in <ref type="figure" target="#fig_9">Fig. 9</ref>). In addition, the performance on GBT is slightly reduced compared with that on CBT, indicating the importance of color information in these tasks.</p><formula xml:id="formula_21">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o) (p)</formula><p>Classification and retrieval of texture patches resampled from the same images are relatively easy tasks. We also tested DISTS on a more challenging large-scale texture database, the Amsterdam Library of Textures (ALOT) <ref type="bibr" target="#b77">[77]</ref>, containing photographs of 250 textured surfaces, from 100 different viewing angles and illumination conditions. Again, we adopted a na?ve k-NN method (k = 100) using our model as the measure of distance, and tested it on 20% of the samples randomly selected from the database. Without training on ALOT, DISTS achieves a reasonable classification accuracy of 0.926, albeit lower than the value of 0.959 achieved by a knowledge-driven method <ref type="bibr" target="#b78">[78]</ref> with handcrafted features and support vector machines, and the value of 0.993 achieved by a data-driven CNN-based method <ref type="bibr" target="#b79">[79]</ref>. The primary cause of errors when using DISTS in this task is that images from the same textured surface can appear quite different under different lighting or viewpoint conditions, as seen in the example in <ref type="figure" target="#fig_0">Fig. 10</ref>. DISTS, which is designed to capture visual appearance only, could likely be improved for this task by fine-tuning the perceptual weights (along with the VGG network parameters) on a small subset of human-labelled ALOT images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Invariance to Geometric Transformations</head><p>Apart from texture similarity, most full-reference IQA measures fail dramatically when the original and distorted images are misregistered, either globally or locally. The underlying reason is again reliance on the assumption of pixel alignment. Although pre-registration can alleviate this issue, it comes with substantial computational complexity, and does not work well in the presence of severe distortions <ref type="bibr" target="#b18">[19]</ref>. In this subsection, we investigated the degree of  invariance of DISTS to geometric transformations that are imperceptible to the visual system. As there are no subject-rated IQA databases designed for this specific purpose, we augmented the LIVE database <ref type="bibr" target="#b60">[61]</ref> (LIVE Aug) with geometric transformations. In real-world scenarios, an image should first undergo geometric transformations (e.g., camera movement) and then distortions (e.g., JPEG compression). We followed the suggestion in <ref type="bibr" target="#b18">[19]</ref>, and implemented an equivalent but much simpler approachdirectly applying the transformations to the original image. Specifically, we augmented reference images using four geometric transformations: 1) shift by 5% pixels in horizontal direction, 2) clockwise rotation by a degree of 3 ? , 3) dilation by a factor of 1.05, and 4) their combination. This yields a set of (4 + 1) ? 779 reference-distortion pairs in the augmented LIVE database. Since the transformations are modest, the quality scores of distorted images with respect to the modified reference images are assumed to be the same as with respect to the original reference image.</p><p>The SRCC results of the augmented LIVE database are shown in <ref type="table" target="#tab_6">Table 5</ref>. We found that data-driven methods based on CNNs significantly outperform traditional ones. Even so, their performance is often made worse by sensitivity to transformations that arises during downsampling without proper Nyquist band limiting. Trained on augmented data  <ref type="figure" target="#fig_1">. 2</ref>). DISTS is seen to perform extremely well across all distortions and exhibit a high degree of robustness to geometric transformations, which we believe arises from 1) replacing max pooling with 2 pooling, 2) using global quality measurements, and 3) optimizing for invariance to texture resampling (see also <ref type="figure" target="#fig_0">Fig. 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Study</head><p>In this subsection, we conducted ablation experiments to single out the individual contributions of key modifications of DISTS, in comparison to the most closely related alternative -LPIPS. We trained a series of intermediate models between LPIPS and DISTS:</p><p>(a) Original LPIPS; (b) Replace max pooling in LPIPS with 2 pooling; (c) Add the input image on the top of (b); (d) Replace the Euclidean distance in LPIPS with local SSIM measurements (within a sliding window of size 11 ? 11) on top of (c); (e) Replace the Euclidean distance in LPIPS with global SSIM measurements on top of (c); (f) Train (c) by adding the E 2 term in Eq. (15); (g) Train (d) by adding the E 2 term; (h) Train (e) by adding the E 2 term, which is equivalent to DISTS.</p><p>Performance of these models is shown in <ref type="table" target="#tab_7">Table 6</ref>, from which we draw several conclusions. First, 2 pooling is slightly better than max pooling. The main motivation of adopting 2 pooling is to de-alias the intermediate representations, as documented in <ref type="bibr" target="#b41">[42]</ref>. Second, incorporating the input image in the representation has little impact on the performance, but it ensures a unique minimum of DISTS, which is beneficial in perceptual optimization <ref type="bibr" target="#b80">[80]</ref>. Third, the global SSIM-like distance outperforms the Euclidean distance, especially in measuring similarity of visual textures and invariance to geometric transformations. We also tested local SSIM measurements within a sliding window size of 11 ? 11 (d), which gives inferior performance. Last, training with the E 2 term is important for texture-related tasks, improving invariance to geometric transformations, although it slightly hurts the performance on standard IQA databases. We concluded that the improved quality prediction and texture similarity performance of DISTS relative to LPIPS is due to the combination of these key modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS</head><p>We have presented a new full-reference IQA method, DISTS, which is the first of its kind with built-in tolerance to texture resampling. Our model unifies structure and texture similarity, providing good predictions of human quality ratings on both textures and natural photographs, is robust to mild geometric distortions, and performs well in texture classification and retrieval. DISTS is based on the pre-trained VGG network for object recognition. By computing the global means of convolution responses at each stage, we established a universal parametric texture model similar to that of Portilla &amp; Simoncelli <ref type="bibr" target="#b14">[15]</ref>. These statistical measurements provide a rich but relatively low-dimensional characterization of texture appearance, as verified using synthesis <ref type="figure">(Fig. 4</ref>). Despite the empirical success, we believe an important direction for future work is to analyze this "black box" to understand 1) what and how certain texture features and attributes are captured by the pre-trained network, and 2) the importance of cascaded convolution and subsampled pooling in summarizing useful texture information. It is also of interest to extend the current model to measure distortions locally, as is done in SSIM. In this case, the distance measure could be reformulated to adaptively select between structure and texture measures as appropriate, instead of linearly combining them with fixed weights.</p><p>The most direct use of IQA measures is for performance assessment and comparison of image processing systems. But perhaps more importantly, they may be used to optimize image processing methods, so as to improve the visual quality of their results. In this context, most existing IQA measures present major obstacles due to the fact that they lack desired mathematical properties that aid optimization (e.g., injectivity, differentiability and convexity). In many cases, they rely on surjective mappings, and minima are non-unique (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Although DISTS enjoys several advantageous mathematical properties, it is still highly non-convex (with abundant saddle points and plateaus), and recovery from random noise using stochastic gradient descent methods (see <ref type="figure" target="#fig_1">Fig. 2</ref>) requires many more iterations than for SSIM. In practice, the larger the weight of the structure term s at the zeroth stage (? 0j in Eq. (6)), the faster the optimization converges. However, to reach a reasonable level of texture invariance, the learned i,j ? ij should be larger than i,j ? ij , hindering optimization. We are currently analyzing DISTS in the context of perceptual optimization. Our initial results indicate that DISTS-based optimization of image processing applications, including denoising, deblurring, super-resolution, and compression,   <ref type="figure" target="#fig_0">= DISTS 0.954 0.954 0.811 0.855 0.830 0.639 0.901 0.923 0.759 0.903 0.910 0.785 0.931 0.928 0.762</ref> can lead to noticeable improvements in visual quality <ref type="bibr" target="#b80">[80]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:2004.07728v3 [cs.CV] 16 Dec 2020 Existing full-reference IQA models are overly sensitive to point-by-point deviations between images of the same texture. (a) A grass image and (b) the same image, distorted by JPEG compression. (c) Resampling of the same grass as in (a). Popular IQA measures, including PSNR, SSIM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Recovery of a reference image by optimization of IQA measures. Recovery is implemented by solving y = arg min y D(x, y) with gradient descent, where D is an IQA distortion measure and x is a given reference image. (a) Reference image. (b) Corrupted initial image y 0 , obtained by compressing the reference image using JPEG at a low bitrate. (c)-(f) Images recovered from (b) by optimizing different metrics (as indicated). (g)Corrupted initial image, obtained by adding white Gaussian noise. (h)-(k) Images recovered from (g) by optimizing indicated metrics. In all cases, the optimization converges, yielding a distortion score substantially lower than that of the initial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Images synthesized to match the mean values of channels up to a given layer (top) or from individual layers (bottom) of the pre-trained VGG network. (a) Reference texture. (b) Up to conv1 2. (c) Up to conv2 2. (d) Up to conv3 3. (e) Up to conv4 3. (f) Up to conv5 3. (g) Only conv1 2. (h) Only conv2 2. (i) Only conv3 3. (j) Only conv4 3. (k) Only conv5 3. Synthesis results for three example texture photographs. (a) Reference textures. (b) Images synthesized using the method of Portilla &amp; Simoncelli [15]. (c) Images synthesized using Gatys et al. [32]. (d) Images synthesized using our texture model (Eq. (4)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>j , respectively. Two small positive constants, c 1 and c 2 , are included to avoid numerical instability when the denominators are close to zero. The normalization mechanisms in Eq. (5) and Eq. (6) serve to equalize the magnitudes of feature maps at different stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Selected feature maps from the six layers of the VGG decomposition of the "buildings" image. (a) Zeroth stage (original image). (b) First stage. (c) Second stage. (d) Third stage. (e) Fourth stage. (f) Fifth stage. The feature map intensities are re-scaled for better visibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?Fig. 6 .</head><label>6</label><figDesc>non-negativity: d(x, y) ? 0; ? symmetry: d(x, y) = d(y, x); ? triangle inequality: d(x, z) ? d(x, y) + d(y, z); ? identity of indiscernibles (i.e., unique minimum): d(x, y) = 0 ? x = y.Proof. The non-negative and symmetric properties are immediately apparent. The identity of indiscernibles is guaranteed due to the injective mapping function and the use VGG-based perceptual representation for the proposed DISTS model. It contains a total of six stages (including the zeroth stage of raw pixels), and the numbers of feature maps at each stage are 3, 64, 128, 256, 512 and 512, respectively. Global texture and structure similarity measurements are made at each stage, and combined with a weighted summation, giving rise to the final model defined in Eq.<ref type="bibr" target="#b6">(7)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison of human mean opinion scores (MOSs) against SSIM, FSIMc, VSI, and DISTS (ours) on the TID2013 database. choice (2AFC) experiment. The evaluation criterion is the 2AFC score [7], which quantifies the proportion of human agreement with the IQA model, computed as pp+(1?p)(1? p), where p is the percentage of human choices in favor of a given image in each pair, andp ? {0, 1} is the preference of the IQA model. Larger values indicate better agreement between model predictions and human judgments. Results are compiled in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>One set of texture images from TQD, ordered according to their rankings by DISTS. (a) Reference image. (b)-(p) Corrupted images ranked by DISTS from high quality to low quality, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Nine non-overlapping patches sampled from an example texture photograph in the Brodatz color texture dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>D = 0.173 (c) D = 0.255 (d) D = 0.398 (e) D = 0.427 Five images of "soil", photographed under different lighting and viewpoint conditions, from the ALOT dataset. We computed the DISTS score for each of the images (b)-(e) with respect to the reference (a). Consistent with the significantly higher values, (d) and (e) are visually distinct from (a), although all of these images are drawn from the same category.(a) SSIM? / DISTS? (b) 0.486 / 0.057 (c) 0.482 / 0.063 (d) 0.493 / 0.064 (e) 0.630 / 0.069 (f) 0.539 / 0.161 (g) 0.637/0.329 (h) 0.705 / 0.270 (i) 0.730 / 0.284 A visual example to demonstrate robustness of DISTS to geometric transformations. (a) Reference image. (b) Translated rightward by 5% pixels. (c) Dilated by a factor 1.05. (d) Rotated by 3 degrees. (e) Cloud movement. (f) Corrupted with additive Gaussian noise. (g) Gaussian blur. (h) JPEG compression. (i) JPEG2000 compression. Below each image are the values of SSIM and DISTS, respectively. SSIM values are similar or better (larger) for the bottom row, whereas our model reports better (smaller) values for the top row, consistent with human perception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>0.769 0.850 0.824 0.626 0.591 0.589 0.452 0.403 0.401 0.302 0.801 0.793 0.629 (b) a + 2 pooling 0.937 0.938 0.770 0.851 0.824 0.626 0.594 0.592 0.459 0.410 0.406 0.305 0.807 0.802 0.633 (c) b + input image 0.935 0.935 0.768 0.851 0.825 0.627 0.582 0.581 0.449 0.410 0.409 0.303 0.795 0.789 0.625 (d) c + local SSIM 0.950 0.951 0.797 0.853 0.828 0.631 0.738 0.744 0.602 0.664 0.667 0.559 0.798 0.790 0.626 (e) c + global SSIM0.955 0.957 0.816 0.859 0.835 0.641 0.868 0.877 0.739 0.780 0.795 0.698 0.899 0.881 0.724 (f) c + E2 term 0.934 0.935 0.768 0.791 0.776 0.608 0.780 0.782 0.630 0.680 0.685 0.588 0.830 0.823 0.655 (g) d + E2 term 0.929 0.931 0.766 0.801 0.783 0.615 0.774 0.778 0.625 0.672 0.678 0.579 0.820 0.816 0.649 (h) e + E2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Keyan Ding, Kede Ma, and Shiqi Wang are with the Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong (e-mail: keyan.ding@my.cityu.edu.hk, kede.ma@cityu.edu.hk, shiqwang@cityu.edu.hk).</figDesc><table /><note>? Eero P. Simoncelli is with the Flatiron Institute of the Simons Foundation, and the Center for Neural Science and the Courant Institute of Mathemat- ical Sciences, New York University, New York, NY 10003, USA (e-mail: eero.simoncelli@nyu.edu).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Performance comparison on three standard IQA databases. Larger PLCC, SRCC and KRCC values indicate better performance. CNN-based methods are highlighted in italics</figDesc><table><row><cell>Method</cell><cell>PLCC</cell><cell>LIVE [61] SRCC</cell><cell>KRCC</cell><cell>PLCC</cell><cell>CSIQ [5] SRCC</cell><cell>KRCC</cell><cell>PLCC</cell><cell cols="2">TID2013 [62] SRCC KRCC</cell></row><row><cell>PSNR</cell><cell>0.865</cell><cell>0.873</cell><cell>0.680</cell><cell>0.819</cell><cell>0.810</cell><cell>0.601</cell><cell>0.677</cell><cell>0.687</cell><cell>0.496</cell></row><row><cell>SSIM [3]</cell><cell>0.937</cell><cell>0.948</cell><cell>0.796</cell><cell>0.852</cell><cell>0.865</cell><cell>0.680</cell><cell>0.777</cell><cell>0.727</cell><cell>0.545</cell></row><row><cell>MS-SSIM [63]</cell><cell>0.940</cell><cell>0.951</cell><cell>0.805</cell><cell>0.889</cell><cell>0.906</cell><cell>0.730</cell><cell>0.830</cell><cell>0.786</cell><cell>0.605</cell></row><row><cell>VSI [64]</cell><cell>0.948</cell><cell>0.952</cell><cell>0.806</cell><cell>0.928</cell><cell>0.942</cell><cell>0.786</cell><cell>0.900</cell><cell>0.897</cell><cell>0.718</cell></row><row><cell>MAD [5]</cell><cell>0.968</cell><cell>0.967</cell><cell>0.842</cell><cell>0.950</cell><cell>0.947</cell><cell>0.797</cell><cell>0.827</cell><cell>0.781</cell><cell>0.604</cell></row><row><cell>VIF [4]</cell><cell>0.960</cell><cell>0.964</cell><cell>0.828</cell><cell>0.913</cell><cell>0.911</cell><cell>0.743</cell><cell>0.771</cell><cell>0.677</cell><cell>0.518</cell></row><row><cell>FSIMc [11]</cell><cell>0.961</cell><cell>0.965</cell><cell>0.836</cell><cell>0.919</cell><cell>0.931</cell><cell>0.769</cell><cell>0.877</cell><cell>0.851</cell><cell>0.667</cell></row><row><cell>NLPD [39]</cell><cell>0.932</cell><cell>0.937</cell><cell>0.778</cell><cell>0.923</cell><cell>0.932</cell><cell>0.769</cell><cell>0.839</cell><cell>0.800</cell><cell>0.625</cell></row><row><cell>GMSD [12]</cell><cell>0.957</cell><cell>0.960</cell><cell>0.827</cell><cell>0.945</cell><cell>0.950</cell><cell>0.804</cell><cell>0.855</cell><cell>0.804</cell><cell>0.634</cell></row><row><cell>DeepIQA [13]</cell><cell>0.940</cell><cell>0.947</cell><cell>0.791</cell><cell>0.901</cell><cell>0.909</cell><cell>0.732</cell><cell>0.834</cell><cell>0.831</cell><cell>0.631</cell></row><row><cell>PieAPP [8]</cell><cell>0.908</cell><cell>0.919</cell><cell>0.750</cell><cell>0.877</cell><cell>0.892</cell><cell>0.715</cell><cell>0.859</cell><cell>0.876</cell><cell>0.683</cell></row><row><cell>LPIPS [7]</cell><cell>0.934</cell><cell>0.932</cell><cell>0.765</cell><cell>0.896</cell><cell>0.876</cell><cell>0.689</cell><cell>0.749</cell><cell>0.670</cell><cell>0.497</cell></row><row><cell>DISTS (ours)</cell><cell>0.954</cell><cell>0.954</cell><cell>0.811</cell><cell>0.928</cell><cell>0.929</cell><cell>0.767</cell><cell>0.855</cell><cell>0.830</cell><cell>0.639</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Performance comparison of various IQA methods on the BAPPS<ref type="bibr" target="#b6">[7]</ref> dataset using the 2AFC score, which quantifies the agreement with human judgments. Values lie in the range [0, 1], with a higher value indicating better agreement</figDesc><table><row><cell></cell><cell cols="2">Synthetic distortions</cell><cell></cell><cell></cell><cell cols="3">Distortions by real-world algorithms</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Traditional CNN-based</cell><cell>All</cell><cell>Super resolution</cell><cell>Video deblurring</cell><cell>Colorization</cell><cell>Frame interpolation</cell><cell>All</cell><cell>All</cell></row><row><cell>Human</cell><cell>0.808</cell><cell>0.844</cell><cell>0.826</cell><cell>0.734</cell><cell>0.671</cell><cell>0.688</cell><cell>0.686</cell><cell cols="2">0.695 0.739</cell></row><row><cell>PSNR</cell><cell>0.573</cell><cell>0.801</cell><cell>0.687</cell><cell>0.642</cell><cell>0.590</cell><cell>0.624</cell><cell>0.543</cell><cell cols="2">0.614 0.633</cell></row><row><cell>SSIM [3]</cell><cell>0.605</cell><cell>0.806</cell><cell>0.705</cell><cell>0.647</cell><cell>0.589</cell><cell>0.624</cell><cell>0.573</cell><cell cols="2">0.617 0.640</cell></row><row><cell>MS-SSIM [63]</cell><cell>0.585</cell><cell>0.768</cell><cell>0.676</cell><cell>0.638</cell><cell>0.589</cell><cell>0.524</cell><cell>0.572</cell><cell cols="2">0.596 0.617</cell></row><row><cell>VSI [64]</cell><cell>0.630</cell><cell>0.818</cell><cell>0.724</cell><cell>0.668</cell><cell>0.592</cell><cell>0.597</cell><cell>0.568</cell><cell cols="2">0.622 0.648</cell></row><row><cell>MAD [5]</cell><cell>0.598</cell><cell>0.770</cell><cell>0.684</cell><cell>0.655</cell><cell>0.593</cell><cell>0.490</cell><cell>0.581</cell><cell cols="2">0.599 0.621</cell></row><row><cell>VIF [4]</cell><cell>0.556</cell><cell>0.744</cell><cell>0.650</cell><cell>0.651</cell><cell>0.594</cell><cell>0.515</cell><cell>0.597</cell><cell cols="2">0.603 0.615</cell></row><row><cell>FSIMc [11]</cell><cell>0.627</cell><cell>0.794</cell><cell>0.710</cell><cell>0.660</cell><cell>0.590</cell><cell>0.573</cell><cell>0.581</cell><cell cols="2">0.615 0.640</cell></row><row><cell>NLPD [39]</cell><cell>0.550</cell><cell>0.764</cell><cell>0.657</cell><cell>0.655</cell><cell>0.584</cell><cell>0.528</cell><cell>0.552</cell><cell cols="2">0.600 0.615</cell></row><row><cell>GMSD [12]</cell><cell>0.609</cell><cell>0.772</cell><cell>0.690</cell><cell>0.677</cell><cell>0.594</cell><cell>0.517</cell><cell>0.575</cell><cell cols="2">0.613 0.633</cell></row><row><cell>DeepIQA [13]</cell><cell>0.703</cell><cell>0.794</cell><cell>0.748</cell><cell>0.660</cell><cell>0.582</cell><cell>0.585</cell><cell>0.598</cell><cell cols="2">0.615 0.650</cell></row><row><cell>PieAPP [8]</cell><cell>0.727</cell><cell>0.770</cell><cell>0.746</cell><cell>0.684</cell><cell>0.585</cell><cell>0.594</cell><cell>0.598</cell><cell cols="2">0.627 0.659</cell></row><row><cell>LPIPS [7]</cell><cell>0.760</cell><cell>0.828</cell><cell>0.794</cell><cell>0.705</cell><cell>0.605</cell><cell>0.625</cell><cell>0.630</cell><cell cols="2">0.641 0.692</cell></row><row><cell>DISTS (ours)</cell><cell>0.772</cell><cell>0.822</cell><cell>0.797</cell><cell>0.710</cell><cell>0.600</cell><cell>0.627</cell><cell>0.625</cell><cell cols="2">0.651 0.689</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="7">Performance comparison on two texture quality databases. Texture</cell></row><row><cell></cell><cell cols="5">similarity models are highlighted in italics</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">SynTEX [71] PLCC SRCC KRCC PLCC SRCC KRCC TQD (proposed)</cell></row><row><cell>SSIM [3]</cell><cell>0.619</cell><cell>0.620</cell><cell>0.446</cell><cell>0.330</cell><cell>0.307</cell><cell>0.185</cell></row><row><cell cols="2">CW-SSIM [25] 0.532</cell><cell>0.497</cell><cell>0.335</cell><cell>0.344</cell><cell>0.325</cell><cell>0.238</cell></row><row><cell cols="2">DeepIQA [13] 0.550</cell><cell>0.512</cell><cell>0.354</cell><cell>0.458</cell><cell>0.444</cell><cell>0.323</cell></row><row><cell>PieAPP [8]</cell><cell>0.719</cell><cell>0.715</cell><cell>0.532</cell><cell>0.721</cell><cell>0.718</cell><cell>0.556</cell></row><row><cell>LPIPS [7]</cell><cell>0.674</cell><cell>0.663</cell><cell>0.478</cell><cell>0.402</cell><cell>0.392</cell><cell>0.301</cell></row><row><cell>STSIM [29]</cell><cell>0.650</cell><cell>0.643</cell><cell>0.469</cell><cell>0.422</cell><cell>0.408</cell><cell>0.315</cell></row><row><cell>NPTSM [69]</cell><cell>0.505</cell><cell>0.496</cell><cell>0.361</cell><cell>0.678</cell><cell>0.679</cell><cell>0.547</cell></row><row><cell>IGSTQA [70]</cell><cell>0.816</cell><cell>0.820</cell><cell>0.621</cell><cell>0.804</cell><cell>0.802</cell><cell>0.651</cell></row><row><cell>DISTS (ours)</cell><cell>0.901</cell><cell>0.923</cell><cell>0.759</cell><cell>0.903</cell><cell>0.910</cell><cell>0.785</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Classification and retrieval performance comparison on the Brodatz</cell></row><row><cell></cell><cell cols="2">texture dataset [75]</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Classification acc. Retrieval mAP CBT GBT CBT GBT</cell></row><row><cell>SSIM [3]</cell><cell>0.397</cell><cell>0.210</cell><cell>0.371</cell><cell>0.145</cell></row><row><cell>CW-SSIM [25]</cell><cell>-</cell><cell>0.424</cell><cell>-</cell><cell>0.351</cell></row><row><cell>DeepIQA [13]</cell><cell>0.388</cell><cell>0.308</cell><cell>0.389</cell><cell>0.293</cell></row><row><cell>PieAPP [8]</cell><cell>0.173</cell><cell>0.115</cell><cell>0.257</cell><cell>0.153</cell></row><row><cell>LPIPS [7]</cell><cell>0.960</cell><cell>0.861</cell><cell>0.951</cell><cell>0.839</cell></row><row><cell>STSIM [29]</cell><cell>-</cell><cell>0.708</cell><cell>-</cell><cell>0.632</cell></row><row><cell>NPTSM [69]</cell><cell>-</cell><cell>0.895</cell><cell>-</cell><cell>0.837</cell></row><row><cell>IGSTQA [70]</cell><cell>-</cell><cell>0.862</cell><cell>-</cell><cell>0.798</cell></row><row><cell>SIFT [76]</cell><cell>0.924</cell><cell>0.928</cell><cell>0.859</cell><cell>0.865</cell></row><row><cell>DISTS (ours)</cell><cell>0.995</cell><cell>0.968</cell><cell>0.988</cell><cell>0.951</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>SRCC comparison of IQA models to human perception using the LIVE database augmented with geometric transformations</figDesc><table><row><cell>Method</cell><cell cols="4">Translation Rotation Dilation Mixed Total</cell></row><row><cell>PSNR</cell><cell>0.159</cell><cell>0.153</cell><cell>0.152</cell><cell>0.146 0.195</cell></row><row><cell>SSIM [3]</cell><cell>0.171</cell><cell>0.168</cell><cell>0.177</cell><cell>0.166 0.190</cell></row><row><cell>MS-SSIM [63]</cell><cell>0.165</cell><cell>0.174</cell><cell>0.198</cell><cell>0.174 0.177</cell></row><row><cell>CW-SSIM [25]</cell><cell>0.207</cell><cell>0.312</cell><cell>0.364</cell><cell>0.219 0.194</cell></row><row><cell>VSI [64]</cell><cell>0.282</cell><cell>0.360</cell><cell>0.372</cell><cell>0.297 0.309</cell></row><row><cell>MAD [5]</cell><cell>0.354</cell><cell>0.630</cell><cell>0.587</cell><cell>0.453 0.327</cell></row><row><cell>VIF [4]</cell><cell>0.296</cell><cell>0.433</cell><cell>0.522</cell><cell>0.387 0.294</cell></row><row><cell>FSIMc [11]</cell><cell>0.380</cell><cell>0.396</cell><cell>0.408</cell><cell>0.365 0.339</cell></row><row><cell>NLPD [39]</cell><cell>0.062</cell><cell>0.074</cell><cell>0.083</cell><cell>0.066 0.112</cell></row><row><cell>GMSD [12]</cell><cell>0.252</cell><cell>0.299</cell><cell>0.303</cell><cell>0.247 0.288</cell></row><row><cell>DeepIQA [13]</cell><cell>0.822</cell><cell>0.919</cell><cell>0.918</cell><cell>0.881 0.859</cell></row><row><cell>PieAPP [8]</cell><cell>0.850</cell><cell>0.903</cell><cell>0.902</cell><cell>0.879 0.874</cell></row><row><cell>LPIPS [7]</cell><cell>0.811</cell><cell>0.908</cell><cell>0.893</cell><cell>0.861 0.779</cell></row><row><cell>GTI-CNN [19]</cell><cell>0.864</cell><cell>0.906</cell><cell>0.904</cell><cell>0.890 0.875</cell></row><row><cell>DISTS (ours)</cell><cell>0.948</cell><cell>0.939</cell><cell>0.946</cell><cell>0.937 0.928</cell></row><row><cell cols="5">by geometric transformations, GTI-CNN [19] achieves desir-</cell></row><row><cell cols="5">able invariance at the cost of discarding perceptually impor-</cell></row><row><cell cols="2">tant features (see Fig</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Ablation experiments: proposed DISTS model (last line) compared to LPIPS (first line), and intermediate variations. All models trained on KADID PLCC SRCC KRCC PLCC SRCC KRCC PLCC SRCC KRCC PLCC SRCC KRCC PLCC SRCC KRCC (a) LPIPS 0.934 0.936</figDesc><table><row><cell></cell><cell cols="2">Quality prediction</cell><cell cols="2">Texture similarity</cell><cell>Geometric invariance</cell></row><row><cell>Model</cell><cell>LIVE [61]</cell><cell>TID2013 [62]</cell><cell>SynTEX [71]</cell><cell>TQD (proposed)</cell><cell>LIVE Aug</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? A new look at signal fidelity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What&apos;s wrong with mean-squared error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Images and Human Vision</title>
		<editor>A. B. Watson</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Most apparent distortion: Full-reference image quality assessment and the role of strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceptually optimized image rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1511" to="1525" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PieAPP: Perceptual image-error assessment through pairwise preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Prashnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mostofi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cluster-based probability model and its application to image and texture processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="284" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Models for static and dynamic texture synthesis in image and video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stojanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1353" to="1365" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural networks for no-reference and full-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="70" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual pattern discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An adaptive linear system framework for image distortion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1160" to="1163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometric transformation invariant image quality assessment using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6732" to="6736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The effects of a visual fidelity criterion of the encoding of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mannos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visible differences predictor: An algorithm for the assessment of image fidelity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Daly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Vision, Visual Processing, and Digital Display III</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="2" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual image distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Vision, Visual Processing, and Digital Display V</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">2179</biblScope>
			<biblScope unit="page" from="127" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Foundations of Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Wandell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Sinauer Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<title level="m">Modern Image Quality Assessment. Morgan &amp; Claypool Publishers</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Translation insensitive image similarity in complex wavelet domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="573" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information content weighting for perceptual image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1185" to="1198" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Group maximum differentiation competition: Model comparison with few samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Perceptual similarity: A texture challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Halley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chantler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structural texture similarity metrics for image analysis and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zujovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2545" to="2558" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Texture features for browsing and retrieval of image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiresolution grayscale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>M?enp??</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep TEN: Texture encoding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="708" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A perception-inspired deep learning framework for predicting perceptual texture similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Maximum differentiation (MAD) competition: A methodology for comparing computational models of perceptual quantities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Eigendistortions of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3530" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nonlinear image representation for efficient perceptual coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Epifanio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="80" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Divisive normalization image quality metric revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu?oz-Mar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="852" to="864" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Perceptual image quality assessment using a normalized Laplacian pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Geodesics of learned representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Signals and Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A convolutional subunit model for neuronal responses in macaque v1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vintch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Movshon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="14" to="829" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end optimized image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1 ? 1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="215" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Invertibility of convolutional generative networks from partial measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ayaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9628" to="9637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Annual Conference on Computer Graphics and Interactive Techniques</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Preattentive texture discrimination with early vision mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="923" to="932" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">What does it take to generate natural textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2230" to="2236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the mathematical properties of the structural similarity index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Vrscay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1488" to="1499" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">KADID-10k: A large-scale artificially distorted IQA database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Image and video quality assessment research at LIVE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<ptr target="http://live.ece.utexas.edu/research/quality/" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Image database TID2013: Peculiarities, results and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Image Communication</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="57" to="77" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Asilomar Conference on Signals, System and Computers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">VSI: A visual saliency-induced index for perceptual image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4270" to="4281" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visibility of wavelet quantization noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villasenor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1164" to="1175" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognition-tangent distance and tangent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="239" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Perceptual losses for realtime style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Content-adaptive nonparametric texture similarity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alfarraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alaudah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Synthesized texture quality assessment via multi-scale spatial and statistical texture attributes of image and gradient magnitude coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="738" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The effect of texture granularity on texture synthesis quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Subedar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Digital Image Processing XXXVIII</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9599</biblScope>
			<biblScope unit="page" from="356" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">High-resolution multi-scale neural texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Snelgrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Asia Technical Briefs</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Special Interest Group on Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="758" to="759" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">New brodatz-based image databases for grayscale color and multiband texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdelmounaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong-Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISRN Machine Vision</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Material-specific adaptation of color invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="306" to="313" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fast features invariant to rotation and scale of texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition, description, and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Comparison of image quality models for optimization of image processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.01338" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1338" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
