<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NITS-VC System for VATEX Video Captioning Challenge 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Thoudam</roleName><forename type="first">Alok</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">National Institute of Technology Silchar Assam</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doren</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">National Institute of Technology Silchar Assam</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">National Institute of Technology Silchar Assam</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NITS-VC System for VATEX Video Captioning Challenge 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C3D</term>
					<term>Encoder-decoder</term>
					<term>LSTM</term>
					<term>VATEX</term>
					<term>Video captioning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video captioning is process of summarising the content, event and action of the video into a short textual form which can be helpful in many research areas such as video guided machine translation, video sentiment analysis and providing aid to needy individual. In this paper, a system description of the framework used for VATEX-2020 video captioning challenge is presented. We employ an encoder-decoder based approach in which the visual features of the video are encoded using 3D convolutional neural network (C3D) and in the decoding phase two Long Short Term Memory (LSTM) recurrent networks are used in which visual features and input captions are fused separately and final output is generated by performing element-wise product between the output of both LSTMs. Our model is able to achieve BLEU scores of 0.20 and 0.22 on public and private test data sets respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, the amount of multimedia data (especially video) over the Internet is increasing day by day. This leads to a problem of automatic classification, indexing and retrieval of the video <ref type="bibr" target="#b13">[14]</ref>. Video captioning is a task of automatic captioning a video by understanding the action and event in the video which can help in the retrieval of the video efficiently through text. On addressing the task of video captioning effectively, the gap between computer vision and natural language can also be minimized. Based on the approaches proposed for video captioning till now, they can be classified into two categories namely, templatebased language model <ref type="bibr" target="#b1">[2]</ref> and sequence learning model <ref type="bibr" target="#b11">[12]</ref>. The template based approaches use predefined templates for generating the captions by fitting the attributes identified in the video. These kinds of approaches need the proper alignment between the words generated for the video and the predefined templates. In contrast to template based approach, the sequence learning based approach learn the sequence of word conditioned on previously generated word and visual feature vector of the video. This approach is commonly used in Machine Translation (MT) where the target language (T) is conditioned on the source language (S).</p><p>Video is a rich source of information which consists of large number of continuous frames, sound and motion. The presence of large number of similar frames, complex actions and events of video makes the task of video captioning challenging. Till now a number of data sets have been introduced by covering variety of domains such as cooking <ref type="bibr" target="#b4">[5]</ref>, social media <ref type="bibr" target="#b7">[8]</ref> and human action <ref type="bibr" target="#b3">[4]</ref>. Based on the data sets available for video captioning, the approaches proposed for video captioning can be categorized into open domain video captioning <ref type="bibr" target="#b18">[19]</ref> and domain specific video captioning <ref type="bibr" target="#b4">[5]</ref>. The process of generating caption for an open domain video is challenging due to the presence of various intra-related action, event and scenes as compared to domain specific video captioning. In this paper, an encoderdecoder based approach is used for addressing the problem of English video captioning in a dataset provided by the VATEX community for multilingual video captioning challenge 2020 <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background Knowledge</head><p>The background of video captioning approaches can be divided into three phases <ref type="bibr" target="#b0">[1]</ref>. The classical video captioning approach phase involves the detection of entities of the video (such as object, actions and scenes) and then map them to a predefined templates. The statistical methods phase, in which the video captioning problem is addressed by employing statistical methods. The last one is deep learning phase. In this phase, many state-of-the-art video captioning framework have been proposed and it is believed that this phase has a capability of solving the problem of automatic open domain video captioning.</p><p>Some classical video captioning approaches are pro-posed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> where motion of vehicles in a video and series of actions in a movie are characterized using natural language respectively. The successful implementation of Deep Learning (DL) techniques in the field of computer vision and natural language processing attract researchers for incorporating DL based techniques for text generation. Most of these DL based approaches for video captioning are inspired by the deep image captioning approaches <ref type="bibr" target="#b19">[20]</ref>. These DL based approaches for video captioning mainly consist of two stages namely, visual feature encoding stage and sequence decoding stage. <ref type="bibr" target="#b16">[17]</ref> proposed a model based on stacked LSTM in which first LSTM layer takes a sequence of frames as an input and second LSTM layer generates the description using first LSTM hidden state representation. The shortcoming of the approach proposed in <ref type="bibr" target="#b16">[17]</ref> is that all frames need to be processed for generating description and since the video consist of many similar frames, there is a chance that final representation of the visual features consist of less relevant information for video captioning <ref type="bibr" target="#b19">[20]</ref>. Some other DL based technique are successfully implemented in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. In this paper, the proposed architecture and performance of the system is presented which is evaluated on VATEX video captioning challenge dataset 2020 1 . Furthermore, in Section 3, description of system used for challenge is given, followed by Section 4 and Section 5 on discussion of performance of the system and conclusion respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VATEX Task</head><p>In this section, detail of visual feature extraction of the video and the implementation of the system are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">System Description</head><p>Visual Feature Encoding: For this task, a traditional encoder-decoder based approach is used. For encoding, the visual feature vector of the input video, firstly, the video is evenly segmented into n segments in the interval of 16 then, using a pre-trained 3D Convolutional Neural Network (C3D), a visual feature vector f = {s 1 , s 2 . . . s n } for video is extracted. The dimension of feature vector for each video is f ? R n?mx where m x is dimension feature vector for each segment. Since, the high dimensional feature vector is prone to carry redundant information and affect the quality of features <ref type="bibr" target="#b19">[20]</ref>, for reducing the dimension of features an average pooling is performed with filter size 5. All the averaged pooled features(f a ) of each segment are concatenated in a sequence to preserve the temporal relation between them and passed to a decoder for caption generation.</p><p>Caption generator: For the decoding, two separate Long Short Term Memory (LSTM) recurrent networks are 1 https://competitions.codalab.org/competitions/ 24360 used as a decoder. In this stage, firstly, all the input captions are passed to a embedding layer to get a dense representation for each word in the input caption. The embedding representation is then passed to both LSTM separately. The first LSTM takes the encoded visual feature vector as an initial stage and for the second LSTM, the visual feature vector concatenated with the output of embedding layer and finally the element wise product is preformed between the output from both LSTM. The unrolling procedure of system is given below:? = W e X + b e (1)</p><formula xml:id="formula_0">z 1 = LST M 1 (?, h i )<label>(2)</label></formula><formula xml:id="formula_1">z 2 = LST M 2 ([?; f a ]) (3) y t = sof tmax(z 1 z 2 )<label>(4)</label></formula><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results and Discussion</head><p>In this section, a brief discussion on dataset used, experimental setup and the output generated by the system are carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>For the evaluation of the system performance, VATEX video captioning dataset is used. This dataset is proposed for motivating multilingual video captioning, in which each video is associated with corresponding 10 captions in both English and Chinese. This dataset consist of two test sets namely, public test set for which reference captions are publicly available and private test set for which reference captions are not publicly available which are on hold for the evaluation in VATEX video captioning challenge 2020. The detailed statistics of the dataset is given in <ref type="table">Table 1</ref>. The system employed in the paper is implemented using QuADro P2000 GPU and tested on both the test sets. <ref type="table">Table 2</ref>. Sample output caption generated by system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VideoId-2-40BG6NPaY</head><p>VideoId-fjErVZXd9e0 VideoId-IwjWR7VJiYY Generated Caption Generated Caption Generated Caption A group of people are skiing down a hill and one of them falls down A man is lifting a heavy weight over his head and then drops it A woman is demonstrating how to apply mascara to her eyelashes (a) (b) (c)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>For the evaluation of generated captions, various evaluation metrics have been proposed. In this paper, for the quantitative evaluation of the generated output, the following evaluation metrics are used: BLEU[13] 2 , METEOR <ref type="bibr" target="#b5">[6]</ref>, CIDEr <ref type="bibr" target="#b15">[16]</ref> and ROUGE-L. BLEU evaluates the part of n-grams (up to four) that are similar in both reference (or a set of references) and hypothesis. CIDEr also measure the n-grams that are common in both hypothesis and the references, but in CIDEr, term frequency-inverse document is used for weighting the n-grams. In METEOR, once the common unigrams are found, then it calculates a score for this matching using a unigram-precision, unigram-recall and measure of fragmentation which is used for evaluating how well-ordered the matched word in generated caption against the reference caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Setup</head><p>Following the system architecture discussed in Section 3.1, after extracting the spatio-temporal features using C3D model pre-trained on Sports-1M dataset <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>, they are passed to caption generator module.</p><p>In the training process, each caption is concatenated by two special marker &lt; BOS &gt; and &lt; EOS &gt; for informing the model about the beginning and ending of caption generation process. We restricted the maximum number of words in a caption to 30, and if the length of the caption is less than desired length the caption is padded with 0. The captions are tokenized using Stanford tokenizer <ref type="bibr" target="#b10">[11]</ref> and only to 15K words with most occurrence are retained. For out-of-vocabulary words, a special tag U KN is used. In the testing phase, the generation of caption starts after watching start marker and the input visual feature vector, in each iteration, the most probable word is sampled out and passed to next iteration with previous generated words and visual feature vector until a special end marker is generated. For loss minimization, cross-entropy loss function is used with ADAM optimizer and learning rate is set to 2 ? 10 ?4 . For reducing the overfitting situation, a dropout of 0.5 is used and the hidden units of both LSTMs are set to 512. The system is trained with different batch size 64 and 256 for 50 epochs each. On analysing the scores of evaluation metrics and the quality of generated captions in terms of coherence and relevancy, smaller batch size performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>The performance of the system on private and public test set is given in <ref type="table" target="#tab_1">Table 3</ref>. In the <ref type="table">Table 2</ref>, sample output captions generated by the system is given along with videoId.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a description of the system which is used for VATEX2020 video captioning challenge is presented. We have used encode-decoder based video captioning framework for the generation of English captions. Our system scored 0.20 and 0.22 BLEU-4 score on public and private video captioning test set respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Performance of the system on public dataset</figDesc><table><row><cell>Evaluation</cell><cell>Proposed System</cell><cell>Proposed System</cell></row><row><cell>Metrics</cell><cell>on public test set</cell><cell>on private test set</cell></row><row><cell>CIDEr</cell><cell>0.24</cell><cell>0.27</cell></row><row><cell>BLEU-1</cell><cell>0.63</cell><cell>0.65</cell></row><row><cell>BLEU-2</cell><cell>0.43</cell><cell>0.45</cell></row><row><cell>BLEU-3</cell><cell>0.30</cell><cell>0.32</cell></row><row><cell>BLEU-4</cell><cell>0.20</cell><cell>0.22</cell></row><row><cell>METEOR</cell><cell>0.18</cell><cell>0.18</cell></row><row><cell>ROUGE-L</cell><cell>0.42</cell><cell>0.43</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">[ ; ] represent the concatenation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tylin/coco-caption</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video description: A survey of methods, datasets, and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayyer</forename><surname>Aafaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Syed Zulqarnain Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Siddharth Narayanaswamy, Dhaval Salvi, et al. Video in sentences out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Burchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Coroian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mussman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1204.2742</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The&quot; inverse hollywood problem&quot;: From video to scripts and storyboards via causal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="132" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradipto</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">F</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dataset for telling the stories of social media videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="968" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithmic characterization of vehicle trajectories from image sequences by motion verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Hellmut</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A novel automatic shot boundary detection algorithm: robust to illumination and motion effect. Signal, Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalton</forename><forename type="middle">Meitei</forename><surname>Thounaojam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saptarshi</forename><surname>Chakraborty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, highquality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic-filtered soft-split-aware video captioning with audio-augmented feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuecong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="35" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
