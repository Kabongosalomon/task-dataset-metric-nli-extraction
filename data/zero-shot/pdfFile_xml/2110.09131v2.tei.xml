<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ENSEMBLING GRAPH PREDICTIONS FOR AMR PARSING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><forename type="middle">Thanh</forename><surname>Lam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Picco</surname></persName>
							<email>gabriele.picco@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufang</forename><surname>Hou</surname></persName>
							<email>yhou@ie.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
							<email>lamnguyen.mltd@ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzung</forename><forename type="middle">T</forename><surname>Phan</surname></persName>
							<email>phandu@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>L?pez</surname></persName>
							<email>vanlopez@ie.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fernandez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Astudillo</forename></persName>
							<email>ramon.astudillo@ibm.com</email>
						</author>
						<title level="a" type="main">ENSEMBLING GRAPH PREDICTIONS FOR AMR PARSING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many machine learning tasks, models are trained to predict structure data such as graphs. For example, in natural language processing, it is very common to parse texts into dependency trees or abstract meaning representation (AMR) graphs. On the other hand, ensemble methods combine predictions from multiple models to create a new one that is more robust and accurate than individual predictions. In the literature, there are many ensembling techniques proposed for classification or regression problems, however, ensemble graph prediction has not been studied thoroughly. In this work, we formalize this problem as mining the largest graph that is the most supported by a collection of graph predictions. As the problem is NP-Hard, we propose an efficient heuristic algorithm to approximate the optimal solution. To validate our approach, we carried out experiments in AMR parsing problems. The experimental results demonstrate that the proposed approach can combine the strength of state-of-the-art AMR parsers to create new predictions that are more accurate than any individual models in five standard benchmark datasets 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ensemble learning is a popular machine learning practice, in which predictions from multiple models are blended to create a new one that is usually more robust and accurate. Indeed, ensemble methods like XGBOOST are the winning solution in many machine learning and data science competitions <ref type="bibr" target="#b10">[Chen and Guestrin, 2016]</ref>. A key reason behind the successes of the ensemble methods is that they can combine the strength of different models to reduce the variance and bias in the final prediction <ref type="bibr">[Domingos, 2000, Valentini and</ref><ref type="bibr" target="#b22">Dietterich, 2004]</ref>. Research in ensemble methods mostly focuses on regression or classification problems <ref type="bibr" target="#b13">[Dong et al., 2020]</ref>. Recently, in many machine learning tasks prediction outputs are provided in a form of graphs. For instance, in Abstract Meaning Representation (AMR) parsing <ref type="bibr" target="#b3">[Banarescu et al., 2013]</ref>, the input is a fragment of text and the output is a rooted, labeled, directed, acyclic graph (DAG). It abstracts away from syntactic representations, in the sense that sentences with similar meaning should have the same AMR. <ref type="figure">Figure 1</ref> shows an AMR graph for the sentence You told me to wash the dog where nodes are concepts and edges are relations.</p><p>AMR parsing is an important problem in natural language processing (NLP) research and it has a broad application in downstream tasks such as question answering <ref type="bibr" target="#b14">[Kapanipathi et al., 2020]</ref> and common sense reasoning <ref type="bibr" target="#b19">[Lim et al., 2020]</ref>. Recent approaches for AMR parsing leverage the advances from pretrained language models <ref type="bibr" target="#b5">[Bevilacqua et al., 2021]</ref> and numerous deep neural network architectures <ref type="bibr">Lam, 2020a, Zhou et al., 2021]</ref>.</p><p>Unlike methods for ensembling numerical or categorical values for regression or classification problems where the mean value or majority votes are used respectively, the problem of graph ensemble is more complicated. For instance, <ref type="figure">Figure  2</ref> show three graphs g 1 , g 2 , g 3 with different structures, having varied number of edges and vertices with different labels.</p><p>In this work, we formulate the ensemble graph prediction as a graph mining problem where we look for the largest common structure among the graph predictions. In general, finding the largest common subgraph is a well-known computationally intractable problem in graph theory. However, for AMR parsing problems where the AMR graphs have labels and a simple tree-alike structure, we propose an efficient heuristic algorithm (Graphene) to approximate the solution of the given problem well. <ref type="figure">Figure 1</ref>: An example AMR graph for the sentence You told me to wash the dog.</p><p>To validate our approach, we collect the predictions from four state-of-the-art AMR parsers and create new predictions using the proposed graph ensemble algorithm. The chosen AMR parsers are the recent state-of-the-art AMR parsers including a seq2seq-based method using BART <ref type="bibr" target="#b5">[Bevilacqua et al., 2021]</ref>, a transition-based approach proposed in <ref type="bibr" target="#b25">[Zhou et al., 2021]</ref> and a graph-based approach proposed in <ref type="bibr" target="#b7">[Cai and Lam, 2020a]</ref>. In addition to those models, we also trained a new seq2seq model based on T5 <ref type="bibr" target="#b20">[Raffel et al., 2020]</ref> to leverage the strength of this pretrained language model.</p><p>The experimental results show that in five standard benchmark datasets, our proposed ensemble approach outperforms the previous state-of-the-art models and achieves new state-of-the-art results in all datasets. For example, our approach achieves new state-of-the-art results with 1.7, 1.5, and 1.3 points better than prior arts in the BIO (under out-ofdistribution evaluation), AMR 2.0, and AMR 3.0 datasets respectively. This result demonstrates the strength of our ensemble method in leveraging the model diversity to achieve better performance. An interesting property of our solution is that it is model-agnostic, therefore it can be used to make an ensemble of existing model predictions without the requirement to have an access to model training. Source code is open-sourced 2 .</p><p>Our paper is organized as follows: Section 2 discusses a formal problem definition and a study on the computational intractability of the formulated problem. The graph ensemble algorithm is described in Section 3. Experimental results are reported in Section 4 while Section 5 discusses related works. The conclusion and future work are discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem formulation</head><p>Denote g = (E, V ) as a graph with the set of edges E and the set of vertices V . Each vertex v ? V and edge e ? E is associated with a label denoted as l(v) and l(e) respectively, where l(.) is a labelling function. Given two graphs g 1 = (E 1 , V 1 ) and g 2 = (E 2 , V 2 ), a vertex matching ? is a bijective function that maps a vertex v ? V 1 to a vertex ?(v) ? V 2 . Example 1. In <ref type="figure">Figure 2</ref>, between g 1 and g 2 there are many possible vertex matches, where ?(g 1 , g 2 ) = [1 ? 3, 2 ? 2, 3 ? 1] is one of them (which can be read as the first vertex of g 1 is mapped to the third vertex of g 2 and so forth). Notice that not all vertices v ? V 1 has a match in V 2 and vice versa. Indeed, in this example, the fourth vertex in g 2 does not have a matched vertex in g 1 .</p><p>Given two graphs g 1 , g 2 and a vertex match ?(g 1 , g 2 ), support of a vertex v with respect to the matching ?, denoted as s ? (v), is equal to 1 if l(v) = l(?(v)) and 0 otherwise. Given an edge e = (v 1 , v 2 ) the support of e with respect to the vertex match ?, denoted as s ? (e), is equal to 1 if l(e) = l((?(v 1 ), ?(v 2 ))) and 0 otherwise. Example 2. In <ref type="figure">Figure 2</ref>, for the vertex match ?(g 1 , g 2 ) = [1 ? 3, 2 ? 2, 3 ? 1], the first vertex in g 1 and the third vertex in g 2 shares the same label A, therefore the support of the given vertex is equal to 1. On the other hand, the third vertex in g 1 and the first vertex in g 2 does not have the same label so their support is equal to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>A graph ensemble example. Each node and edge of g occurs in at least two out of three graphs g 1 , g 2 , g 3 . Therefore, g is ?-supported where ? = 2 by the given set of graphs. Graph g is also the graph with the largest sum of supports among all ?-supported graphs. The tables show the node and edge support (votes) are updated in each step of the Graphene algorithm when g 1 is a pivot graph.</p><p>Between two graphs, there are many possible vertex matches, the best vertex match is defined as the one that has the maximal total vertex support and edge support. In our discussion, when we mention a vertex match we always refer to the best vertex match.</p><p>Denote G = {g 1 = (E 1 , V 1 ), g 2 = (E 2 , V 2 ), ? ? ? , g m = (E m , V m )} as a set of m graphs. Given any graph g = (E, V ), for every g i denote ? i (g, g i ) as the best vertex match between g and g i . The total support of a vertex v ? V or an edge e ? E is defined as follows:</p><formula xml:id="formula_0">? support(e) = m i=1 s ?i (e) ? support(v) = m i=1 s ?i (v)</formula><p>Given a support threshold ?, a graph g is called ?-supported by G if for any node v ? V or any edge e ? E, support(v) ? ? and support(e) ? ?. Example 3. In <ref type="figure">Figure 2</ref>, graph g is ?-supported by G = {g 1 , g 2 , g 3 } where ? = 2.</p><p>Intuitively, an ensemble graph g should have as many common edges and vertices with all the graph predictions as possible. Therefore, we define the graph ensemble problem as follows: Problem 1 (Graph ensemble). Given a support threshold ? and a collection of graphs G, find the graph g that is ?-supported by G and has the largest sum of vertex and edge supports. Theorem 1. Finding the optimal ?-supported graph with the largest total of support is NP-Hard.</p><p>Proof. We prove the NP-Hardness by reduction to the Maximum Common Edge Subgraph (MCES) problem, which is known to be an NP-Complete problem <ref type="bibr" target="#b2">[Bahiense et al., 2012]</ref>. Given two graphs g 1 and g 2 , the MCES problem finds a graph g that is a common subgraph of g 1 and g 2 and the number of edges in g is the largest. Consider the following instance of the Graph Ensemble problem with ? = 2, and G = {g 1 , g 2 } created from the graphs in the MCES problem. Assume that all vertices and all edges of g 1 and g 2 have the same label A.</p><p>Since ? = 2, a ?-supported graph is also a common subgraph between g 1 and g 2 and vice versa. Denote g s and g e as the common subgraph between g 1 and g 2 with the largest support and the largest common edge, respectively. We can show that g s has as many edges as g e . In fact, since g s is the largest supported common subgraph there is no vertex v ? g e such that v ? g s because otherwise we can add v to g s to create a larger supported graph. For any edge e = (v 1 , v 2 ) ? g e , since both vertices v 1 and v 2 also appear in g s , the edge e = (v 1 , v 2 ) must also be part of g s otherwise we can add this edge to g s to create a subgraph with a larger support. Therefore, g s has as many edges as g e , which is also a solution to the MCES problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph ensemble algorithm</head><p>In this section, we discuss a heuristic algorithm based on the strategy "Please correct me if I am wrong!" to solve Problem 1. The main idea is to improve a pivot graph based on other graphs. Specifically, starting with a pivot graph g i (i = 1, 2, ? ? ? , m), we collect the votes from the other graphs for every existing vertex and existing/non-existing edges to correct g i . We call the proposed algorithm Graphene which stands for Graph Ensemble algorithm. The key steps of the algorithm are provided in the pseudo-code in Algorithm 1.</p><p>Algorithm 1: Graph ensemble with the Graphene algorithm. Input: a set of graphs G = {g 1 , g 2 , ? ? ? , g m } and the support threshold ? Output: an ensemble graph g e Algorithm: Graphene(G, ?)</p><formula xml:id="formula_1">for i ? 1 to m do g pivot ? g i V ? Initialise(g pivot ) for j ? 1 to m do if j = i then V ? V ? getVote(?(g pivot , g j )) end g e i ? F ilter(V, ?) end g e ? the</formula><p>graph with the largest support among g e 1 , ? ? ? , g e m Return g e For example, in <ref type="figure">Figure 2</ref>, the algorithm starts with the first graph g 1 and considers it as a pivot graph g pivot . In the first step, it creates a table to keep voting statistics V initialized with the vote counts for every existing vertex and edge in g pivot . To draw additional votes from the other graphs, it performs the following subsequent steps:</p><p>? Call the function ?(g 1 , g i ) (i = 2, 3, ? ? ? , m) to get the best bijective mapping ? between the vertices of two graphs g 1 and g i (with a little bit abuse of notation we drop the index i from ? i when g i and g pivot are given in the context). For instance, the best vertex match between g 1 and g 2 is ? = 1 ? 3, 2 ? 2, 3 ? 1 because that vertex match has the largest number of common labeled edges and vertices.</p><p>? Enumerate the matching vertices and edges to update the voting statistics accordingly. For instance, since the vertex 3 in g 1 with label B is mapped to the vertex 1 in g 2 with label C, a new candidate label C is added to the table for the given vertex. For the same reason, we add a new candidate label Z for the edge (1, 2). For all the other edges and vertices where the labels are matched the votes are updated accordingly.</p><p>Once the complete voting statistics V is available, the algorithm filters the candidate labels of edges and vertices using the provided support threshold ? by calling the function F ilter(V, ?) to obtain an ensemble graph g e i . For special cases, when disconnected graphs are not considered as a valid output, we keep all edges of the pivot graph even its support is below the threshold. On the other hand, for the graph prediction problem, where a graph is only considered a valid graph if it does not have multiple edges between two vertices and multiple labels for any vertex, we remove all candidate labels for vertices and edges except the one with the highest number of votes.</p><p>Assume that the resulting ensemble graph that is created by using g i as the pivot graph is denoted as g e i . The final ensemble graph g e is chosen among the set of graphs g e 1 , g e 2 , ? ? ? , g e m as the one with the largest total support. Recall that ?(g pivot , g i ) finds the best vertex match between two graphs. In general, the given task is computationally intractable. However, for labeled graphs like AMR a heuristic was proposed  to approximate the best match by a hill-climbing algorithm. It first starts with the candidate with labels that are mostly matched. The initial match is modified iteratively to optimize the total number of matches with a predefined number of iterations (default value set to 5). This algorithm is very efficient and effective, it was used to calculate the Smatch score in  so we reuse the same implementation to approximate ?(g pivot , g i ) (report on average running time can be found in the supplementary materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare our Graphene algorithm against four previous state-of-the-art models on different benchmark datasets. Below we describe our experimental settings.  <ref type="bibr" target="#b5">[Bevilacqua et al., 2021]</ref>, tackles Text-to-AMR and AMR-to-Text as a symmetric transduction task. The authors show that with a pretrained encoder-decoder model, it is possible to obtain state-of-the-art performances in both tasks using a simple seq2seq framework by predicting linearized graphs. In our experiments, we used the pretrained models provided in <ref type="bibr" target="#b5">[Bevilacqua et al., 2021]</ref> 3 . In addition, we trained 3 more models using different random seeds following the same setup described in <ref type="bibr" target="#b5">[Bevilacqua et al., 2021]</ref>. Blink  was used to add wiki tags to the predicted AMR graphs as a post-processing step. T5 The T5 model, presented in <ref type="bibr" target="#b20">[Raffel et al., 2020]</ref>, introduces a unified framework that models a wide range of NLP tasks as a text-to-text problem. We follow the same idea proposed in <ref type="bibr">[Xu et al., 2020</ref>] to train a model to transfer a text to a linearized AMR graph based on T5-Large. The data is preprocessed by linearization and removing wiki tags using the script provided in <ref type="bibr">[amr]</ref>. In addition to the main task, we added a new task that takes as input a sentence and predicts the concatenation of word senses and arguments provided in the English Web Treebank dataset <ref type="bibr">[goo]</ref>. The model is trained with 30 epochs. We use ADAM optimization with a learning rate of 1e-4 and a mini-batch size of 4. Blink  was used to add wiki tags to the predicted AMR graphs during post-processing.</p><p>APT <ref type="bibr" target="#b25">[Zhou et al., 2021]</ref> proposed a transition-based AMR parser 4 based on Transformer <ref type="bibr" target="#b24">[Vaswani et al., 2017]</ref>. It combines hard-attentions over sentences with a target side action pointer mechanism to decouple source tokens from node representations. For model training in our experiments, we use the setup described in <ref type="bibr" target="#b25">[Zhou et al., 2021</ref>] and added 70K model-annotated silver data sentences to the training data, which was created from the 85K sentence set in  with self-learning described in the paper.</p><p>Cai&amp;Lam The model proposed in <ref type="bibr" target="#b8">[Cai and Lam, 2020b]</ref> treats AMR parsing as a series of dual decisions (i.e., which parts of the sequence to abstract, and where in the graph to construct) on the input sequence and constructs the AMR graph incrementally. Following <ref type="bibr" target="#b8">[Cai and Lam, 2020b]</ref>, we use Stanford CoreNLP 5 for tokenization, lemmatization, part-of-speech tagging, and named entity recognition. We apply the pretrained model provided by the authors 6 to all testing datasets and follow the same pre-processing and post-processing steps for graph re-categorization.</p><p>Graphene (our algorithm) The only hyperparameter of the Graphene algorithm is the threshold ?. A popular practice for ensemble methods via voting strategy <ref type="bibr" target="#b13">[Dong et al., 2020]</ref> is to consider the labels that get at least 50% of the total number of votes, therefore we set the threshold ? such that ? m ? 0.5 (where m is the number of models in the ensemble). In all experiments, we used a Tesla GPU V100 for model training and used 8 CPUs for making an ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation</head><p>We use the script 7 provided in <ref type="bibr" target="#b11">[Damonte et al., 2017]</ref> to calculate the Smatch score , the most relevant metric for measuring the similarity between the predictions and the gold AMR graphs. The overall Smatch score can be broken down into different dimensions, including the followings sub-metrics:</p><p>? Unlabeled (Unl.): Smatch score after removing all edge labels Similarly to <ref type="bibr" target="#b5">[Bevilacqua et al., 2021]</ref>, we use five standard benchmark datasets [dat] to evaluate our approach. <ref type="table">Table 1</ref> shows the statistics of the datasets. AMR 2.0 and AMR 3.0 are divided into train, development and testing sets and we use them for in-distribution evaluation in Section 4.2. Furthermore, the models trained on AMR 2.0 training data are used to evaluate out-of-distribution prediction on the BIO, the LP and the New3 dataset (See Section 4.3). <ref type="table">Table 1</ref>: Benchmark datasets. All instances of BIO, LP, and New3 are used to test models in out-of-distribution evaluation. For AMR 2.0 and 3.0, the models are trained on the training dataset, validated on the development dataset. We report results on testing sets in the in-distribution evaluation.</p><p>Datasets AMR 2.0 AMR 3.0 BIO Little Prince (LP) New3 Training 36,521 55,635 n/a n/a n/a Dev 1,368 1,722 n/a n/a n/a Test 1,371 1,898 6,952 1,562 527</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">In-distribution evaluation</head><p>In the same spirit of <ref type="bibr" target="#b5">[Bevilacqua et al., 2021]</ref>, we evaluate the approaches when training and test data belong to the same domain. <ref type="table" target="#tab_1">Table 2</ref> shows the results of the models on the test split of the AMR 2.0 and AMR 3.0 datasets. The metrics reported for SPRING correspond to the model with the highest Smatch score among the 4 models(the checkpoint plus the 3 models with different random seeds). For the ensemble approach, we report the result when Graphene is an ensemble of four SPRING checkpoints, denoted as Graphene 4S. The ensemble of all the models including four SPRING checkpoints, APT, T5, and Cai&amp;Lam is denoted as Graphene All. For the AMR 3.0 dataset, the Cai&amp;Lam model is not available so the reported result corresponds to an ensemble of all six models.</p><p>We can see that Graphene successfully leverages the strength of all the models and provides better prediction both in terms of the overall Smatch score and sub-metrics. In both datasets, we achieve the state-of-the-art results with performance gain of 1.6 and 1.2 Smatch points in AMR 2.0 and AMR 3.0 respectively. <ref type="table" target="#tab_1">Table 2</ref> shows that by combining predictions from four checkpoints of the SPRING model, Graphene 4S provides better results than any individual models. The result is improved further when increasing the number of ensemble models, indeed Graphene All improves Graphene 4S further and outperforms the individual models in terms of the overall Smatch score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Out-of-distribution evaluation</head><p>In contrast to in-distribution evaluation, we use the models trained with AMR 2.0 data to collect AMR predictions for the testing datasets in the domains that differ from the AMR 2.0 dataset. The purpose of the experiment is to evaluate the ensemble approach under out-of-distribution settings. <ref type="table" target="#tab_2">Table 3</ref> shows the results of our experiments. Similar to the in-distribution experiments, the Graphene 4S algorithm achieves better results than other individual models, while the Graphene All approach improves the given results further. We achieve the new state-of-the-art results in these benchmark datasets (under out-of-distribution settings). This result has an important practical implication because in practice it is very common not to have labeled AMR data for domain-specific texts. After all, the labeling task is very time-demanding. Using the proposed ensemble methods we can achieve better results with domain-specific data not included in the training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">How the ensemble algorithm works</head><p>We explore a few examples to demonstrate the reason why the ensemble method works. <ref type="figure" target="#fig_1">Figure 3</ref> shows a sentence with a gold AMR in Penman format and a list of AMRs corresponding to the prediction of SPRING <ref type="bibr" target="#b5">[Bevilacqua et al., 2021]</ref>, T5 <ref type="bibr" target="#b20">[Raffel et al., 2020]</ref>, APT <ref type="bibr" target="#b25">[Zhou et al., 2021]</ref>, Cai and Lam <ref type="bibr" target="#b8">[Cai and Lam, 2020b]</ref> parser and the ensemble graph given by Graphene.</p><p>In this particular example, with the sentence "They want money, not the face", the AMR prediction from SPRING is inaccurate. Graphene corrects the prediction thanks to the votes given from the other models. In particular, the label and of the root node z 0 of SPRING prediction was corrected to contrast ? 01 because T5, APT and Cai&amp;Lam parsers all vote for contrast ? 01. On the other hand, the labels : op1 and : op2 of the edges (z 0 , z 1 ) and (z 0 , z 4 ) were modified to have the correct labels : ARG1 and : ARG2 accordingly thanks to the votes from the other models. We can also see that even though the Cai&amp;Lam method misses polarity prediction, since the other models predict polarity correctly, the ensemble prediction does not inherit this mistake. Putting everything together, the prediction from Graphene perfectly matches with the gold AMR graph in this example.  The Graphene algorithm searches for the graph that has the largest support from all individual graphs. One question that arises from this is whether the support is correlated with the accuracy of AMR parsing. <ref type="table" target="#tab_3">Table 4</ref> shows the support and the Smatch score of three models in the standard benchmark datasets. The first model is SPRING, while the second one denoted as Graphene SPRING pivot starts with a SPRING prediction as a pivot and corrects the prediction using votes from other models. The last model corresponds to the Graphene algorithm that polls the votes while considering every prediction as a pivot for correction and selecting the best one. Since Graphene looks for the best pivot to have better-supported ensemble graphs, the total supports of the Graphene predictions are larger than the Graphene SPRING pivot predictions. From the table, we can also see that the total support is highly correlated to the Smatch score. Namely, Graphene has higher support in all the benchmark datasets and a higher Smatch score than Graphene SPRING pivot. This experiment suggests that by optimizing the total support we can obtain the ensemble graphs with higher Smatch score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison to Barzdins and Gosko [2016]</head><p>Barzdins and Gosko [2016] 8 proposed a character-level based neural method for parsing texts into AMRs. In order to improve the robustness of the neural parser, the authors proposed an ensemble technique which selects among the prediction graphs the one that has the highest average Smatch when it is compared against the other predictions. The key difference between Barzdins' approach and our approach is that while our solution first modifies the predictions to create new prediction candidates for ensemble prediction, Barzdins' approach directly selects a prediction among existing predictions.</p><p>We provided experimental results to compare to <ref type="bibr" target="#b4">Barzdins and Gosko [2016]</ref> in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> to <ref type="bibr" target="#b4">Barzdins and Gosko [2016]</ref>. Recall that our Graphene algorithm has two stages. In the first one, it modifies all the predictions to make new pivot graphs that are most supported by the other graphs. In the second stage it chooses the best candidate among the pivot and original predictions with respect to the average support. The second stage is very similar to <ref type="bibr" target="#b4">Barzdins and Gosko [2016]</ref>, the only difference is in the objective function where support was used instead of Smatch. For general graph prediction problems when the evaluation metrics is not known in advance Graphene can rely on the support. However, for AMR parsing problem where Smatch is the evaluation metrics, Graphene can also use Smatch as the criteria to select the best candidate similar to <ref type="bibr" target="#b4">Barzdins and Gosko [2016]</ref> does, we denote this version of Graphene as Graphene_Smatch, the other version that uses support is denoted as Graphene_Support.</p><p>As we can see in both <ref type="table" target="#tab_1">Table 2 and Table 3</ref>, Bazdins's method works very well for 4 out of 5 datasets. Since it targets optimizing the Smatch, it provides better predictions than Graphene_Support except for the BIO dataset. However, when Graphene uses Smatch to select the best candidate from the pivoting and the original predictions, Graphene_Smatch provides slightly better results in AMR 2.0, AMR 3.0 and LP, and outperform Bazdins's method in BIO and New3 datasets.</p><p>Discussion Our hypothesis is that if the predictions of each individual model are similar to each other and accurate, Bazdins methods and Graphene_Smatch are comparable. This can be clearly observed in AMR 2.0, AMR 3.0 and LP. But when the predictions from the models are not accurate and differ, the modification step in Graphene helps to correct the predictions and thus it provides much better results in BIO and New3 dataset. Although both algorithms search for a graph that is most similar to all the prediction, Graphene search space is extended to pivoting graphs besides existing graphs so it provides better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Ensemble learning. Ensemble learning is a popular machine learning approach that combines predictions from different learners to make a more robust and more accurate prediction. Many ensembling approaches have been proposed, such as bagging <ref type="bibr" target="#b6">[Breiman, 1996]</ref> or boosting <ref type="bibr" target="#b22">[Schapire and Freund, 2013]</ref>, the winning solutions in many machine learning competitions <ref type="bibr" target="#b10">[Chen and Guestrin, 2016]</ref>. These methods are proposed mainly for regression or classification problems. Recently, structure output prediction emerges as an important problem thanks to the advances in deep learning research. To apply popular ensembling techniques such as bagging or boosting, it is important to study the ensemble method for combining structure predictions.</p><p>Ensemble structure prediction. Previous studies have explored various ensemble learning approaches for dependency and constituent parsing: <ref type="bibr" target="#b21">[Sagae and Lavie, 2006</ref>] proposes a reparsing framework that takes the output from different parsers and maximizes the number of votes for a well-formed dependency or constituent structure; <ref type="bibr" target="#b16">[Kuncoro et al., 2016]</ref> uses minimum Bayes risk inference to build a consensus dependency parser from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. Note that a syntactic tree is a special graph structure in which nodes for a sentence from different parsers are roughly the same. In contrast, we propose an approach to ensemble graph predictions in which both graph nodes and edges can be different among base predictions.</p><p>Ensemble methods for AMR parsing. Parsing text to AMR is an important research problem. State-of-the-art approaches in AMR parsing are divided into three categories. Sequence to sequence models <ref type="bibr" target="#b5">[Bevilacqua et al., 2021</ref><ref type="bibr" target="#b15">, Konstas et al., 2017</ref><ref type="bibr" target="#b23">, Van Noord and Bos, 2017</ref><ref type="bibr">, Xu et al., 2020</ref> consider the AMR parsing as a machine translation problem that translates texts to AMR graphs. The transition-based methods <ref type="bibr" target="#b25">[Zhou et al., 2021]</ref> predicts a sequence of actions given the input text, and then the action sequence is turned into an AMR graph using an oracle decoder. Lastly, graph-based methods <ref type="bibr" target="#b8">[Cai and Lam, 2020b]</ref> directly construct the AMR graphs from textual data. All these methods are complementary to each other and thus ensemble methods can leverage the strength of these methods to create a better prediction, as demonstrated in this paper. Ensemble of AMR predictions from a single type of model is studied in <ref type="bibr" target="#b25">[Zhou et al., 2021]</ref> where the authors demonstrated that by combining predictions from three different model's checkpoints they gain performance improvement in the final prediction. However, ensemble in sequential decoding requires that all predictions are from the same type of models. It is not applicable for cases when the predictions are from different types of models such as seq2seq, transition-based or graph-based models. In contrast to that approach, our algorithm is model-agnostic, i.e. it can combine predictions from different models. In our experiments, we have demonstrated the benefit of combining predictions from different models, with additional gains in performance compared to the ensemble of predictions from a single model's checkpoints.</p><p>Comparison to Bazdins et al. <ref type="bibr" target="#b4">Barzdins and Gosko [2016]</ref> proposed a character-level based neural method for parsing texts into AMRs. In order to improve the robustness of the neural parser, the authors proposed an ensemble technique which selects among the prediction graphs the one that has the highest average SMATCH when it is compared against the other predictions. The key difference between Barzdins' approach and our approach is that while our solution modifies the predictions to create new prediction candidates for ensemble prediction, Barzdins' approach only selects a prediction among existing predictions. In order to demonstrate how creating new prediction candidates from existing predictions before selecting the best candidates help achieving a better prediction we have discussed the new results added to <ref type="table" target="#tab_1">Table 2 and 3.</ref> 6 Conclusions and future work</p><p>In this paper, we formulate the graph ensemble problem, study its computational intractability, and provide an algorithm for constructing graph ensemble predictions. We validate our approach with AMR parsing problems. The experimental results show that the proposed approach outperforms the previous state-of-the-art AMR parsers and achieves new state-of-the-art results in five different benchmark datasets. We demonstrate that the proposed ensemble algorithm not only works well for in-distribution but also for out-of-distribution evaluations. This result has a significant practical impact, especially when applying the proposed method to domain-specific texts where training data is not available. Moreover, our approach is model-agnostic, which means that it can be used to combine predictions from different models without the requirement of having an access to model training. In general, our approach provides the basis for graph ensemble, studying classical ensemble techniques such as bagging, boosting, or stacking for graph ensemble is a promising future research direction that is worth considering to improve the results further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble Graph Prediction Supplementary Material</head><p>A Running time <ref type="figure" target="#fig_2">Figure 4</ref> shows the average running time of the Graphene algorithm. The horizontal axis corresponds to the average graph size (the number of triples) while the vertical axis shows the average running time (in seconds). We can see that the running time depends on the average size of the AMR graphs. Since AMR graph size is proportional to the input sentence length, the largest average graph has around 50 triples. Graphene requires less than 2 seconds on an 8-core CPU machine to make an ensemble from 7 models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B On the support threshold</head><p>The popular VotingClassifier algorithm implemented in scikit-learn 9 follows the majority vote rule where the label with the most votes is selected as the final prediction. Therefore, we apply the same rule in our experimental settings where setting theta = 0.5 is equivalent to the majority vote rule in classification problems.</p><p>If there is an independent validation set, this hyper-parameter can be tuned to choose the right theta value for that dataset. For example, in the AMR 2.0 dataset, the results of ensembling 4 Spring models, APT model, and T5 models on the validation set (the dev split) when theta is varied are reported in <ref type="table" target="#tab_4">Table 5</ref>. Based on this result on an independent dev set, theta=0.5 is the right choice for AMR 2.0. Note that setting theta is a trade-off between precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Comparison with median baselines</head><p>Beside the Graphene 4S baseline, we provide the results in <ref type="table" target="#tab_5">Table 6</ref> of the following baseline approaches:</p><p>? Uniform sampling: for each set of predictions we sample the graph uniformly at random, this approach is equivalent to the "median" representative from a set. ? Ideal median: assumes that the gold AMRs are available for the test set (hence named as "ideal"). We computed the Smatch of each prediction with the gold AMR and use the AMR with the median Smatch score as the final prediction. D Pivot selection <ref type="figure">Figure 5</ref> shows the pie-charts with the percentage summarizing the number of times that the prediction created when each algorithm is chosen as a pivot graph is selected as the final prediction. Notice that the order of the algorithms matters because when tight happens, the ensemble is chosen from the first algorithm in the list.</p><p>The results show that all algorithms contribute to the final predictions. In the Bio dataset where the test data is from a specific domain that differs from the training data domain, Graphene benefits from the model diversity when it leverages predictions from all models effectively. We down-sampled the AMR 2.0 training data with sample rates 0.6 and 0.8. Then 4 Spring models with different random seeds and the T5 model were trained on these two sample sets. The Smatch score on AMR 2 test sets and on the out-of-distribution sets (LP, New3, Bio) are reported in <ref type="table" target="#tab_6">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Robustness on down-sampled training data</head><p>Compared to the best individual models, Graphene is more robust and 1.35, 2.86, 0.92, and 0.83 points better when the sample rate is equal to 0.6. While compared to the best individual models, Graphene is more robust and 1.27, 2.73, 1.27 and 0.39 points better better when the sample rate is equal to 0.8. This result demonstrates that the proposed method is robust with respect to smaller training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Ties broken arbitrarily</head><p>When many ensemble graphs have the same support, Graphene chooses the ensemble graph created when the first model in the list is chosen as the pivot. <ref type="table" target="#tab_7">Table 8</ref> shows the results when each model is put first in the list. If we have a validation set like the case with AMR 2.0 or AMR 3.0, we can tune the right input order to achieve the best performance on the validation set.</p><p>In case there is no validation set available, to mitigate the impact of random input order, we can break the ties arbitrarily, the results of ties broken arbitrarily are reported in <ref type="table" target="#tab_8">Table 9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Support and Smatch</head><p>We have shown in <ref type="table" target="#tab_3">Table 4</ref> that the average total support is highly correlated with the Smatch score. We performed statistical significant tests to support the given hypothesis. Below is the correlation between the "Normalised total support" (the total support normalised to the size of the graph) and the Smatch score, together with the p-value for each dataset:</p><p>? The overall correlation between the "Normalised total support" and the Smatch score, together with the p-value for all datasets is: Pearson correlation =0.67 , p-value=0.0</p><p>Figure 5: Pie charts shows the percentage of the number of times each model was selected as the best pivot in the Graphene algorithm. Notice that when tight happens, the ensemble created when first algorithm is considered as the pivot is selected as the final ensemble graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>No WSD (NWSD): Smatch score while ignoring Propbank senses ? NE: F-score on the named entity recognition (:name roles) ? Wikification (Wiki.): F-score on the wikification (:wiki roles) ? Negations (Neg.): F-score on the negation detection (:polarity roles) 3 Available for download at https://github.com/SapienzaNLP/spring 4 Available under https://github.com/IBM/ transition-amr-parser. 5 Available at https://github.com/stanfordnlp/stanza/ 6 The model "AMR2.0+BERT+GR" can be downloaded from https://github.com/jcyk/AMR-gs 7 https://github.com/mdtux89/amr-evaluation ? Concepts (Con.): F-score on the concept identification task ? Reentrancy (Reen.): Smatch computed on reentrant edges only ? SRL: Smatch computed on :ARG-i roles only 4.1.3 Datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The gold AMR and the ensemble AMR graph of SPRING, T5, APT and Cai&amp;Lam using the Graphene algorithm for the sentence "They want money, not the face".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Average running time of the Graphene algorithm versus the average graph size in the LP, New3, AMR 3.0, AMR 2.0, and BIO datasets respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>AMR 2.0: Pearson correlation =0.60 , p-value = 2.7e-117 ? AMR 3.0: Pearson correlation =0.49 , p-value = 2.6e-137 ? BIO: Pearson correlation =0.55 , p-value = 0.0 ? LP: Pearson correlation =0.56, p-value =3.4e-130 ? New3: Pearson correlation = 0.73, p-value=5.1e-191</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on the test splits of the AMR 2.0 and AMR 3.0 dataset. 88.63 74.78 81.49 75.48 83.55 Graphene Smatch 84.87 87.76 85.31 90.57 88.81 74.73 82.15 74.06 83.72</figDesc><table><row><cell>Models</cell><cell cols="2">Smatch Unl.</cell><cell cols="2">NWSD Con. NE</cell><cell>Neg.</cell><cell>Wiki. Reen. SRL</cell></row><row><cell>AMR 2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPRING</cell><cell>84.22</cell><cell cols="2">87.38 84.72</cell><cell cols="2">89.98 90.77 72.65 82.76 74.30 82.89</cell></row><row><cell>APT</cell><cell>82.70</cell><cell cols="2">86.18 83.23</cell><cell cols="2">89.48 90.20 67.27 78.87 73.19 82.01</cell></row><row><cell>T5</cell><cell>82.98</cell><cell cols="2">86.17 83.43</cell><cell cols="2">89.85 90.65 73.43 77.99 72.44 82.02</cell></row><row><cell>Cai&amp;Lam</cell><cell>80.15</cell><cell cols="2">83.60 80.66</cell><cell cols="2">87.39 82.25 78.09 85.36 66.46 77.35</cell></row><row><cell>Graphene 4S</cell><cell>84.78</cell><cell cols="2">87.96 85.29</cell><cell cols="2">90.64 92.19 75.22 83.88 71.42 83.46</cell></row><row><cell cols="2">Graphene Support 85.85</cell><cell cols="2">88.68 86.35</cell><cell cols="2">91.23 92.30 77.01 84.63 74.49 84.41</cell></row><row><cell>Barzdins</cell><cell>85.93</cell><cell cols="2">88.85 86.63</cell><cell cols="2">91.16 92.51 75.73 84.01 76.53 84.64</cell></row><row><cell cols="2">Graphene Smatch 86.26</cell><cell cols="2">89.03 86.75</cell><cell cols="2">91.39 92.47 76.72 84.61 76.25 84.85</cell></row><row><cell>AMR 3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPRING</cell><cell>83.25</cell><cell cols="2">86.40 83.71</cell><cell cols="2">89.38 87.80 72.94 81.22 73.33 81.97</cell></row><row><cell>APT</cell><cell>80.57</cell><cell cols="2">83.96 81.07</cell><cell cols="2">88.38 86.82 68.69 76.88 70.78 80.17</cell></row><row><cell>T5</cell><cell>82.17</cell><cell cols="2">85.22 82.66</cell><cell cols="2">89.03 86.99 72.59 73.78 72.18 81.18</cell></row><row><cell>Graphene 4S</cell><cell>83.77</cell><cell cols="2">86.89 84.23</cell><cell cols="2">90.09 88.27 74.60 81.92 70.22 82.46</cell></row><row><cell cols="2">Graphene Support 84.41</cell><cell cols="2">87.35 84.83</cell><cell cols="2">90.51 88.64 74.76 82.25 71.93 83.15</cell></row><row><cell>Barzdins</cell><cell>84.74</cell><cell cols="2">87.66 85.21</cell><cell>90.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of out-of-distribution evaluation on the BIO, New3, and Little Prince dataset.</figDesc><table><row><cell>Models</cell><cell cols="2">Smatch Unl.</cell><cell cols="2">NWSD Con. NE</cell><cell>Neg.</cell><cell>Wiki. Reen. SRL</cell></row><row><cell>BIO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPRING</cell><cell>60.52</cell><cell cols="2">65.33 61.42</cell><cell cols="3">67.76 33.92 65.68 3.80</cell><cell>51.19 62.86</cell></row><row><cell>APT</cell><cell>51.23</cell><cell cols="2">56.27 51.81</cell><cell cols="3">58.22 15.68 52.91 3.62</cell><cell>43.53 54.24</cell></row><row><cell>T5</cell><cell>58.89</cell><cell cols="2">63.86 59.69</cell><cell cols="3">66.63 30.42 65.11 2.46</cell><cell>48.56 61.47</cell></row><row><cell>Cai&amp;Lam</cell><cell>42.22</cell><cell cols="2">49.78 42.85</cell><cell>47.10 5.19</cell><cell cols="2">51.42 7.32</cell><cell>39.23 51.00</cell></row><row><cell>Graphene 4S</cell><cell>61.51</cell><cell cols="2">66.22 62.28</cell><cell cols="3">68.48 33.02 68.24 4.46</cell><cell>50.40 63.70</cell></row><row><cell cols="2">Graphene Support 62.29</cell><cell cols="2">66.89 63.07</cell><cell cols="3">68.64 32.62 69.48 4.54</cell><cell>52.06 64.21</cell></row><row><cell>Barzdins</cell><cell>62.05</cell><cell cols="2">66.84 62.88</cell><cell cols="3">68.43 32.73 68.11 4.23</cell><cell>52.22 64.08</cell></row><row><cell cols="2">Graphene Smatch 62.80</cell><cell cols="2">67.39 63.58</cell><cell cols="3">68.94 32.82 69.13 4.37</cell><cell>52.58 64.56</cell></row><row><cell>New3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPRING</cell><cell>74.66</cell><cell cols="2">78.99 75.21</cell><cell cols="3">82.38 67.52 67.48 67.20 66.47 75.65</cell></row><row><cell>APT</cell><cell>71.06</cell><cell cols="2">75.92 71.58</cell><cell cols="3">80.34 65.65 67.08 57.14 63.02 73.40</cell></row><row><cell>T5</cell><cell>73.04</cell><cell cols="2">77.30 73.68</cell><cell cols="3">82.65 68.24 64.20 56.42 64.65 75.03</cell></row><row><cell>Cai&amp;Lam</cell><cell>60.81</cell><cell cols="2">66.00 61.29</cell><cell cols="3">72.79 45.60 59.57 46.39 57.70 68.87</cell></row><row><cell>Graphene 4S</cell><cell>74.84</cell><cell cols="2">79.23 75.30</cell><cell cols="3">82.56 69.98 69.51 68.34 63.53 76.31</cell></row><row><cell cols="2">Graphene Support 75.60</cell><cell cols="2">79.64 76.14</cell><cell cols="3">83.08 68.40 69.62 67.98 67.16 76.88</cell></row><row><cell>Barzdins</cell><cell>75.87</cell><cell cols="2">80.22 76.41</cell><cell cols="3">83.39 72.00 68.75 67.52 68.54 77.52</cell></row><row><cell cols="2">Graphene Smatch 76.32</cell><cell cols="2">80.26 76.86</cell><cell cols="3">83.62 70.60 68.39 67.93 68.93 77.79</cell></row><row><cell>Little Prince</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPRING</cell><cell>77.85</cell><cell cols="2">82.31 78.85</cell><cell cols="3">84.68 60.53 70.72 60.53 68.28 77.78</cell></row><row><cell>APT</cell><cell>75.21</cell><cell cols="2">80.07 76.12</cell><cell cols="3">85.29 65.15 67.92 69.70 63.28 75.31</cell></row><row><cell>T5</cell><cell>77.66</cell><cell cols="2">81.99 78.53</cell><cell cols="3">85.12 58.06 72.33 59.35 67.03 78.30</cell></row><row><cell>Cai&amp;Lam</cell><cell>71.03</cell><cell cols="2">75.91 72.07</cell><cell cols="3">80.18 22.73 57.51 31.50 59.29 72.02</cell></row><row><cell>Graphene 4S</cell><cell>77.91</cell><cell cols="2">82.40 78.86</cell><cell cols="3">84.91 61.54 73.58 60.65 64.77 78.12</cell></row><row><cell cols="2">Graphene Support 78.54</cell><cell cols="2">82.81 79.44</cell><cell cols="3">85.52 64.05 75.11 63.45 67.83 78.72</cell></row><row><cell>Barzdins</cell><cell>79.21</cell><cell cols="2">83.47 80.12</cell><cell cols="3">85.74 59.21 74.06 61.84 70.45 79.35</cell></row><row><cell cols="2">Graphene Smatch 79.52</cell><cell cols="2">83.68 80.44</cell><cell cols="3">86.10 63.16 75.11 63.89 70.19 79.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The average total support and Smatch score of the prediction from SPRING, Graphene with SPRING as a pivot and Graphene considering every prediction as a pivot respectively. The support is highly correlated with Smatch score.84.08 136.90 83.14 166.86 60.52 69.33 77.85 118.27 74.66 SPR. pivot 172.70 84.70 139.42 83.73 169.97 61.56 70.97 78.22 120.85 74.83 Graphene 175.73 85.85 142.07 84.43 179.38 62.29 72.64 78.54 123.62 75.60</figDesc><table><row><cell></cell><cell>AMR 2.0</cell><cell>AMR 3.0</cell><cell>BIO</cell><cell>LP</cell><cell>New3</cell></row><row><cell></cell><cell>Sup. Smat.</cell><cell>Sup. Smat.</cell><cell>Sup. Smat.</cell><cell>Sup. Smat.</cell><cell>Sup. Smat.</cell></row><row><cell>SPRING</cell><cell>170.15</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The results of ensembling 4 Spring models, APT model, and T5 models on the validation set (the dev split) when ? is varied. On this dev set, ? = 0.5 is a proper choice for AMR 2.0.</figDesc><table><row><cell></cell><cell cols="5">? = 0.1 ? = 0.3 ? = 0.5 ? = 0.7 ? = 0.9</cell></row><row><cell>Smatch</cell><cell>81.64</cell><cell>85.01</cell><cell>85.49</cell><cell>85.12</cell><cell>83.68</cell></row><row><cell cols="2">Precision 76.13</cell><cell>83.62</cell><cell>85.86</cell><cell>86.53</cell><cell>86.54</cell></row><row><cell>Recall</cell><cell>88.00</cell><cell>86.55</cell><cell>85.13</cell><cell>83.75</cell><cell>81.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Comparison with median baselines</cell></row><row><cell></cell><cell cols="3">AMR 2.0 AMR 3.0 BIO</cell><cell>new3 LP</cell></row><row><cell cols="2">uniform Sampling 82.58</cell><cell>82.98</cell><cell cols="2">56.00 71.18 76.17</cell></row><row><cell>Ideal median</cell><cell>83.80</cell><cell>83.66</cell><cell cols="2">57.72 73.06 77.14</cell></row><row><cell>Graphene</cell><cell>85.85</cell><cell>84.41</cell><cell cols="2">62.29 75.60 78.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>When the training data is down-sampled, the gain of using Graphene is enlarged.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Sample rate 0.6</cell><cell>Sample rate 0.8</cell></row><row><cell cols="3">Methods/Data AMR 2.0 BIO</cell><cell>New3 LP</cell><cell>AMR 2.0 BIO</cell><cell>New3 LP</cell></row><row><cell>SPRING 603</cell><cell>82.40</cell><cell cols="3">58.22 73.81 76.75 83.28</cell><cell>58.85 74.13 76.96</cell></row><row><cell>SPRING 703</cell><cell>82.74</cell><cell cols="3">57.93 73.49 76.60 83.43</cell><cell>59.50 74.71 77.40</cell></row><row><cell>SPRING 803</cell><cell>82.85</cell><cell cols="3">58.72 73.42 76.68 83.39</cell><cell>58.97 73.57 77.39</cell></row><row><cell>SPRING 903</cell><cell>82.81</cell><cell cols="3">58.80 74.09 76.96 83.12</cell><cell>57.52 73.50 76.70</cell></row><row><cell>T5</cell><cell>82.08</cell><cell cols="3">56.46 72.84 76.31 82.59</cell><cell>58.69 73.42 77.70</cell></row><row><cell>Graphene</cell><cell>84.20</cell><cell cols="3">61.66 75.01 77.79 84.70</cell><cell>62.23 75.98 78.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Results of Graphene when each model is put first in the list. 85.77 85.81 85.87 85.65 85.67 85.11 AMR 3.0 84.44 84.42 84.34 84.44 84.35 84.18 NA Bio 62.38 62.41 62.39 62.44 62.38 62.41 62.34 LP 78.65 78.63 78.75 78.70 78.65 78.23 77.75 New3 75.65 75.69 75.88 75.82 75.78 75.48 74.92</figDesc><table><row><cell cols="2">Grapphene Models S</cell><cell>S703 S803 S903 T5</cell><cell>APT Cai&amp;Lam</cell></row><row><cell>AMR 2.0</cell><cell>85.66</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Results of Graphene when ties are broken arbitrarily.</figDesc><table><row><cell cols="4">Grapphene Models AMR 2.0 AMR 3.0 Bio</cell><cell>LP</cell><cell>New3</cell></row><row><cell>Graphene 4SATC</cell><cell>85.52</cell><cell>84.27</cell><cell cols="2">62.39 78.50 75.66</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This ArXiv paper is an updated version of the published paper at NeurIPS 2021 https://openreview.net/forum?id= lmm2W2ICtjk. The paper discusses related work by<ref type="bibr" target="#b4">Barzdins and Gosko [2016]</ref> in Section 5 and provides detailed discussion and comparative studies in Subsection 4.5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/IBM/graph_ensemble_learning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We would like to thank Juri Opitz for pointing us to missing references as a very useful feedback for our published work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://amr.isi.edu/download.html" />
		<title level="m">AMRLIB data preprocessing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>English Web Treebank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<ptr target="https://catalog.ldc.upenn.edu/ldc2012t13.URLhttps://catalog.ldc.upenn.edu/LDC2012T13" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The maximum common edge subgraph problem: A polyhedral investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Bahiense</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordana</forename><surname>Mani?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breno</forename><surname>Piva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cid C De</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2523" to="2541" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on amr parsing accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guntis</forename><surname>Barzdins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didzis</forename><surname>Gosko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01278</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rexhina</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AMR parsing via graph-sequence iterative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.119</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.119" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="1290" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AMR parsing via graph-sequence iterative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.119</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.119" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="1290" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P13-2131" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An incremental parser for Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/E17-1051" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="536" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified bias-variance decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th International Conference on Machine Learning</title>
		<meeting>17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Question answering over knowledge bases by leveraging semantic parsing and neuro-symbolic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saswati</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01707</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural AMR: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling an ensemble of greedy dependency parsers into one MST parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1180</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1180" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pushing the limits of AMR parsing with self-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Revanth</forename><surname>Gangi Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3208" to="3214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient one-pass end-to-end entity linking for questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">I know what you asked: Graph path learning using AMR for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonna</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kisu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuiseok</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.222</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.222" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="2459" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parser combination by reparsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N06-2033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bias-variance analysis of support vector machines for the development of svm-based ensemble methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="725" to="775" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Boosting: Foundations and algorithms. Kybernetes</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09980</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, and Guodong Zhou</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Improving AMR parsing with sequence-tosequence pre-training. EMNLP</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AMR parsing with action-pointer transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2021)</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2021)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5585" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
