<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
							<email>liuchunxiao@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The training of many existing end-to-end steering angle prediction models heavily relies on steering angles as the supervisory signal. Without learning from much richer contexts, these methods are susceptible to the presence of sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes. In this paper, we considerably improve the accuracy and robustness of predictions through heterogeneous auxiliary networks feature mimicking, a new and effective training method that provides us with much richer contextual signals apart from steering direction. Specifically, we train our steering angle predictive model by distilling multi-layer knowledge from multiple heterogeneous auxiliary networks that perform related but different tasks, e.g., image segmentation or optical flow estimation. As opposed to multitask learning, our method does not require expensive annotations of related tasks on the target set. This is made possible by applying contemporary off-the-shelf networks on the target set and mimicking their features in different layers after transformation. The auxiliary networks are discarded after training without affecting the runtime efficiency of our model. Our approach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming the previous best by a large margin of 12.8% and 52.1% 1 , respectively. Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Autonomous driving is conventionally formulated and solved as a collection of sub-problems, including perception, decision, path planning, and control <ref type="bibr" target="#b14">(Paden et al. 2016)</ref>. Recent approaches address the problem in an end-to-end manner, in which a convolutional neural network (CNN) is trained end-to-end to map raw visual observations (e.g., images or videos) obtained from a single front-facing camera directly to steering commands <ref type="bibr" target="#b1">(Bojarski et al. 2016)</ref>.</p><p>Steering angle is often used as the sole supervisory signal for training a network <ref type="bibr" target="#b1">(Bojarski et al. 2016;</ref><ref type="bibr" target="#b15">Pomerleau 1989)</ref>. Some studies improve the training by multi-task learning <ref type="bibr" target="#b5">(Chowdhuri, Pankaj, and Zipser 2017;</ref><ref type="bibr" target="#b22">Yang et al. 2018)</ref>, i.e., requiring the network to predict additional labels Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="bibr">1</ref> We compare the mean absolute error against the top entry 'komanda' on Udacity leaderboard <ref type="bibr" target="#b20">(Udacity 2018)</ref>, which uses a 3D CNN with LSTM. such as vehicle speed and steering torque. These supervisory signals are informative but do not ensure effective representation learning to capture rich environmental contexts, e.g., physical scene constraints or coexistence of scene objects, which are crucial for driving. Without such spatial and object awareness, existing methods often fail in challenging cases that involve severe lighting changes, strong shadows, sharp turns, or busy traffic. In <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, we show some of the challenging cases on which a baseline fails.</p><p>A plausible way to solve the problem above is by widening the scope of multi-task learning from speed or torque to more complex tasks such as scene segmentation, lane detection, or optical flow estimation. These tasks capture scene structure and object motion that likely benefit steering angle prediction. However, introducing side tasks for multitask learning requires one to collect extra task-specific annotations for the target scene, a process that is both laborious and expensive. An alternative approach is to pre-train a network with related tasks such as scene segmentation and fine-tune the model to the steering angle prediction task. This method relaxes the need of target scene annotations since pre-training can exploit data collected from a different scene. Our experiments, however, show that this indirect approach only improves steering angle prediction marginally.</p><p>In this study, we train our model, FM-Net, with a new and effective technique that brings drastic improvement to the performance of end-to-end steering angle prediction. Unlike multi-task learning, our method does not require additional annotations of side tasks on target scene. This is made possible by drawing inspiration from Hinton et al.'s seminal work on knowledge distillation <ref type="bibr" target="#b8">(Hinton, Vinyals, and Dean 2015)</ref>. In contrast to <ref type="bibr" target="#b8">(Hinton, Vinyals, and Dean 2015)</ref> that distils knowledge in an ensemble of large models into a single small model, we propose 'heterogeneous auxiliary networks feature mimicking', which allows the learning of a steering angle predictive model by distilling knowledge from heterogeneous off-the-shelf networks. Specifically, there are many strong and state-of-the-art networks such as PSPNet <ref type="bibr" target="#b24">(Zhao et al. 2017)</ref> for segmentation, and FlowNet2 <ref type="bibr" target="#b10">(Ilg et al. 2017)</ref> for optical flow estimation. Applying these networks on the target data can generate features that are highly indicative of the final prediction of steering angles. This is evident from the embedding of these features shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. As can be observed, both high-and low-layer features are structured  <ref type="bibr" target="#b20">(Udacity 2018</ref>) that is trained with steering angle alone tends to fail in challenging cases.</p><p>(b) Deep feature embeddings of auxiliary networks. The embedding of features extracted from PSPNet <ref type="bibr" target="#b24">(Zhao et al. 2017)</ref> and FlowNet2 <ref type="bibr" target="#b10">(Ilg et al. 2017)</ref>. Each point corresponds to a deep feature vector of a video frame. Every point is encoded by colour so that points to the most positive steering angle are lilac and points with the most negative steering angle are aqua. As can be observed, features at different layers are highly indicative of steering angle prediction. and meaningful. Clearly, the high and middle levels contain direct hints for angle prediction. Low-level features are more scattered but they still show well-clustered embeddings.</p><p>In this work, we aim to train a more accurate steering angle predictive model by enforcing it to approximate the multi-layer representation of those well-established auxiliary networks. In contrast to <ref type="bibr" target="#b8">(Hinton, Vinyals, and Dean 2015)</ref>, we find an entirely different application of mimicking heterogeneous networks for learning rich contexts. In addition, we explore the use of deep features extracted from different layers of auxiliary networks as targets beyond the logits (the inputs to the final softmax) as proposed by <ref type="bibr" target="#b8">(Hinton, Vinyals, and Dean 2015)</ref>. We summarize the contributions of this paper as follows: 1) We present an effective training method that drastically improves the performance of end-to-end steering angle prediction through mimicking features from well-established and cheap-to-access auxiliary networks. The auxiliary network is only used in the training stage and brings no computation cost during the deployment. 2) We demonstrate through extensive and systematic experiments that mimicking can be conducted simultaneously from heterogeneous auxiliary networks. In addition, mimicking can be performed at multiple layers of an auxiliary network while still benefiting the main network. This allows our main network to acquire rich contexts of different natures and spatial resolutions.</p><p>3) The mimicking process is non-trivial as the original features extracted from different layers of auxiliary networks are high-dimensional. We show effective ways of pooling these features to a lower dimension for regularizing and training a 3D CNN for steering angle prediction.</p><p>Apart from auxiliary network mimicking, we show that both network choice and initialization play a crucial role in prediction performance. The deepest network in the lit-img steer vs gt-vs gt-seg aux-out (1)</p><p>(2) (3) (4) <ref type="figure">Figure 2</ref>: Four main paradigms of learning steering angle prediction. Each subfigure shows the training sources and prediction tasks. Six abbreviations in the legend denote image sequence, steering angle, vehicle state, ground-truth vehicle state, ground-truth segmentation labels and output features of auxiliary networks, respectively. The last paradigm 4 is proposed in this paper. A dotted line means that the corresponding source is dropped after training. erature is fewer than 10 layers <ref type="bibr" target="#b1">(Bojarski et al. 2016)</ref>. We advance the state-of-the-art by proposing a 50-layer 3D ResNet. We inflate our 3D convolutional network from a 2D ImageNet model. This initialization scheme, which was originally proposed for action recognition <ref type="bibr" target="#b2">(Carreira and Zisserman 2017)</ref>, provides us with a strong ResNet-based model for representation learning and mimicking. Extensive experiments are conducted on two public datasets, namely, Udacity <ref type="bibr" target="#b20">(Udacity 2018)</ref> and Comma.ai <ref type="bibr" target="#b18">(Santana and Hotz 2016)</ref>. Our method surpasses previous methods by a large margin and records a new state-of-the-art in steering angle prediction, with a mean absolute error (MAE) of 1.62 and 0.70 and root mean square error (RMSE) of 2.35 and 0.98.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>End-to-end learning for self driving. Learn-to-steer with end-to-end optimization was first demonstrated in <ref type="bibr" target="#b15">(Pomerleau 1989)</ref>. The study utilized a shallow neural network to predict actuation from images. Although the driving conditions were quite simple, its appealing performance showcased the possibility of applying neural networks to autonomous driving. A similar idea is later presented by exploiting a deep CNN to output steering commands from images <ref type="bibr" target="#b1">(Bojarski et al. 2016</ref>). Due to its good prediction results achieved in highway driving, the structure (a network with a normalization layer, five convolutional layers, and three fully connected layers) has become the base model for many studies <ref type="bibr" target="#b21">(Xu et al. 2017;</ref><ref type="bibr" target="#b4">Chi and Mu 2017)</ref>. We found that there is a lack of new models publicly available for our problem. Most of the models are fewer than 10 layers and do not adopt a contemporary architecture like ResNet <ref type="bibr" target="#b7">(He et al. 2016)</ref>. In this study, we contribute a 50-layer 3D ResNet model for steering angle prediction and make it available to the research community 2 . Currently, there are four main paradigms adopted for training an end-to-end steering angle prediction model (see <ref type="figure">Fig. 2</ref>). A direct mapping of image sequences to steering angles is the simplest form. Researchers soon found that using steering commands as the sole supervisory signal is too weak to train deep networks. Some studies incorporated multi-task learning in the pipeline <ref type="bibr" target="#b4">(Chi and Mu 2017;</ref><ref type="bibr" target="#b21">Xu et al. 2017;</ref><ref type="bibr" target="#b22">Yang et al. 2018;</ref><ref type="bibr" target="#b5">Chowdhuri, Pankaj, and Zipser 2017)</ref> to prevent over-fitting and improved the prediction accuracy of steering angles. The second and third paradigms in <ref type="figure">Fig. 2</ref> correspond to these variants. For instance, Chowdhuri et al. <ref type="bibr" target="#b5">(Chowdhuri, Pankaj, and Zipser 2017)</ref> proposed a light-weight Z2Color network and executed steering commands by utilizing vehicle-state indicators (behavioral modalities) as secondary input data besides image sequences. To supplement visual inputs, in <ref type="bibr" target="#b22">(Yang et al. 2018</ref>) the steering prediction and speed prediction were simultaneously learnt in the framework of multi-task learning. To leverage auxiliary tasks for feature learning, Xu et al. <ref type="bibr" target="#b21">(Xu et al. 2017</ref>) developed a FCN-LSTM framework, which learnt jointly from steering loss and image segmentation loss. However, this method needs extra labels for the auxiliary segmentation task. Our proposed framework learns directly from off-the-shelf models without any extra labeling for target scene apart from the ground-truth steering angles. It belongs to the last paradigm in <ref type="figure">Fig. 2</ref>. The main difference between our scheme and multi-task learning is that extra labels are not needed during training and image sequences are the sole input of the network. Network and feature mimicking. Network mimicking is originally introduced for small networks to distil knowledge from an ensemble of large networks for network acceleration and compression <ref type="bibr" target="#b8">(Hinton, Vinyals, and Dean 2015;</ref><ref type="bibr" target="#b0">Ba and Caruana 2014)</ref> by forcing a small network to mimic outputs of large networks. In <ref type="bibr" target="#b16">(Romero et al. 2014</ref>) the mimicking idea was applied in image classification, where a student network was required to learn the intermediate output of a teacher network. This mimicking strategy is also known as feature mimicking. Li et al. <ref type="bibr" target="#b13">(Li, Jin, and Yan 2017)</ref> further extended feature mimicking into object detection tasks, where a small network was used to mimic spatially sampled features of large networks. Despite the progress made in feature mimicking, existing mimicking approaches are limited to transferring knowledge from large networks to a small network, and these networks share a common task. In <ref type="bibr" target="#b19">(Saurabh, Judy, and Jitendra 2016)</ref>, feature mimicking was used to teach a new CNN for a new image modality (like depth images), by teaching the network to reproduce the mid-level semantic representations learned from a well labeled image modality. In this work, we demonstrate the possibility of transferring knowledge between large networks that perform heterogeneous tasks. We also explore feature mimicking at different layers of a teacher network to distil richer information for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology Preliminary</head><p>The general objective of steering angle prediction is to predict the angle p given a video frame x. Typically, one would use a video clip, x = (x 1 , x 2 , . . . , x N ) as input to encapsulate the temporal information, and then learn a function F : x ? p for prediction. Recent studies use convolutional networks as F for end-to-end prediction. Multi-task learning <ref type="bibr" target="#b5">(Chowdhuri, Pankaj, and Zipser 2017;</ref><ref type="bibr" target="#b22">Yang et al. 2018)</ref> assumes additional targets apart from steering angles. Ex-amples of such targets include speed of the vehicle, steering wheel torque, or GPS trajectory. It can also take a richer form such as a sequence of scene segmentation maps. Here, we use b l to denote the l-th additional target label. The function becomes F :</p><formula xml:id="formula_0">x ? (p, {b l } L l=1 ),</formula><p>where L is the number of additional tasks.</p><p>In this study, we follow existing practices to train the proposed FM-Net to make prediction on steering angle, speed of the vehicle, and steering wheel torque. The ground-truth and metadata are readily available from many benchmark datasets such as Udacity <ref type="bibr" target="#b20">(Udacity 2018)</ref> and Comma.ai <ref type="bibr" target="#b18">(Santana and Hotz 2016)</ref>. The key difference of our work is that we regularize the learning of our model by requiring it to approximate the features extracted from different layers of heterogeneous auxiliary networks. We introduce our method in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-layer Feature Mimicking from Heterogeneous Networks</head><p>We first provide an overview of our framework that is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We denote the main network FM-Net as M . It is a 50-layer 3D ResNet with a Long-Short-Term Memory (LSTM) module <ref type="bibr" target="#b17">(Sak, Senior, and Beaufays 2014)</ref>. We provide architectural details of this network in the next subsection. At each time step, a sequence of N video frames x is fed to the main network. Fully-connected layers are introduced after the convolutional layers to transform the feature maps to a compact feature vector. To further capture temporal dynamics, which is crucial for smooth angle prediction, a LSTM module is added thereafter. Three vehicle state indicators, namely steering angle, speed of the vehicle, and steering wheel torque are predicted according to the extracted feature vectors and predicted vehicle state indicators of the last time step. These previous vehicle states will contribute to the prediction of our network. We next detail our approach on heterogeneous feature mimicking. Auxiliary networks. In our approach, apart from the main network, we assume a set of K heterogeneous auxiliary networks A = {A 1 , A 2 . . . A K } during training stage. These auxiliary networks are chosen from off-the-shelf networks that achieve strong performance in their respective task, e.g., PSPNet <ref type="bibr" target="#b24">(Zhao et al. 2017)</ref> for image segmentation and FlowNet2 <ref type="bibr" target="#b10">(Ilg et al. 2017;</ref><ref type="bibr" target="#b9">Hui, Tang, and Loy 2018)</ref> optical flow estimation. In this study, we use two auxiliary networks, i.e., PSPNet and FlowNet2. They represent a wellsuited choice of auxiliary networks as PSPNet captures the scene semantical structure while FlowNet2 encapsulates information on moving objects. It is interesting to see that both networks have strong generalization capability when they are applied on the unseen target data, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> and <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Mimicking features from both of these networks contribute to performance improvement of the main network, as we will show in our experiments. Note that although we show two auxiliary networks in our study, one can easily generalize to more networks. These auxiliary networks will be discarded after training.</p><p>Given an auxiliary network A k , we can extract features from its different feature layers. Formerly, we denote fea-  <ref type="bibr" target="#b10">(Ilg et al. 2017;</ref><ref type="bibr" target="#b9">Hui, Tang, and Loy 2018)</ref>. Transformation layers are introduced to ensure a low yet compatible spatial dimension for feature mimicking at low-middle-high mimicking paths. The input is a sequence of images and the prediction outputs are steering angles, speed of vehicle, and steering wheel torque. The details of the architecture and mimicking paths are shown in <ref type="table" target="#tab_1">Table 1</ref>.  tures extracted from j-th layer of A k as f jk . We use N A k to denote the number of layers from which features are extracted from A k . Here, f jk can be feature maps (output of convolution layers) or feature vectors (output of fully connected layers). To perform multi-layer feature mimicking, we pair each of the N A k layers of A k with a corresponding layer of the main network M . Similar to the way we obtain f jk from the auxiliary network, we extract features at designated layers of the main network and obtain features e j for its j-layer. To examine the usefulness of different layers more conveniently, we assume three paired levels low-middle-high from both the main network and auxiliary networks. These three levels are chosen according to the network depth (for high-level mimicking) and the size of receptive fields (for low-and middle-level mimicking). While our current design is shown effective, this is by no means the only option. More levels can be attempted. With different designs of transformation layers (described next), features of different scales are applicable too. In the experimental section, we will systematically examine the benefits of mimicking different individual level of features.</p><p>Transformation layers. We need to address two issues during the process of feature mimicking. Firstly, the source features generated by auxiliary networks are of high dimensionality. For instance, the output dimension of the first convolution block of PSPNet is 179?179?128. Simply approximat-ing the features would cause difficulty in training the main network. Secondly, we wish to retain the spatial information encoded in the feature maps extracted from both main network and auxiliary networks. The spatial information is crucial to provide contextual information of a driving scene. To address the two issues, we propose to insert two transformation layers between each of the low-middle-high mimicking paths, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. A transformation layer ? is designated for the main network, whilst another type of transformation layer ? is designed for auxiliary networks. The common goals of these transformation layers are (1) to reduce the dimensionality of the original feature maps, and (2) to retain sufficient spatial information of the original feature maps. The parameters in these transformation layers are learned in the training process and add little extra computation to training.</p><p>For the transformation layer ? of main network, we use 1?1 convolution operation to reduce the number of channels and an upsampling/downsampling operation to alter the size of features. For the transformation layer ? of auxiliary networks, we employ an average pooling operation to reduce the number of channels and an upsampling/downsampling operation. If the sizes of main network's features or auxiliary networks' features are the same as the targeted sizes, then we do not perform upsampling/downsampling operation. Note that average pooling is used instead of 1 ? 1 convolution because we need a deterministic target for training the FM-Net. Average pooling meets this purpose while compacting the redundant features of auxiliary networks. Loss. The overall loss comprises of three terms: (1)</p><formula xml:id="formula_1">L = L steer (p,p) steering loss + L l=1 ? l L multi (b l ,b l ) multi-task loss + K k=1 N A k j=1 ? k L mimic (?(e j ), ?(f jk ))</formula><p>The first term is the steering angle prediction loss, which is typically defined as a L2 loss, andp are angle predictions produced by the main network. The second term is the multitask loss, whereb l are predictions on the l-th task. Note that there are a total of L tasks. In our case, L = 2 as we have speed and torque predictions as the additional tasks. The third term is the feature mimicking loss, which we use L2 loss. Transformation operations are represented as ?(?) and ?(?). The parameters ? l and ? k balance the influence of multi-task loss and feature mimicking loss on the final prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Initialization and Multi-Stage Training</head><p>From our experiments, we found that a good initialization is important to the convergence of a very deep 3D CNN since the parameters of the network are significantly more in comparison to a 2D network. In our experiments, without a proper initialization, the FM-Net (50-layer 3D ResNet) suffers from convergence problem and even yields a result poorer than a shallow network. In this work, we follow (Carreira and Zisserman 2017) to initialize our network. Specifi-cally, we first load the weights of a 2D ResNet-50 model that has been pre-trained on ImageNet <ref type="bibr" target="#b12">(Krizhevsky, Sutskever, and Hinton 2012)</ref> in our 3D network. We then copy the weights of a w ? w kernel w times along the time dimension and normalize the weights by w so that a sequence of video frames will get the same response as it goes through a 2D network. Besides, the stride of both convolution layers and max-pooling layers is set as 1 in the temporal dimension so that the input sequence length does not decrease.</p><p>After initialization, we train the network based on Eqn. (1). We found that a multi-stage training scheme works well in practice. In particular, the loss defined in Eqn. (1) contains three terms. In the first stage, we optimize FM-Net based on the first two terms L steer and L multi . In the second stage, we train the network by using all the terms including L mimic . Introducing L mimic at the very beginning of the training yields slightly inferior results to the proposed twostage strategy. We conjecture that feature mimicking from heterogeneous networks (i.e., optimizing against L mimic ) a relatively harder and more complex task in comparison to learning steering angles, speed, and torque. Thus the main network should behave relatively well in steering angle prediction before performing feature mimicking, else feature mimicking would be less efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Datasets. We perform evaluations on two standard benchmarks widely-used in the community, namely Udacity <ref type="bibr" target="#b20">(Udacity 2018)</ref> and Comma.ai <ref type="bibr" target="#b18">(Santana and Hotz 2016)</ref> for evaluation. They are the largest steering angle prediction datasets by far. Note that the Berkeley Deep Drive (BDD) dataset ) provides vehicle turning directions (i.e., go straight, stop, turn left / right) instead of steering wheel angles. Nonetheless, we conducted experiments on this dataset and provide the results. We show that heterogeneous feature mimicking still helps even in a much larger dataset (BDD dataset provides more than 7 M video frames).</p><p>The Udacity dataset is mainly composed of video frames taken from urban roads. It provides a total number of 404,916 video frames for training and 5,614 video frames for testing. This dataset is challenging due to severe lighting changes, sharp road curves and busy traffic. The images of Comma.ai dataset are mainly captured from highway and urban roads. It contains 11 video clips within 7.5 hours driving. Busy traffic conditions make this dataset challenging. Note that there is no official and publicly available partition setting for this dataset. For fair comparisons, we benchmark our method and variants using a common setting. Specifically, we use 5% of each of the 11 clips for validation and testing, chosen randomly as a continuous chunk. The remaining frames are used for training. As pre-processing, we downsample the clips by a factor of two in time and remove video frames whose speed is less than 15 m/s to discard erroneous steering readings. As a result, we obtain a total number of 341,663 frames for training and 23,642 frames for testing. We release the data partitions on our project page 3 . Some typical frames of these two datasets are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. Implementation details. To facilitate the training of our network, the pixel values of input video frames are normalized to lie in <ref type="bibr">[-1, 1]</ref>. Frames in Udacity are resized to 160 ? 160. We follow the common practice <ref type="bibr" target="#b20">(Udacity 2018)</ref> to use video clips of 10 frames each as inputs (i.e., N = 10). The balancing parameters ? l is set as 1.0 for both the speed and torque prediction tasks, while ? k is set as 0.2 for all auxiliary networks. In Udacity, three vehicle states, i.e., steering angle, torque and speed are used as targets, while we only use steering angle and speed in Comma.ai, since it does not provide steering torque. A training batch for our network contains 16 video clips. The learning rate is set as 10 ?4 in first 30 training episodes and reduced to 10 ?6 thereafter. Evaluation metrics. We follow existing studies and use mean absolute error (MAE) and root mean square error (RMSE) as metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparative Evaluations</head><p>We compare the proposed method with state-of-the-art approaches on two publicly available datsets, i.e., Udacity and Comma.ai. The results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. Udacity. We compare with (Kim and Canny 2017) based on the results reported in their paper. We obtain the results of (Udacity 2018) by using its codes shared by the top team on official Udacity GitHub 4 . All baselines use the same train/test partition. The baseline 3D CNN + LSTM <ref type="bibr" target="#b20">(Udacity 2018)</ref> is the best existing method on this dataset. It is evident from <ref type="table" target="#tab_2">Table 2</ref> that a deeper model (our 50-layer ResNet) is advantageous than 3D CNN. In particular, the MAE is reduced from 2.5598 to 1.9167, a relative improvement of 25%. Adding LSTM to model the temporal information further improves the result of 3D ResNet from MAE of 1.9167 to 1.7147. The best result is yielded by the proposed FM-Net, which is based on 3D ResNet + LSTM but further enhanced with heterogeneous feature mimicking. Comma.ai. Making comparisons on this dataset is more challenging as there are no official or publicly available train/test partition settings. Owing to this reason, we did not include the results reported by <ref type="bibr" target="#b11">Kim et al. (Kim and Canny 2017)</ref> in <ref type="table" target="#tab_2">Table 2</ref> to avoid unfair comparison. Based on our own partition setting, we run the code of the best baseline in Udacity, i.e., 3D CNN + LSTM <ref type="bibr" target="#b20">(Udacity 2018)</ref>, on this data and report its results. Again, 3D ResNet and 3D ResNet + LSTM outperform the shallower baselines, and FM-Net with heterogeneous feature mimicking achieves the best performance. Noticeably, with feature mimicking, we bring down the MAE by 12% (from 0.7989 to 0.7048), which is significant. BDD100K. As can be seen from <ref type="table" target="#tab_2">Table 2</ref>, it is apparent that feature mimicking can bring considerable performance gains to 3D ResNet + LSTM and outperforms all previous algorithms. The results validate the effectiveness of our proposed feature mimicking method. Qualitative results. We show qualitative results of FM-Net with and without feature mimicking in <ref type="figure" target="#fig_5">Fig. 5</ref>. It can be observed that FM-Net with heterogeneous feature mimicking 4 https://github.com/udacity/self-driving-car/.  <ref type="bibr" target="#b11">(Kim and Canny 2017)</ref>. Note that the proposed FM-Net is based on 3D ResNet+LSTM but further enhanced with heterogeneous feature mimicking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Udacity MAE RMSE CNN + FCN ? <ref type="bibr" target="#b1">(Bojarski et al. 2016)</ref> 4.1200 4.8300 CNN + LSTM <ref type="bibr" target="#b11">(Kim and Canny 2017)</ref> 4.1500 4.9300 CNN + Attention <ref type="bibr" target="#b11">(Kim and Canny 2017)</ref>   <ref type="bibr" target="#b21">(Xu et al. 2017)</ref> 82.03% CNN + LSTM <ref type="bibr" target="#b21">(Xu et al. 2017)</ref> 81.23% 3D CNN + LSTM <ref type="bibr" target="#b20">(Udacity 2018)</ref> 82.94% 3D ResNet + LSTM (ours) 83.69% FM-Net (ours) 85.03%</p><p>yields much stable manoeuvre in driving albeit challenging road conditions including curved roads and extremely dark scenes. The observations are consistent on both Udacity and Comma.ai datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>The effectiveness of heterogeneous feature mimicking.</p><p>We summarize the performance of mimicking features from different layers of auxiliary networks in <ref type="table">Table.</ref> 3. We have a few observations. (1) Feature mimicking is beneficial since the one without feature mimicking results in the lowest performance.</p><p>(2) High-level feature mimicking (PH+FH) brings slightly more benefits than mid-(PM+FM) and lowlevel (PL+FL) feature mimicking judging from RMSE. This may be explained from the observation that high-level features contain more semantical meanings than those of midand low-levels, as supported by the feature embeddings shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Another reason could be that it is more fruitful to regularize the high-level features of FM-Net rather than the mid-and low-level features.</p><p>(3) Comparing PH + PM + PL and FH + FM + FL, we see no obvious difference between using either PSPNet or FlowNet2 as the sole auxiliary network although the results obtained from using FlowNet2 is marginally better. (4) The best performance is achieved when we use both auxiliary networks and activate   the low-middle-high mimicking paths. Mimicking features at different levels from different networks help FM-Net to capture more diverse contextual information, e.g. object motion and scene structure, at different feature resolutions. <ref type="table" target="#tab_4">Table 3</ref> studies some representative combinations of mimicking paths. Next, we further examine the performance of FM-Net when we only allow a single mimicking path chosen from low-middle-high of an auxiliary network. The results are shown in <ref type="figure">Fig. 6</ref>. It is observed that high-level mimicking paths are generally superior to the mid-and low-level paths. It is interesting to learn that regularizing low-level features with mimicking brings more benefits than mid-level features do. This is an intriguing observation that worths further investigations.</p><p>Feature mimicking v.s. pre-training. Pre-training is an alternative approach to introduce a side task indirectly without performing annotations on the target set -we can pretrain the FM-Net using the same image segmentation task on Cityscape dataset <ref type="bibr" target="#b6">(Cordts et al. 2016)</ref> as in the PSP-Net and subsequently fine-tune FM-Net on the steering an-  <ref type="figure">Figure 6</ref>: Comparative results of activating a single mimicking path chosen from low-middle-high of an auxiliary network.</p><p>gle prediction task. In this way, we wish to observe if the network could still benefit from the segmentation task. We compare feature mimicking with this approach and report the results in <ref type="table" target="#tab_6">Table 4</ref>. We include FM-Net without both Cityscape pre-training 5 and feature mimicking as a baseline. In this comparison, the FM-Net variant with feature mimicking only mimics features from PSPNet (i.e., PH+PM+PL in <ref type="table" target="#tab_4">Table 3</ref>). All three methods in <ref type="table" target="#tab_4">Table 3</ref> used ImageNet initialization. As can be observed, pre-training only yields very marginal improvement. By contrast, feature mimicking brings a higher gain to FM-Net. The results suggest that this na?ve pre-training scheme may not be the most effective way in our problem context: (1) the side task pre-training employs Cityscape, which introduces a domain gap when we applied the pre-trained network on Udacity and Comma.ai;</p><p>(2) the network structure of FM-Net is not optimal for direct learning from the image segmentation task. Feature mimicking alleviates the two aforementioned issues as it approximates PSPNet's features by using target data as input. And it focuses to approximate low-middle-high features well rather do well on the segmentation tasks itself, thus network architecture becomes a less crucial issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Contextual learning from side networks is a meaningful exploration not attempted before. We have presented a novel scheme of training very deep 3D CNN for the task on endto-end steering angle prediction. Specifically, we found that approximating multi-level features from heterogeneous auxiliary networks provide strong supervisory signals and regularization to the main network. In our experiments, we have shown that PSPNet and FlowNet2 help our FM-Net to learn better in capturing contextual information such as scene structure and object motion. With the proposed heterogeneous feature mimicking, the proposed FM-Net achieves a new state-of-the-art on both Udacity and Comma.ai benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Failure cases of baseline model. A baseline model (3D CNN + LSTM)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Feature Mimicking from Heterogeneous Auxiliary Network. The main network is FM-Net that takes an architecture of 3D ResNet + LSTM. The two auxiliary networks are PSPNet<ref type="bibr" target="#b24">(Zhao et al. 2017)</ref> and FlowNet2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Auxiliary networks PSPNet and FlowNet2 show satisfactory generalization on unseen target data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of FM-Net with and without heterogeneous feature mimicking (FM) on (a) Udacity and (b) Comma.ai test sets. The top row shows steering angles over time (x axis denotes frame and y axis represents steering angle).Ground-truth (gt) is represented in blue line. We selected a few representative examples to highlight the advantages of using feature mimicking. The bar charts in the second row provide closer observations on the predictions and errors (represented by the error bar).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>PSPNet FlowNet2 Input frame Output First frame Second frame Output Udacity Comma.ai Input frame Output</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The different dimensions (w ? w ? c, where w ? w is the size of feature maps and c is the number of channels) produced by the transformation layers at low-middlehigh mimicking paths. Here, the output dimension of PSP-Net is the same for Udacity and Comma.ai, but different on FlowNet because PSPNet normalizes the input image to the same resolution so there is no discrepancy of output dimensions on Udacity and Comma.ai.</figDesc><table><row><cell cols="4">Auxiliary Mimicking Output Dimension (w ? w ? c)</cell></row><row><cell>Networks</cell><cell>Path</cell><cell>Udacity</cell><cell>Comma.ai</cell></row><row><cell></cell><cell>Low</cell><cell cols="2">32 ? 40 ? 16 12 ? 20 ? 32</cell></row><row><cell>FlowNet</cell><cell>Middle</cell><cell>8 ? 10 ? 32</cell><cell>12 ? 20 ? 32</cell></row><row><cell></cell><cell>High</cell><cell>8 ? 10 ? 32</cell><cell>3 ? 5 ? 64</cell></row><row><cell></cell><cell>Low</cell><cell cols="2">30 ? 30 ? 16</cell></row><row><cell>PSPNet</cell><cell>Middle</cell><cell cols="2">30 ? 30 ? 16</cell></row><row><cell></cell><cell>High</cell><cell cols="2">30 ? 30 ? 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art methods on Udacity, Comma.ai and BDD100K datasets. ? indicates the results are copied from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Performance comparison of mimicking features</cell></row><row><cell cols="3">from different layers of auxiliary networks. We use "P" and</cell></row><row><cell cols="3">"F" to denote PSPNet and FlowNet, respectively. The abbre-</cell></row><row><cell cols="3">viation is used along with "L" and "M" and "H" to represent</cell></row><row><cell cols="3">low-middle-high . For instance, PSPNet feature mimicking</cell></row><row><cell cols="3">(high-level) is abbreviated as "PH". Full feature mimicking</cell></row><row><cell cols="2">means "PH + PM + PL + FH + FM + FL".</cell><cell></cell></row><row><cell>Method</cell><cell>Udacity MAE RMSE</cell><cell>Comma.ai MAE RMSE</cell></row><row><cell>Without feat. mimick</cell><cell cols="2">1.7147 2.4899 0.7989 1.1519</cell></row><row><cell>PH + FH</cell><cell cols="2">1.6826 2.4013 0.7514 1.0836</cell></row><row><cell>PM + FM</cell><cell cols="2">1.6928 2.4659 0.7749 1.0836</cell></row><row><cell>PL + FL</cell><cell cols="2">1.6869 2.4521 0.7627 1.1204</cell></row><row><cell>PH + PM + PL</cell><cell cols="2">1.6653 2.3847 0.7315 1.0574</cell></row><row><cell>FH + FM + FL</cell><cell cols="2">1.6573 2.3746 0.7259 1.0215</cell></row><row><cell cols="3">With full feat. mimick 1.6236 2.3549 0.7048 0.9831</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparing heterogeneous feature mimicking and network pre-training. Variants (A) without both Cityscape pre-training and feature mimicking, (B) with Cityscape pretraining only, and (C) with feature mimicking only.</figDesc><table><row><cell>FM-Net</cell><cell>Udacity MAE RMSE</cell><cell>Comma.ai MAE RMSE</cell></row><row><cell cols="3">Variant A 1.7147 2.4899 0.7989 1.1519</cell></row><row><cell cols="3">Variant B 1.7125 2.4614 0.7842 1.0908</cell></row><row><cell cols="3">Variant C 1.6653 2.3847 0.7315 1.0574</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code is available at https://cardwing.github.io/projects/FM-Net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Project page: https://cardwing.github.io/projects/FM-Net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The mIOUs of FM-Net with Cityscape pre-training only (ResNet-50, with Large FOV, without data augmentation, ASPP and CRF) in the validation and testing set of Cityscape are 67.2 and 66.4, respectively, which are comparable with the state-of-the-art method(Chen et al. 2018) (ResNet-101, with data augmentation,  Large FOV, ASPP and CRF, validation: 71.4, testing: 70.4).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Del Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep steering: Learning endto-end driving model from spatial and temporal visual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chowdhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pankaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zipser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05581</idno>
		<title level="m">Multi-modal multi-task deep learning for autonomous driving</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STAT</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interpretable learning for selfdriving cars by visualizing causal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2942" to="2950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7341" to="7349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of motion planning and control techniques for self-driving urban vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>??p</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yershov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="55" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Alvinn: An autonomous land vehicle in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="305" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning a driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01230</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saurabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Judy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jitendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Udacity</surname></persName>
		</author>
		<ptr target="https://github.com/udacity/self-driving-car/.Accessed" />
		<imprint>
			<date type="published" when="2018-02-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2174" to="2182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-toend multi-modal multi-task vehicle control for self-driving cars with visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<title level="m">BDD100K: A diverse driving video database with scalable annotation tooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
