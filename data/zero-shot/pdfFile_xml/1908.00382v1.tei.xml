<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Science IntelliCloud Technology Co.Ltd</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Dalian University of Technology</orgName>
								<orgName type="institution" key="instit2">? University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric occupancy and semantic category of a 3D scene. It helps intelligent devices to understand and interact with the surrounding scenes. Due to the high-memory requirement, current methods only produce low-resolution completion predictions, and generally lose the object details. Furthermore, they also ignore the multiscale spatial contexts, which play a vital role for the 3D inference. To address these issues, in this work we propose a novel deep learning framework, named Cascaded Context Pyramid Network (CCPNet), to jointly infer the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The proposed CCPNet improves the labeling coherence with a cascaded context pyramid. Meanwhile, based on the low-level features, it progressively restores the fine-structures of objects with Guided Residual Refinement (GRR) modules. Our proposed framework has three outstanding advantages: (1) it explicitly models the 3D spatial context for performance improvement; (2) full-resolution 3D volumes are produced with structure-preserving details;</p><p>(3) light-weight models with low-memory requirements are captured with a good extensibility. Extensive experiments demonstrate that in spite of taking a single-view depth map, our proposed framework can generate high-quality SSC results, and outperforms state-of-the-art approaches on both the synthetic SUNCG and real NYU datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human can perceive the real-world through 3D views with partial observations. For example, one can capture the geometry of rigid objects by only seeing the corresponding 2D images. Thus, understanding and reconstructing a 3D scene from its partial observations is a valuable technique for many computer vision and robotic applications, such as object localization, visual reasoning and indoor navigation. As an encouraging direction, Semantic Scene Completion (SSC) has draw more and more attentions in recent years. It aims to simultaneously predict the volumetric occupancy * Prof. Lu is the corresponding author. Email: lhchuan@dlut.edu.cn. and semantic category of a 3D scene. Given a single depth image, several outstanding works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref> have been proposed for single-view SSC. By designing 3D Convolutional Neural Networks (CNNs), these methods can automatically predict the semantic labels or complete 3D shapes of the objects in the scene. However, it is not a trivial task to utilize 3D CNNs for the SSC task. Vanilla 3D CNNs are locked in the cubic growth of computational and memory requirements with the increase of voxel resolution. Thus, current methods inevitably limit the resolution of predictions and the depth of 3D CNNs, which leads to wrong labels and missing shape details in the completion results.</p><p>To achieve better SSC results, several works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20]</ref> introduce the 2D semantic segmentation as an auxiliary, which takes an additional RGB image and applies complex 2D CNNs for semantic enhancement. These methods can fully exploit the high-resolution input, however, they ignore the 3D context information of the scene. Thus, only based on the 2D input image, they may not infer the invisible object parts of the complex scene. Recently, Song et al. <ref type="bibr" target="#b31">[32]</ref> show that global 3D context helps the prediction of SSC. However, the 3D CNN used in their work simply adopts the dilated convolutions <ref type="bibr" target="#b33">[34]</ref>, and concatenates the multi-stage features for predictions. It only considers the global semantics, which result in low-resolution predictions, and lose the scene details. In this work, we find that both local geometric details and multi-scale 3D contexts of the scene play a vital role in the SSC task. The local geometric details help the SSC system to identify the fine-structured objects. The multi-scale 3D contexts can enhance the spatial coherence and infer the occluded objects from the scene layout. However, designing a framework that can efficiently integrate both characteristics is still a challenging task.</p><p>To address above problems, we propose a novel deep learning framework, named Cascaded Context Pyramid Network (CCPNet), for single depth image based SSC. The proposed CCPNet effectively learns both local geometry details and multi-scale 3D contexts from the training dataset. For semantic confusing objects, the CCPNet improves the prediction coherence with an effective self-cascaded context pyramid. The self-cascaded pyramid helps the model to reduce the semantic gap of different contexts and cap-ture the hierarchical dependencies among the objects and scenes <ref type="bibr" target="#b23">[24]</ref>. In addition, we introduce a Guided Residual Refinement (GRR) module to progressively restore the finestructures of complex objects. The GRR corrects the latent fitting using low-level features, and avoids the high computational cost and memory consumption of the 3D CNN. With this module, the CCPNet can output full-resolution completion results and show much better accuracy than vanilla 3D networks. Experimental results demonstrate that our approach outperforms other state-of-the-art methods on both synthetic and real datasets. With only a single depth map, our method generates high-quality SSC results with much better accuracy and faster inference.</p><p>In summary, our contributions are three folds:</p><p>? We propose a novel cascaded context pyramid network (CCPNet) for efficient 3D semantic scene completion. The CCPNet automatically integrates both local geometric details and multi-scale 3D contexts of the scene in a self-cascaded manner.</p><p>? We also propose an efficient guided residual refinement (GRR) module for restoring fine-structures of objects and full-resolution predictions. The GRR progressively refines the objects with low-level features and light-weight residual connections, improving both computational efficiency and completion accuracy.</p><p>? Extensive experiments on public synthetic and real benchmarks demonstrate that our proposed approach achieves superior performance over other state-of-theart methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review related work on analyzing and completing a 3D scene from depth images. For more details, we refer the readers to <ref type="bibr" target="#b16">[17]</ref> for a survey of deep learning based 3D data processing.</p><p>Semantic Scene Analysis. In recent years, many deep learning based methods have been proposed for semantic scene analysis with a depth image or RGB-D image pair. In general, 2D image-based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref> treat the depth image as additional information, and adopt complex 2D CNNs for semantic scene analysis tasks, e.g., salient object detection, semantic segmentation and scene completion. Meanwhile, several works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1]</ref> extract deep features from the depth image and the RGB image separately, then fuse them for multi-mode complementarity. Although effective, 2D image-based methods ignore the spatial occupancy of objects, and can not fully exploit the depth information. While 3D volume-based methods usually convert the depth image into a volumetric representation, and exploit rich handcrafted 3D features <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref> or learned 3D CNNs <ref type="bibr" target="#b30">[31]</ref> for detecting 3D objects. Although existing methods can detect and segment visible 3D objects and scenes, they cannot infer the objects that are totally occluded. Instead, our method can predict the semantic labels and 3D shapes for both visible and invisible objects.</p><p>3D Scene Completion. Semantic scene completion is a fundamental task in understanding 3D scenes. To achieve this goal, Zheng et al. <ref type="bibr" target="#b40">[41]</ref> first complete the occluded objects with a set of pre-defined rules, and then refine the completion results by physical reasoning. Geiger and Wang <ref type="bibr" target="#b5">[6]</ref> propose a high-order graphical model to jointly reason about the layout, objects and superpixels in the scene image. Their model leverages detailed 3D geometry of scenes, and explicitly enforces occlusion and visibility constraints. Then, Firman et al. <ref type="bibr" target="#b3">[4]</ref> utilize the random forest to infer the occluded 3D object shapes from a single depth image. These methods are based on handcrafted features, and perform semantic scene segmentation and completion in two separate steps. Recently, Song et al. <ref type="bibr" target="#b31">[32]</ref> propose the Semantic Scene Completion Network (SSCNet) to simultaneously predict the semantic labels and volumetric occupancy of the 3D objects from a single depth image. Although this method unifies the semantic segmentation and voxel completion, the expensive 3D CNN limits the input resolution and network depth. Thus the SSCNet only produces low-resolution predictions and generally lacks of object details. By combining the 2D CNN and 3D CNN, Guo and Tong <ref type="bibr" target="#b9">[10]</ref> propose the View-Volume Network (VVNet) to efficiently reduce the computation cost and enhance the network depth. Garbade et al. <ref type="bibr" target="#b4">[5]</ref> propose a two-stream approach that jointly leverages the depth and semantic information. They first construct an incomplete 3D semantic tensor for the inferred 2D semantic information, and then adopt a vanilla 3D CNN to infer the complete 3D semantic tensor. Liu et al. <ref type="bibr" target="#b22">[23]</ref> propose a task-disentangled framework to sequentially carry out the 2D semantic segmentation, 2D-3D re-projection and 3D semantic scene completion. However, their multi-stage method may cause the error accumulation, producing mislabeling completion results. Similarly, Li et al. <ref type="bibr" target="#b19">[20]</ref> introduce a Dimensional Decomposition Residual Network (DDRNet) for the 3D SSC task. Based on the factorized and dilated convolutions <ref type="bibr" target="#b1">[2]</ref>, they utilize the multiscale feature fusion mechanism for depth and color images.</p><p>Although effective, current methods only consider the global semantics, which usually result in low-resolution predictions and lose the scene details. Different from previous works, we propose to integrate both local geometric details and multi-scale 3D contexts of the scene for the SSC task. To reduce the semantic gaps of multi-scale 3D contexts, we propose a self-cascaded context aggregation method to generate coherent labeling results. Meanwhile, the local geometric details are also incorporated to identify  the fine-structured objects in a coarse-to-fine manner. We note that the proposed modules are general-purpose for 3D CNNs. Thus, they can be easily applied to other 3D tasks. Given a single-view depth map of a 3D scene, the goal of our CCPNet is to map the voxels in the view frustum to one of the semantic labels C = [c 0 , c 1 , ..., c N +1 ], where N is number of semantic categories and c 0 stands for empty voxels. Our CCPNet is a self-cascaded pyramid structure to successively aggregate multi-scale 3D contexts and local geometry details for full-resolution scene completions. It consists of three key components, i.e., 3D Dilated Convolution Encoder (DCE), Cascaded Context Pyramid (CCP), and Guided Residual Refinement (GRR). Functionally, the DCE adopts multiple dilated convolutions with separated kernels to extract 3D feature representations from singleview depth images. Then, the CCP performs the sequential global-to-local context aggregation to improve the labeling coherence. After the context aggregation, the GRR is introduced to refine the target objects using low-level features learned by the shallow layers. In the following subsections, we will describe these components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cascaded Context Pyramid Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Dilated Convolution Encoder</head><p>Input Tensor Generation. For the input of our frontend 3D DCE, we follow previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> and rotate the 3D scene to align with the gravity and room orientation based on the Manhattan assumption. We consider the absolute dimensions of the 3D space with 4.8 m horizontally, 2.88 m vertically, and 4.8 m in depth. Each 3D scene is encoded into a flipped Truncated Signed Distance Function (fTSDF) <ref type="bibr" target="#b31">[32]</ref> with grid size 0.02 m, truncation value 0.24 m, resulting in a 240 ? 144 ? 240 tensor as the network input. Our method produces the completion result with the same resolution as input. However, due to the fully convolutional structure and the light-weight network design, our method certainly can take larger depth images as input, even full-resolution depth maps (e.g., 427?561 from depth sensors). During the model training, we render depth maps from virtual viewpoints of 3D scenes and voxelize the full 3D scenes with object labels as ground truth.</p><p>Encoder Structure. Processing 3D data needs large memories and huge computations. To reduce the memoryrequirement, we propose a light-weight encoder to extract the 3D feature representations of scenes, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. As demonstrated in dense labeling tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37]</ref>, large contexts can provide valuable information for understanding the scenes. For the 3D scenes and depth images, spatial context is more useful due to the lack of high frequency signals. To effectively learn spatial contextual information, we make sure our encoder has a big enough receptive field. A direct method is using the 3D dilated convolution proposed in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32]</ref>, which can exponentially expand the receptive field without a loss of resolution or coverage. However, the computation of 3D dilated convolutions is rather huge, because we need to perform convolutions with large volumes. To address this problem, we propose the 3D dilated convolutions with separated kernels. More <ref type="figure">Figure 2</ref>. Comparison of (a) Vanilla 3D convolution <ref type="bibr" target="#b17">[18]</ref>, (b) 3D dilated convolution <ref type="bibr" target="#b31">[32]</ref> and (c) Our proposed method.</p><formula xml:id="formula_0">H W C k k k d d C C W W H H s (a) (b) (c)</formula><p>specifically, we first separate the input tensor into several subvolumes, then apply the 3D dilated kernels to each subvolume for the convolutions. The reasons are two-fold. On the one hand, our method can reduce the model parameters and computations, and inherit all characteristics of dilated convolutions. On the other hand, our method considers the characteristic of depth profiles, in which the depth values are continuous only in neighbour regions. <ref type="figure">Fig. 2</ref> shows the differences of vanilla 3D convolution <ref type="bibr" target="#b17">[18]</ref>, 3D dilated convolution <ref type="bibr" target="#b31">[32]</ref> and our proposed method. To build our 3D DCE, we stack the proposed 3D dilated convolution several times with 3D pooling. Besides, to avoid the extreme separation, we reduce the number of subvolumes along with the network depth. The detailed parameters are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cascaded Context Pyramid</head><p>For scene completion, different objects have very different physical 3D sizes and visual orientations. This implies that the model needs to capture information at different contexts in order to recognize objects reliably. Besides, for confusing manmade objects in indoor scenes, obtaining coherent labeling results is not easily accessible, because they are of high intra-class variance and low inter-class variance. Therefore, it is insufficient to use only the single-scale and global information of the target objects <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24]</ref>. We need to introduce multi-scale context information, which characterizes the underlying dependencies between an object and its surroundings. However, it is very hard to retain the hierarchical dependencies in contexts of different scales, using common fusion strategies (e.g., direct stack <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40]</ref>). To address this issue, we propose a novel self-cascaded context pyramid architecture, as shown in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>. Different from previous methods, our method sequentially aggregates the global-to-local contexts while well retains the hierarchical dependencies, i.e., the underlying inclusion and location relationship among the objects and scenes in different scales.</p><p>Architecture Details. To build the context pyramid, we perform 3D dilated convolutions on the last pooling layer of the 3D DCE to capture multi-scale contexts. By setting varied dilation rates <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6</ref> and 1 in the experiments) and feature reduction layers, a series of 3D feature maps with global-to-local contexts are generated. The large-scale context contains more semantics and wider visual cues, while the small-scale context retains object geometry details. Meanwhile, the obtained feature maps with multi-scale contexts can be aligned automatically due to their equal resolution. To well retain the hierarchical dependencies of multi-scale contexts, we sequentially aggregate them in a self-cascaded pyramid manner. Formally, it can be described as:</p><formula xml:id="formula_1">3D DCE Context1 Context2 ContextN Context3 ? Aggregated Context 3D DCE Context1 Context2 ContextN Context3 ? (a) (b) Aggregated Context</formula><formula xml:id="formula_2">X sa = f (? ? ? f (f (X 1 ? X 2 ) ? X 3 ) ? ? ? ? ? X n ), d 1 &gt; d 2 &gt; d 3 &gt; ? ? ? &gt; d n .<label>(1)</label></formula><p>where X n denotes the n-scale context, X sa is the final aggregated context and d n is the dilation rate for extracting the context X n . ? denotes the element-wise summation. f denotes the Basic Residual Block (BRB) <ref type="bibr" target="#b15">[16]</ref>, as shown in <ref type="figure" target="#fig_3">Fig. 4 (a)</ref>. In our proposed method, we first aggregate the large-scale context with big dilation rates, then the context with small dilation rates. This aggregation rule is consistent with the human visual mechanism, i.e., large-scale context could play a guiding role in integrating small-scale context. We also notice that there are other outstanding structures for multi-scale contexts, such as PPM <ref type="bibr" target="#b39">[40]</ref> and ASPP <ref type="bibr" target="#b1">[2]</ref>, as shown in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>. In order to aggregate information with different contexts, they add a layer that parallelly concatenates the feature maps with different receptive fields:</p><formula xml:id="formula_3">X pa = g([X 1 , X 2 , X 3 , ? ? ? , X n ]), d 1 &gt; d 2 &gt; d 3 &gt; ? ? ? &gt; d n .<label>(2)</label></formula><p>where g denotes the aggregation function, which usually is</p><formula xml:id="formula_4">an 1 ? 1 ? 1 convolutional layer. [? ? ? ]</formula><p>is the concatenation operation in channel-wise. However, our proposed selfcascaded pyramid architecture has several advantages: 1) Our self-cascaded strategy enhances the hierarchical dependencies in different context scales. Thus, it is more effective than the parallel strategies such as PSPNet <ref type="bibr" target="#b39">[40]</ref>, DeepLab variants <ref type="bibr" target="#b1">[2]</ref>, which directly fuse the multi-scale contexts with large semantic gaps; 2) Our method introduces more complicated nonlinear operations (Equ. 1), thus it has a stronger capacity to model the relationship of different contexts than simple convolution operations. 3) By adopting the summation, the sequential aggregation significantly reduces the parameters and computations. Experiments also verify the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Guided Residual Refinement</head><p>Besides semantic confusing categories, fine-structured objects also increase the difficulty for accurate labeling in 3D scenes. However, current methods usually produce lowresolution predictions, thus it is very hard to retain the finegrained details of objects. To address this problem, we propose to reuse low-level features with the Guided Residual Refinement (GRR), as shown in the bottom of <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, the rich low-level features are progressively reintroduced into the prediction stream by guided residual connections. As a result, the coarse feature maps can be refined and the low-level details can be restored for fullresolution predictions. The used Guided Residual Block (GRB) is shown in <ref type="figure" target="#fig_3">Fig. 4 (b)</ref>, which can be formulated as:</p><formula xml:id="formula_5">X = X ? G,<label>(3)</label></formula><formula xml:id="formula_6">X rf = ReLu(X ?XT anh(X) ? h(X))<label>(4)</label></formula><p>= ReLu(X(I ? T anh(X)) ? h(X))</p><p>= ReLu(X G ? h(X)).</p><p>where X is the input semantic context feature and G is the guidance feature coming from a shallower layer. ? denotes the element-wise summation and h is the standard non-linear transform in residual blocks. X rf is the refined feature map. ReLu(?) and T anh(?) are the rectified linear unit and hyperbolic tangent activation, respectively. To restore finer details with the shallower layer, we first integrate the input feature and the guidance (Equ. 3), then we introduce an auxiliary connection to the BRB <ref type="bibr" target="#b15">[16]</ref>. More specifically, we use the hyperbolic tangent activation to amplify the integrated features (resulting inX G ), as shown in <ref type="figure" target="#fig_3">Fig. 4  (b)</ref> and Equ. 4-6. It is very beneficial to fuse low-level features by the guided refinement strategy. On the one hand, the feature maps of X and G represent different semantics at varied levels. Thus, due to their inherent semantic gaps, directly stacking all these features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref> may not be an efficient strategy. In the proposed method, the influence of semantic gaps is alleviated when a residual iteration strategy is adopted <ref type="bibr" target="#b6">[7]</ref>. On the other hand, the feature amplification connection enhances the effect of low-level details and gradient propagations, which helps the effectively end-to-end training. There also exist effective refinement strategies for detail enhancement <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref>. However, they are very different from ours. First, our strategy focuses on amplifying low-level features considering the 3D data properties, e.g., high computation and memory requirements. In contrast, previous methods introduce complex refinement modules, which are hardly executable for the 3D data. Besides, we only choose specific shallow layers for the refinement, as shown in the bottom of <ref type="figure" target="#fig_0">Fig. 1</ref>. Other methods incorporate all the hierarchical layers that inevitably contain boundary noises <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22]</ref>. To build our model, several GRB modules are elaborately embedded in the prediction part, which can greatly prevent the fitting residual from accumulating. As a result, the proposed CCPNet effectively works in a coarseto-fine labeling manner for full-resolution predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Training</head><p>Given the training dataset (i.e., the paired depth images and ground truth volumetric labels of 3D scenes), our proposed CCPNet can be trained in an end-to-end manner. We adopt the voxel-wise softmax loss function <ref type="bibr" target="#b31">[32]</ref> for the network training. The loss can be expressed as:</p><formula xml:id="formula_9">L(p, y) = i,j,k w ijk L sm (p ijk , y ijk ),<label>(7)</label></formula><p>where L sm is the softmax cross-entropy loss, y ijk is the ground truth label, p ijk is the predicted probability of the voxel at coordinates (i, j, k). The weight w ijk ? {0, 1} is used to balance the loss between different semantic categories. Due to the sparsity of 3D data, the ratio of empty vs. occupied voxels is extremely imbalanced. To address this problem, we follow <ref type="bibr" target="#b31">[32]</ref> and randomly sample the training voxels with a 2:1 ratio to ensure that each mini-batch has a balanced set of empty and occupied examples.  The real NYU dataset <ref type="bibr" target="#b28">[29]</ref> includes 1449 depth images captured by the Kinect depth sensor. The depth images are partitioned into 795 for training and 654 for test. Following previous works, we adopt the ground truth completion and segmentation from <ref type="bibr" target="#b8">[9]</ref>. Some labeled volumes and their corresponding depth images are not well aligned in the NYU dataset. Thus, we also use the NYU CAD dataset <ref type="bibr" target="#b3">[4]</ref>, in which the depth map is rendered from the label volume. The NYU dataset is challenging due to the unavoidably measurement errors in the depth images collected by Kinect. As in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>, we also pre-train the network on the SUNCG dataset before fine-tuning it on the NYU dataset.</p><p>Evaluation Metrics. We mainly follow <ref type="bibr" target="#b31">[32]</ref> and report the precision, recall, and Intersection over Union (IoU) of the compared methods. The IoU measures the overlapped ratio between intersection and union of the positive prediction volume and the ground truth volume. In this work, two tasks are considered: Scene Completion (SC) and Semantic Scene Completion (SSC). For the SC task, we treat all voxels as binary predictions, i.e., occupied or non-occupied. The ground truth volume includes all the occluded voxels in the view frustum. For the SSC task, we report the IoU of each class, and average them to get the mean IoU.</p><p>Training Protocol. We implement our CCPNet in the modified Caffe toolbox <ref type="bibr" target="#b18">[19]</ref> for 3D data processing. We perform experiments on a quad-core PC with an Intel i4790 CPU and one NVIDIA TITAN X GPU (12G memory). For the CCPNet, we initialize the weights by the "msra" method <ref type="bibr" target="#b14">[15]</ref>. During the training, we use the standard SGD method with a batch size 4, momentum 0.9 and weight decay 0.0005. We set the base learning rate to 0.01. For the SUNCG dataset, we train the CCPNet with 200K iterations and change the learning rate to 0.001 after 150K iterations. To reduce the performance bias, we evaluate the results at every 5K steps after 180K iterations, and average them as the final results. For both the NYU Kinect and NYU CAD datasets, we follow previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23]</ref>, and fine-tune the CPPNet pre-trained from the SUNCG dataset with 10K iterations. After that, we test the models at every 2K iterations and pick the best one as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparison on the SUNCG dataset.</head><p>For the SUNCG dataset, we compare our proposed CCPNet with SSCNet <ref type="bibr" target="#b31">[32]</ref>, VVNet <ref type="bibr" target="#b9">[10]</ref>, DCRF <ref type="bibr" target="#b35">[36]</ref>, ESSCNet <ref type="bibr" target="#b34">[35]</ref> and SATNet <ref type="bibr" target="#b22">[23]</ref> for both SC and SSC tasks. As shown in Tab. 1, our approach achieves the best performance in both SC and SSC tasks. Compared to the SSCNet, the overall   <ref type="table">Table 3</ref>. The performances of different scene completion methods on the NYU CAD dataset. The best results are in bold.</p><p>IoUs of our CCPNet significantly increase about 18% and 28% for SC and SSC tasks, respectively. In spite of taking a single depth map, our approach gets higher IoUs than the RGB-D based SATNet (Ours 91.4% vs. SATNet 78.5%). Our approach also perform better than the previous best ES-SCNet with a considerable margin. Tab. 1 also lists the IoU for each object category. Our approach also achieves the highest IoUs in each category. Thus, the quantitative results demonstrate that our approach is superior in 3D SSC. <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates the qualitative results on the SUNCG dataset. Although previous methods works well for many scenes, they usually fail in the objects which have complex structures and confusing semantics (the first and second rows). In contrast, our method leverages the low-level features and multi-scale contexts to overcome these difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison on the NYU dataset.</head><p>For the NYU dataset, we compare our CCPNet with other outstanding methods. Tab. 2 and Tab. 3 illustrate the performances on the NYU Kinect and NYU CAD datasets, respectively. From the results, we can see that our CCP-Net also achieves the best performance. For the SC task, it outperforms the SSCNet (8.4% on NYU Kinect and 12.1% on NYU CAD) when only the NYU dataset is used as the training data. Meanwhile, even the SSCNet uses the additional SUNCG training dataset, our CCPNet still achieves a substantial improvement (7% on NYU Kinect and 9.2% on NYU CAD). We observe that the SSCNet achieves a rather high recall but a low precision for the SC task. Our model pre-trained with the SUNCG dataset achieves better performances, and outperforms previous best methods, i.e., VVNet and SATNet, with a large margin.</p><p>For the SSC task, our approach achieves 41.3% on NYU Kinect and 55.0% on NYU CAD, and outperforms the SS-CNet [32] by 10.8% and 15%, respectively. With the same training data, our approach constantly performs better than existing best methods with a considerable margin. Tab. 2 and Tab. 3 also include the results of each category. In general, our method tends to predict more occluded voxels than previous methods, such as window, chair and furniture. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the qualitative results in which cluttered scene completions can be observed. Our method performs substantially better than other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Efficiency Analysis</head><p>Current methods usually depend on expensive 3D CNNs and feature concatenations, while our CCPNet utilizes a light-weight 3D dilated encoder and a self-cascaded pyramid. Thus, it significantly reduces memory requirement and computational cost for inference. Tab. 4 lists the parameters and computations of different methods. Our CCPNet achieves much better accuracy, and significantly reduces the model parameters, and speeds up for inference. </p><p>(1)    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To verify the effect of our proposed modules, we also perform ablation experiments on the SUNCG dataset.</p><p>Separated Convolution Kernels. Based on the SSC-Net <ref type="bibr" target="#b31">[32]</ref>, we replace the 3D dilated convolutions of SSCNet with our proposed separated kernels. For simplification, we set the number of subvolumes to 4. Tab. 5 shows the quantitative performances. For SC and SSC tasks, our method has fewer parameters and computations, while provides 3.3% and 6.1% IoU improvements compared to the SSCNet.</p><p>Cascaded Context Pyramid. To verify the effect of our CCP, we replace the CCP with the outstanding PPM <ref type="bibr" target="#b39">[40]</ref> and ASPP <ref type="bibr" target="#b1">[2]</ref> modules, and keep other modules unchanged. The first three rows of Tab. 6 show the quantitative results. With the PPM and ASPP, the IoUs of the CCPNet decrease 4.1% and 2.3% for the SC task, respectively. For the SSC task, it has a similar trend, which proves that our CCP is more effective. Note that the PPM and ASPP need more memories and parameters for the context aggregation.</p><p>Guided Residual Refinement. To evaluate the effect of our GRR, we compare the performances with different refinements. As shown in the 4-th row of Tab. 6, with the BRBs, the CCPNet shows worse results, decreasing 8.1% and 8.4% for SC and SSC, respectively. However, when introducing the guidance (the 5-th row), the model shows significant improvements for both SC and SSC tasks. Only  <ref type="table">Table 6</ref>. Ablation results of components on the SUNCG dataset.</p><p>with the feature amplification (the 6-th row), we observe a considerable improvement compared to the BRBs. A possible reason is that it is not enough for the detail recovery when only amplifying on the 3D context information. However, with the whole GRB, our approach shows best results. Full-Resolution Prediction. To evaluate the benefits of full-resolutions, we also re-implement our approach with the quarter and half resolution. To achieve this goal, we remove the corresponding layers after the deconvolution operations in <ref type="figure" target="#fig_0">Fig. 1</ref>. The last two rows of Tab. 6 show the performances. From the results, we can see that the lowresolution-based model shows worse performances. The main reason is that it cannot preserve the geometric details. However, our model still performs better than most state-ofthe-art methods. This further demonstrates the effectiveness of our proposed modules. With full-resolution outputs, our model can fully exploit the geometric details, improving the IoUs by 4.9% and 5.1% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel deep learning framework, named CCPNet, for full-resolution 3D SSC. The CCPNet is a self-cascaded pyramid structure to successively aggregate multi-scale 3D contexts and local geometry details. Extensive experiments on both synthetic and real benchmarks demonstrate that our CCPNet significantly improves the semantic completion accuracy, reduces the computational cost, and offers high-quality completion results with full-resolution. In the future work, we will explore color information for semantic and boundary enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of our Cascaded Context Pyramid Network (CCPNet). Taking a single-view depth map as input, the CCPNet predicts the occupancy and object labels for each voxel in the view frustum. With light-weight operations, the CCPNet can produce full-resolution 3D completion results. The convolution parameters are shown as (number of filters, kernel size, stride, dilation, number of subvolumes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>illustrates the overall architecture of our CCPNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of different multi-scale context aggregation methods. (a) Our self-cascaded context aggregation approach, which reduces the semantic gaps of different scales. (b) Existing parallel concatenations, such as PSPNet<ref type="bibr" target="#b39">[40]</ref>, Deeplab variants<ref type="bibr" target="#b1">[2]</ref>. "Context" denotes the dilated convolution for context extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The used residual modules in our CCPNet. (a) The Basic Residual Block (BRB)<ref type="bibr" target="#b15">[16]</ref>. (b) The proposed Guided Residual Block (GRB). In the GRB, we add a tangent function-based connection to amplify the fused features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Completion results with different methods on the SUNCG dataset. From the left to right: (a) Input Depth; (b) fTSDF Surface; (c) Ground Truth; (d) SSCNet [32]; (e) VVNet [10]; (f) ESSCNet [35]; (g) Ours. It can be observed that, our results constantly contain more accurate and detailed structures compared to the baselines. The figure is best viewed in color with 200% zooming-in. ground truth volumes. The test set consists of totally 470 pairs sampled from 170 non-overlap scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results on NYUCAD. From left to right: Input RGB-D image, ground truth, results obtained by our approach, and results obtained by SSCNet<ref type="bibr" target="#b33">[34]</ref>. Overall, our completed semantic 3D scenes are less cluttered and show a higher voxel class accuracy compared to SSCNet. Refer to Section 4.4 for the detailed analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Completion results with different methods on the NYU dataset. From the left to right: (a) Input Depth; (b) fTSDF Surface; (c) Ground Truth; (d) SSCNet [32]; (e) DDRNet [20]; (f) VVNet [10]; (g) Ours. The figure is best viewed in color with 200% zooming-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The synthetic SUNCG dataset<ref type="bibr" target="#b30">[31]</ref> consists of 45622 indoor scenes. Technically, the depth images and semantic scene volumes can be acquired by setting different camera orientations. Following previous useful works<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref>, we adopt the same training/test split for our network training and evaluation. More specifically, the training set contains about 150K depth images and the corresponding scene completion semantic scene completion Methods prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. SSCNet [32] 76.3 95.2 73.5 96.3 84.9 56.8 28.2 21.3 56.0 52.7 33.7 10.9 44.3 25.4 46.4 VVNet [10] 90.8 91.7 84.0 98.4 87.0 61.0 54.8 49.3 83.0 75.5 55.1 43.5 68.8 57.7 66.7 DCRF [36] ---95.4 84.3 57.7 24.5 28.2 63.4 55.3 34.5 19.6 45.8 28.7 48.8 ESSCNet [35] 92.6 90.4 84.5 96.6 83.7 74.9 59.0 55.1 83.3 78.0 61.5 47.4 73.5 62.9 70.5 The performances of different scene completion methods on the SUNCG dataset. The best results are in bold. Semantic scene completion results generated by different methods for SUNCG and NYU datasets. Justin Th JunYoung Gwak, Daeyun Shin, and Derek Hoiem. Com 3d object shape from one depth image. In CVPR, pages 2493, 2015. [ Silberman et al., 2012 ] Nathan Silberman, Derek Hoiem,</figDesc><table><row><cell>4. Experiments</cell></row><row><cell>4.1. Experimental Settings</cell></row><row><cell>Datasets.</cell></row></table><note>[ Lin et al., 2013 ] Dahua Lin, Sanja Fidler, and Raquel Urtasun. Holistic scene understanding for 3d object detection with rgbd cameras. In ICCV, pages 1417-1424, 2013.[ Liu et al., 2017 ] Fangyu Liu, Shuaipeng Li, Liqiang Zhang, Chenghu Zhou, Rongtian Ye, Yuebin Wang, and Jiwen Lu.[ Rock et al., 2015 ] Jason Rock, Tanmay Gupta,meet Kohli, and Rob Fergus. Indoor segmentation and s inference from rgbd images. In ECCV, pages 746-760, 20 [ Song and Xiao, 2016 ] Shuran Song and Jianxiong Xiao. Sliding Shapes for amodal 3D object detection in RGB ages. In CVPR, pages 808-816, 2016.[ Song et al., 2017 ] Shuran Song, Fisher Yu, Andy Zeng, A Chang, Manolis Savva, and Thomas Funkhouser. Semanti completion from a single depth image. In CVPR, pages</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. 56.6 15.1 94.6 24.7 10.8 17.3 53.2 45.9 15.9 13.9 31.1 12.6 30.5</figDesc><table><row><cell>scene completion</cell><cell cols="3">semantic scene completion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods prec. recall Lin et al. [21] Trained on NYU 58.5 49.9 36.4 0.0 Geiger and Wang [6] NYU 65.7 58.0 44.4 10.2 62.5 19.1 5.8 11.7 13.3 14.1</cell><cell>9.4 8.5</cell><cell>29.0 24.0 40.6 27.7</cell><cell>6.0 7.0</cell><cell>7.0 6.0</cell><cell>16.2 22.6</cell><cell>1.1 5.9</cell><cell>12.0 19.6</cell></row><row><cell cols="8">SSCNet [32] SSCNet [32] SSCNet [32] 92.9 CSSCNet [8] NYU 57.0 94.5 55.1 15.1 94.7 24.4 0.0 SUNCG 55.6 91.9 53.2 5.8 81.8 19.6 5.4 NYU+SUNCG 59.3 NYU+SUNCG 62.5 82.3 54.3 ----VVNet [10] NYU+SUNCG 69.8 83.1 61.1 19.3 94.8 28.0 12.2 19.6 57.0 50.5 17.6 11.9 35.6 15.3 32.9 12.6 32.1 35.0 13.0 7.8 27.1 10.1 24.7 12.9 34.4 26.0 13.6 6.1 9.4 7.4 20.2 -------27.5 DCRF [36] NYU ---18.1 92.6 27.1 10.8 18.8 54.3 47.9 17.1 15.1 34.7 13.0 31.8 TS3D,V2 [5] NYU 65.7 87.9 60.4 8.9 94.0 26.4 16.1 14.2 53.5 45.8 16.4 13.0 32.9 12.7 30.4 TS3D,V3+ [5] NYU 64.9 88.8 60.2 8.2 94.1 26.4 19.2 17.2 55.5 48.4 16.4 22.0 34.0 17.1 32.6 ESSCNet [35] NYU 71.9 71.9 56.2 17.5 75.4 25.8 6.7 15.3 53.8 42.4 11.2 0.0 33.4 11.8 26.7 SATNet [23] NYU+SUNCG 67.3 85.8 60.6 17.3 92.1 28.0 16.6 19.3 57.5 53.8 17.7 18.5 38.4 18.9 34.4 DDRNet [20] NYU 71.5 80.8 61.0 21.1 92.2 33.5 6.8 14.8 48.3 42.3 13.2 13.9 35.3 13.2 30.4 Ours NYU 74.2 90.8 63.5 23.5 96.3 35.7 20.2 25.8 61.4 56.1 18.1 28.1 37.8 20.1 38.5 Ours NYU+SUNCG 78.8 94.3 67.1 25.5 98.5 38.8 27.1 27.3 64.8 58.4 21.5 30.1 38.4 23.8 41.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The performances of different scene completion methods on the NYU Kinect dataset. The best results are in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">scene completion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">semantic scene completion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Trained on</cell><cell cols="14">prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg.</cell></row><row><cell>Zheng et al. [41] Firman et al. [4]</cell><cell>NYU NYU</cell><cell>60.1 66.5</cell><cell>46.7 34.6 69.7 50.8</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>SSCNet [32] SSCNet [32] VVNet [10] DCRF [36] TS3D,V2 [5] TS3D,V3+ [5] DDRNet [20] Ours Ours</cell><cell cols="2">NYU NYU+SUNCG 75.4 75.0 NYU+SUNCG 86.4 NYU -NYU 81.2 NYU 80.2 NYU 88.7 NYU 91.3 NYU+SUNCG 93.4</cell><cell cols="13">92.3 70.3 96.3 73.2 32.5 92.6 40.2 8.9 ----92.0 80.3 ------35.5 92.6 52.4 10.7 40.0 60.0 62.5 34.0 ----33.9 57.0 59.5 28.3 ----93.6 76.9 33.9 93.4 47.0 26.4 27.9 61.7 51.7 27.6 27.3 44.4 21.8 42.1 ----8.1 44.8 25.1 40.0 ----9.4 49.2 26.5 43.0 94.4 76.5 34.4 93.6 47.7 31.8 32.2 65.2 54.2 30.7 32.5 50.1 30.7 45.7 88.5 79.4 54.1 91.5 56.4 14.9 37.0 55.7 51.0 28.8 9.2 44.1 27.8 42.8 92.6 82.4 56.2 94.6 58.7 35.1 44.8 68.6 65.3 37.6 35.5 53.1 35.2 53.2 91.2 85.1 58.1 95.1 60.5 36.8 47.2 69.3 67.7 39.8 37.6 55.4 37.6 55.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Yuebin Wang, and Jiwen Lu. 3dcnn-dqn-rnn: A deep reinforcement learning framework for semantic parsing of large-scale 3d point clouds. In ICCV, pages 5678-5687, 2017. [Nguyen et al., 2016] D. T. Nguyen, B. S. Hua, M. K. Tran, Q. H. Pham, and S. K. Yeung. A field model for repairing 3d shapes. In CVPR, pages 5676-5684, 2016. [Qi et al., 2016] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, pages 652-660, 2016.</figDesc><table><row><cell>SUNCG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell>(c)</cell><cell></cell><cell>(d)</cell><cell></cell><cell>(e)</cell><cell></cell><cell></cell><cell>(f)</cell><cell>(g)</cell><cell></cell><cell></cell></row><row><cell>NYU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ceil.</cell><cell>Ceil</cell><cell>floor floor</cell><cell>wall wall</cell><cell>wind. window</cell><cell>Chair</cell><cell>chair bed</cell><cell>bed sofa</cell><cell>table</cell><cell>sofa</cell><cell>tvs</cell><cell>table furn</cell><cell>TVs objects</cell><cell>furn.</cell><cell>obj.</cell></row><row><cell cols="14">Figure 3: Semantic scene completion results generated by different methods for SUNCG and NYU datasets.</cell><cell></cell></row><row><cell cols="7">[Jimenez Rezende et al., 2016] Danilo Jimenez Rezende, S. M. Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, and Nicolas Heess. Unsupervised learning of 3d structure from im-ages. In NIPS, pages 4996-5004. 2016.</cell><cell cols="8">[Rock et al., 2015] Jason Rock, Tanmay Gupta, Justin Th JunYoung Gwak, Daeyun Shin, and Derek Hoiem. Com 3d object shape from one depth image. In CVPR, pages 2493, 2015.</cell></row><row><cell cols="7">[Kalogerakis et al., 2017] Evangelos Averkiou, Subhransu Maji, and Siddhartha Chaudhuri. 3D shape Kalogerakis, Melinos segmentation with projective convolutional networks. In CVPR, pages 3779-3788, 2017. [Lin et al., 2013] Dahua Lin, Sanja Fidler, and Raquel Urtasun. Holistic scene understanding for 3d object detection with rgbd cameras. In ICCV, pages 1417-1424, 2013.</cell><cell cols="8">[Silberman et al., 2012] Nathan Silberman, Derek Hoiem, meet Kohli, and Rob Fergus. Indoor segmentation and s inference from rgbd images. In ECCV, pages 746-760, 20 [Song and Xiao, 2016] Shuran Song and Jianxiong Xiao. Sliding Shapes for amodal 3D object detection in RGB ages. In CVPR, pages 808-816, 2016. [Song et al., 2017] Shuran Song, Fisher Yu, Andy Zeng, A Chang, Manolis Savva, and Thomas Funkhouser. Semanti completion from a single depth image. In CVPR, pages 1754, 2017.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">[Tulsiani et al., 2017] Shubham Tulsiani, Tinghui Zhou, Ale Efros, and Jitendra Malik. Multi-view supervision for view reconstruction via differentiable ray consistency. In pages 2626-2634, 2017.</cell></row><row><cell>(4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">[Wang et al., 2017] Weiyue Wang, Qiangui Huang, Suya Chao Yang, and Ulrich Neumann. Shape inpainting us works. In ICCV, pages 2298-2306, 2017. generative adversarial network and recurrent convolution</cell></row><row><cell cols="7">[Qi et al., 2017] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learn-ing on point sets in a metric space. In NIPS, pages 5105-5114, 2017. [Ren and Sudderth, 2016] Zhile Ren and Erik B. Sudderth. Three-dimensional object detection and layout prediction using clouds of oriented gradients. In CVPR, pages 1525-1533, 2016. (5)</cell><cell cols="8">[Wu et al., 2015] Zhirong Wu, Shuran Song, Aditya Khosl guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d sha A deep representation for volumetric shape modeling. In pages 1912-1920, 2015. 1704, 2016. construction without 3d supervision. In NIPS 2016, pages Perspective transformer nets: Learning single-view 3d ob [Yan et al., 2016] X. Yan, J. Yang, E. Yumer, Y. Guo, and H</cell></row><row><cell>(6)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">(a) RGB and Depth images</cell><cell cols="3">(b) Ground truth</cell><cell cols="2">(c) Ours</cell><cell></cell><cell></cell><cell cols="2">(d) Results of SSCNet</cell><cell></cell><cell></cell></row></table><note>[Liu et al., 2017] Fangyu Liu, Shuaipeng Li, Liqiang Zhang, Chenghu Zhou, Rongtian Ye,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Comparison of efficiency with different methods. Quantitative results on separated convolution kernels.</figDesc><table><row><cell>Methods</cell><cell cols="4">SC-IoU SSC-IoU Params/k FLOPs/G</cell></row><row><cell>SSCNet [32] SSCNet [32]+SK</cell><cell>73.5 76.8</cell><cell>46.4 52.5</cell><cell>930 532</cell><cell>163.8 100.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depthcomp: realtime depth image completion based on prior semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>I?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03550</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint 3d object and layout inference from a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Highway and residual networks learn unrolled iterative estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07771</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic scene completion combining colour and depth: preliminary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B S</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Predicting complete 3d models of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.02437</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">View-volume network for semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning advances in computer vision with 3d data: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ioannidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chatzilari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nikolopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rgbd based dimensional decomposition residual network for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic labeling in very high resolution images via a self-cascaded convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS JPRS</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1525" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient semantic scene completion network with spatial group convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="733" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic scene completion with dense crf from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="page" from="182" to="195" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep gated attention networks for large-scale street-level scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="702" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Agile amulet: Real-time salient object detection with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond point clouds: Scene understanding by reasoning geometry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
