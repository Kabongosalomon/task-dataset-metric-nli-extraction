<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Diffusion Models for High Fidelity Image Generation Figure 1: A cascaded diffusion model comprising a base model and two super-resolution models. * . Equal contribution Figure 2: Selected synthetic 256?256 ImageNet samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-17">17 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
							<email>jonathanho@google.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
							<email>sahariac@google.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
							<email>williamchan@google.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
							<email>davidfleet@google.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
							<email>mnorouzi@google.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
							<email>salimans@google.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>?2021</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Ho</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Saharia</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Norouzi</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Saharia</roleName><surname>Ho</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fleet, Norouzi</roleName><forename type="first">Salimans</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Diffusion Models for High Fidelity Image Generation Figure 1: A cascaded diffusion model comprising a base model and two super-resolution models. * . Equal contribution Figure 2: Selected synthetic 256?256 ImageNet samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-17">17 Dec 2021</date>
						</imprint>
					</monogr>
					<note>32?32 64?64 256?256 Class ID = 213 &quot;Irish Setter&quot; Model 1 Model 2 Model 3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>generative models</term>
					<term>diffusion models</term>
					<term>score matching</term>
					<term>iterative refinement</term>
					<term>super-resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64?64, 3.52 at 128?128 and 4.88 at 256?256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256?256, outperforming VQ-VAE-2. Figure 2: Selected synthetic 256?256 ImageNet samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Diffusion models <ref type="bibr" target="#b25">(Sohl-Dickstein et al., 2015)</ref> have recently been shown to be capable of synthesizing high quality images and audio <ref type="bibr" target="#b2">(Chen et al., 2021;</ref><ref type="bibr" target="#b10">Ho et al., 2020;</ref><ref type="bibr" target="#b14">Kong et al., 2021;</ref><ref type="bibr" target="#b28">Song and Ermon, 2020)</ref>: an application of machine learning that has long been dominated by other classes of generative models such as autoregressive models, GANs, VAEs, and flows <ref type="bibr" target="#b1">(Brock et al., 2019;</ref><ref type="bibr" target="#b5">Dinh et al., 2017;</ref><ref type="bibr" target="#b7">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b9">Ho et al., 2019;</ref><ref type="bibr" target="#b12">Kingma and Dhariwal, 2018;</ref><ref type="bibr" target="#b13">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b19">Razavi et al., 2019;</ref><ref type="bibr" target="#b30">van den Oord et al., 2016a</ref><ref type="bibr">van den Oord et al., ,b, 2017</ref>. Most previous work on diffusion models demonstrating high quality samples has focused on data sets of modest size, or data with strong conditioning signals. Our goal is to improve the sample quality of diffusion models on large high-fidelity data sets for which no strong conditioning information is available. To showcase the capabilities of the original diffusion formalism, we focus on simple, straightforward techniques to improve the sample quality of diffusion models; for example, we avoid using extra image classifiers to boost sample quality metrics <ref type="bibr" target="#b19">Razavi et al., 2019)</ref>.</p><p>Our key contribution is the use of cascades to improve the sample quality of diffusion models on class-conditional ImageNet . Here, cascading refers to a simple technique to model high resolution data by learning a pipeline of separately trained models at multiple resolutions; a base model generates low resolution samples, followed by super-resolution models that upsample low resolution samples into high resolution samples. Sampling from a cascading pipeline occurs sequentially, first sampling from the low resolution base model, followed by sampling from super-resolution models in order of increasing resolution. While any type of generative model could be used in a cascading pipeline (e.g., <ref type="bibr" target="#b15">Menick and Kalchbrenner, 2019;</ref><ref type="bibr" target="#b19">Razavi et al., 2019)</ref>, here we restrict ourselves to diffusion models. Cascading has been shown in recent prior work to improve the sample quality of diffusion models <ref type="bibr" target="#b22">(Saharia et al., 2021;</ref>; our work here concerns the improvement of diffusion cascading pipelines to attain the best possible sample quality.</p><p>The simplest and most effective technique we found to improve cascading diffusion pipelines is to apply strong data augmentation to the conditioning input of each superresolution model. We refer to this technique as conditioning augmentation. In our experiments, conditioning augmentation is crucial for our cascading pipelines to generate high quality samples at the highest resolution. With this approach we attain FID scores on classconditional ImageNet generation that are better than BigGAN-Deep <ref type="bibr" target="#b1">(Brock et al., 2019)</ref> at any truncation value, and classification accuracy scores that are better than VQ-VAE-2 <ref type="bibr" target="#b19">(Razavi et al., 2019)</ref>. We empirically find that conditioning augmentation is effective because it alleviates compounding error in cascading pipelines due to train-test mismatch, sometimes referred to as exposure bias in the sequence modeling literature <ref type="bibr" target="#b0">(Bengio et al., 2015;</ref><ref type="bibr" target="#b17">Ranzato et al., 2016)</ref>.</p><p>The key contributions of this paper are as follows:</p><p>? We show that our Cascaded Diffusion Models (CDM) yield high fidelity samples superior to <ref type="bibr">BigGAN-deep (Brock et al., 2019)</ref> and VQ-VAE-2 <ref type="bibr" target="#b19">(Razavi et al., 2019)</ref> in terms of FID score <ref type="bibr" target="#b8">(Heusel et al., 2017)</ref> and classification accuracy score <ref type="bibr" target="#b18">(Ravuri and Vinyals, 2019)</ref>, the latter by a large margin. We achieve these results with pure generative models that are not combined with any classifier.</p><p>? We introduce conditioning augmentation for our super-resolution models, and find it critical towards achieving high sample fidelity. We perform an in-depth exploration of augmentation policies, and find Gaussian augmentation to be a key ingredient for low resolution upsampling, and Gaussian blurring for high resolution upsampling. We also show how to efficiently train models amortized over varying levels of conditioning augmentation to enable post-training hyperparameter search for optimal sample quality.</p><p>Section 2 reviews recent work on diffusion models. Section 3 describes the most effective types of conditioning augmentation that we found for class-conditional ImageNet generation. Section 4 contains our sample quality results, ablations, and experiments on additional datasets. Appendix A contains extra samples and Appendix B contains details on hyperparameters and architectures. High resolution figures and additional supplementary material can be found at https://cascaded-diffusion.github.io/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We begin with background on diffusion models, their extension to conditional generation, and their associated neural network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Diffusion Models</head><p>A diffusion model <ref type="bibr" target="#b25">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b10">Ho et al., 2020)</ref> is defined by a forward process that gradually destroys data x 0 ? q(x 0 ) over the course of T timesteps</p><formula xml:id="formula_0">q(x 1:T |x 0 ) = T t=1 q(x t |x t?1 ), q(x t |x t?1 ) = N (x t ; 1 ? ? t x t?1 , ? t I)</formula><p>and a parameterized reverse process p ? (x 0 ) = p ? (x 0:T ) dx 1:T , where</p><formula xml:id="formula_1">p ? (x 0:T ) = p(x T ) T t=1 p ? (x t?1 |x t ), p ? (x t?1 |x t ) = N (x t?1 ; ? ? (x t , t), ? ? (x t , t)).</formula><p>The forward process hyperparameters ? t are set so that x T is approximately distributed according to a standard normal distribution, so p(x T ) is set to a standard normal prior as well. The reverse process is trained to match the joint distribution of the forward process by optimizing the evidence lower bound</p><formula xml:id="formula_2">(ELBO) ?L ? (x 0 ) ? log p ? (x 0 ): L ? (x 0 ) = E q L T (x 0 ) + t&gt;1 D KL (q(x t?1 |x t , x 0 ) p ? (x t?1 |x t )) ? log p ? (x 0 |x 1 ) (1) where L T (x 0 ) = D KL (q(x T |x 0 ) p(x T )).</formula><p>The forward process posteriors q(x t?1 |x t , x 0 ) and marginals q(x t |x 0 ) are Gaussian, and the KL divergences in the ELBO can be calculated in closed form. Thus it is possible to train the diffusion model by taking stochastic gradient steps on random terms of Eq. (1). As previously suggested <ref type="bibr" target="#b10">(Ho et al., 2020;</ref>, we use the reverse process parameterizations</p><formula xml:id="formula_3">? ? (x t , t) = 1 ? ? t x t ? ? t ? 1 ?? t ? (x t , t) ? ii ? (x t , t) = exp(log? t + (log ? t ? log? t )v i ? (x t , t))</formula><p>where ? t = 1 ? ? t ,? t = t s=1 ? s , and? t = 1?? t?1 1??t ? t . Sample quality can be improved, at the cost of log likelihood, by optimizing modified losses instead of the ELBO. The particular form of the modified loss depends on whether we are learning ? ? or treating it as a fixed hyperparameter (and whether ? ? is learned is itself considered a hyperparameter choice that we set experimentally). For the case of non-learned ? ? , we use the simplified loss</p><formula xml:id="formula_4">L simple (?) = E x 0 , ?N (0,I),t?U ({1,...,T }) ? ( ?? t x 0 + ? 1 ?? t , t) ? 2</formula><p>which is a weighted form of the ELBO that resembles denoising score matching over multiple noise scales <ref type="bibr" target="#b10">(Ho et al., 2020;</ref><ref type="bibr" target="#b27">Song and Ermon, 2019)</ref>. For the case of learned ? ? , we employ a hybrid loss  implemented using the expression</p><formula xml:id="formula_5">L hybrid (?) = L simple (?) + ?L vb (?) where L vb = E x 0 [L ? (x 0 )]</formula><p>and a stop-gradient is applied to the ? term inside L ? . Optimizing this hybrid loss has the effect of simultaneously learning ? ? using L simple and learning ? ? using the ELBO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Diffusion Models</head><p>In the conditional generation setting, the data x 0 has an associated conditioning signal c, for example a label in the case of class-conditional generation, or a low resolution image in the case of super-resolution <ref type="bibr" target="#b22">(Saharia et al., 2021;</ref>. The goal is then to learn a conditional model p ? (x 0 |c). To do so, we modify the diffusion model to include c as input to the reverse process:</p><formula xml:id="formula_6">p ? (x 0:T |c) = p(x T ) T t=1 p ? (x t?1 |x t , c), p ? (x t?1 |x t , c) = N (x t?1 ; ? ? (x t , t, c), ? ? (x t , t, c)) L ? (x 0 |c) = E q L T (x 0 ) + t&gt;1 D KL (q(x t?1 |x t , x 0 ) p ? (x t?1 |x t , c)) ? log p ? (x 0 |x 1 , c) .</formula><p>The data and conditioning signal (x 0 , c) are sampled jointly from the data distribution, now called q(x 0 , c), and the forward process q(x 1:T |x 0 ) remains unchanged. The only modification that needs to be made is to inject c as a extra input to the neural network function approximators: instead of ? ? (x t , t) we now have ? ? (x t , t, c), and likewise for ? ? . The particular architectural choices for injecting these extra inputs depends on the type of the conditioning c, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Architectures</head><p>The current best architectures for image diffusion models are U-Nets <ref type="bibr" target="#b20">(Ronneberger et al., 2015;</ref><ref type="bibr" target="#b24">Salimans et al., 2017)</ref>, which are a natural choice to map corrupted data x t to reverse process parameters (? ? , ? ? ) that have the same spatial dimensions as x t . Scalar conditioning, such as a class label or a diffusion timestep t, is provided by adding embeddings into intermediate layers of the network <ref type="bibr" target="#b10">(Ho et al., 2020)</ref>. Lower resolution image conditioning is provided by channelwise concatenation of the low resolution image, processed by bilinear or bicubic upsampling to the desired resolution, with the reverse process input x t , as in the SR3 <ref type="bibr" target="#b22">(Saharia et al., 2021)</ref> and Improved DDPM  models. See <ref type="figure" target="#fig_0">Fig. 3</ref> for an illustration of the SR3-based architecture that we use in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conditioning Augmentation in Cascaded Diffusion Models</head><p>Suppose x 0 is high resolution data and z 0 is its low resolution counterpart. We use the term cascading pipeline to refer to a sequence of generative models. At the low resolution we have a diffusion model p ? (z 0 ), and at the high resolution, a super-resolution diffusion model p ? (x 0 |z 0 ). The cascading pipeline forms a latent variable model for high resolution data; i.e., p ? (x 0 ) = p ? (x 0 |z 0 )p ? (z 0 ) dz 0 . It is straightforward to extend this to more than two resolutions. It is also straightforward to condition an entire cascading pipeline on class information or other conditioning information c: the models take on the form p ? (z 0 |c) and p ? (x 0 |z 0 , c), each using the conditioning mechanism described in Section 2.2. An example cascading pipeline is depicted in <ref type="figure" target="#fig_1">Fig. 4</ref>. Cascading pipelines have been shown to be useful with other generative model families <ref type="bibr" target="#b15">(Menick and Kalchbrenner, 2019;</ref><ref type="bibr" target="#b19">Razavi et al., 2019)</ref>. A major benefit to training a cascading pipeline over training a standard model at the highest resolution is that most of the modeling capacity can be dedicated to low resolutions, which empirically are the most important for sample quality, and training and sampling at low resolutions tends to be the most computationally efficient. In addition, cascading allows the individual models to be trained independently, and architecture choices can be tuned at each specific resolution for the best performance of the entire pipeline.  The most effective technique we found to improve the sample quality of cascading pipelines is to train each super-resolution model using data augmentation on its low resolution input. We refer to this general technique as conditioning augmentation. At a high level, for some super-resolution model p ? (x 0 |z) from a low resolution image z to a high resolution image x 0 , conditioning augmentation refers to applying some form of data augmentation to z. This augmentation can take any form, but what we found most effective at low resolutions is adding Gaussian noise (forward process noise), and for high resolutions, randomly applying Gaussian blur to z. In some cases, we found it more practical to train super-resolution models amortized over the strength of conditioning augmentation and pick the best strength in a post-training hyperparameter search for optimal sample quality. Details on conditioning augmentation and its realization during training and sampling are given in the following sections.</p><formula xml:id="formula_7">(x t , z) N 2 , M 1 ( N 2 ) 2 , M 2 ( N K ) 2 , M K ( N K ) 2 , M K ( N K ) 2 , 2 ? M K ( N 2 ) 2 , 2 ? M 2 N 2 , 2 ? M 1 x t?1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Blurring Augmentation</head><p>One simple instantiation of conditioning augmentation is augmentation of z by blurring. We found this to be most effective for upsampling to images with resolution 128?128 and 256?256. More specifically, we apply a Gaussian filter of size k and sigma ? to obtain z b . We use a filter size of k = (3, 3) and randomly sample ? from a fixed range during training. We perform hyper-parameter search to find the range for ?. During training, we apply this blurring augmentation to 50% of the examples. During inference, no augmentation is applied to low resolution inputs. We explored applying different amounts of blurring augmentations during inference, but did not find it helpful in initial experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Truncated Conditioning Augmentation</head><p>Here we describe what we call truncated conditioning augmentation, a form of conditioning augmentation that requires a simple modification to the training and architecture of the super-resolution models, but no change to the low resolution model at the initial stage of the cascade. We found this method to be most useful at resolutions smaller than 128?128. Normally, generating a high resolution sample x 0 involves first generating z 0 from the low resolution model p ? (z 0 ), then feeding that result into the super-resolution model p ? (x 0 |z 0 ). In other words, generating a high resolution sample is performed using ancestral sampling from the latent variable model</p><formula xml:id="formula_8">p ? (x 0 ) = p ? (x 0 |z 0 )p ? (z 0 ) dz 0 = p ? (x 0 |z 0 )p ? (z 0:T ) dz 0:T .</formula><p>(For simplicity, we have assumed that the low resolution and super-resolution models both use the same number of timesteps T .) Truncated conditioning augmentation refers to truncating the low resolution reverse process to stop at timestep s &gt; 0, instead of 0; i.e.,</p><formula xml:id="formula_9">p s ? (x 0 ) = p ? (x 0 |z s )p ? (z s ) dz s = p ? (x 0 |z s )p ? (z s:T ) dz s:T .<label>(2)</label></formula><p>The base model is now p ? (z s ) = p ? (z s:T )dz s+1:T , and the super-resolution model is now</p><formula xml:id="formula_10">p ? (x 0 |z s ) = p(x T ) T t=1 p ? (x t?1 |x t , z s )dx 1:T , where p ? (x t?1 |x t , z s ) = N (x t?1 ; ? ? (x t , t, z s , s), ? ? (x t , t, z s , s)).</formula><p>The reason truncating the low resolution reverse process is a form of data augmentation is that the training procedure for p ? (x 0 |z s ) involves conditioning on noisy z s ? q(z s |z 0 ), which, up to scaling, is z 0 augmented with Gaussian noise.</p><p>To be more precise about training a cascading pipeline with truncated conditioning augmentation, let us examine the ELBO for p s ? (x 0 ) in Eq.</p><p>(2). We can treat p s ? (x 0 ) as a VAE with a diffusion model prior, a diffusion model decoder, and the approximate posterior</p><formula xml:id="formula_11">q(x 1:T , z 1:T |x 0 , z 0 ) = T t=1 q(x t |x t?1 )q(z t |z t?1 ),</formula><p>which runs forward processes independently on a low and high resolution pair. The ELBO is</p><formula xml:id="formula_12">? log p s ? (x 0 ) ? E q L T (z 0 ) + t&gt;s D KL (q(z t?1 |z t , z 0 ) p ? (z t?1 |z t )) ? log p ? (x 0 |z s ) , where L T (z 0 ) = D KL (q(z T |z 0 ) p(z T ))</formula><p>. Note that the sum over t is truncated at s, and the decoder p ? (x 0 |z s ) is the super-resolution model conditioned on z s . The decoder itself has an</p><formula xml:id="formula_13">ELBO of the form ? log p ? (x 0 |z s ) ? L ? (x 0 |z s ), where L ? (x 0 |z s ) = E q L T (x 0 ) + t&gt;1 D KL (q(x t?1 |x t , x 0 ) p ? (x t?1 |x t , z s )) ? log p ? (x 0 |x 1 , z s ) .</formula><p>Thus we have an ELBO for the combined model</p><formula xml:id="formula_14">? log p s ? (x 0 ) ? E q L T (z 0 ) + t&gt;s D KL (q(z t?1 |z t , z 0 ) p ? (z t?1 |z t )) + L ? (x 0 |z s ) .<label>(3)</label></formula><p>It is apparent that optimizing Eq.</p><p>(3) trains the low and high resolution models separately.</p><p>For a fixed value of s, the low resolution process is trained up to the truncation timestep s, and the super-resolution model is trained on a conditioning signal corrupted using the low resolution forward process stopped at timestep s.</p><p>In practice, since we pursue sample quality as our main objective, we do not use these ELBO expressions directly when training models with learnable reverse process variances. Rather, we train on the "simple" unweighted loss or the hybrid loss described in Section 2, and the particular loss we use is considered a hyperparameter reported in Appendix B.</p><p>We would like to search over multiple values of s to select for the best sample quality. To make this search practical, we avoid retraining models by amortizing a single super-resolution model over uniform random s at training time. Because each possible truncation time corresponds to a distinct super-resolution task, the super-resolution model for ? ? and ? ? must take z s as input along with s, and this can be accomplished using a single network with an extra time embedding input for s. We leave the low resolution model training unchanged, because the standard diffusion training procedure already trains with random s. The complete training procedure for a two-stage cascading pipeline is listed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Non-truncated Conditioning Augmentation</head><p>Another form of conditioning augmentation, which we call non-truncated conditioning augmentation, uses the same model modifications and training procedure as truncated conditioning augmentation (Section 3.2). The only difference is at sampling time. Instead of truncating the low resolution reverse process, in non-truncated conditioning augmentation we always sample z 0 using the full, non-truncated low resolution reverse process; then we corrupt z 0 using the forward process into z s ? q(z s |z 0 ) and feed the corrupted z s into the super-resolution model.</p><p>The main advantage of non-truncated conditioning augmentation over truncated conditioning augmentation is a practical one during the search phase over s. In the case of truncated augmentation, if we want to run the super-resolution model over all s in parallel, we must store all low resolution samples z s for all values of s considered. In the case of </p><formula xml:id="formula_15">(z 0 , c) ? p(z, c)</formula><p>Sample low-resolution image and label 3:</p><formula xml:id="formula_16">t ? U({1, . . . , T }) 4: ? N (0, I) 5: z t = ?? t z 0 + ? 1 ?? t 6: ? ? ? ? ?? ? ? (z t , t, c) ? 2</formula><p>Simple loss (can be replaced with a hybrid loss) 7: until converged 8: repeat</p><p>Train super-resolution model (in parallel with the base model) 9:</p><formula xml:id="formula_17">(x 0 , z 0 , c) ? p(x, z, c)</formula><p>Sample low-and high-resolution images and label 10:</p><formula xml:id="formula_18">s, t ? U({1, . . . , T }) 11: z , x ? N (0, I)</formula><p>Note: z , x should have the same shapes as z 0 , x 0 , respectively 12:</p><formula xml:id="formula_19">z t = ?? s z 0 + ? 1 ?? s z</formula><p>Apply Gaussian conditioning augmentation 13: for t = T, . . . , s + 1 do 4:</p><formula xml:id="formula_20">x t = ?? t x 0 + ? 1 ?? t x 14: ? ? ? ? ?? ? ? (x t , t, z s , s, c) ? x</formula><formula xml:id="formula_21">z t?1 ? p ? (z t?1 |z t , c) 5:</formula><p>end for 6: else 7:</p><p>for t = T, . . . , 1 do 8:</p><formula xml:id="formula_22">z t?1 ? p ? (z t?1 |z t , c) 9: end for 10: z s ? q(z s |z 0 )</formula><p>Overwrite previously sampled value of z s 11: end if 12: x T ? N (0, I) 13: for t = T, . . . , 1 do 14:</p><formula xml:id="formula_23">x t?1 ? p ? (x t?1 |x t , z s , c) 15: end for 16: return x 0</formula><p>non-truncated augmentation, we need to store the low resolution samples just once, since sampling z s ? q(z s |z 0 ) is computationally inexpensive. These sampling procedures are listed in Algorithm 2.</p><p>Truncated and non-truncated conditioning augmentation should perform similarly because z s and z s should have similar marginal distributions if the low resolution model is trained well enough. Indeed, in Section 4.3, we empirically find that sample quality metrics are similar for both truncated and non-truncated conditioning augmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We designed experiments to improve the sample quality metrics of cascaded diffusion models on class-conditional ImageNet generation. Our cascading pipelines consist of class-conditional diffusion models at all resolutions, so class information is injected at all resolutions: see <ref type="figure" target="#fig_1">Fig. 4</ref>. Our final ImageNet results are described in Section 4.1.</p><p>To give insight into our cascading pipelines, we begin with improvements on a baseline non-cascaded model at the 64?64 resolution (Section 4.2), then we show that cascading up to 64?64 improves upon our best non-cascaded 64?64 model, but only in conjunction with conditioning augmentation. We also show that truncated and non-truncated conditioning augmentation perform equally well (Section 4.3), and we study random Gaussian blur augmentation to train super-resolution models to resolutions of 128?128 and 256?256 (Section 4.4). Finally, we verify that conditioning augmentation is also effective on the LSUN dataset <ref type="bibr" target="#b35">(Yu et al., 2015)</ref> and therefore is not specific to ImageNet (Section 4.5).</p><p>We cropped and resized the ImageNet dataset <ref type="bibr" target="#b21">(Russakovsky et al., 2015)</ref> in the same manner as BigGAN <ref type="bibr" target="#b1">(Brock et al., 2019)</ref>. We report Inception scores using the standard practice of generating 50k samples and calculating the mean and standard deviation over 10 splits <ref type="bibr" target="#b23">(Salimans et al., 2016)</ref>. Generally, throughout our experiments, we selected models and performed early stopping based on FID score calculated over 10k samples, but all reported FID scores are calculated over 50k samples for comparison with other work <ref type="bibr" target="#b8">(Heusel et al., 2017)</ref>. The FID scores we used for model selection and reporting model performance are calculated against training set statistics according to common practice, but since this can be seen as overfitting on the performance metric, we additionally report model performance using FID scores calculated against validation set statistics. We also report results on Classification Accuracy Score (CAS), which was proposed by <ref type="bibr" target="#b18">Ravuri and Vinyals (2019)</ref> due to their findings that non-GAN models may score poorly on FID and IS despite generating visually appealing samples and that FID and IS are not correlated (sometimes anti-correlated) with performance on downstream tasks. <ref type="table" target="#tab_4">Table 1a</ref> reports the main results on the cascaded diffusion model (CDM ), for the 64?64, 128?128, and 256?256 ImageNet dataset resolutions, along with baselines. CDM outperforms BigGAN-deep in terms of FID score on the image resolutions considered, but GANs perform better in terms of Inception score when their truncation parameter is optimized for Inception score <ref type="bibr" target="#b1">(Brock et al., 2019)</ref>. We also outperform concurrently released diffusion models that do not use classifier guidance to boost sample quality scores . See <ref type="figure">Fig. 8</ref> for a qualitative assessment of sample quality and diversity compared to VQ-VAE-2 <ref type="bibr" target="#b19">(Razavi et al., 2019)</ref> and <ref type="bibr">BigGAN-deep (Brock et al., 2019)</ref>, and see Figs. 5 and 6 for examples of generated images. <ref type="table" target="#tab_4">Table 1b</ref> reports the results on Classification Accuracy Score (CAS) <ref type="bibr" target="#b18">(Ravuri and Vinyals, 2019)</ref> for our models at the 128?128 and 256?256 resolutions. We find that CDM outperforms VQ-VAE-2 and BigGAN-deep at both resolutions by a significant margin on the CAS metric, suggesting better potential performance on downstream tasks. <ref type="figure">Figure 7</ref> compares class-wise classification accuracy scores between classifiers trained on real training data, and CDM samples. The CDM classifier outperforms real data on 96 classes compared to 6 and 31 classes by BigGAN-deep and VQ-VAE-2 respectively. We also show samples from classes with best and worst accuracy scores in Appendix <ref type="figure" target="#fig_8">Figure 11</ref> and 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Cascading Pipeline Results</head><p>Our cascading pipelines are structured as a 32?32 base model, a 32?32?64?64 superresolution model, followed by 64?64?128?128 or 64?64?256?256 super-resolution models. Models at 32?32 and 64?64 resolutions use 4000 diffusion timesteps and architectures similar to DDPM <ref type="bibr" target="#b10">(Ho et al., 2020)</ref> and Improved DDPM . Models at 128?128 and 256?256 resolutions use 100 sampling steps, determined by post-training hyperparameter search (Section 4.4), and they use architectures similar to SR3 <ref type="bibr" target="#b22">(Saharia et al., 2021)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class ID Classification Accuracy</head><p>Real Training Data CDM Samples <ref type="figure">Figure 7</ref>: Classwise Classification Accuracy Score comparison between real data (blue) and generated data (red) at the 256?256 resolution. Accompanies <ref type="table" target="#tab_4">Table 1b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Model Improvements</head><p>To set a strong baseline for class-conditional ImageNet generation at the 64?64 resolution, we reproduced and improved upon a 4000 timestep non-cascaded 64?64 class-conditional diffusion model from Improved DDPM . Our reimplementation used dropout and was trained longer than reported by Nichol and Dhariwal; we found that adding dropout generally slowed down convergence of FID and Inception scores, but improved their best values over the course of a longer training period. We further improved the training set FID score and Inception score by adding noise to the trained model's samples using the forward process to the 2000 timestep point, then restarting the reverse process from that point. See <ref type="table" target="#tab_5">Table 2a</ref> for the resulting sample quality metrics.   4.06 Improved DDPM  2.92 ADM  2.07 CDM (ours)  <ref type="bibr" target="#b1">(Brock et al., 2019)</ref> 27 317 VQ-VAE-2 <ref type="bibr" target="#b19">(Razavi et al., 2019)</ref> 31.11 Improved DDPM  12.26 SR3 <ref type="bibr" target="#b22">(Saharia et al., 2021)</ref> 11.30 ADM  10.94 100.98 ADM+upsampling  7   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Conditioning Augmentation Experiments up to 64?64</head><p>Building on our reimplementation in Section 4.2, we verify in a small scale experiment that cascading improves sample quality at the 64?64 resolution. We train a two-stage cascading pipeline that comprises a 16?16 base model and a 16?16?64?64 super-resolution model. The super-resolution model architecture is identical to the best 64?64 non-cascaded baseline model in Section 4.2, except for the trivial modification of adding in the low resolution image conditioning information by channelwise concatenation at the input (see Section 2). See <ref type="table" target="#tab_5">Table 2b</ref> and <ref type="figure" target="#fig_7">Fig. 9</ref> for the results of this 16?16?64?64 cascading pipeline. Interestingly, we find that without conditioning augmentation, the cascading pipeline attains lower sample quality than the non-cascaded baseline 64?64 model; the FID score, for example, degrades from 2.35 to 6.02. With sufficient conditioning augmentation, however, the sample quality of the cascading pipeline becomes better than the non-cascaded baseline. We train two super-resolution models with non-truncated conditioning augmentation, one at truncation time s = 101 and another at s = 1001 (we could have amortized both into a single model, but we chose not to do so in this experiment to prevent potential model capacity issues from confounding the results). The first model achieves better sample quality than the non-augmented model but is still worse than the non-cascaded baseline. The second model achieves a FID score of 2.13, outperforming the non-cascaded baseline. Conditioning augmentation is therefore crucial to improve sample quality in this particular cascading pipeline.</p><p>To further improve sample quality at the 64?64 resolution, we found it helpful to increase model sizes and to switch to a cascading pipeline starting with a 32?32 base resolution model. We train a 32?32 base model applying random left-right flips, which we found to help 32?32 scores at the expense of longer training times. Training without random flips, the   <ref type="table" target="#tab_5">Table 2b</ref>.</p><p>best 32?32 resolution FID score is 1.25 at 300k training steps, while training with random flips it is 1.11 at 700k training steps. The 32?32?64?64 super-resolution model is now amortized over the truncation time s by providing s as an extra time embedding input to the network (Section 2), allowing us to perform a more fine grained search over s without retraining the model. <ref type="table" target="#tab_7">Table 3a</ref> displays the resulting sample quality scores for both truncated and non-truncated augmentation. The sample quality metrics improve and then degrade non-monotonically as the truncation time is increased. This indicates that moderate amounts of conditioning augmentation are beneficial to sample quality of the cascading pipeline, but too much conditioning augmentation causes the super-resolution model to behave as a non-conditioned model unable to benefit from cascading. For comparison, <ref type="table" target="#tab_7">Table 3b</ref> shows sample quality when the super-resolution model is conditioned on ground truth data instead of generated data.</p><p>Here, sample quality monotonically degrades as truncation time is increased. Conditioning augmentation is therefore useful precisely when conditioning on generated samples, so as a technique it is uniquely suited to cascading pipelines.</p><p>Based on these findings on non-monotonicity of sample quality with respect to truncation time, we conclude that conditioning augmentation works because it alleviates compounding error from a train-test mismatch for the super-resolution model. This occurs when lowresolution model samples are out of distribution compared to the ground truth data on which the super-resolution model is trained. A sufficient amount of Gaussian conditioning augmentation prevents the super-resolution model from attempting to upsample erroneous, out-of-distribution details in the low resolution generated samples. In contrast, sample quality degrades monotonically with respect to truncation time when conditioning the super-resolution model on ground truth data, because there is no such train-test mismatch. <ref type="table" target="#tab_7">Table 3a</ref> additionally shows that truncated and non-truncated conditioning augmentation are approximately equally effective at improving sample quality of the cascading pipeline, albeit at different values of the truncation time parameter. Thus we generally recommend non-truncated augmentation due to its practical benefits described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments at 128?128 and 256?256</head><p>While we found Gaussian noise augmentation to be a key ingredient to boost the performance of our cascaded models at low resolutions, our initial experiments with similar augmentations for 128?128 and 256?256 upsampling yielded negative results. Hence, we explore Gaussian blurring augmentation for these resolutions. As mentioned in Section 3.1, we apply the blurring augmentation 50% of the time during training, and use no blurring during inference. We explored other settings (e.g. applying blurring to all training examples, and using varying amounts of blurring during inference), but found this to be most effective in our initial experiments. <ref type="table" target="#tab_8">Table 4a</ref> shows the results of applying Gaussian blur augmentation to the 64?64 ? 256?256 super-resolution model. While any amount of blurring helps improve the scores of the 256?256 samples over the baseline model with no blur, we found that sampling ? ? U(0.4, 0.6) gives the best results. <ref type="table" target="#tab_8">Table 4b</ref> shows further improvements from class conditioning, large batch training, and random flip augmentation for the super-resolution model. While we find class conditioning helpful for upsampling at low resolution settings, it is   interesting that it still gives a huge boost to the upsampling performance at high resolutions even when the low resolution inputs at 64?64 can be sufficiently informative. We also found increasing the training batch size from 256 to 1024 further improved performance by a significant margin. We also obtain marginal improvements by training the super-resolution model on randomly flipped data.</p><p>Since the sampling cost increases quadratically with the target image resolution, we attempt to minimize the number of denoising iterations for our 64?64 ? 256?256 and 64?64 ? 128?128 super-resolution models. To this end, we train these super-resolution models with continuous noise conditioning, like <ref type="bibr" target="#b22">Saharia et al. (2021)</ref> and <ref type="bibr" target="#b2">Chen et al. (2021)</ref>, and tune the noise schedule for a given number of steps during inference. This tuning is relatively inexpensive as we do not need to retrain the models. We report all results using 100 inference steps for these models. <ref type="figure">Figure 10</ref> shows FID vs number of inference steps for our 64?64 ? 256?256 model. The FID score deteriorates marginally even when using just 4 inference steps. Interestingly, we do not observe any concrete improvement in FID by increasing the number of inference steps from 100 to 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments on LSUN</head><p>While the main results of this work are on class-conditional ImageNet generation, here we study the effectiveness of non-truncated conditioning augmentation for a 64?64?128?128 cascading pipeline on the LSUN Bedroom and Church datasets <ref type="bibr" target="#b35">(Yu et al., 2015)</ref> in order to verify that conditioning augmentation is not an ImageNet-specific method. LSUN Bedroom and Church are two separate unconditional datasets that do not have any class labels, so our study here additionally verifies the effectiveness of conditioning augmentation for unconditional generation. <ref type="table" target="#tab_10">Table 5</ref> displays our LSUN sample quality results, which confirm that a nonzero amount of conditioning augmentation is beneficial to sample quality. (The relatively large FID scores between generated examples and the validation sets are explained by the fact that the LSUN Church and Bedroom validation sets are extremely small, consisting of only 300 examples each.) We observe a similar effect as our ImageNet results in <ref type="table" target="#tab_7">Table 3b</ref>: because the super-resolution model is conditioned on base model samples, the sample quality improves then degrades non-monotonically as the truncation time s is increased. See Appendix A for examples of images generated by our LSUN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>One way to formulate cascaded diffusion models is to modify the original diffusion formalism of a forward process q(x 0:T ) at single resolution so that the transition q(x t |x t?1 ) performs downsampling at certain intermediate timesteps, for example at t ? S := {T /4, 2T /4, 3T /4}. The reverse process would then be required to perform upsampling at those timesteps, similar to our cascaded models here. However, there is no guarantee that the reverse transitions at the timesteps in S are conditional Gaussian, unlike the guarantee for reverse transitions at other timesteps for sufficiently slow diffusion. By contrast, our cascaded diffusion model construction dedicates entire conditional diffusion models for these upsampling steps, so it is specified more flexibly.  Recent interest in diffusion models <ref type="bibr" target="#b25">(Sohl-Dickstein et al., 2015)</ref> started with work connecting diffusion models to denoising score matching over multiple noise scales <ref type="bibr" target="#b10">(Ho et al., 2020;</ref><ref type="bibr" target="#b27">Song and Ermon, 2019)</ref>. There have been a number of improvements and alternatives proposed to the diffusion framework, for example generalization to continuous time <ref type="bibr" target="#b29">(Song et al., 2021b</ref>), deterministic sampling <ref type="bibr" target="#b26">(Song et al., 2021a)</ref>, adversarial training <ref type="bibr" target="#b11">(Jolicoeur-Martineau et al., 2021)</ref>, and others <ref type="bibr" target="#b6">(Gao et al., 2021)</ref>. For simplicity, we base our models on DDPM <ref type="bibr" target="#b10">(Ho et al., 2020)</ref> with modifications from Improved DDPM  to stay close to the original diffusion framework.</p><p>Cascading pipelines have been investigated in work on VQ-VAEs (van den Oord et al., 2016c; <ref type="bibr" target="#b19">Razavi et al., 2019)</ref> and autoregressive models <ref type="bibr" target="#b15">(Menick and Kalchbrenner, 2019)</ref>. Cascading pipelines have also been investigated for diffusion models, such as SR3 <ref type="bibr" target="#b22">(Saharia et al., 2021)</ref>, Improved DDPM , and concurrently in ADM . Our work here focuses on improving cascaded diffusion models for ImageNet generation and is distinguished by the extensive study on conditioning augmentation and deeper cascading pipelines. Our conditioning augmentation work also resembles scheduled sampling in autoregressive sequence generation <ref type="bibr" target="#b0">(Bengio et al., 2015)</ref>, where noise is used to alleviate the mismatch between train and inference conditions. Concurrent work  showed that diffusion models are capable of generating high quality ImageNet samples using an improved architecture, named ADM, and a classifier guidance technique in which a class-conditional diffusion model sampler is modified to simultaneously take gradient steps to maximize the score of an extra trained image classifier. By contrast, our work focuses solely on improving sample quality by cascading, so we avoid introducing extra model elements such as the image classifier. We are interested in avoiding classifier guidance because the FID and Inception score sample quality metrics that we use to evaluate our models are themselves computed on activations of an image classifier trained on ImageNet, and therefore classifier guidance runs the risk of cheating these metrics.</p><p>Avoiding classifier guidance comes at the expense of using thousands of diffusion timesteps in our low resolution models, where ADM uses hundreds. ADM with classifier guidance outperforms our models in terms of FID and Inception scores, while our models outperform ADM without classifier guidance as reported by <ref type="bibr">Dhariwal and Nichol.</ref> Our work is a showcase of the effectiveness of cascading alone in a pure generative model, and since classifier guidance and cascading complement each other as techniques to improve sample quality and can be applied together, we expect classifier guidance would improve our results too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have shown that cascaded diffusion models are capable of outperforming state-of-the-art generative models on the ImageNet class-conditional generation benchmark when paired with conditioning augmentation, our technique of introducing data augmentation into the conditioning information of super-resolution models. Our models outperform BigGAN-deep and VQ-VAE-2 as measured by FID score and classification accuracy score. We found that conditioning augmentation helps sample quality because it combats compounding error in cascading pipelines due to train-test mismatch in super-resolution models, and we proposed practical methods to train and test models amortized over varying levels of conditioning augmentation.</p><p>Although there could be negative impact of our work in the form of malicious uses of image generation, our work has the potential to improve beneficial downstream applications such as data compression while advancing the state of knowledge in fundamental machine learning problems. We see our results as a conceptual study of the image synthesis capabilities of diffusion models in their original form with minimal extra techniques, and we hope our work serves as inspiration for future advances in the capabilities of diffusion models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 ImageNet</head><p>Here we give the hyperparameters of the models in our ImageNet cascading pipelines. Each model in the pipeline is described by its diffusion process, its neural network architecture, and its training hyperparameters. Architecture hyperparameters, such as the base channel count and the list of channel multipliers per resolution, refer to hyperparameters of the U-Net in DDPM and related models <ref type="bibr" target="#b10">(Ho et al., 2020;</ref><ref type="bibr" target="#b22">Saharia et al., 2021;</ref><ref type="bibr" target="#b24">Salimans et al., 2017)</ref>. The cosine noise schedule and the hybrid loss method of learning reverse process variances are from Improved DDPM . Some models are conditioned on? t for post-training sampler tuning <ref type="bibr" target="#b2">(Chen et al., 2021;</ref><ref type="bibr" target="#b22">Saharia et al., 2021)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>32?32 base model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 LSUN</head><p>Here we give the hyperparameters of our LSUN Bedroom and Church pipelines. We used the same hyperparameters for both datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>64?64 base model</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The U-Net architecture used in each model of a CDM pipeline. The first model is a class-conditional diffusion model that receives the noisy image x t and the class label y and as input. (The class label y and timestep t are injected into each block as an embedding, not depicted here). The remaining models in the pipeline are class-conditional super-resolution models that receive x t , y, and an additional upsampled low-resolution image z as input. The downsampling/upsampling blocks adjust the image input resolution N ? N by a factor of 2 through each of the K blocks. The channel count at each block is specified using channel multipliers M 1 , M 2 , ..., M K , and the upsampling pass has concatenation skip connections to the downsampling pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Detailed CDM pipeline for generation of class conditional 256?256 images. The first model is a class-conditional diffusion model, and it is followed by a sequence of two class-conditional super-resolution diffusion models. Each model has a U-Net architecture as depicted inFig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label></label><figDesc>15: until converged Algorithm 2 Sampling from a two-stage CDM with Gaussian conditioning augmentation Require: c: class label Require: s: conditioning augmentation truncation time 1: z T ? N (0, I) 2: if using truncated conditioning augmentation then 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Classwise Synthetic 256?256 ImageNet images. Each row represents a specific ImageNet class. Classes from top to bottom -Flamingo (130), White Wolf (270), Tiger (292), Monarch Butterfly (323), Zebra (340) and Dome (538).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Classwise Synthetic 256?256 ImageNet images. Each row represents a specific ImageNet class. Classes from top to bottom -Greenhouse (580), Model T (661), Streetcar (829), Comic Book (917), Crossword Puzzle (918), Cheeseburger (933).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. All base resolution and super-resolution models are conditioned on class labels. See Appendix B for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>super-resolution, s = 0 (c) 16?16?64?64 super-resolution, s = 101 (d) 16?16?64?64 super-resolution, s = 1001</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Generated images for varying amounts of conditioning augmentation (nontruncated) in a small-scale 16?16?64?64 pipeline for ablation purposes. Accompanies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Samples from classes with best relative classification accuracy score. Each row represents a specific ImageNet class. Classes from top to bottom -Tiger Cat (282), Gong (577), Coffee Mug (504), Squirrel Monkey (382), Miniature Schnauzer (196) and Corn (987).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Samples from classes with worst relative classification accuracy score. Each row represents a specific ImageNet class. Classes from top to bottom -Letter Opener (623), Plate (923), Overskirt (689), Tobacco Shop (860), Black-and-tan Coonhound (165) and Bathtub (435).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Samples from LSUN 128x128: bedroom subset (first six rows) and church subset (last six rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>-</head><label></label><figDesc>Noise schedule: linear</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>-</head><label></label><figDesc>Noise schedule: cosine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Training a two-stage CDM with Gaussian conditioning augmentation</figDesc><table><row><cell>1: repeat</cell><cell>Train base model</cell></row><row><cell>2:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Main results. Numbers are bolded only when at least two are available for comparison. CAS for real data and other models are from<ref type="bibr" target="#b18">Ravuri and Vinyals (2019)</ref>.</figDesc><table><row><cell>CDM (ours)</cell><cell>VQ-VAE-2</cell><cell>BigGAN-deep</cell></row><row><cell cols="3">Figure 8: Comparing the quality and diversity of model samples in selected 256?256 Ima-</cell></row><row><cell cols="3">geNet classes {Tench(0), Goldfish(1) and Ostrich(9)}. VQVAE-2 and BigGAN</cell></row><row><cell cols="2">samples are taken from Razavi et al. (2019).</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>64?64 ImageNet sample quality: ablations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: 64?64 ImageNet sample quality: large scale experiment comparing truncated</cell></row><row><cell>and non-truncated conditioning augmentation for 32?32?64?64 cascading, using</cell></row><row><cell>amortized truncation time conditioning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>256?256 ImageNet sample quality: experiments on 64?64 ? 256?256 superresolution.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>128?128 LSUN sample quality: non-truncated conditioning augmentation for a 64?64?128?128 cascading pipeline using the base model for low resolution conditioning.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jascha Sohl-Dickstein, Douglas Eck and the Google Brain team for feedback, research discussions and technical assistance.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Samples</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">WaveGrad: Estimating gradients for waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04933</idno>
		<title level="m">Hierarchical autoregressive image models with auxiliary decoders</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat gans on image synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Density estimation using Real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning energy-based models by diffusion recovery likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T D</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Auto-encoding variational Bayes. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<title level="m">DiffWave: A Versatile Diffusion Model for Audio Synthesis. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Sequence level training with recurrent neural networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification Accuracy Score for Conditional Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<title level="m">Image super-resolution via iterative refinement</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PixelCNN++: Improving the Pixel-CNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<title level="m">Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00953</idno>
		<title level="m">Latent optimisation for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
