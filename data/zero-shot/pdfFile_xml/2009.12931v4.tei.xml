<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification and understanding of cloud structures via satellite images with EfficientUNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tashin</forename><surname>Ahmed</surname></persName>
							<email>tashinahmed@aol.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noor</forename><surname>Hossain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuri</forename><surname>Sabab</surname></persName>
							<email>nsabab@aol.com</email>
						</author>
						<title level="a" type="main">Classification and understanding of cloud structures via satellite images with EfficientUNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-EfficientNet</term>
					<term>UNet</term>
					<term>clouds</term>
					<term>Dice coefficient</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Climate change has been a common interest and the forefront of crucial political discussion and decision-making for many years. Shallow clouds play a significant role in understanding the Earth's climate, but they are challenging to interpret and represent in a climate model. By classifying these cloud structures, there is a better possibility of understanding the physical structures of the clouds, which would improve the climate model generation, resulting in a better prediction of climate change or forecasting weather update. Clouds organise in many forms, which makes it challenging to build traditional rule-based algorithms to separate cloud features. In this paper, classification of cloud organization patterns was performed using a new scaled-up version of Convolutional Neural Network (CNN) named as EfficientNet as the encoder and UNet as decoder where they worked as feature extractor and reconstructor of fine grained feature map and was used as a classifier, which will help experts to understand how clouds will shape the future climate. By using a segmentation model in a classification task, it was shown that with a good encoder alongside UNet, it is possible to obtain good performance from this dataset. Dice coefficient has been used for the final evaluation metric, which gave the score of 66.26% and 66.02% for public and private (test set) leaderboard on Kaggle competition respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Clouds perform an essential role to control the radiation of the sun as well as controlling the radiation that goes back to the atmosphere. The more energy that is trapped inside the planet, the warmer the atmosphere becomes, giving rise to sea level via meltdown of polar ice caps and contributing to global warming. The less energy that is trapped, the colder the temperature becomes. Understanding the structure of the clouds gives a better insight into the planet's weather. Hence it is crucial to climatologists <ref type="bibr" target="#b0">[1]</ref>. Albedo is a measure of how much energy is reflected without being absorbed <ref type="bibr" target="#b1">[2]</ref>. White surfaces reflect the most energy; hence it has a high albedo, while dark surfaces absorb most energy, indicating a low albedo. Earth's albedo is 0.3, which indicate the warming of the climate <ref type="bibr" target="#b2">[3]</ref>. Interpreting cloud structures provide useful insight into the abuse of Earth's climate and risks associated with it. Satellite images of clouds give a broader picture of the atmosphere, and interpreting the images provide information on the current situation of the planet.</p><p>It is assumed that as the overall temperature of Earth increases, it will evaporate more water from the oceans, resulting in more clouds with different structures and variations <ref type="bibr" target="#b3">[4]</ref>. Climate change could depend on the general effect of clouds shapes and other features like more or less abundant, thicker  <ref type="bibr" target="#b4">[5]</ref>.</p><p>These feathery clouds are made of icicles and have lengthy narrow strips that are also known as mare's tails. Dispersed cirrus clouds indicates good weather. A continuous growth of a cirrus cloud which looks like a cover of the web indicates increasing humidity in air and arrival of a potential storm. Cirrostratus clouds lay themselves out across the sky with their scattered thin lines which gives the air a subtle white look. These are often the signs of upcoming rain. Their translucency offers the sun and moon to be seen easily through the cloud. Visibility of a cirrostratus cloud normally can be confirmed twelve to twenty four hours before the arrival of a rain or snow <ref type="bibr" target="#b5">[6]</ref>. Cirrocomulus is another type of high cloud which tends to be a cluster of white lines that oftentimes can be seen as orderly aligned. In tropical climates these could be the mark of an incoming hurricane <ref type="bibr" target="#b6">[7]</ref>. Transverse cirrus bands could provide turbulence in planes and the heat maps annually show the spatial patterns where the bands are quite consistently forming <ref type="bibr" target="#b7">[8]</ref>. There are many more forms of clouds which determine the weather and provides an indication of any natural disaster, which can be handled in a low-risk manner if correctly detected beforehand.</p><p>Formation of clouds and detecting their structures and patterns beforehand allows a lot of high-risk activities to be avoided previously. Detecting the electrical discharge density of atmosphere and checking for convective patterns could reduce risk of accidents <ref type="bibr" target="#b8">[9]</ref>. Aircraft flights are a high-risk activity that carries thousands of passengers at a given interval of time, and flying through clouds is similar to driving a car through a thick fog -it is difficult to see what is ahead, making it a challenging maneuver to accomplish, as a consequence, an essential part of becoming a pilot is knowing about cloud formations <ref type="bibr" target="#b9">[10]</ref>. Cloud structures like Cumulonimbus are a direct threat to aircraft <ref type="bibr" target="#b10">[11]</ref>. Cloud-borne updrafts and downdrafts create fast and unforeseeable outcomes to the lift force on aircraft wings which causes turbulence. These changes cause the plane to lurch and jump about during flight, known as turbulence, which sometimes makes less experienced pilots lose control of the craft, and the result is often fatal, with high casualties. Cargo ships, on the other hand, rely a lot on the weather of the sea, which is often unpredictable and ever-changing. Cargoes usually have a tight schedule and delay causes a loss of hundreds of thousands of dollars in fuel consumption. Storms also delay shipping which contributes to the loss of millions of dollars. An early prediction of storms or change in weather saves many lives and millions of dollars.</p><p>Interpretation of satellite images of cloud structures requires the expertise of a well-trained meteorologist, although it is not feasible and not always readily available. An intelligent automated system to interpret the satellite images, therefore, becomes a promising alternative with ease of access and hence becomes quite desirable for the understanding of cloud structures.</p><p>In this paper, a model has been developed to classify the clouds into four categories using satellite images, with classification architecture like EfficientNet and segmentation architecture UNet <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>The research has the following contributions:</p><p>? An approach capable of learning from satellite images to classify four classes of cloud shapes. ? Smart augmentation approach using Albumentation has been applied to expand the dataset for accurate model training.</p><p>? Usage of segmentation model on a classification task to show that a good encoder provides effective result with UNet as decoder. ? Experimental details and comparison has been provided with straightforward use of EfficientNet models (B0-B5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATASET</head><p>The dataset consists of satellite images, courtesy of NASA Worldview, gathered from Kaggle competition "Understanding Clouds from satellite images". The images contain clouds of four classes namely: Fish, Flower, Sugar and Gravel. The images were taken from three regions, spanning 21 ? longitude and 14 ? latitude. The true color images were taken from two polar-orbiting satellites, Terra and Aqua. An image might be attached from two orbits, due to the small footprint of Moderate Resolution Imaging Spectroradiometer (MODIS) onboard these satellites. The remaining area, which has not been covered by two succeeding orbits, is marked black, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The dataset was split into train and validation of 80:20. The training sample was perfectly balanced with 22184 images, which consists of 5546 ? 4 images, where each class has 5546 images. All images are of the same size, which is 1400?2100 pixels. 68 scientists labelled images in the train set, and 3 individual ones labelled each image. Certain augmentation techniques were applied to the dataset. Augmentation is a process to artificially expand the size of the dataset by creating modified data which improves the performance of the model to generalise. Albumentations library has been used for augmentation <ref type="bibr" target="#b13">[14]</ref>. This library efficiently implements an abundant variations in image transformation that are performance optimized. The images were augmented into four types randomly for each train data: horizontal flip, vertical flip, random 20 ? rotation and grid distortion. The count of train image data doubled after performing augmentation. Also some adjustment have been done in image size to feed into EfficentUNet architecture mentioned in Subsection III-F3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Public and Private Leaderboard Score</head><p>Since the dataset was gathered from a public Kaggle competition, there were two sets of scores that competed among others. Public LB 1 scores are those which are shown while the competition is ongoing. It shows the outcome from a subset of the test dataset. Private LB scores get generated after the competition is over, that provides scores on the remaining test dataset. As a result, public LB scores are usually better than the private. For this particular competition, private score was calculated on 75% of the test data.</p><p>Pixel encoding technique was followed to participate in the submission of the competition since the image sizes were too large for Kaggle system <ref type="bibr" target="#b14">[15]</ref>. As a result, instead of submitting an exhaustive list of indices for segmentation, pairs of values were submitted, which contained the start position and the run length of the image pixels. For example, a pair value of (1, 3) indicates that the pixel starts at 1 and run 3 pixels. The competition also required a space-delimited list of pairs. The predicted encodings were scaled by 0.25 per side, which scaled down the images of size 1400 ? 2100 pixels in both train and test set to 350?525 pixels, hence allowing the scope to achieve reasonable submission evaluation times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dice coefficient</head><p>The evaluation metric used in this paper is the Dice coefficient. It was applied to compare the pixel-wise agreement within a predicted segmentation and corresponding ground truth, using the following equation:</p><formula xml:id="formula_0">2 * |X ? Y | |X|+|Y |<label>(1)</label></formula><p>Here X defines the set of pixels predicted, whereas Y defines the ground truth of the training set. When X and Y are empty, the dice coefficient is defined as 1. The LB score provides the mean of Dice coefficients for each (Image, Label) pair in the test data.</p><p>Dice coefficients are slightly different from the more popular evaluation metric: accuracy of a model. They are used to quantify the performance of image segmentation methods. Some ground truth regions are annotated in the images, and then an automated algorithm is allowed to do it. The algorithm is validated by calculating the dice score, which is a measure of how similar the objects are and is calculated by the overlap of the two segmentations divided by the total size of the two objects <ref type="bibr" target="#b15">[16]</ref>. Dice coefficient works better in segmentation because of its ease of differentiation, as a result it is more preferable over Jaccard's index, another evaluation metric similar to Dice coefficient <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimizer</head><p>Rectified Adam (RAdam) was used instead of Adam as an optimizer for high accuracy and fewer epochs <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>Categorical Cross-entropy (CCE) has been applied as a loss function as it is a multi-label classification task. To evaluate the difference between two probability distributions, Categorical Cross-entropy, also known as Softmax Loss, is used.</p><formula xml:id="formula_1">Loss = ? i=1 output size y i . log? i<label>(2)</label></formula><p>Here? i determines the i-th scalar value in the output of the model, y i denotes the analogous target value, and number of scalar values in output of the model is provided by output size.</p><p>It's an important function in order to calculate and distinguish two probability distribution of discrete nature. y i specifies probability that event i occurred and sum of all y i is 1, indicating that precisely one event occurred. The function has a negative sign which confirms that the loss decreases as the distributions move closer to one another.</p><p>Softmax activation function is recommended with CCE, as it adjusts the output of the model and verifies if it contains the right properties, as positive outputs are desirable so that logarithm of every output value? i exists. Comparing two probability distribution is the major objective of this particular loss function. For the classification part loss score was calculated from the CCE and for the segmentation part it was calculated as CCE ? 0.7 + DICE ? 0.3. DSC is a measure of overlap between corresponding pixel values of prediction and ground truth respectively. Range of DSC is between 0 and 1 as it is understandable from Subsection III-B and the larger the better. So, Dice Loss (DICE) try to maximize the overlap between the above mentioned two sets (predcitions and ground truth pixel values) <ref type="bibr" target="#b18">[19]</ref>. <ref type="table">Conv 3 X 3  MBConv1, 3 X 3  MBConv6, 3 X 3  MBConv6, 3 X 3  MBConv6, 5 X 5  MBConv6, 3 X 3  MBConv6, 5 X 5  MBConv6, 5 X 5  MBConv6, 5 X 5  MBConv6, 5 X 5  MBConv6, 5 X 5  MBConv6, 5 X 5  MBConv6, 5 X 5  MBConv6, 3 X 3  MBConv6,  3 X 3  MBConv6, 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Precision-Recall (PR) Curve</head><p>Precision-Recall (PR) curve demonstrates the relationship between positive predictive value (precision) and sensitivity (recall) of a machine learning model. Precision gives the percentage of the relevant outcome, while recall indicates the total consistent result. PR curve is a vital evaluation metric as it provides a more informative view of an algorithm's performance. X axis shows recall while the Y axis shows precision.</p><p>PR curve visualizes the set-off between recall and precision for various thresholds. A high area under the curve (AUC) shows both higher precision and recall, where they refer to a flat false-negative and low false-positive rate respectively in terms of high recall and precision. Since cloud structures are complex, it was essential to understand whether the implemented model was correctly detecting the shapes and evaluating true positive, true negative scores appropriately, hence PR curve was a crucial evaluation metric to understand the learning rate of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Description of Applied Architectures</head><p>1) EfficientNet: EfficientNet presented by Google AI research is considered as a group of CNN models, but with subtle improvements, it works better than its predecessors <ref type="bibr" target="#b11">[12]</ref>. It consists of 8 variations from B0 to B7, where each subsequent model number refers to variants with more parameters and higher accuracy. EfficientNet works in three ways:</p><p>? Depthwise + Pointwise Convolution: Depthwise convolution performs independently over each channel of input. This is a spatial convolution. Pointwise convolution projects the channel's output by the depthwise convolution onto a new channel space. This is a 1?1 convolution. ? Inverse Res: ResNet blocks consist of a layer that squeezes the channels a layer that extends the channels. In this way, it links skip connections to rich channel layers <ref type="bibr" target="#b19">[20]</ref>. ? Linear Bottlneck: In each block, it uses linear activation in the last layer to prevent loss of information from ReLU <ref type="bibr" target="#b20">[21]</ref>. As mentioned earlier, EfficientNet has 8 variations, B0 -B7, among which, first 6 models have been explored in this paper.</p><p>Due to the rise in complexity, the remaining models were ignored as they produced underappreciated results with poor performance while absorbing precious runtime. The layers in each of the models (B0 -B7) can be created by using 5 standard modules shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depthwise Conv2D</head><p>Batch  The individual modules are further used in various order to create sub-blocks, as shown in <ref type="figure">Fig. 4</ref>. It's easy to observe the difference among the models, with a gradual increase in the number of sub-blocks. The foundation for EfficientNet is MBConv layer which is an inverted residual block originally applied in MobileNetV2 <ref type="bibr" target="#b21">[22]</ref>. Basic building block of EfficientNet-B0 with respect to MBConv layers have been shown in <ref type="figure">Fig. 2</ref>. The 8 models of EfficientNet (B0 -B7) share the common blocks with subtle complexities in their architectures.</p><p>EfficientNet is a scaled-up neural network architecture, where the models scale all dimensions with a compound coefficient, which is a newly proposed method known as Compound Scaling <ref type="bibr" target="#b22">[23]</ref>. Here scaling up is defined as a systematic, principled scaling of three factors, which are depth, width and resolution.</p><p>? Width scale adds more feature maps in each layer. .</p><p>Every architecture has similarity with its earlier versions. The only difference is the different feature maps that increase the number of parameters. All the models have the same architecture as its previous one, except for the multiplied block (x2) that expands and covers more blocks. This gives a lot of parameters to be used in a calculation, making it a very robust model. It's not difficult to observe the changes among all the models, and they gradually increased the number of sub-blocks <ref type="bibr" target="#b11">[12]</ref>. Starting from EfficientNet-B0, compound scaling method was used to scale up with two steps:</p><p>? Step 1 coefficient was fixed to 1 assuming twice more resources to be available, and it enforces a small grid search for the networks depth, width and resolution constants.</p><formula xml:id="formula_2">? Step 2</formula><p>The constants then get fixed and scaled up the baseline network with different coefficient to obtain the successive versions from B1 to B7. EfficientNet was prioritized in this paper due to limitations of Kaggle notebook as well. It was also the core reason why the remaining EfficientNet models were avoided, since they calculate a substantial amount of parameters, which takes a lot of processing power and time, producing a disappointing outcome. While calculating mVA for EfficientNet models, fine-tuning has been applied alongside baseline versions for each models. Imagenet pretrained model has been used while fine-tuning <ref type="bibr" target="#b23">[24]</ref>.</p><p>2) UNet: UNet is built on the fully convolutional network. The structure of UNet was remodelled to work with fewer trained data which produce more accurate segmentation. The goal of UNet is enhancing a contracting layer to sequential layers. Upsampling operators are used for substituting those layers instead of pooling operations which produces better resolution of the output. A successive convolution layer learns to construct accurate output based on the data. <ref type="bibr" target="#b12">[13]</ref>.</p><p>In the upsampling segment, UNet has a substantial number of feature channels which allows higher resolution layers to have propagated context information. As a result, the extensive path becomes almost symmetric to the shrinking path <ref type="bibr" target="#b24">[25]</ref>.</p><p>The U-shaped architecture of UNet is produced by the contracting and expansive path. Contracting path is created by a typical convolutional neural network where each network is followed by a max-pooling operation and a rectified linear unit (ReLU). Throughout the contraction, spatial information is decreased while feature information is increased <ref type="bibr" target="#b12">[13]</ref>.</p><p>3) EfficientUNet: In typical UNet expansion path and the contracting path is almost symmetric. Decoder module is the same as the original UNet in this work and in the contracting path EfficientNet has been adopted as the encoder. Details of the proposed architecture including the number of channels, levels, resolution of each feature map are illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. Initial size of input images is 1400 ? 2100 which is resized to 1312 ? 2080 for ease of processing. First, feature map of the last logit of the encoder is upsampled bi-linearly by a factor of two and then concatenated with encoders feature map maintaining the same spatial resolution. Before upsampling by the factor of two it is followed by 3 ? 3 convolution layers. This process gets repeated until the segmentation map equals to the size of the input image reconstructs. Unlike the original UNet, this architecture is asymmetric where contracting path is deeper than expansion path. Addition of robust architecture like EfficientNet as encoder improved overall performance of the algorithm <ref type="bibr" target="#b25">[26]</ref>.</p><p>IV. RESULTS AND DISCUSSION Experimentation has been undertaken by applying six different versions of EfficentNet architectures mentioned previously.</p><p>Mean Validation Accuracy (mVA) has been measured for all EfficentNet architectures for both baseline and fine-tuned versions. The outcome of this part of the inspection has been presented in <ref type="table">Table I</ref> Cross validation result along with the public and private LB score (DSC) has been presented in <ref type="table">Table II</ref>.</p><p>The mean validation accuracy (mVA) score was best for the B0 model of EfficientNet architecture, as shown in <ref type="table">Table I</ref>, although the other versions (B1-B5) came close on both private and public leaderboard scores, as shown in <ref type="table">Table II</ref>. Cross validation scores were not stable since the image segmentation wasn't reliable, with fluctuations in the outcome, having only 63.89% in B0 while the other models had a higher value. As a result it was important to use an efficient segmentation approach to reach a stable and accurate result, as shown in <ref type="table">Table III</ref>.  <ref type="figure">6</ref>. Highest precision (0.74) corresponding to recall threshold was obtained for class: Sugar and lowest for class: Fish which was 0.55, while the maximum and minimum recall for class: Flower was 0.49 and class: Fish was 0.22.</p><p>The main objective of this experiment was to express in a meaningful manner that EfficientNet alone doesn't provide a satisfying outcome in a dataset like this, despite being a substantially efficient classification layer. We showed in our research that by initiating a different segmentation architecture like UNet alongside EfficientNet, we can improve the performance on datasets like the one used in this project. It is also worth noting that instead of using UNet as a segmentation architecture alone, we used EfficientNet's encoding capability and realized that EfficientUnet shows significant improvement on the outcome. By using a classification layer as an encoder and a segmentation architecture as decoder, we boosted the performance of both the models and made them work together for a better performance.</p><p>By boosting the peformance of both the architectures, using them as an encoder and a decoder, we proved that using them in such manner allows faster performance for memory intensive datasets, and can be used in various sectors such as medical image analysis, cellular image classification for cancer detection where the data are usually segmented.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper implements classification of satellite images of cloud structures into four different classes: Sugar, Gravel, Fish and Flower. 6 versions of EfficientNet from B0 to B5 were used as encoder and UNet as decoder has been applied. Dice coefficient was used as the evaluation metric. The scores used were compared with both public and private LB scores of Kaggle competition. By using a segmentation model like UNet in a classification problem, it was demonstrated that with a good encoder good performance can be achieved from the dataset.</p><p>Although EfficientNet was used in this paper, it could be replaced with a different model as well but was not tested in this research. Also, a good segmentation of the images boost the output of the classification drastically, which was also demonstrated in this paper. PR-Curves were generated to demonstrate the relation between precision and recall as it shows the compromise between precision and recall for various threshold.</p><p>In future, estimation of the distribution of classes and adjustment of the validation set could be implemented accordingly. As the complex architectures of EfficientNet gave less appreciating results because of default coefficients, altering these hyperparameters according to the dataset will hopefully improve the outcome. Exploring gradient weighted class activation mapping to generate a baseline, which is a class explainability technique, could be achieved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Masked images of four different class samples or thinner and altitude positions. There are several types of cloud structures. Out of all top level clouds Cirrus clouds are the most abundant. Cirrus means a "curl of hair"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Common modules which were used to implement layers of all 8 models of EfficientNet ? Module 1 acts as the starting block for the sub-blocks. ? Module 2 acts as the initializing point for the first subblock of all the 7 main blocks except the 1st block. ? Module 3 is used as a skip connection block for all the sub-blocks. ? Module 4 combines the skip connections that occurred in the first sub-blocks. ? Module 5 combines each sub-block that is connected to its previous sub-block in a skip connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>? 5 Fig. 4 .</head><label>54</label><figDesc>Depth scale adds more layers to the network. ? Resoultion scale increase resolution of input images. Sub-blocks using individual modules which are presented infigure 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Architecture of EfficientUNet with EfficientNet-B0 framework for semantic segmentation. Blocks of EfficientNet-B0 as encoder has been presented in figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.</figDesc><table><row><cell>EfficientNet</cell><cell cols="2">mean validation accuracy</cell></row><row><cell>Version</cell><cell cols="2">Baseline Fine Tuned</cell></row><row><cell>B0</cell><cell>0.670</cell><cell>0.836</cell></row><row><cell>B1</cell><cell>0.656</cell><cell>0.829</cell></row><row><cell>B2</cell><cell>0.657</cell><cell>0.827</cell></row><row><cell>B3</cell><cell>0.659</cell><cell>0.825</cell></row><row><cell>B4</cell><cell>0.640</cell><cell>0.798</cell></row><row><cell>B5</cell><cell>0.601</cell><cell>0.732</cell></row><row><cell></cell><cell>TABLE I</cell><cell></cell></row><row><cell cols="3">MEAN VALIDATION ACCURACY SCORES ON EFFICIENTNET VERSION</cell></row><row><cell></cell><cell>B0-B5.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>PRIVATE AND PUBLIC DSC LEADERBOARD SCORES OF EFFICIENTNET ARCHITECTURE FOR CLASSIFICATION.It became visible that an improved segmentation of images stabilized the scores, with a gradual trend in the scores and model B0 showing the best response in both cross validation and LB values, with 66.54% in cross validation, 66.02% in private and 66.26% in public leaderboard. The other models were not far behind either, and the scores decreased in a descending order with B1 being the second-best model with 66.01% on cross validation, 65.53% on private and 65.98% on public leaderboard. Recall curves for all four classes are shown inFig.</figDesc><table><row><cell>EfficientNet</cell><cell>Cross</cell><cell>LB score</cell><cell></cell></row><row><cell>Version</cell><cell>Validation</cell><cell>Private</cell><cell>Public</cell></row><row><cell>B0</cell><cell>0.6389</cell><cell cols="2">0.64595 0.65936</cell></row><row><cell>B1</cell><cell>0.6423</cell><cell cols="2">0.64640 0.65849</cell></row><row><cell>B2</cell><cell>0.6351</cell><cell cols="2">0.64551 0.65801</cell></row><row><cell>B3</cell><cell>0.6311</cell><cell cols="2">0.64585 0.65820</cell></row><row><cell>B4</cell><cell>0.6294</cell><cell cols="2">0.63911 0.65563</cell></row><row><cell>B5</cell><cell>0.6255</cell><cell cols="2">0.64059 0.65325</cell></row><row><cell></cell><cell cols="2">TABLE II</cell><cell></cell></row><row><cell>CROSS VALIDATION, EfficientNet</cell><cell>Cross</cell><cell cols="2">LB score</cell></row><row><cell>Version</cell><cell>Validation</cell><cell cols="2">Private Public</cell></row><row><cell>B0</cell><cell>0.6689</cell><cell>0.6611</cell><cell>0.6650</cell></row><row><cell>B1</cell><cell>0.6601</cell><cell>0.6553</cell><cell>0.6598</cell></row><row><cell>B2</cell><cell>0.6589</cell><cell>0.6570</cell><cell>0.6578</cell></row><row><cell>B3</cell><cell>0.6588</cell><cell>0.6500</cell><cell>0.6582</cell></row><row><cell>B4</cell><cell>0.6425</cell><cell>0.6417</cell><cell>0.6421</cell></row><row><cell>B5</cell><cell>0.6322</cell><cell>0.6319</cell><cell>0.6333</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell></row><row><cell cols="4">CROSS VALIDATION, PRIVATE AND PUBLIC DSC LB SCORES OF</cell></row><row><cell cols="4">EFFICIENTUNET FOR CLASSIFICATION</cell></row><row><cell>Precision-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>656 X 1040 X 32 656 X 1040 X 16 328 X 520 X 24 164 X 260 X 40 164 X 260 X 80 82 X 130 X 112 41 X 65 X 192 41 X 65 X 320 82 X 130 X 256 164 X 260 X 128 328 X 520 X 64 656 X 1040 X 32 1312 X 2080 X 16</head><label></label><figDesc></figDesc><table><row><cell>1312 X 2080 X 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Image</cell></row><row><cell>Conv2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv2D</cell></row><row><cell>Block 1</cell><cell>Block 2</cell><cell>Block 3</cell><cell>Block 4</cell><cell>Concat</cell><cell>Up-Conv2D</cell><cell>Concat</cell><cell>Up-Conv2D</cell><cell>Concat</cell><cell>Up-Conv2D</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Block 5</cell><cell>Up-Conv2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Block 6</cell><cell>Concat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Block 7</cell><cell>Up-Conv2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">LB: Leaderboard</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank Kaggle for hosting such a great contest and for their kernel (notebook). We also thank Max Planck Institute for Meteorology for providing the dataset. We also admire Pavel Yakubovskiy for sharing his contribution in GitHub repository from where we managed to access the pretrained imagenet weights.</p><p>Source code of this project is available at https://github.com/ TashinAhmed/CloudsClassification.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Thin liquid water clouds: Their importance and our challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vogelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cady-Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khaiyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liljegren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pollution and the planetary albedo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Twomey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmos. Environ</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1251" to="1256" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Changes in earth&apos;s reflectance over the past two decades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pall?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montanes-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koonin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">5675</biblScope>
			<biblScope unit="page" from="1299" to="1301" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Increased insolation threshold for runaway greenhouse processes on earth-like planets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leconte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Forget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Charnay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wordsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pottier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">504</biblScope>
			<biblScope unit="issue">7479</biblScope>
			<biblScope unit="page" from="268" to="271" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A summary of the physical properties of cirrus clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="970" to="978" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noctilucent clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schr?der</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Noctilucent Clouds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cloud distributions in the vicinity of jet streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Mclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="579" to="583" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detection of transverse cirrus bands in satellite imagery using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Geosciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new classification approach for detecting severe weather patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R T</forename><surname>De Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stephany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; geosciences</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="158" to="165" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Impact of aerosol particles on cloud formation: Aircraft measurements in china</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmospheric Environment</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="665" to="672" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The ice particle threat to engines in flight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Strapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th AIAA Aerospace Sciences Meeting and Exhibit</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">206</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
	</analytic>
	<monogr>
		<title level="m">Fig. 6. PR-Curves for All Four Classes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Method of encoding image pixel values for storage as compressed digital data and method of decoding the compressed digital data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">812</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using of jaccard coefficient for keywords similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niwattanakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singthongchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Naenudorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international multiconference of engineers and computer scientists</title>
		<meeting>the international multiconference of engineers and computer scientists</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="380" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations,&quot; in Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Compounding the performance improvements of assembled techniques in a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06268</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yakubovskiy</surname></persName>
		</author>
		<ptr target="https://github.com/qubvel/segmentationmodels" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eff-unet: A novel architecture for semantic segmentation in unstructured environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Innani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gajre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Talbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="358" to="359" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
