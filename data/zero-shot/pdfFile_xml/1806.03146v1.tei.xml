<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Message Passing with Edge Updates for Predicting Properties of Molecules and Materials</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">Bj?rn</forename><surname>J?rgensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">Wedel</forename><surname>Jacobsen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Physics Technical</orgName>
								<orgName type="institution">University of Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikkel</forename><forename type="middle">N</forename><surname>Schmidt</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Applied Mathematics and Computer Science Technical</orgName>
								<orgName type="institution">University of Denmark</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Message Passing with Edge Updates for Predicting Properties of Molecules and Materials</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural message passing on molecular graphs is one of the most promising methods for predicting formation energy and other properties of molecules and materials. In this work we extend the neural message passing model with an edge update network which allows the information exchanged between atoms to depend on the hidden state of the receiving atom. We benchmark the proposed model on three publicly available datasets (QM9, The Materials Project and OQMD) and show that the proposed model yields superior prediction of formation energies and other properties on all three datasets in comparison with the best published results. Furthermore we investigate different methods for constructing the graph used to represent crystalline structures and we find that using a graph based on K-nearest neighbors achieves better prediction accuracy than using maximum distance cutoff or the Voronoi tessellation graph.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The current workhorse for screening of new molecules and materials is Density Functional Theory (DFT), but machine learning methods show a potential for trading DFT accuracy for a several orders of magnitude decrease in computation time. Until recently machine learning methods for predicting properties of molecules and materials have been based on hand crafted feature descriptors, such as Coulomb matrix <ref type="bibr" target="#b17">(Rupp et al., 2012)</ref>, bag-of-bonds <ref type="bibr" target="#b7">(Hansen et al., 2015)</ref>, fingerprints (e.g. <ref type="bibr" target="#b15">Rogers and Hahn 2010)</ref> and histogram of angles <ref type="bibr" target="#b4">(Faber et al., 2017)</ref>. With the availability of large databases of molecules and materials we are now seeing a shift towards data-driven representation learning as we have seen in the computer vision field.</p><p>The graph neural network model was introduced by <ref type="bibr" target="#b6">Gori et al. (2005)</ref>; <ref type="bibr" target="#b20">Scarselli et al. (2009)</ref> and regained attention when <ref type="bibr" target="#b13">Li et al. (2015)</ref> expanded the model for different graph problems. In the last few years a number of graph-based models for molecules have been proposed <ref type="bibr" target="#b3">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b10">Kearnes et al., 2016;</ref><ref type="bibr">Sch?tt et al., 2017a,b;</ref><ref type="bibr" target="#b5">Gilmer et al., 2017)</ref>. These models can all be cast into the framework of message passing on a molecular graph as shown by <ref type="bibr" target="#b5">Gilmer et al. (2017)</ref>.</p><p>The molecular graphs used as input for these models are either the topological graph defined by the chemical bonds (with or without bond lengths) or a fully connected graph where the pairwise distances between all the atoms are used as edge features. The topological approach does not directly apply to crystalline structures because the chemical bonds are less well-defined and the fully connected approach is impossible because the structure is infinite. The first application of neural message passing for materials is SchNet <ref type="bibr" target="#b23">(Sch?tt et al., 2017c)</ref> where a constant cutoff distance is used. As noted by <ref type="bibr" target="#b23">Sch?tt et al. (2017c)</ref> this may lead to "isolated atoms" if the cutoff distance is too small and the computational burden increases with the size of the cutoff. This motivates us to investigate different ways to define the molecular graphs based on nearest neighbor cutoff. Concurrently with the work presented in this paper <ref type="bibr" target="#b25">Xie and Grossman (2018)</ref> have also employed neural message passing on graphs based on Voronoi tessellation and K-nearest neighbors, but they do not include a comparison between the methods.</p><p>The edge neural network with set-to-set readout function (enn-s2s)  demonstrated state of the art prediction accuracy on the 13 properties of the QM9 <ref type="bibr" target="#b14">(Ramakrishnan et al., 2014;</ref><ref type="bibr" target="#b16">Ruddigkeit et al., 2012)</ref> dataset consisting of 134k molecules. The recently proposed SchNet <ref type="bibr">(Sch?tt et al., 2017b,c)</ref> network improves the accuracy on eight out of the twelve properties. In both of these models the information exchanged between the atoms in the message passing scheme depends on the representation of the sending atom and the edge feature on which the message is passed, but is independent of the representation of the receiving atom. <ref type="bibr" target="#b5">Gilmer et al. (2017)</ref> proposed the use of a "pair message" network that includes the state of the receiving atom, but its predicting performance is inferior to the enn-s2s network. In this work we propose to extend the SchNet model with an edge update network such that the edge feature depends on the representation of the atoms that the edge connects. This in turn means the information exchanged between the atoms also depends on the receiving atom. Edge update networks was also utilised in the Weave module proposed by <ref type="bibr" target="#b10">Kearnes et al. (2016)</ref>, but there the edges are forced to be undirected and the prediction accuracy is below that of enn-s2s .</p><p>We benchmark the proposed edge update network on QM9 molecules, Materials Project <ref type="bibr" target="#b9">(Jain et al., 2013)</ref> and OQMD <ref type="bibr" target="#b19">(Saal et al., 2013;</ref><ref type="bibr" target="#b12">Kirklin et al., 2015)</ref> datasets and it shows an improvement over current state of the art results on all three datasets. For crystalline structures we show that it is beneficial to use a K-nearest neighbor graph rather than a graph defined by a constant cutoff distance as used in previous work <ref type="bibr" target="#b23">(Sch?tt et al., 2017c)</ref>.</p><p>The paper is organised as follows. We present the proposed model within the neural message passing framework in Section 2 and we show how it is different from other models within the same framework in Section 3. We introduce the three datasets in Section 4, which are used for benchmarking the model in Section 5 and we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Message Passing Neural Networks</head><p>We describe message passing neural networks similarly to  as a model operating on a graph G with vertex features x v and edge features ? vw . Each vertex has a hidden state h t v and each edge has a hidden state e t vw which are updated in a number of interaction steps T . Vertices are updated using a message function M t (?) and a state transition function S t (?)</p><formula xml:id="formula_0">m t+1 v = w?N (v) M t (h t v , h t w , e t vw ),<label>(1)</label></formula><formula xml:id="formula_1">h t+1 v = S t h t v , m t+1 v ,<label>(2)</label></formula><p>where N (v) denotes the neighbourhood of v, i.e. the vertices that have an edge to v. The edges are updated by an edge update function E t (?) that depends of the previous edge state and the states of the sending and receiving vertices of the current step</p><formula xml:id="formula_2">e t+1 vw = E t h t+1 v , h t+1 w , e t vw .<label>(3)</label></formula><p>After the T interaction steps a readout function R(?) is applied which maps the set of vertex states into a single entity?</p><formula xml:id="formula_3">= R {h T v ? G} .<label>(4)</label></formula><p>The readout function must be invariant to permutation of the vertex set, which is often achieved via summation over the vertex features. The functions M t , S t , E t and R are all implemented as neural networks with trainable parameters and can be optimised using gradient descent. Some models, for example Deep Tensor Neural Network <ref type="bibr" target="#b21">(Sch?tt et al., 2017a)</ref>, use weight sharing across the interaction steps, thus M t ? M , S t ? S and E t ? E for all time steps t. A range of graph convolution models <ref type="bibr" target="#b3">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b13">Li et al., 2015;</ref><ref type="bibr" target="#b0">Battaglia et al., 2016;</ref><ref type="bibr" target="#b10">Kearnes et al., 2016;</ref><ref type="bibr" target="#b21">Sch?tt et al., 2017a)</ref> including Laplacian based models <ref type="bibr" target="#b2">(Bruna et al., 2013)</ref> can be cast into this message passing framework as shown by <ref type="bibr" target="#b5">Gilmer et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SchNet with Edge Update Network</head><p>We now describe how our model fits into the message passing framework described above. As vertex input features, x v , we use the atomic numbers which are translated into an embedding vector for each atomic number as in <ref type="bibr">(Sch?tt et al., 2017a,b,c)</ref>. The initial hidden state h 0 v = (x v ) is thus the result of a lookup function : Z ? R C . The hidden state is a representation of the atom and its chemical environment and the idea behind the message passing interaction steps is to refine this representation based on the surrounding atoms and their chemical environments. Since the interaction between atoms depends on the distance between them (Coulomb's law) we use the interatomic distances as initial edge features. We then expand the initial edge feature using a radial basis function. Denoting the distance between atom v and w as d vw , the edge between node v and w in the graph has initial feature vector</p><formula xml:id="formula_4">(? vw ) k = exp ? (d vw ? (?? min + k?)) 2 ? , k = 0 . . . k max<label>(5)</label></formula><p>where ? min , ?, and k max are chosen such that the centers of the functions covers the range of the input features. In all our experiments we set ? min = 0 ?, ? = 0.1 ?, k max = 150. This expansion makes it easier for the neural network model to decorrelate input and output similar to how 1-hot-encoding is preferred over integer coding for categorical features.</p><p>The role of the hidden edge representation e t vw is to control how the two connected atoms interact. The idea of using an edge update network is to let the updated atomic hidden states influence this interaction. We use an edge update function at each interaction step implemented as a two layer feed-forward neural network. The input to the network is the concatenation of the edge representation and the hidden states of the receiving and sending vertices. Thus</p><formula xml:id="formula_5">e 0 vw = E 0 (h 0 v , h 0 w , ? vw ) = g(W 0 E2 g(W 0 E1 (h 0 v ; h 0 w ; ? vw ))),<label>(6)</label></formula><p>and similarly for the subsequent steps</p><formula xml:id="formula_6">e t+1 vw = g(W t+1 E2 g(W t+1 E1 (h t+1 v ; h t+1 w ; e t vw ))),<label>(7)</label></formula><p>where g(x) = ln(e x +1) ? ln <ref type="formula" target="#formula_1">(2)</ref> is the shifted soft-plus activation function (?; ?) denotes vector concatenation and {W 0 E1 , W 0 E2 , W t+1 E1 , W t+1 E2 } are trainable weight matrices. This update makes the network edges directional.</p><p>The message function itself is only a function of the sending node and can be written as</p><formula xml:id="formula_7">M t (h t v , h t w , e t vw ) = M t (h t w , e t vw ) = (W t 1 h t w ) g(W t 3 g(W t 2 e t vw )),<label>(8)</label></formula><p>where denotes element-wise multiplication and {W t 1 , W t 2 , W t 3 } are trainable weight matrices. This message function enables the interpretation of the function on the right hand side of the element-wise multiplication as a filter-generating function <ref type="bibr">(Sch?tt et al., 2017b,c)</ref> </p><formula xml:id="formula_8">f t g (e t vw ) = g(W t 3 g(W t 2 e t vw )</formula><p>), a continuous analogous to the filters applied over a discrete domain in a convolutional neural network for image processing. We visualize this function towards the end of this section. The shifted softplus activation function is chosen to follow <ref type="bibr">Sch?tt et al. (2017b,c)</ref> and can be seen as an infinitely differentiable alternative to the rectified linear unit (ReLU) activation function.</p><p>The state transition function applies a two layer neural network on the sum of incoming messages and adds this to the current hidden state as in Residual Networks <ref type="bibr" target="#b8">(He et al., 2015)</ref>:</p><formula xml:id="formula_9">S t h t v , m t+1 v = h t v + W t 5 g(W t 4 m t+1 v ),<label>(9)</label></formula><p>where {W t 4 , W t 5 } are trainable weight matrices. After a number of interaction steps T all the information about the property we want to predict must be contained in the set of hidden node states. We apply a readout function for which we use a two layer neural network that maps the hidden representation to a scalar and finally we sum over the contribution from each atom, i.e.</p><formula xml:id="formula_10">R h T v ? G = h T v ?G W 7 g(W 6 h T v ),<label>(10)</label></formula><p>where {W 7 , W 6 } are also trainable weight matrices. The model architecture and flow of computations is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The original SchNet architecture is obtained if we "remove" the edge update function, i.e. by setting E t (h t v , h t w , e t?1 vw ) = e t?1 vw . This property enables direct comparison between the two models.</p><p>We want to visualize the filter generating function to qualitatively assess what the model learns, but because of the edge update network, the edge feature of the first layer e 0 vw does not only depend on the distance d vw between the two atoms, but also on the atom embeddings, i.e.</p><formula xml:id="formula_11">f 0 g (d vw ) = f 0 g (E 0 (h 0 v , h 0 w , RBF(d vw ))),<label>(11)</label></formula><p>where RBF(?) is the radial basis function expansion defined in (5). We can now plot the filter response of the first layer as a function of the sending and receiving atoms and the distance between them as shown in <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="figure">Figure 3</ref>. We see that the filter-generating function is almost identical across the sending atom species but the variation across the receiving atoms is more significant. This indicates that the model has learned to shape the messages depending on the receiving atom. The ability to condition the filter on the pair of sending and receiving atoms is the key difference between the proposed model and the SchNet model which is using the same filter for all combinations of sending and receiving atoms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Our model is closely related to the other message passing neural network models used for molecular properties prediction. However, the only message passing neural network that has explored the use of edge updates is the Weave module <ref type="bibr" target="#b10">(Kearnes et al., 2016)</ref></p><formula xml:id="formula_12">. The message function is M t (h t v , h t w , e t vw ) = ?(W t 1 e t vw ) and the state transition function is S t (h t v , m t+1 v ) = ?(W t 3 (?(W t 2 h t v ); m t+1 v ))</formula><p>where ?(?) is the rectified linear unit activation function, (?; ?) denotes vector concatenation and {W t 1 , W t 2 , W t 3 } are weight matrices. Unlike our model where the edge updates are interleaved with the node updates, the edge updates are done in parallel, i.e.   <ref type="figure">Figure 3</ref>: Filter variation for all 64 learned filters at the first layer of the message passing architecture trained on prediction of formation energy U 0 of the QM9 dataset. Each row of pixels corresponds to one of the 64 filters and the color encodes the deviation (?0.4 is blue and 0.4 is red) from the corresponding average filter (averaged over sending and receiving atom). The enumeration of the filters is arbitrary and we have sorted them according to the value of the deviation at 1.0 for the filter H?H. The filter generating function shows a higher dependence on the species of the receiving atom than the species of the sending atom.</p><formula xml:id="formula_13">e t+1 vw = E t h t v , h t w , e t vw (12) = ?(W t 6 (?(W t 4 (h v , h w )) + ?(W t 4 (h w , h v )), ?(W t 5 e t<label>vw ))</label></formula><p>Another difference is that the edge updates in the Weave module are invariant to permutation of h v and h w and the edges are thus undirected by design. The Weave module does not use the Euclidean distance between the atoms as input features, but uses the graph distance on the chemical graph. The model was tested on a range of different classification tasks as well as drug efficacy, photovoltaic efficiency and solubility regression tasks. The model was reimplemented by <ref type="bibr" target="#b4">Faber et al. (2017)</ref> and <ref type="bibr" target="#b5">Gilmer et al. (2017)</ref>. The version by <ref type="bibr" target="#b4">Faber et al. (2017)</ref> includes a few modifications. The edges are no longer forced to be undirected, i.e.</p><formula xml:id="formula_14">e t+1 vw = E t h t v , h t w , e t vw<label>(13)</label></formula><p>= ?(W t 6 (?(W t 4 (h v , h w )), ?(W t 5 e t vw ))) and the Euclidean distance between the atoms is included in the message function by division of the edge feature with a range of different powers of the distance d vw :</p><formula xml:id="formula_15">M t (h t v , h t w , e t vw ) = ? concatenate k?{0,1,2,3,6} W t 1 e t vw d k vw<label>(14)</label></formula><p>Both versions of the graph convolution model based on the Weave module shows similar performance on the QM9 regression benchmark <ref type="bibr" target="#b4">Faber et al., 2017)</ref>, but the mean absolute prediction error is significantly higher than that of the enn-s2s and SchNet models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>We benchmark the proposed model on three publicly available datasets.</p><p>Quantum Machines 9 (QM9) <ref type="bibr" target="#b14">(Ramakrishnan et al., 2014;</ref><ref type="bibr" target="#b16">Ruddigkeit et al., 2012)</ref> The dataset contains 133 885 examples of stable small organic molecules with up to 7 heavy atoms (CONF) and The Materials Project <ref type="bibr" target="#b9">(Jain et al., 2013)</ref> This dataset contains geometries and formation energies of 69 640 inorganic compounds with input structures taken from the ICSD database <ref type="bibr" target="#b1">(Bergerhoff et al., 1983)</ref>. We use the latest version of the database (version 2.0.0). The target property is heat of formation. The number of examples is reduced to 69 539 after we exclude all materials with noble gases (He, Ne, Ar, Kr, Xe) because they occur so infrequently in the dataset. This brings the number of different elements in the dataset down to 84. Following <ref type="bibr" target="#b23">(Sch?tt et al., 2017c)</ref>  Open Quantum Materials Database (OQMD) <ref type="bibr" target="#b19">(Saal et al., 2013;</ref><ref type="bibr" target="#b12">Kirklin et al., 2015)</ref> Is also a database of inorganic structures. We have extracted the database from the supplemental material of <ref type="bibr" target="#b24">(Ward et al., 2017)</ref>. Again we consider materials with noble gases as outliers and we also exclude (highly unstable) materials with a heat of formation of more than 5 eV/atom. We thus exclude 210 out of 435 792 examples. As with the materials project we use 20% (87 116) of the examples as a test set, 5000 for validation and the remaining 343 466 for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We train the models with ADAM (Kingma and Ba, 2014) with initial learning rate 5 ? 10 ?4 or 1 ? 10 ?4 (when the higher learning rate leads to instability) and decrease the learning rate by multiplying with 0.96 every 100 000 gradient steps. We use a minibatch size of 32, run the optimization for up to 1 ? 10 7 gradient steps, compute the validation error every 50 000 steps and terminate if there was no improvement within the last 1 ? 10 6 steps. When applying the model on the two materials datasets (Materials Project and OQMD) we use an average over messages rather than a sum in (1) because <ref type="bibr" target="#b23">Sch?tt et al. (2017c)</ref> found that it increases the stability of the optimization. For these datasets (as well as for some of the QM9 properties, specifically ? HOMO , ? LUMO , ??, ZPVE, ?, ? and R 2 ) we also take the average rather than the sum in the readout function (10) because the target property is formation energy per atom. When the readout function uses a sum we first estimate the target mean and standard deviation atom-wise, i.e.? = i ti ni ,? = i ( ti ni ??) 2 where t i and n i are the target and number of atoms for the ith training sample. The targets are then normalised using t i = ti?ni? ? such that each scalar of the sum (10) is expected to have zero mean and unit variance for a given sample with known n i . When the readout function uses the average rather than a sum the targets are normalised to zero mean and unit variance in the "usual" way, irrespectively of the number of atoms. For the QM9 experiments we use a hidden state representation of dimension C = 64 and <ref type="table">Table 2</ref>: Mean absolute error of formation energy predictions for V-RF, SchNet and the proposed model. For QM9 the error is in meV and for the Materials Project and OQMD the numbers are in meV/atom. The lowest error is highlighted in bold. We have obtained the V-RF results by running the implementation provided by the authors <ref type="bibr" target="#b24">(Ward et al., 2017)</ref>, while SchNet results have been obtained by running our own SchNet implementation. The numbers in parenthesis are the estimated 95th percentile, which have been obtained by sampling the test set (with replacement) 1 ? 10 6 times.</p><p>Model for the two materials datasets we increase the dimensionality to C = 256 because the number of different elements in these datasets is 84 rather than 5 in QM9. In agreement with the results shown in <ref type="bibr" target="#b23">(Sch?tt et al., 2017c)</ref> and also noticed in  we did not see a gain in prediction accuracy when using more than 3 interaction steps, so the results presented here are are all with T = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Edge Update Network</head><p>To assess the effect of the edge update network we train the proposed model with and without the edge update network to predict formation energies of the three datasets. Without the edge update network the model reduces to the SchNet model. For the two materials datasets (Materials Project and OQMD) we use a cutoff distance of 5 ? as used in <ref type="bibr" target="#b23">(Sch?tt et al., 2017c)</ref> when constructing the graphs. In the benchmark we also include a state of the art non-deep learning, graph-based method, which creates a graph based on the Voronoi diagram, extracts a number of features from the graph and uses a random forest regression model for making predictions and we refer to this method as V-RF <ref type="bibr" target="#b24">(Ward et al., 2017)</ref>. The model also uses the spatial information, but the interatomic distances are normalised such that the model's predictions are independent of the scaling. We benchmark all three models on the same training and test set, but V-RF also uses the validation set for training because early stopping is not used for this model. The mean absolute error for the test set predictions are shown in <ref type="table">Table 2</ref>. In all three benchmarks we observe a big improvement in prediction accuracy when using the edge update network. This is not only due to the increase in number of parameters, because merely increasing the number of interaction steps does not improve the performance of the model.</p><p>We also train the proposed model to predict the 12 properties of the QM9 dataset. The results of SchNet <ref type="bibr" target="#b23">(Sch?tt et al., 2017c)</ref> and enn-s2s  are included as references. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. The proposed model demonstrates a significant improvement in the prediction of 9 out of 12 properties and matches the existing results for the remaining 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Choosing The Cutoff</head><p>We want to investigate the importance of choosing a cutoff when constructing the graph used as input to the neural message passing models. The choice of cutoff is important because the computational complexity of the algorithm increases linearly with the number of edges in the graph. On the other hand if the number of edges is too small the interaction between the nodes of the graph may be too limited. In <ref type="bibr" target="#b23">(Sch?tt et al., 2017c)</ref> and in our experiments above a constant cutoff distance of 5 ? is used. We use the OQMD dataset for this experiment and use the formation energy as the target for predictions. With this cutoff distance some of the atoms of the dataset becomes isolated and further increasing the cutoff distance comes with prohibitive computational cost. Alternatively we can use a K-nearest cutoff method such that each atom receives messages from the K nearest neighboring atoms no matter how far away they are. This makes the connections between atoms asymmetrical, but that is not necessarily a problem. Finally we can use the neighbors as obtained through Voronoi tessellation, i.e. two atoms are connected if they are neighboring cells in a Voronoi diagram. The connections are symmetrical and we also avoid isolated atoms. We use the software package Voro++ <ref type="bibr" target="#b18">(Rycroft, 2009</ref>) to compute the tessellation.  <ref type="bibr" target="#b23">(Sch?tt et al., 2017c)</ref> and    The prediction accuracy of the model when using different cutoff methods is shown in <ref type="figure">Figure 4</ref>. We also show the average number of incoming edges across all atoms in the OQMD dataset in <ref type="figure">Figure 5</ref>, which serves as a proxy for the computational complexity. The results shows that the K-nearest cutoff is more efficient in terms of achieving a low MAE for a given average number of edges per atom and we achieve the lowest error (13.7 meV) with K = 24. This is not only caused by eliminating the "isolated atoms problem", because the Voronoi tessellation method is also without this problem. One reason could be that training the model is more stable when the number of incoming messages is constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed a novel neural message passing model for molecules and materials by extending the SchNet model with an edge update network that allows the information exchanged between atoms to be dependent on the sending and receiving atom. This simple extension leads to a remarkable improvement in prediction accuracy. By inspecting the learned filters for the molecular formation energy prediction task we found that the edge updates in the first layer has a higher dependence on the receiving atom than the sending atom. Through numerical simulations we demonstrate improvements in accuracy on formation energy prediction of molecules and materials across all three benchmark datasets. We also highlight the problem of choosing the cutoff when constructing the graphs used as input for the model. We found that using K-nearest neighbors cutoff yields lower error than using a constant cutoff distance. We believe these results are important for applications of message passing neural networks for predicting properties of molecules and materials and hope to see more applications and further improvements of the model architecture in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flow of computations of the proposed model. The dimension of the hidden state for each atom state is C and the "fc"-blocks are fully connected layers annotated with the output dimension and the applied activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example of one of the learned filters (out of 64) at the first layer of the message passing architecture trained on prediction of formation energy U 0 of the QM9 dataset. The filter depends on the embedding of the sending (the rows) and the receiving (the columns) nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>we randomly sample 60 000 of the examples to use as the training set. Of the remaining examples we use a set of 5000 for validation and the remaining 4539 examples are used for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>10.5 (11.1) 22.7 (24.0) 14.9 (15.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>MAE of formation energy predictions on OQMD using different cutoffs (cutoff distance in ?ngstr?m, K-nearest and Voronoi) for constructing the graphs. The black dots show the mean absolute error for the validation set used for early stopping. The lowest error is 13.7 meV/atom using K-nearest cutoff with K=24. Average number of incoming messages per atom for all atoms in OQMD. The error bars show the standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Target properties of the QM9 dataset. Target Description ? HOMO Energy of highest occupied molecular orbital (HOMO) ? LUMO Energy of lowest occupied molecular orbital (LUMO)</figDesc><table><row><cell>??</cell><cell>Difference between LUMO and HOMO</cell></row><row><cell cols="2">ZPVE Zero point vibrational energy</cell></row><row><cell>?</cell><cell>Dipole moment</cell></row><row><cell>?</cell><cell>Isotropic polarizability</cell></row><row><cell>R 2</cell><cell>Electronic spatial extent</cell></row><row><cell>U 0</cell><cell>Internal energy at 0 K</cell></row><row><cell>U</cell><cell>Internal energy at 298.15 K</cell></row><row><cell>H</cell><cell>Enthalpy at 298.15 K</cell></row><row><cell>G</cell><cell>Free energy at 298.15 K</cell></row><row><cell>C v</cell><cell>Heat capacity at 298.15 K</cell></row><row><cell cols="2">up to 29 atoms in total including H. The 12 target properties for each example are shown in Table</cell></row><row><cell cols="2">1. All properties are calculated at the B3LYP/6-31G(2df,p) level of quantum chemistry. Following</cell></row><row><cell cols="2">Gilmer et al. (2017); Sch?tt et al. (2017c) we randomly select a training set of 110 000 molecules,</cell></row></table><note>a validation set of 10 000 molecules and the remaining 13 885 examples are used for testing. The validation set is used for early stopping and model selection.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean absolute error of predictions for different target properties of the QM9 dataset using 110k training examples. The lowest error is highlighted in bold. SchNet and enn-s2s results are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>respectively. The numbers in parenthesis are the estimated 95th percentile, which have been obtained by sampling the test set (with replacement) 1 ? 10 6 times.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Target Unit</cell><cell cols="3">SchNet enn-s2s Proposed (95th)</cell></row><row><cell></cell><cell></cell><cell cols="2">? HOMO meV</cell><cell>41</cell><cell>43</cell><cell>36.7</cell><cell>(37.3)</cell></row><row><cell></cell><cell></cell><cell cols="2">? LUMO meV</cell><cell>34</cell><cell>37</cell><cell>30.8</cell><cell>(31.3)</cell></row><row><cell></cell><cell></cell><cell>??</cell><cell>meV</cell><cell>63</cell><cell>69</cell><cell>58.0</cell><cell>(58.9)</cell></row><row><cell></cell><cell></cell><cell cols="2">ZPVE meV</cell><cell>1.7</cell><cell>1.5</cell><cell>1.49</cell><cell>(1.52)</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>Debye</cell><cell>0.033</cell><cell>0.030</cell><cell>0.029</cell><cell>(0.029)</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>Bohr 3</cell><cell>0.235</cell><cell>0.092</cell><cell>0.077</cell><cell>(0.082)</cell></row><row><cell></cell><cell></cell><cell>R 2</cell><cell>Bohr 2</cell><cell>0.073</cell><cell>0.180</cell><cell>0.072</cell><cell>(0.075)</cell></row><row><cell></cell><cell></cell><cell>U 0</cell><cell>meV</cell><cell>14</cell><cell>19</cell><cell>10.5</cell><cell>(11.1)</cell></row><row><cell></cell><cell></cell><cell>U</cell><cell>meV</cell><cell>19</cell><cell>19</cell><cell>10.6</cell><cell>(11.2)</cell></row><row><cell></cell><cell></cell><cell>H</cell><cell>meV</cell><cell>14</cell><cell>17</cell><cell>11.3</cell><cell>(11.9)</cell></row><row><cell></cell><cell></cell><cell>G</cell><cell>meV</cell><cell>14</cell><cell>19</cell><cell>12.2</cell><cell>(12.7)</cell></row><row><cell></cell><cell></cell><cell>C v</cell><cell cols="2">cal/molK 0.033</cell><cell>0.040</cell><cell>0.032</cell><cell>(0.033)</cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAE [meV/atom]</cell><cell>10 15 20 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>d=3.5 d=4 d=5 d=6</cell><cell cols="2">Cutoff K=6 K=12 K=24 K=48 Voronoi</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Preprint.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>1612.00222</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The inorganic crystal structure data base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bergerhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sievers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="69" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Spectral networks and locally connected networks on graphs. 1312.6203</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno>1509.09292</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prediction errors of molecular machine learning models lower than hybrid DFT error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5255" to="5264" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning predictions of molecular properties: Accurate Many-Body potentials and nonlocality in chemical space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Von Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. Chem. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2326" to="2331" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Materials Project: A materials genome approach to accelerating materials innovation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hautier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cholia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ceder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APL Materials</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11002</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Aided Mol. Des</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The open quantum materials database (OQMD): assessing the accuracy of DFT formation energies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kirklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Doak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aykol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>R?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Computational Materials</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15010</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno>1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and accurate modeling of molecular atomization energies with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">58301</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VORO++: a three-dimensional voronoi cell library in c++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Rycroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41111</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Materials design and discovery with High-Throughput density functional theory: The open quantum materials database (OQMD)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kirklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aykol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1501" to="1509" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantumchemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">SchNet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<idno>1706.08566</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SchNet -a deep learning architecture for molecules and materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<idno>1712.06113</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Including crystal structure attributes in machine learning models of formation energies via voronoi tessellations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. B Condens. Matter</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24104</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
