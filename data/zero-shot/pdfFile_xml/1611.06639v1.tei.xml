<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-11-21">21 Nov 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
							<email>zhoupeng2013@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country>China (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
							<email>zhenyu.qi@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country>China (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
							<email>zhengsuncong@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country>China (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
							<email>jiaming.xu@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country>China (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
							<email>hongyun.bao@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country>China (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<email>xubo@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country>China (</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-11-21">21 Nov 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variablelength text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is an essential component in many NLP applications, such as sentiment analysis <ref type="bibr" target="#b23">(Socher et al., 2013)</ref>, relation extraction <ref type="bibr" target="#b32">(Zeng et al., 2014)</ref> and spam detection <ref type="bibr" target="#b27">(Wang, 2010)</ref>. Therefore, it has attracted considerable attention from many researchers, and various types of models have been proposed. As a traditional method, the bag-of-words (BoW) model treats texts as unordered sets of words <ref type="bibr" target="#b26">(Wang and Manning, 2012)</ref>. In this way, however, it fails to encode word order and syntactic feature.</p><p>Recently, order-sensitive models based on neural networks have achieved tremendous success for text classification, and shown more significant progress compared with BoW models. The challenge for textual modeling is how to capture features for different text units, such as phrases, sentences and documents. Benefiting from its recurrent structure, RNN, as an alternative type of neural networks, is very suitable to process the variable-length text.</p><p>RNN can capitalize on distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. This matrix includes two dimensions: the time-step dimension and the feature vector dimension, and it will be updated in the process of learning feature representation. Then RNN utilizes 1D max pooling operation <ref type="bibr" target="#b11">(Lai et al., 2015)</ref> or attention-based operation <ref type="bibr" target="#b37">(Zhou et al., 2016)</ref>, which extracts maximum values or generates a weighted representation over the time-step dimension of the matrix, to obtain a fixed-length vector. Both of the two operators ignore features on the feature vector dimension, which maybe important for sentence representation, therefore the use of 1D max pooling and attention-based operators may pose a serious limitation.</p><p>Convolutional Neural Networks (CNN) <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b8">Kim, 2014)</ref> utilizes 1D convolution to perform the feature mapping, and then applies 1D max pooling operation over the time-step dimension to obtain a fixed-length output. However the elements in the matrix learned by RNN are not independent, as RNN reads a sentence word by word, one can effectively treat the matrix as an 'image'. Unlike in NLP, CNN in image processing tasks <ref type="bibr" target="#b14">(LeCun et al., 1998;</ref> applies 2D convolution and 2D pooling operation to get a representation of the input. It is a good choice to utilize 2D convolution and 2D pooling to sample more meaningful features on both the time-step dimension and the feature vector dimension for text classification.</p><p>Above all, this paper proposes Bidirectional Long Short-Term Memory Networks with Two-Dimensional Max Pooling (BLSTM-2DPooling) to capture features on both the time-step dimension and the feature vector dimension. It first utilizes Bidirectional Long Short-Term Memory Networks (BLSTM) to transform the text into vectors. And then 2D max pooling operation is utilized to obtain a fixed-length vector. This paper also applies 2D convolution (BLSTM-2DCNN) to capture more meaningful features to represent the input text.</p><p>The contributions of this paper can be summarized as follows:</p><p>? This paper proposes a combined framework, which utilizes BLSTM to capture long-term sentence dependencies, and extracts features by 2D convolution and 2D max pooling operation for sequence modeling tasks. To the best of our knowledge, this work is the first example of using 2D convolution and 2D max pooling operation in NLP tasks.</p><p>? This work introduces two combined models BLSTM-2DPooling and BLSTM-2DCNN, and verifies them on six text classification tasks, including sentiment analysis, question classification, subjectivity classification, and newsgroups classification. Compared with the state-of-the-art models, BLSTM-2DCNN achieves excellent performance on 4 out of 6 tasks. Specifically, it achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.</p><p>? To better understand the effect of 2D convolution and 2D max pooling operation, this paper conducts experiments on Stanford Sentiment Treebank fine-grained task. It first depicts the performance of the proposed models on different length of sentences, and then conducts a sensitivity analysis of 2D filter and max pooling size.</p><p>The remainder of the paper is organized as follows. In Section 2, the related work about text classification is reviewed. Section 3 presents the BLSTM-2DCNN architectures for text classification in detail. Section 4 describes details about the setup of the experiments. Section 5 presents the experimental results. The conclusion is drawn in the section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning based neural network models have achieved great improvement on text classification tasks. These models generally consist of a projection layer that maps words of text to vectors. And then combine the vectors with different neural networks to make a fixed-length representation. According to the structure, they may divide into four categories: Recursive Neural Networks (RecNN 1 ), RNN, CNN and other neural networks.</p><p>Recursive Neural Networks: RecNN is defined over recursive tree structures. In the type of recursive models, information from the leaf nodes of a tree and its internal nodes are combined in a bottom-up manner. <ref type="bibr" target="#b23">Socher et al. (2013)</ref> introduced recursive neural tensor network to build representations of phrases and sentences by combining neighbour constituents based on the parsing tree. <ref type="bibr" target="#b5">Irsoy and Cardie (2014)</ref> proposed deep recursive neural network, which is constructed by stacking multiple recursive layers on top of each other, to modeling sentence.</p><p>Recurrent Neural Networks: RNN has obtained much attention because of their superior ability to preserve sequence information over time. <ref type="bibr">Tang et al. (2015)</ref> developed target dependent Long Short-Term Memory Networks (LSTM (Hochreiter and Schmidhuber, 1997)), where target information is automatically taken into account. <ref type="bibr" target="#b24">Tai et al. (2015)</ref> generalized LSTM to Tree-LSTM where each LSTM unit gains information from its children units. <ref type="bibr" target="#b37">Zhou et al. (2016)</ref> introduced BLSTM with attention mechanism to automatically select features that have a decisive effect on classification. <ref type="bibr" target="#b29">Yang et al. (2016)</ref> introduced a hierarchical network with two levels of attention mechanisms, which are word attention and sentence attention, for document classification. This paper also implements an attention-based model BLSTM-Att like the model in <ref type="bibr" target="#b37">Zhou et al. (2016)</ref>.</p><p>Convolution Neural Networks: <ref type="bibr">CNN (LeCun et al., 1998</ref>) is a feedforward neural network with 2D convolution layers and 2D pooling layers, originally developed for image processing. Then CNN is applied to NLP tasks, such as sentence classification <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b8">Kim, 2014)</ref>, and relation classification <ref type="bibr" target="#b32">(Zeng et al., 2014)</ref>. The difference is that the common CNN in NLP tasks is made up of 1D convolution layers and 1D pooling layers. <ref type="bibr" target="#b8">Kim (2014)</ref> defined a CNN architecture with two channels. <ref type="bibr" target="#b7">Kalchbrenner et al. (2014)</ref> proposed a dynamic k-max pooling mechanism for sentence modeling. <ref type="bibr" target="#b33">(Zhang and Wallace, 2015)</ref> conducted a sensitivity analysis of one-layer CNN to explore the effect of architecture components on model performance. Yin and Sch?tze (2016) introduced multichannel embeddings and unsupervised pretraining to improve classification accuracy. <ref type="bibr" target="#b33">(Zhang and Wallace, 2015)</ref> conducted a sensitivity analysis of one-layer CNN to explore the effect of architecture components on model performance.</p><p>Usually there is a misunderstanding that 1D convolutional filter in NLP tasks has one dimension. Actually it has two dimensions (k, d), where k, d ? R. As d is equal to the word embeddings size d w , the window slides only on the time-step dimension, so the convolution is usually called 1D convolution. While d in this paper varies from 2 to d w , to avoid confusion with common CNN, the convolution in this work is named as 2D convolution. The details will be described in Section 3.2.</p><p>Other Neural Networks: In addition to the models described above, lots of other neural networks have been proposed for text classification. <ref type="bibr" target="#b6">Iyyer et al. (2015)</ref> introduced a deep averaging network, which fed an unweighted average of word embeddings through multiple hidden layers before classification. <ref type="bibr" target="#b36">Zhou et al. (2015)</ref> used CNN to extract a sequence of higher-level phrase representations, then the representations were fed into a LSTM to obtain the sentence representation.</p><p>The proposed model BLSTM-2DCNN is most relevant to DSCNN  and RCNN <ref type="bibr" target="#b28">(Wen et al., 2016)</ref>. The difference is that the former two utilize LSTM, bidirectional RNN respectively, while this work applies BLSTM, to capture long-term sentence dependencies. After that the former two both apply 1D convolution and 1D max pooling operation, while this paper uses 2D convolution and 2D max pooling operation, to obtain the whole sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>As shown in <ref type="figure">Figure 1</ref>, the overall model consists of four parts: BLSTM Layer, Two-dimensional Convolution Layer, Two dimensional max pooling Layer, and Output Layer. The details of different components are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BLSTM Layer</head><p>LSTM was firstly proposed by <ref type="bibr">Hochreiter and Schmidhuber (1997)</ref> to overcome the gradient vanishing problem of RNN. The main idea is to introduce an adaptive gating mechanism, which decides the degree to keep the previous state and memorize the extracted features of the current data input. Given a sequence S = {x 1 , x 2 , . . . , x l }, where l is the length of input text, LSTM processes it word by word. At time-step t, the memory c t and the hidden state h t are updated with the following equations: </p><formula xml:id="formula_0">? ? ? ? i t f t o t c t ? ? ? ? = ? ? ? ? ? ? ? tanh ? ? ? ? W ? [h t?1 , x t ] (1) c t = f t ? c t?1 + i t ?? t (2) h t = o t ? tanh(c t )<label>(3)</label></formula><p>where x t is the input at the current time-step, i, f and o is the input gate activation, forget gate activation and output gate activation respectively,? is the current cell state, ? denotes the logistic sigmoid function and ? denotes element-wise multiplication. For the sequence modeling tasks, it is beneficial to have access to the past context as well as the future context. <ref type="bibr" target="#b22">Schuster and Paliwal (1997)</ref> proposed BLSTM to extend the unidirectional LSTM by introducing a second hidden layer, where the hidden to hidden connections flow in opposite temporal order. Therefore, the model is able to exploit information from both the past and the future.</p><p>In this paper, BLSTM is utilized to capture the past and the future information. As shown in <ref type="figure">Figure  1</ref>, the network contains two sub-networks for the forward and backward sequence context respectively. The output of the i th word is shown in the following equation:</p><formula xml:id="formula_1">h i = [ ? ? h i ? ? ? h i ]<label>(4)</label></formula><p>Here, element-wise sum is used to combine the forward and backward pass outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Neural Networks</head><p>Since BLSTM has access to the future context as well as the past context, h i is related to all the other words in the text. One can effectively treat the matrix, which consists of feature vectors, as an 'image', so 2D convolution and 2D max pooling operation can be utilized to capture more meaningful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Two-dimensional Convolution Layer</head><p>A matrix H = {h 1 , h 2 , . . . , h l }, H ? R l?d w , is obtained from BLSTM Layer, where d w is the size of word embeddings. Then narrow convolution is utilized <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014)</ref> to extract local features over H. A convolution operation involves a 2D filter m ? R k?d , which is applied to a window of k words and d feature vectors. For example, a feature o i,j is generated from a window of vectors H i:i+k?1, j:j+d?1 by</p><formula xml:id="formula_2">o i,j = f (m ? H i:i+k?1, j:j+d?1 + b)<label>(5)</label></formula><p>where i ranges from 1 to (l ? k + 1), j ranges from 1 to (d w ? d + 1), ? represents dot product, b ? R is a bias and an f is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of the matrix H to produce a feature map O:</p><formula xml:id="formula_3">O = [o 1,1 , o 1,2 , ? ? ? , o l?k+1,d w ?d+1 ]<label>(6)</label></formula><p>with O ? R (l?k+1)?(d w ?d+1) . It has described the process of one convolution filter. The convolution layer may have multiple filters for the same size filter to learn complementary features, or multiple kinds of filter with different size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Two-dimensional Max Pooling Layer</head><p>Then 2D max pooling operation is utilized to obtain a fixed length vector. For a 2D max pooling p ? R p 1 ?p 2 , it is applied to each possible window of matrix O to extract the maximum value:</p><formula xml:id="formula_4">p i,j = down(O i:i+p 1 , j:j+p 2 )<label>(7)</label></formula><p>where down(?) represents the 2D max pooling function, i = (1, 1 + p 1 , ? ? ? , 1 + (l ? k + 1/p 1 ? 1) ? p 1 ), and j = (1, 1 + p 2 , ? ? ? , 1 + (d w ? d + 1/p 2 ? 1) ? p 2 ). Then the pooling results are combined as follows:</p><formula xml:id="formula_5">h * = [p 1,1 , p 1,1+p 2 , ? ? ? , p 1+(l?k+1/p 1 ?1)?p 1 ,1+(d w ?d+1/p 2 ?1)?p 2 ]<label>(8)</label></formula><p>where h * ? R, and the length of h * is ?l ? k + 1/p 1 ? ? ?d w ? d + 1/p 2 ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Output Layer</head><p>For text classification, the output h * of 2D Max Pooling Layer is the whole representation of the input text S. And then it is passed to a softmax classifier layer to predict the semantic relation label? from a discrete set of classes Y . The classifier takes the hidden state h * as input:</p><formula xml:id="formula_6">p (y|s) = sof tmax W (s) h * + b (s) (9) y = arg max yp (y|s)<label>(10)</label></formula><p>A reasonable training objective to be minimized is the categorical cross-entropy loss. The loss is calculated as a regularized sum:</p><formula xml:id="formula_7">J (?) = ? 1 m m i=1 t i log(y i ) + ? ? 2 F<label>(11)</label></formula><p>where t ? R m is the one-hot represented ground truth, y ? R m is the estimated probability for each class by softmax, m is the number of target classes, and ? is an L2 regularization hyper-parameter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The proposed models are tested on six datasets. Summary statistics of the datasets are in <ref type="table">Table 1</ref>.</p><p>? MR 2 : Sentence polarity dataset from <ref type="bibr" target="#b20">Pang and Lee (2005)</ref>. The task is to detect positive/negative reviews.</p><p>? SST-1 3 : Stanford Sentiment Treebank is an extension of MR from <ref type="bibr" target="#b23">Socher et al. (2013)</ref>. The aim is to classify a review as fine-grained labels (very negative, negative, neutral, positive, very positive).</p><p>? SST-2: Same as SST-1 but with neutral reviews removed and binary labels (negative, positive). For both experiments, phrases and sentences are used to train the model, but only sentences are scored at test time <ref type="bibr" target="#b23">(Socher et al., 2013;</ref><ref type="bibr" target="#b12">Le and Mikolov, 2014)</ref>. Thus the training set is an order of magnitude larger than listed in table 1.</p><p>? Subj 4 : Subjectivity dataset <ref type="bibr" target="#b19">(Pang and Lee, 2004)</ref>. The task is to classify a sentence as being subjective or objective.</p><p>? TREC 5 : Question classification dataset <ref type="bibr" target="#b16">(Li and Roth, 2002)</ref>. The task involves classifying a question into 6 question types (abbreviation, description, entity, human, location, numeric value).</p><p>? 20Newsgroups 6 : The 20Ng dataset contains messages from twenty newsgroups. We use the bydate version preprocessed by <ref type="bibr" target="#b0">Cachopo (2007)</ref>. We select four major categories (comp, politics, rec and religion) followed by <ref type="bibr" target="#b2">Hingmire et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Embeddings</head><p>The word embeddings are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data <ref type="bibr" target="#b25">(Turian et al., 2010)</ref>. In particular, our experiments utilize the GloVe embeddings 7 trained by <ref type="bibr" target="#b21">Pennington et al. (2014)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyper-parameter Settings</head><p>For datasets without a standard development set we randomly select 10% of the training data as the development set. The evaluation metric of the 20Ng is the Macro-F1 measure followed by the state-ofthe-art work and the other five datasets use accuracy as the metric. The final hyper-parameters are as follows.</p><p>The dimension of word embeddings is 300, the hidden units of LSTM is 300. We use 100 convolutional filters each for window sizes of (3,3), 2D pooling size of (2,2). We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0. For regularization, we employ Dropout operation  with dropout rate of 0.5 for the word embeddings, 0.2 for the BLSTM layer and 0.4 for the penultimate layer, we also use l2 penalty with coefficient 10 ?5 over the parameters.</p><p>These values are chosen via a grid search on the SST-1 development set. We only tune these hyperparameters, and more finer tuning, such as using different numbers of hidden units of LSTM layer, or using wide convolution <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014)</ref>, may further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Performance</head><p>This work implements four models, BLSTM, BLSTM-Att, BLSTM-2DPooling, and BLSTM-2DCNN. <ref type="table" target="#tab_2">Table 2</ref> presents the performance of the four models and other state-of-the-art models on six classification tasks. The BLSTM-2DCNN model achieves excellent performance on 4 out of 6 tasks. Especially, it achieves 52.4% and 89.5% test accuracies on SST-1 and SST-2 respectively. BLSTM-2DPooling performs worse than the state-of-the-art models. While we expect performance gains through the use of 2D convolution, we are surprised at the magnitude of the gains. BLSTM-CNN beats all baselines on SST-1, SST-2, and TREC datasets. As for Subj and MR datasets, BLSTM-2DCNN gets a second higher accuracies. Some of the previous techniques only work on sentences, but not paragraphs/documents with several sentences. Our question becomes whether it is possible to use our models for datasets that have a substantial number of words, such as 20Ng and where the content consists of many different topics. For that purpose, this paper tests the four models on document-level dataset 20Ng, by treating the document as a long sentence. Compared with RCNN <ref type="bibr" target="#b11">(Lai et al., 2015)</ref>, BLSTM-2DCNN achieves a comparable result.</p><p>Besides, this paper also compares with ReNN, RNN, CNN and other neural networks:</p><p>? Compared with ReNN, the proposed two models do not depend on external language-specific features such as dependency parse trees.</p><p>? CNN extracts features from word embeddings of the input text, while BLSTM-2DPooling and BLSTM-2DCNN captures features from the output of BLSTM layer, which has already extracted features from the original input text.</p><p>? BLSTM-2DCNN is an extension of BLSTM-2DPooling, and the results show that BLSTM-2DCNN can capture more dependencies in text.</p><p>? AdaSent utilizes a more complicated model to form a hierarchy of representations, and it outperforms BLSTM-2DCNN on Subj and MR datasets. Compared with DSCNN , BLSTM-2DCNN outperforms it on five datasets.</p><p>Compared with these results, 2D convolution and 2D max pooling operation are more effective for modeling sentence, even document. To better understand the effect of 2D operations, this work conducts a sensitivity analysis on SST-1 dataset. ReNN RNTN <ref type="bibr" target="#b23">(Socher et al., 2013)</ref> 45.7 85.4 ----DRNN <ref type="bibr" target="#b5">(Irsoy and Cardie, 2014)</ref> 49.8 86.6 ----CNN DCNN <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014)</ref> 48.5 86.8 -93.0 --CNN-non-static <ref type="bibr" target="#b8">(Kim, 2014)</ref> 48.0 87.2 93.4 93.6 --CNN-MC <ref type="bibr" target="#b8">(Kim, 2014)</ref> 47.4 88.1 93.2 92 --TBCNN <ref type="bibr" target="#b18">(Mou et al., 2015)</ref> 51.4 87.9 -96.0 --Molding-CNN <ref type="bibr" target="#b15">(Lei et al., 2015)</ref> 51.2 88.6 ----CNN-Ana <ref type="bibr">Wallace, 2015) 45.98 85.45 93.66 91.37 81.02 -MVCNN (Yin and</ref><ref type="bibr">Sch?tze, 2016)</ref> 49.6 89.4 93.9 ---RNN RCNN <ref type="bibr" target="#b11">(Lai et al., 2015)</ref> 47.21 ----96.49 S-LSTM  -81.9 ----LSTM <ref type="bibr" target="#b24">(Tai et al., 2015)</ref> 46.4 84.9 ----BLSTM <ref type="bibr" target="#b24">(Tai et al., 2015)</ref> 49.1 87.5 ----Tree-LSTM <ref type="bibr" target="#b24">(Tai et al., 2015)</ref> 51.0 88.0 ----LSTMN <ref type="bibr" target="#b1">(Cheng et al., 2016)</ref> 49.3 87.3 ----Multi-Task <ref type="bibr" target="#b17">(Liu et al., 2016)</ref> 49.6 87.9 94.1 ---Other PV <ref type="bibr" target="#b12">(Le and Mikolov, 2014)</ref> 48.7 87.8 ----DAN <ref type="bibr" target="#b6">(Iyyer et al., 2015)</ref> 48.2 86.8 ---combine-skip <ref type="bibr" target="#b9">(Kiros et al., 2015)</ref> --93.6 92.2 76.5 -AdaSent  --95.5 92.4 83.1 -LSTM-RNN <ref type="bibr" target="#b13">(Le and Zuidema, 2015)</ref> 49.9 88.0 ----C-LSTM <ref type="bibr" target="#b36">(Zhou et al., 2015)</ref> 49   <ref type="bibr" target="#b23">(Socher et al., 2013)</ref>. DRNN: Deep recursive neural networks for compositionality in language <ref type="bibr" target="#b5">(Irsoy and Cardie, 2014)</ref>. DCNN: A convolutional neural network for modeling sentences <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014)</ref>. CNN-nonstatic/MC: Convolutional neural networks for sentence classification <ref type="bibr" target="#b8">(Kim, 2014)</ref>. TBCNN: Discriminative neural sentence modeling by tree-based convolution <ref type="bibr" target="#b18">(Mou et al., 2015)</ref>. Molding-CNN: Molding CNNs for text: non-linear, non-consecutive convolutions <ref type="bibr" target="#b15">(Lei et al., 2015)</ref>. CNN-Ana: A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification <ref type="bibr" target="#b33">(Zhang and Wallace, 2015)</ref>. MVCNN: Multichannel variable-size convolution for sentence classification (Yin and Sch?tze, 2016). RCNN: Recurrent Convolutional Neural Networks for Text Classification <ref type="bibr" target="#b11">(Lai et al., 2015)</ref>. S-LSTM: Long short-term memory over recursive structures . LSTM/BLSTM/Tree-LSTM: Improved semantic representations from tree-structured long shortterm memory networks <ref type="bibr" target="#b24">(Tai et al., 2015)</ref>. LSTMN: Long short-term memory-networks for machine reading <ref type="bibr" target="#b1">(Cheng et al., 2016)</ref>. Multi-Task: Recurrent Neural Network for Text Classification with Multi-Task Learning <ref type="bibr" target="#b17">(Liu et al., 2016)</ref>. PV: Distributed representations of sentences and documents <ref type="bibr" target="#b12">(Le and Mikolov, 2014)</ref>. DAN: Deep unordered composition rivals syntactic methods for text classification <ref type="bibr" target="#b6">(Iyyer et al., 2015)</ref>. combine-skip: skip-thought vectors <ref type="bibr" target="#b9">(Kiros et al., 2015)</ref>. AdaSent: Selfadaptive hierarchical sentence model . LSTM-RNN: Compositional distributional semantics with long short term memory <ref type="bibr" target="#b13">(Le and Zuidema, 2015)</ref>. C-LSTM: A C-LSTM Neural Network for Text Classification <ref type="bibr" target="#b36">(Zhou et al., 2015)</ref>. DSCNN: Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents . longer than 45 words. The accuracy here is the average value of the sentences with length in the window [l ? 2, l + 2]. Each data point is a mean score over 5 runs, and error bars have been omitted for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Sentence Length</head><p>It is found that both BLSTM-2DPooling and BLSTM-2DCNN outperform the other two models. This suggests that both 2D convolution and 2D max pooling operation are able to encode semantically-useful structural information. At the same time, it shows that the accuracies decline with the length of sentences increasing. In future work, we would like to investigate neural mechanisms to preserve long-term dependencies of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of 2D Convolutional Filter and 2D Max Pooling Size</head><p>We are interested in what is the best 2D filter and max pooling size to get better performance. We conduct experiments on SST-1 dataset with BLSTM-2DCNN and set the number of feature maps to 100.</p><p>To make it simple, we set these two dimensions to the same values, thus both the filter and the pooling are square matrices. For the horizontal axis, c means 2D convolutional filter size, and the five different color bar charts on each c represent different 2D max pooling size from 2 to 6. <ref type="figure" target="#fig_3">Figure 3</ref> shows that different size of filter and pooling can get different accuracies. The best accuracy is 52.6 with 2D filter size (5,5) and 2D max pooling size (5,5), this shows that finer tuning can further improve the performance reported here. And if a larger filter is used, the convolution can detector more features, and the performance may be improved, too. However, the networks will take up more storage space, and consume more time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper introduces two combination models, one is BLSTM-2DPooling, the other is BLSTM-2DCNN, which can be seen as an extension of BLSTM-2DPooling. Both models can hold not only the time-step dimension but also the feature vector dimension information. The experiments are conducted on six text classificaion tasks. The experiments results demonstrate that BLSTM-2DCNN not only outperforms RecNN, RNN and CNN models, but also works better than the BLSTM-2DPooling and DSCNN . Especially, BLSTM-2DCNN achieves highest accuracy on SST-1 and SST-2 datasets. To better understand the effective of the proposed two models, this work also conducts a sensitivity analysis on SST-1 dataset. It is found that large filter can detector more features, and this may lead to performance improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A BLSTM-2DCNN for the seven word input sentence. Word embeddings have size 3, and BLSTM has 5 hidden units. The height and width of convolution filters and max pooling operations are 2, 2 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>on 6 billion tokens of Wikipedia 2014 and Gigaword 5. Words not present in the set of pre-trained words are initialized by randomly sampling from uniform distribution in [?0.1, 0.1]. The word embeddings are fine-tuned during training to improve the performance of classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>depicts the performance of the four models on different length of sentences. In the figure, the x-axis represents sentence lengths and y-axis is accuracy. The sentences collected in test set are no</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Prediction accuracy with different size of 2D filter and 2D max pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Summary statistics for the datasets. c: number of target classes, l: average sentence length, m: maximum sentence length, train/dev/test: train/development/test set size, |V |: vocabulary size, |V pre |: number of words present in the set of pre-trained word embeddings, CV: 10-fold cross validation.</figDesc><table><row><cell>Data</cell><cell>c</cell><cell>l</cell><cell>m</cell><cell>train</cell><cell>dev</cell><cell>test</cell><cell>|V |</cell><cell>|V pre |</cell></row><row><cell cols="3">SST-1 5 18</cell><cell>51</cell><cell cols="5">8544 1101 2210 17836 12745</cell></row><row><cell cols="3">SST-2 2 19</cell><cell>51</cell><cell>6920</cell><cell cols="4">872 1821 16185 11490</cell></row><row><cell>Subj</cell><cell cols="2">2 23</cell><cell>65</cell><cell>10000</cell><cell>-</cell><cell cols="3">CV 21057 17671</cell></row><row><cell cols="3">TREC 6 10</cell><cell>33</cell><cell>5452</cell><cell>-</cell><cell>500</cell><cell>9137</cell><cell>5990</cell></row><row><cell>MR</cell><cell cols="2">2 21</cell><cell>59</cell><cell>10662</cell><cell>-</cell><cell cols="3">CV 20191 16746</cell></row><row><cell cols="5">20Ng 4 276 11468 7520</cell><cell cols="4">836 5563 51379 30575</cell></row><row><cell>Table 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Training is done through stochastic gradient descent over shuffled mini-batches with the AdaDelta (Zeiler, 2012) update rule. Training details are further introduced in Section 4.3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification results on several standard benchmarks. RNTN: Recursive deep models for semantic compositionality over a sentiment treebank</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To avoid confusion with RNN, we named Recursive Neural Networks as RecNN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.cs.cornell.edu/people/pabo/movie-review-data/ 3 http://nlp.stanford.edu/sentiment/ 4 http://www.cs.cornell.edu/people/pabo/movie-review-data/ 5 http://cogcomp.cs.illinois.edu/Data/QA/QC/ 6 http://web.ist.utl.pt/acardoso/datasets/ 7 http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank anonymous reviewers for their constructive comments. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving methods for single-label text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana Margarida De Jesus Cardoso</forename><surname>Cachopo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Universidade T?cnica de Lisboa</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Document classification by topic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hingmire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="877" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and J?rgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
	<note>Irsoy and Cardie2014</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Iyyer et al.2015</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuidema2015] Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02510</idno>
		<title level="m">Compositional distributional semantics with long short term memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04112</idno>
		<title level="m">Molding cnns for text: non-linear, nonconsecutive convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth2002] Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05101</idno>
		<title level="m">Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Recurrent neural network for text classification with multi-task learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01106</idno>
		<title level="m">Discriminative neural sentence modeling by tree-based convolution</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
	<note>Pang and Lee2004</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>Pang and Lee2005</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Paliwal1997</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<idno>arXiv:1512.01100</idno>
	</analytic>
	<monogr>
		<title level="m">Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2015. Target-dependent sentiment classification with long short term memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2012] Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Don&apos;t follow me: Spam detection in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Hai</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Cryptography (SECRYPT), Proceedings of the 2010 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06905</idno>
		<title level="m">Learning text representation using recurrent convolutional neural network with highway layers</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multichannel variable-size convolution for sentence classification</title>
		<idno type="arXiv">arXiv:1603.04513</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: An adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wallace2015] Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03820</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dependency sensitive convolutional neural networks for modeling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1512" to="1521" />
		</imprint>
	</monogr>
	<note>Rui Zhang, Honglak Lee, and Dragomir Radev</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05070</idno>
		<title level="m">Self-adaptive hierarchical sentence model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08630</idno>
		<title level="m">A c-lstm neural network for text classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention-based bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 54th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
