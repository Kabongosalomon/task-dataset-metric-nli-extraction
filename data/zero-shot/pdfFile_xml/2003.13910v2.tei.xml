<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Multi-modal Fusion Network for Semantic Scene Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">KLISS</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">KLISS</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
							<email>gaoyue@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">KLISS</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-based Multi-modal Fusion Network for Semantic Scene Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an end-to-end 3D convolutional network named attention-based multi-modal fusion network (AMFNet) for the semantic scene completion (SSC) task of inferring the occupancy and semantic labels of a volumetric 3D scene from single-view RGB-D images. Compared with previous methods which use only the semantic features extracted from RGB-D images, the proposed AMFNet learns to perform effective 3D scene completion and semantic segmentation simultaneously via leveraging the experience of inferring 2D semantic segmentation from RGB-D images as well as the reliable depth cues in spatial dimension. It is achieved by employing a multi-modal fusion architecture boosted from 2D semantic segmentation and a 3D semantic completion network empowered by residual attention blocks. We validate our method on both the synthetic SUNCG-RGBD dataset and the real NYUv2 dataset and the results show that our method respectively achieves the gains of 2.5% and 2.6% on the synthetic SUNCG-RGBD dataset and the real NYUv2 dataset against the state-of-the-art method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Understanding and reconstructing a 3D scene from partial observations is a very important technique which has received increasing research attention in recent years due to its commercial potential in a large variety of robotics and vision tasks such as robotic navigation <ref type="bibr" target="#b7">(Gupta, Arbelaez, and Malik 2013)</ref>, autonomous driving <ref type="bibr" target="#b13">(Laugier et al. 2011)</ref>, and scene reconstruction <ref type="bibr" target="#b10">(Hays and Efros 2007;</ref><ref type="bibr" target="#b9">Han et al. 2019)</ref>. Given a single depth image or RGB-D images of a 3D scene, many papers <ref type="bibr" target="#b7">(Gupta, Arbelaez, and Malik 2013;</ref><ref type="bibr" target="#b18">Ren, Bo, and Fox 2012;</ref><ref type="bibr">Firman et al. 2016)</ref> have been proposed to complete or segment the 3D scene with neural networks. More recently, a set of methods <ref type="bibr" target="#b20">(Song et al. 2017;</ref><ref type="bibr" target="#b17">Liu et al. 2018;</ref><ref type="bibr">Guo and Tong 2018;</ref><ref type="bibr" target="#b25">Zhang et al. 2018;</ref><ref type="bibr" target="#b15">Li et al. 2019b</ref>) have been developed to automatically predict the semantic labels, together with completing the 3D geometry, of the objects in a 3D scene from a single view of the scene using convolutional neural networks.</p><p>This line of literature focusing on the task of semantic scene completion (SSC) was initiated by those meth- * Corresponding author  <ref type="figure">Figure 1</ref>: Given RGB-D images (a-b) as input, the proposed AMFNet can produce more accurate scene completion and scene segmentation result (e) than the previous methods (e.g., SSCNet <ref type="bibr">(Song et al. 2017) (d)</ref>). Directly boosting the AMFNet from the 2D segmentation ground truth of the input RGB-D images can even produce a result closer to the ground truth (c).</p><p>ods <ref type="bibr" target="#b20">(Song et al. 2017;</ref><ref type="bibr" target="#b25">Zhang et al. 2018)</ref> which take as input only the depth information. Several later works <ref type="bibr" target="#b17">(Liu et al. 2018;</ref><ref type="bibr" target="#b15">Li et al. 2019b;</ref><ref type="bibr" target="#b6">Guedes, de Campos, and Hilton 2018)</ref> argue that the RGB color information, which is often captured together with the depth information by an RGB-D sensor, could be used to improve the performance of the networks which take only the depth information. The experimental results from these works reveal that the RGB color channels provide complementary information to the depth information and higher accuracy of SSC can be achieved by using the fused information.</p><p>In this work, we propose a novel approach for the task of semantic scene completion, which makes use of not only the information of RGB color channels and depth but also the experience of deep network learning 2D semantic segmentation. Specifically, we train a model that leverages 2D semantic segmentation information to guide both 3D completion and semantic labeling (semantic segmentation) in the SSC task. Some prior works <ref type="bibr" target="#b24">Yang et al. 2018;</ref><ref type="bibr" target="#b14">Li et al. 2019a;</ref><ref type="bibr" target="#b23">Xiong et al. 2019)</ref> show that 2D segmenta-  <ref type="figure">Figure 2</ref>: Architecture of AMFNet. Taking RGB-D images (separated to a RGB and a HHA image) as input, AMFNet predicts voxel occupancy and object labels of the scene simultaneously. It boosts the 3D completion and segmentation from an initial 3D semantic feature volume produced by computing the 2D-3D projection of the results of a 2D segmentation network.</p><p>tion is more accurate than the 3D SSC task. Our model is based on the assumption that the image features which are reliable for 2D semantic segmentation should also be reliable for semantic scene completion. With this assumption, we propose a novel two-branch multi-modal fusion network where the 3D segmentation information, which is boosted from 2D segmentation, could guide both 3D completion and semantic labeling feasibly. Moreover, due to the inherent sparsity of 3D data (most voxels in a 3D scene are empty), we believe that some features (e.g., color and texture) associated with the empty voxels are valueless. Therefore, we propose a residual attention block (RAB) based 3D network to make full use of reliable depth cues in the spatial dimension for semantic scene completion.</p><p>The proposed network contains three sequential modules: a 2D segmentation module which extracts 2D image features and produces 2D semantic labels, a 2D-3D projection layer which generates a 3D semantic feature volume from the output of the 2D segmentation module, and an attentiondriven two-branch 3D volume network (named 3D volume network) which infers a complete 3D volume with semantic labels from the initial 3D scene.</p><p>We validate our approach on both the synthetic SUNCG-RGBD dataset <ref type="bibr" target="#b17">(Liu et al. 2018</ref>) and the real NYUv2 dataset <ref type="bibr" target="#b19">(Silberman et al. 2012)</ref>, and the results show that our method achieves a gain of 2.5% on the SUNCG-RGBD dataset and a gain of 2.6% on the NYUv2 dataset against the state-of-the-art method <ref type="bibr" target="#b15">(Li et al. 2019b)</ref>. Our analytical study also observes that directly boosting the AMFNet from the 2D segmentation ground truth of the input RGB-D images (also see <ref type="figure">Figure 1</ref>(f)) can achieve a gain of 3.8% on the synthetic SUNCG-RGBD dataset and a gain of 4.3% on the real NYUv2 dataset against the state-of-the-art method <ref type="bibr" target="#b15">(Li et al. 2019b)</ref>.</p><p>Our main contributions can be summarized as follows: ? A multi-modal fusion network boosted from 2D semantic segmentation is proposed for semantic scene completion. Compared to previous works <ref type="bibr" target="#b20">(Song et al. 2017;</ref><ref type="bibr" target="#b17">Liu et al. 2018;</ref><ref type="bibr" target="#b15">Li et al. 2019b)</ref>, the major advantage of this network is that 2D semantic segmentation is used to guide and improve the 3D feature extraction for the SSC task. We demonstrate this advantage can significantly boost the overall performance of the task of SSC by improving both scene completion and segmentation. ? A residual attention block (RAB) based network is proposed for scene semantic completion. Our experiments show that the proposed RAB is particularly effective for some particular objects in a 3D scene. ? The proposed end-to-end framework achieves state-ofthe-art performance on two large-scale datasets, i.e., the NYUv2 <ref type="bibr" target="#b19">(Silberman et al. 2012</ref>) and the SUNCG-RGBD <ref type="bibr" target="#b17">(Liu et al. 2018)</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Semantic Scene Completion The pioneering work for the task of semantic scene completion was proposed by <ref type="bibr" target="#b20">Song et al. in (Song et al. 2017)</ref>. Although this method achieves good performance in terms of performing both scene completion and semantic segmentation simultaneously from a single-view depth image within a task, it requires an extremely time-consuming data pre-processing step of computing the flipped truncated signed distance function (fTSDF) used for eliminating intense gradients and view dependency. Motivated by the fact that most of the voxels in the 3D volume are empty and useless, Zhang et al. further improved the SSCNet by applying a spatial group convolution (SGC) in EsscNet . This method divides the input fTSDF into different groups and then forwards them to a 3D sub-manifold sparse convolutional network <ref type="bibr" target="#b5">(Graham, Engelcke, and van der Maaten 2018)</ref>. A limitation of the Es-scNet is that the input features are empirically divided into several groups, which might lead to performance degradation.</p><p>More close to our work, SATNet <ref type="bibr" target="#b17">(Liu et al. 2018</ref>) is a model that performs the SSC task from 2D semantic features. This model contains three sequential modules similar to the proposed AMFNet: a 2D network to extract semantic features from RGB-D images, a 2D-3D projection layer, and a 3D network to complete the scene. Generally, the SATNet has a data processing flow (i.e., boosting 3D completion and segmentation from the 2D-3D projection of the 2D semantic features) similar to the 3D-semantic completion branch (the bottom one in <ref type="figure">Figure 2</ref>) of AMFNet. DDRNet <ref type="bibr" target="#b15">(Li et al. 2019b</ref>) is a light-weight deep model with an architecture similar to SATNet, of which the major contribution is a dimensional decomposition residual (DDR) block, which reduces the network parameters dramatically by decomposing the traditional 3D convolution block into consecutive layers channel-wise. The proposed AMFNet follows data processing flow similar to SATNet and DDRNet, but rather than perform 3D scene completion and semantic segmentation in a single branch, AMFNet employs two separated branches for 3D scene completion and semantic segmentation respectively, which enables the 2D semantic segmentation information to be used to explicitly guide the 3D scene completion and semantic segmentation simultaneously.</p><p>Attention Mechanism A large number of CNN-based methods <ref type="bibr" target="#b21">(Wang et al. 2017;</ref><ref type="bibr" target="#b12">Hu, Shen, and Sun 2018)</ref> use the attention mechanism to improve the performance of the classification task. Hu et al. <ref type="bibr" target="#b12">(Hu, Shen, and Sun 2018)</ref> employ an attention scheme which infers the importance of multiple channels for image classification. This method achieves significant gains in classification accuracy by making the model focus on meaningful regions of the image and ignores meaningless regions with the attention scheme. Woo et al. <ref type="bibr" target="#b22">(Woo et al. 2018)</ref> propose an attention scheme named convolutional bottleneck attention module (CBAM) to estimate channelwise attention weight and spatial-wise attention weight. Inspired by the great success of the application of attention mechanism in the image classification task and the inherent sparsity of 3D data, we believe that the ability of the attention mechanism, which makes the model focus on important parts, maybe also helpful for the task of SSC. In our network, to inject the attention into the 3D volume network without significant parameter increase, we graft the CBAM attention module onto a computation-efficient residual block to form a novel attention-injected residual block (RAB) and use this RAB for 3D feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this section, we first present the structure of the proposed AMFNet and then detail each module.</p><p>The proposed AMFNet, as illustrated in <ref type="figure">Figure 2</ref>, mainly contains three sequential modules: a 2D segmentation network, a 2D-3D projection layer, and a two-branch 3D volume network. This network takes single-view RGB-D images as input and outputs occupancy and semantic labels for all voxels in the scene. The whole network can be trained in an end-to-end manner. We next introduce each module in the sequence of the flow of data processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Segmentation Network</head><p>The 2D segmentation network extracts 2D geometry features and performs 2D semantic segmentation from the input RGB-D images. Since it is unlikely that a CNN would automatically learn to compute the properties of geocentric pose  that emphasize complementary discontinuities in the RGB-D image (e.g., depth, surface normal and height), we first compute a three-channel HHA <ref type="bibr" target="#b8">(Gupta et al. 2014</ref>) encoding result from the input depth image. After that, the RGB image and HHA image are fed into two 2D segmentation networks that have the same structure but do not share weights, and their individual outputs are fused together by a convolution layer with a kernel size of 1. In our implementation, the 2D segmentation network has an encoder-decoder structure where the encoder uses ResNet-101 <ref type="bibr" target="#b11">(He et al. 2016)</ref>, and the decoder contains a series of up-sampling convolutions and an atrous spatial pyramid pooling (ASPP) module <ref type="bibr" target="#b0">(Chen et al. 2016;</ref> which can extract multi-scale features. The output of the 2D segmentation network is a feature map with 64 channels and a semantic segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D-3D Projection Layer</head><p>We employ a projection layer similar to those in SAT-Net <ref type="bibr" target="#b17">(Liu et al. 2018)</ref> and DDRNet <ref type="bibr" target="#b15">(Li et al. 2019b</ref>) to generate an initial 3D semantic feature volume (can be separated into a 3D semantic volume and a 3D feature volume, as shown in <ref type="figure">Figure 2</ref>). For each input RGB-D image, given the intrinsic and extrinsic camera matrix, a 3D volume can be directly computed from the depth information (see <ref type="bibr">SAT-Net (Liu et al. 2018</ref>) for reference). Since each pixel in the RGB-D images correspond to a tensor in the 2D feature map as well as a semantic label in the 2D segmentation map, every feature tensor and semantic label can be projected into the 3D volume at the location with the same depth value.</p><p>In this way, we obtain a view-independent 3D semantic feature volume and a 3D semantic volume (only visible voxels have semantic labels), as shown in <ref type="figure">Figure 2</ref>. The 2D-3D projection operation can be recognized as reshaping the feature maps during the forward propagation. During back propagation, we only map the gradient of the voxels on the visible surface (discarding the gradient of other voxels) to the 2D pixels and continue the back propagation in the 2D network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Volume Network</head><p>The 3D volume network, which takes the output of the 2D-3D projection layer as input, contains two branches: one for 3D guidance information and the other for 3D semantic completion. At the end of the two branches, the outputs of the guidance branch and semantic completion branch are element-wise multiplied and forwarded into a softmax layer to generate a 3D volume with semantic labels.</p><p>3D-Guidance Branch The 3D-guidance branch is used to provide the guidance information for the branch of 3D semantic completion, which is boosted from an initial 3D semantic volumetric scene where visible voxels have initial semantic labels. The initial 3D semantic volume is first encoded by a one-hot encoder to achieve an ROI region (3D bounding box) for a specific category from the initial 3D semantic volume. For a dataset containing N different categories, the 3D semantic volume could be encoded by a volume with N channels. Each channel of the volume can be used to represent the ROI region of a specific category. For each channel, the value of the voxels within the bounding box of the corresponding category is set to one, otherwise, it is set to zero. The one-hot encoding introduces spatial boundary constraints into the network for each category, which improves the prediction of 3D semantic volume. (See the experimental analysis for more details.) The encoded volume is then fed into three 3D dense convolution layers and a sigmoid layer to obtain the output features of the 3Dguidance branch. It is worth mentioning that we do not perform the one-hot encoding with instance-level semantic information which may provide more accurate boundary constraints because both datasets in our experiment do not provide instance-level segmentation information.</p><p>3D Semantic Completion Branch The 3D-semantic completion branch, which takes the initial 3D feature volume as input, is mainly used to infer the voxel occupancy of the 3D scene. The data processing flow is illustrated in <ref type="figure">Figure 2</ref>. The input 3D features are firstly forwarded to four RAB modules, and the outputs of each block are concatenated and forwarded to a 3D convolution layer with a kernel size of 1 for multi-level feature fusion. After that, the fused feature is further forwarded to a 3D ASPP block (i.e., multiple parallel atrous convolutional layers with different dilatation rates, which is a powerful way to handle objects with various sizes) to exploit multi-scale features. The output of the ASPP block is lastly forwarded to four cascaded 3D convolution layers to generate high-level features for the fusion with the 3D-Guidance branch. In this branch, the attention focusing on reliable depth cues in the spatial dimension are mainly captured by the RAB modules. Our experiments in the next section will show the attention scheme contributes much to the overall performance (3% gains on average on both the synthetic SUNCG-RGBD and the real NYUv2 dataset on the metric of semantic scene completion). We next detail the RAB block. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our RAB block consists of two sequential blocks: a DDR block and an attention block. Moreover, a shortcut connection is used to achieve the effect of the residual block <ref type="bibr" target="#b11">(He et al. 2016)</ref>. Our RAB can be functionally formulated as:</p><formula xml:id="formula_0">y = A(D(x)) + x (1)</formula><p>where x is input, y is output, and D and A denote the DDR block and attention block, respectively. Instead of the 3D-ResNet blocks, we utilize the DDR block proposed in <ref type="bibr" target="#b15">(Li et al. 2019b)</ref>, as shown in <ref type="figure" target="#fig_1">Figure 3 (b)</ref>, to reduce the model parameters. The DDR block decomposes a 3D convolution with a kernel size of 3 ? 3 ? 3 into three consecutive layers with kernel sizes of 1 ? 1 ? 3, 1 ? 3 ? 1 and 3 ? 1 ? 1 respectively, so that the parameters could be reduced significantly.</p><p>The attention block used in RAB is a 3D adaption of the 2D attention block originally proposed in <ref type="bibr" target="#b22">(Woo et al. 2018)</ref>. It sequentially captures channel-wise attention and spatialwise attention. The former takes into account the channel importance of the 3D features while the latter infers the spatial importance of the 3D features. Specifically, in our attention block, the 3D features of each channel are firstly aggregated by using average pooling and max pooling to generate two descriptors. Then the aggregated descriptors are forwarded to a multi-layer perceptron (MLP) to extract hidden features. The outputs of the MLP are element-wise added as the channel-wise attention. The generated channelwise attention is multiplied by the 3D features to generate the channel-refined 3D features. In practice, the MLP has two hidden layers, and we choose 1/8 of the number of input feature channels as the hidden layer size. The spatial-wise attention is achieved via the following steps: the channelrefined 3D features are first aggregated by channel-wise average pooling and max pooling, and then the generated descriptors are forwarded to two convolution layers (the kernel sizes are 5 ? 5 ? 5 and 1 ? 1 ? 1, respectively) to generate the spatial-wise attention. In the last stage of RAB, the input 3D features are multiplied by the attention weight to achieve the 3D features with both channel-wise and spatial-wise attention injected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We follow <ref type="bibr" target="#b20">(Song et al. 2017)</ref> to process the data and compute the 2D-3D projection mapping for each sample in advance following SATNet <ref type="bibr" target="#b17">(Liu et al. 2018)</ref>. The training procedure consists of two steps. We first pre-train the 2D segmentation network with the supervision of 2D semantic segmentation ground truth, and then train the whole model endto-end. We use cross-entropy loss and an SGD optimizer with a momentum of 0.9, a weight decay of 5e-4, and a batch size of 1. The learning rate of the 2D segmentation network and 3D scene completion network is 0.001 and 0.01, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, the proposed method is evaluated and compared with the state-of-the-art methods on two public datasets, i.e., the NYUv2 dataset <ref type="bibr" target="#b19">(Silberman et al. 2012</ref>) and the SUNCG-RGBD dataset <ref type="bibr" target="#b17">(Liu et al. 2018</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Metrics</head><p>Datasets We evaluate our method on the NYUv2 and the SUNCG-RGBD datasets. The NYUv2 <ref type="bibr" target="#b19">(Silberman et al. 2012</ref>) is a real scene dataset, consisting of 1449 indoor scenes. The dataset is divided into 795 training and 654 testing samples, each scene associated with RGB-D images. We obtain the ground truth of the SSC task by following <ref type="bibr" target="#b20">(Song et al. 2017)</ref>. NYUv2 is a challenging dataset due to the complexity of the indoor scene and the measurement errors in the depth images caused by the Kinect data collection. SUNCG-RGBD, a synthetic dataset proposed by <ref type="bibr" target="#b17">Liu et al. (Liu et al. 2018)</ref>, is a subset of the SUNCG dataset <ref type="bibr" target="#b20">(Song et al. 2017)</ref>. It consists of 13011 training samples and 499 testing samples.</p><p>Metrics We mainly validate the proposed framework on two tasks, scene completion and semantic scene completion, as previous methods. We use the voxel-level intersection over union (IoU) between ground truth labels and predicted labels as the evaluation metric on both tasks. Specifically, for the task of semantic scene completion, we evaluate the IoU of each category on both the observed and occluded voxels. For the task of scene completion, we treat all nonempty voxels as the same category and evaluate the IoU of the binary predictions on the occluded voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Comparison</head><p>Comparison on NYUv2 dataset <ref type="table">Table 1</ref> presents the comparison results of both the scene completion and the semantic scene completion on the NYUv2 dataset. Compared with previous methods, our model achieves state-of-the-art performance on the task of semantic scene completion and ranks second on the task of scene completion. Compared to DDRNet <ref type="bibr" target="#b15">(Li et al. 2019b)</ref>, which also uses RGB-D images as input, our method achieves gains of 2.6% on the task of semantic scene completion. The quantitative result also reveals that the proposed method demonstrates superior performance against previous methods on the categories with small physical size such as chair (a gain of 4.9% is achieved) and the categories with variational depth information such as window (a gain of 5.1% is achieved). We attribute this improvement to the application of the attention block and the guidance branch in the 3D network. The attention block can make the model focus on significant parts. Therefore, the features of small objects can be acquired more effectively and further achieve better semantic scene completion performance. Meanwhile, the guidance branch, which is boosted with the sophisticated 2D segmentation network, can improve the task of 3D semantic scene completion by providing reliable guidance information for the 3D semantic scene completion branch. In the discussion section, we will validate this explanation by ablation study.</p><p>Comparison on SUNCG-RGBD dataset <ref type="table" target="#tab_3">Table 2</ref> presents the performance of our method on the SUNCG-RGBD dataset with comparison to some previous approaches. Compared with these baseline methods, our model achieves the best performance on both the tasks of scene completion and semantic scene completion. Specifically, our method outperforms the previous SSCNet by significant performance margin, which are 9.6% gains on the semantic scene completion task and 13.0% gains on the scene completion task. Compared to another recent</p><formula xml:id="formula_1">(a) RGB Image (b) GT of 2D Segmentation (c) Ground Truth (f) Ours (d) SSCNet (g) Ours (w/o-Attn) (h) Ours (w/o-Seg) (i) Ours (Seg-GT) (e) SATNet<label>(3)</label></formula><p>(2)</p><p>(1) (6) (5) (4) <ref type="figure">Figure 4</ref>: Qualitative results on the NYUv2 dataset. From left to right: RGB image, ground truth of 2D segmentation result, ground truth of SSC task, results generated by SSCNet <ref type="bibr" target="#b20">(Song et al. 2017)</ref>, SATNet <ref type="bibr" target="#b17">(Liu et al. 2018)</ref>, our approach, our approach with attention block removed, our approach with 3D-guidance branch removed, and our approach with the result of the 2D semantic segmentation module (see <ref type="figure">Figure 2</ref>) replaced by the ground truth.</p><p>model SATNet, our method still has a distinct advantage (2.5% gains on semantic scene completion and 0.6% gains on scene completion). Consistent with the observations on the NYUv2 dataset, our model performs well on those categories like chair and window and respectively achieves gains of 0.9% and 4% against SATNet. <ref type="figure">Figure 4</ref> visualizes the qualitative results of the semantic scene completion task generated by the proposed method and two previous methods, SSCNet and SATNet, on a set of representative samples from the NYUv2 dataset. It can be easily seen that our method has achieved better performance than SSCNet and SATNet. For example, our method produced much better results for objects with relatively large physical size, such as the wall and floor, in terms of segmentation consistency, as shown in row (1) and row (4). Similarly, our method can achieve a clearer 3D shape boundary of small objects, such as chair in row (1) and obj. in row (3). We attribute this to the guidance branch, which can provide the boundary constraints. In row (2), it can be seen from the RGB image of the scene that there is a lamp within the green dashed-line box, but it is regarded as background in the ground truth of both 2D segmentation and 3D semantic scene completion. Our model can reconstruct it well as shown in column (f) (see the red dashed-line box). This indicates that our model is capable of recognizing and reconstructing objects with small physical size in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>How useful is the 3D-guidance branch?</p><p>To investigate if the 3D-guidance branch can be helpful for the semantic scene completion task, we study the performance of our method without the 3D-guidance branch. Specifically, we remove the 3D-guidance branch in <ref type="figure">Figure 2</ref> and leave only the 3D-semantic completion branch. In <ref type="table" target="#tab_3">Table 1 and Table 2</ref>, we show the quantitative performance of our model without the 3D-guidance branch (denoted as Ours (w/o-Seg)). Its performance on average for 3D semantic scene completion is 45.5% on the SUNCG-RGBD dataset, and 29.9% on the NYUv2 dataset. In contrast, the model with the guidance branch can achieve 1.8% and 3.1% performance improvements against the one with the 3D-guidance branch removed on the two datasets, respectively. Compared to the basic model (denoted as Ours (basic)) where both the attention block and the guidance branch are removed from the proposed architecture, the 3D-guidance branch can respectively improve the performance by 2.1% and 0.7% on the SUNCG-RGBD dataset and the NYUv2 dataset. (visualized from the output of the 3D-guidance branch). The color bar, which quantifies the category probability, is shown on the right as a reference. It can be seen that the output of the 3D-guidance branch does provide reliable guidance information for the task of SSC.</p><p>In <ref type="figure" target="#fig_2">Figure 5</ref>, we visualize the results of the outputs of the 3D-guidance branch. We can see that the 3D-guidance branch can accurately focus on the corresponding 3D regions and provide reliable constraints of object boundaries, which makes the 3D semantic scene completion network able to determine the accurate voxel occupancy. Therefore it benefits both the scene completion and semantic scene completion task. In <ref type="figure">Figure 4</ref>, we visually demonstrate the effect of the 3D-guidance branch. In row (3) and row (5), the model with 3D-guidance branch removed is not able to maintain the integrity of the bed and wall, as shown in column (h). However, with the 3D-guidance branch added, the proposed method can address this problem, as shown in column (f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How useful are the residual attention blocks?</head><p>Previous works have proved that the attention mechanism can greatly help the 2D image classification task. We here investigate if the attention mechanism can benefit the 3D semantic scene completion task. To achieve this purpose, we evaluate the performance of the proposed architecture with the attention block removed (i.e., only the DDR block in <ref type="figure" target="#fig_1">Figure 3</ref> (a) remains). In <ref type="table" target="#tab_3">Table 1 and Table 2</ref>, we report the performance of our model with attention mechanism removed (denoted as Ours (w/o-Attn)). The average performance of this model is 44.3% on the SUNCG-RGBD dataset and 30.1% on the NYUv2 dataset. Conversely, the model with the attention mechanism added can achieve 3% and 2.9% performance improvements on the two datasets, respectively. Compared with the basic model (denoted as Ours (basic)), the attention block can improve the performance 0.5% and 3.3% on the two datasets, respectively.</p><p>We guess this performance improvement is probably because the proposed RAB calculates both spatial-wise and channel-wise weights and is able to make the network focus on meaningful parts instead of empty voxels. Moreover, the attention mechanism in this work can improve the performance of the network on small objects, such as chairs and tables. For example, in row (3) of <ref type="figure">Figure 4</ref>, it can be seen from the RGB image that there is a pile of paper on the cabinet. The proposed method can recognize and reconstruct it while the one with attention block removed cannot complete it completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is it possible to improve AMFNet?</head><p>Our proposed AMFNet improves the performance of SSC by mainly leveraging 2D semantic segmentation results to provide guidance information and applying attention mechanisms to enhance the capability of feature extraction. Here we envision potential improvements in two aspects.</p><p>(1) In <ref type="table">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref>, we report the performance of the architecture which uses the ground truth of the 2D semantic segmentation as input to the 3D-guidance branch during the inferring stage (denoted as Ours (Seg-GT)). It can be seen that the ground truth segmentation can improve the performance of the proposed model by 1.8% and 1.2% on the SUNCG-RGBD and NYUv2 datasets respectively, which indicates an accessible way to improve our model is to enhance the performance of the 2D segmentation network.</p><p>(2) In the 3D-guidance branch we perform a one-hot encoding to the semantic segmentation results of the visible voxels in a 3D scene, which may lead to inaccurate object constraints for the scenes where the objects belonging to the same category are spatially separated. In these scene samples, a large range of empty voxels between these objects might be encoded into the corresponding category and lead to inaccurate object boundaries. Therefore, another accessible way to improve our model is to use instance segmentation information for one-hot encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we introduce a novel network named AMFNet for the task of semantic scene completion. With the guidance of 2D semantic segmentation and the help of the attention focusing on reliable depth cues in the spatial dimension, AMFNet is capable of improving the completion and segmentation accuracy simultaneously by making full use of the information of the input RGB-D images. The major technique contributions of AMFNet include a two-branch fusion network boosted with 2D semantic segmentation and 3D semantic completion network empowered by residual attention blocks. Experimental results on the SUNCG-RGBD dataset and NYUv2 dataset demonstrate AMFNet achieves state-of-the-art performance. Ablation study and visualization results also show that the two-branch network and the attention-injected network have significant contributions to the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Ground Truth of SSC (b) Depth (HHA) Image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the proposed residual attention block (RAB). RAB has a structure similar to the DDR block<ref type="bibr" target="#b15">(Li et al. 2019b</ref>) but with both channel-wise and spatial-wise attention injected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the output of 3D-guidance branch. (a-b) The RGB-D images. (c) Ground truth of SSC. (d-e) 3D segmentation results of category bed and furn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. 61.0 21.1 92.2 33.5 6.8 14.8 48.3 42.3 13.2 13.9 35.3 13.2 30.4 Ours 67.9 82.3 59.0 16.7 89.2 27.3 19.2 20.2 56.1 50.4 15.1 13.5 36.8 18.0 33.0</figDesc><table><row><cell></cell><cell cols="2">scene completion</cell><cell></cell><cell cols="3">semantic scene completion</cell><cell></cell><cell></cell></row><row><cell cols="3">Method prec. recall Lin et al. (2013) 58.5 49.9 36.4 0.0</cell><cell>11.7 13.3 14.1</cell><cell>9.4</cell><cell>29.0 24.0</cell><cell>6.0</cell><cell>7.0</cell><cell>16.2</cell><cell>1.1</cell><cell>12.0</cell></row><row><cell cols="2">Geiger et al. (2015) 65.7</cell><cell cols="2">58.0 44.4 10.2 62.5 19.1 5.8</cell><cell>8.5</cell><cell>40.6 27.7</cell><cell>7.0</cell><cell>6.0</cell><cell>22.6</cell><cell>5.9</cell><cell>19.6</cell></row><row><cell>SSCNet (2017)</cell><cell>57.0</cell><cell cols="2">94.5 55.1 15.1 94.7 24.4 0.0</cell><cell cols="3">12.6 32.1 35.0 13.0</cell><cell>7.8</cell><cell cols="2">27.1 10.1 24.7</cell></row><row><cell>EsscNet (2018)</cell><cell>71.9</cell><cell cols="2">71.9 56.2 17.5 75.4 25.8 6.7</cell><cell cols="3">15.3 53.8 42.4 11.2</cell><cell>0</cell><cell cols="2">33.4 11.8 26.7</cell></row><row><cell cols="4">DDRNet (2019b) 80.8 Ours (w/o-Attn) 71.5 64.5 86.5 58.6 21.3 90.3 26.1 7.7</cell><cell cols="3">18.0 53.8 48.4 13.0</cell><cell>0</cell><cell cols="2">36.7 16.3 30.1</cell></row><row><cell>Ours (w/o-Seg)</cell><cell>68.5</cell><cell cols="8">78.6 57.3 13.7 94.7 26.9 14.8 12.7 48.5 39.8 10.4 12.2 36.2 19.2 29.9</cell></row><row><cell>Ours (basic)</cell><cell>64.0</cell><cell cols="5">86.6 58.0 20.9 94.1 27.1 15.7 12.4 46.0 45.1 13.9</cell><cell>0.3</cell><cell cols="2">32.3 15.7 29.4</cell></row><row><cell>Ours (Seg-GT)</cell><cell>66.3</cell><cell cols="8">80.5 57.2 20.0 78.7 27.3 20.5 21.8 56.5 53.9 19.5 18.8 40.1 19.5 34.2</cell></row><row><cell></cell><cell cols="7">Table 1: Results on the NYUv2 dataset. Bold numbers represent the best scores.</cell><cell></cell></row><row><cell></cell><cell cols="2">scene completion</cell><cell></cell><cell cols="3">semantic scene completion</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="9">prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg.</cell></row><row><cell>SSCNet (2017)</cell><cell>43.5</cell><cell cols="7">90.7 41.5 64.9 60.1 57.6 25.2 25.5 40.4 37.9 23.1 29.8 45.7</cell><cell>4.7</cell><cell>37.7</cell></row><row><cell>SATNet (2018)</cell><cell>56.7</cell><cell cols="8">91.7 53.9 65.5 60.7 50.3 56.4 26.1 47.3 43.7 30.6 37.2 44.9 30.0 44.8</cell></row><row><cell>Ours</cell><cell>57.5</cell><cell cols="8">91.6 54.5 80.4 69.1 55.0 60.4 27.0 42.2 46.7 32.5 42.3 36.9 27.4 47.3</cell></row><row><cell>Ours (w/o-Attn)</cell><cell>54.8</cell><cell cols="8">94.7 53.2 79.6 66.9 51.7 60.2 26.5 38.2 45.5 24.5 27.9 38.1 28.7 44.3</cell></row><row><cell>Ours (w/o-Seg)</cell><cell>54.1</cell><cell cols="8">94.0 52.4 76.6 61.0 53.7 54.7 26.2 37.3 49.4 31.8 43.7 40.7 25.0 45.5</cell></row><row><cell>Ours (basic)</cell><cell>47.5</cell><cell cols="8">96.4 46.7 71.0 49.7 41.4 57.3 21.6 41.3 46.1 30.1 44.8 39.9 21.4 42.2</cell></row><row><cell>Ours (Seg-GT)</cell><cell>60.6</cell><cell cols="8">89.1 56.3 81.3 68.5 54.1 61.8 30.2 45.9 50.7 34.3 42.7 41.9 28.4 49.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the SUNCG-RGBD dataset (we do not compare our method to EsscNet and DDRNet because neither of them report their performances on this dataset). Bold numbers represent the best scores.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Funds of China (61671267).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured Prediction of Unobserved Voxels from a Single Depth Image</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint 3D Object and Layout Inference from a Single RGB-D Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">View-volume Network for Semantic Scene Completion from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B S</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04735</idno>
		<idno>arXiv:1806.05361</idno>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Guo, Y.-X., and Tong, X</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Semantic Scene Completion Combining Colour and Depth</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning of Volume-guided Progressive View Inpainting for 3D Point Scene Completion from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scene Completion Using Millions of Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic Analysis of Dynamic Scenes and Collision Risks Assessment to Improve Driving Safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Paromtchik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrollaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Yoder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mekhnacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>N?gre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4" to="19" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dfanet: Deep Feature Aggregation for Real-time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9522" to="9531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RGBD Based Dimensional Decomposition Residual Network for 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7693" to="7702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">See and Think: Disentangling Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RGB-(D) Scene Labeling: Features and Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic Scene Completion from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Residual Attention Network for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional Block Attention Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">So</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Upsnet: A Unified Panoptic Segmentation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8818" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Denseaspp for Semantic Segmentation in Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient Semantic Scene Completion Network with Spatial Group Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="733" to="749" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
