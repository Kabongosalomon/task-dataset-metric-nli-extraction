<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><forename type="middle">Di</forename><surname>Liello</surname></persName>
							<email>luca.diliello@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>2 Amazon Alexa AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Garg</surname></persName>
							<email>sidgarg@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
							<email>lucas@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
						</author>
						<title level="a" type="main">Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An important task for designing QA systems is answer sentence selection (AS2): selecting the sentence containing (or constituting) the answer to a question from a set of retrieved relevant documents. In this paper, we propose three novel sentence-level transformer pre-training objectives that incorporate paragraph-level semantics within and across documents, to improve the performance of transformers for AS2, and mitigate the requirement of large labeled datasets. Specifically, the model is tasked to predict whether: (i) two sentences are extracted from the same paragraph, (ii) a given sentence is extracted from a given paragraph, and (iii) two paragraphs are extracted from the same document. Our experiments on three public and one industrial AS2 datasets demonstrate the empirical superiority of our pre-trained transformers over baseline models such as RoBERTa and ELECTRA for AS2. 9 https://github.com/alexa/wqa_tanda 10</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) finds itself at the core of several commercial applications, for e.g., virtual assistants such as Google Home, Alexa and Siri. Answer Sentence Selection (AS2) is an important task for QA Systems operating on unstructured text such as web documents. When presented with a set of relevant documents for a question (retrieved from a web index), AS2 aims to find the best answer sentence for the question.</p><p>The recent popularity of pre-trained transformers <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr" target="#b5">Clark et al., 2020)</ref>, has made them the de-facto approach for most QA tasks, including AS2. Several research works <ref type="bibr">(Garg et al., 2020;</ref><ref type="bibr">Laskar et al., 2020;</ref><ref type="bibr">Lauriola and Moschitti, 2021)</ref> fine-tune transformers for AS2, by posing it as a sentence-pair task and per- * Work done as an intern at Amazon Alexa AI ? Work completed at Amazon Alexa AI forming inference over the encoded representations of the question and answer candidates. AS2 is a knowledge-intensive complex reasoning task, where the answer candidates for a question can stem from multiple documents, possibly on different topics linked to concepts in the question. While there have been recent works <ref type="bibr">(Ginzburg et al., 2021;</ref><ref type="bibr" target="#b2">Caciularu et al., 2021)</ref> proposing pretraining strategies for obtaining multi-document aware document representations over long input encoders such as the Longformer <ref type="bibr" target="#b0">(Beltagy et al., 2020)</ref>, there has been limited research <ref type="bibr">(Giorgi et al., 2021)</ref> on enhancing sentence-pair representations with paragraph and document level semantics.</p><p>Furthermore, obtaining high quality human labeled examples for AS2 is expensive and time consuming, due to the large number of answer candidates to be annotated for each question. Domainspecific AS2 datasets such as WikiQA <ref type="bibr">(Yang et al., 2015)</ref> and <ref type="bibr">TREC-QA (Wang et al., 2007)</ref> only contain a few thousand questions. <ref type="bibr">Garg et al. (2020)</ref> show that effectively fine-tuning pre-trained transformers on these domain specific AS2 datasets requires an intermediate fine-tuning transfer on a large scale AS2 dataset (ASNQ).</p><p>Towards improving the downstream performance of pre-trained transformers for AS2 and mitigating the requirement of large scale labeled data for fine-tuning, we propose three novel sentencelevel transformer pre-training objectives, which can incorporate paragraph-level semantics across multiple documents. Analogous to the sentence-pair nature of AS2, we design our pre-training objectives to operate over a pair of input text sequences. The model is tasked with predicting: (i) whether the sequences are two sentences extracted from the same paragraph, (ii) whether the first sequence is a sentence that is extracted from the second sequence (paragraph), and (iii) whether the sequences are two paragraphs belonging to the same document.</p><p>We evaluate our paragraph-aware pre-trained transformers for AS2 on three popular public datasets: ASNQ, WikiQA and TREC-QA; and one industrial QA benchmark 1 . Results show that our pre-training can improve the performance of finetuning baseline transformers such as RoBERTa and ELECTRA on AS2 by ?3?4% points without requiring any additional data (labeled/unlabeled).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Answer Sentence Selection (AS2) Earlier approaches for AS2 used CNNs (Severyn and <ref type="bibr">Moschitti, 2015)</ref> or alignment networks <ref type="bibr">(Shen et al., 2017a;</ref><ref type="bibr">Tran et al., 2018)</ref> to learn and score question and answer representations. Since then, compareand-aggregate architectures have also been extensively studied <ref type="bibr">(Wang and Jiang, 2017;</ref><ref type="bibr" target="#b1">Bian et al., 2017;</ref><ref type="bibr">Yoon et al., 2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answer Sentence Selection (AS2)</head><p>In this section we formally define the task of AS2. Given a question q and a set of answer candidates A={a 1 , . . ., a n }, the objective is to select the candidate? ? A that best answers q. AS2 can be modeled as a ranking task over A to learn a scoring function f : Q?A ? R that predicts the probability f (q, a) of an answer candidate a being correct. The best answer? corresponds to argmax n i=1 f (q, a i ). Pre-trained transformers are used as QA pair encoders for AS2 to approximate the function f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sentence-Level Pre-training Objectives</head><p>Documents are typically organized into paragraphs, by humans, to address the document's general topic from different viewpoints. We propose three pretraining objectives to exploit the intrinsic information contained in the structure of documents. For all these objectives, we provide a pair of text se-quences as input to the transformer to jointly reason over them, analogous to the AS2 task.</p><p>Spans in Same Paragraph (SSP) Given two sequences (A, B) as input to the transformer, the objective is to predict if A and B belong to the same paragraph in a document. To create positive pairs (A, B), given a document D, we extract two small, contiguous and disjoint subsets of sentences to be used as A and B from a single paragraph P i ? D.</p><p>To create negative pairs, we sample spans of sentences B from different paragraphs P j , j = i in the same document D (hard negatives) and also from different documents (easy negatives). The negative pairs correspond to (A, B ). Posing the above pre-training objective in terms of spans (instead of sentences) allows for modifying the lengths of the inputs A, B (by changing number of sentences ?A, B). When fine-tuning transformers for AS2, typically the question is provided as the first input and a longer answer candidate/paragraph is provided as the second input. For our experiments (Sec 5), we use a longer span for input B than A.</p><p>Span in Paragraph (SP) Given two sequences (A, B) as input to the transformer, the objective is to predict if A is a span of text extracted from a paragraph B in a document. To create positive pairs (A, B), given a paragraph P i in a document D, we extract a small contiguous span of sentences A from it and create the input pair as (A, P i \ A). To create negative pairs, we select other paragraphs P j , j = i in the same document D and remove a randomly chosen span A from each of them. The negative pairs correspond to (A, P j \ A ). This is necessary to ensure that the model does not simply recognize whether the second input is a complete paragraph or a clipped version. To create easy negatives, we use the above approach for paragraphs P j sampled from documents other than D.</p><p>Paragraphs in Same Document (PSD) Given two sequences (A, B) as input to the transformer, the objective is to predict if A and B are paragraphs belonging to the same document. To create positive pairs (A, B), given a document D k , we randomly select paragraphs P i , P j ? D k and obtain a pair (P i , P j ). To create negative pairs, we randomly select P j / ? D k , and obtain a pair (P i , P j ). 82.5 (2.0) 88.6 (1.4) 90.0 (1.4) 88.5 (1.9) 89.6 (0.7) 93.5 (0.9) +1.4% +1.5% +1.3% (Ours) ELECTRA + SP 65.0 (0.2) 69.0 (0.1) 75.1 (0.1) 81.8 (2.3) 88.1 (1.5) 89.5 (1.5) 91.2 (1.5) 90.3 (0.7) 94.6 (0.7) +1.4% +1.5% +1.3% (Ours) ELECTRA + PSD 65.3 (0.4) 68.9 (0.3) 75.1 <ref type="bibr">(0.3)</ref> 78.6 (0.7) 85.6 (0.7) 87.3 (0.6) 85.9 (2.2) 87.9 (1.1) 92.</p><formula xml:id="formula_0">2 (1.1) +1.6% +1.6% +1.3% (Ours) ELECTRA + All 65.0 (0.3) 69.3 (0.2) 75.2 (0.2) 80.8 (1.9) 87.3 (1.2) 88.7 (1.1) 92.6 (1.8) 90.4 (0.4) 95.5 (1.0) +1.5% +1.6% +1.4% TANDA ELECTRA - - - 85.6 (1.1) 90.2 (0.8) 91</formula><p>.4 (0.7) 92.6 (1.5) 91.6 (0.7) 95.5 (0.7) +1.9% +1.6% +1.5% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup and Details</head><p>We use our 3 pre-training objectives: SSP, SP and PSD, for both RoBERTa and ELECTRA, obtaining 6 different continuously pre-trained models. We set the maximum pre-training steps to 400k for SSP and 200k for SP and PSD. This corresponds to each model processing ?210B tokens during pre-training, which is about 10% of the ?2100B tokens used for pre-training RoBERTa. Notice also that the compute FLOPs are even less than the 10% of the original training because we used a shorted max sequence length. More details about the continuous pre-training hyper-parameters are given in Appendix B. We also combine all 3 objectives together (SSP+SP+PSD) for both RoBERTa and ELECTRA, with the same setting as SSP. We fine-tune each of our pre-trained models on all four AS2 datasets (with early stopping on the dev set) and compute results on their respective test splits. Baselines We use RoBERTa and ELECTRA models as baselines. We also use TANDA <ref type="bibr">(Garg et al., 2020)</ref>, the state of the art for AS2, as an upperbound baseline as it uses an additional intermediate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We present results of our pre-trained models on the AS2 datasets in <ref type="table" target="#tab_2">Table 1</ref>. We observe that the models trained with our pre-training objectives significantly outperform the baseline models when fine-tuned for the AS2 tasks. For example, on ASNQ, using our SP objective with RoBERTa-Base gains 2.3% in P@1 over the baseline RoBERTa-Base model. On WikiQA, the performance gap is even larger with the SSP objective corresponding to 4.6% points for RoBERTa-Base and 5.4% for ELECTRA-Base over the corresponding baselines. Performance improvements on TREC-QA and WQA are smaller but consistent, around 1% and 0.6% in P@1. Combining SSP+SP+PSD together consistently achieves either the best results (TREC-QA and WQA), or close to the best results (ASNQ and WikiQA).</p><p>For questions in ASNQ and WikiQA, all candidate answers are extracted from a single Wikipedia document, while for TREC-QA and WQA, candidate answers come from multiple documents extracted from heterogeneous web sources. By design of our objectives SSP, SP and PSD, they perform differently when fine-tuning on different datasets. For example, SSP aligns well with ASNQ and Wik-iQA as they contain many negative candidates, per question, extracted from the same document as the positive (i.e, 'hard' negatives). As per our design of the SSP objective, for every positive sequence pair, we sample 2 'hard' negatives coming from the same document as the positive pair. The presence of hard negatives is of particular importance for WikiQA and ASNQ, as it forces the models to learn and contrast more subtle differences between answer candidates, which might likely be more related as they come from the same document.   On the other hand, PSD is designed so as to see paragraphs from same or different documents (with no analogous concept of 'hard' negatives of SSP and SP). For this reason, PSD is better aligned for fine-tuning on datasets where candidates are extracted from multiple documents, such as WQA and TREC-QA.</p><p>Comparison with TANDA For RoBERTa, our pre-trained models can surprisingly improve/achieve comparable performance to TANDA. Note that our models achieve this performance without using the latter's additional ?20M labeled ASNQ QA pairs. This lends support to our pre-training objectives mitigating the requirement of large scale labeled data for AS2 fine-tuning. For ELECTRA, we only observe comparable performance to TANDA for WQA and TREC-QA.</p><p>Ablation: MLM-only Pre-training To mitigate any improvements stemming from the specific data sampling techniques used by our objectives, we pretrain 3 models (starting from RoBERTa-Base) with the same data sampling as each of the SSP, SP and PSD models, but only using the MLM objective.</p><p>We report results in <ref type="table" target="#tab_4">Table 2</ref>, and observe that, almost always, models pre-trained only with MLM under-perform models trained with SSP, SP and PSD objectives in addition to MLM. Thus, the empirical improvements of our methods are derived from the novel pre-training objectives, and not data sampling. Surprisingly, for some models, the MLM-only continuous pre-training performs worse than the baseline RoBERTa-Base. We believe that restarting the training with a different learning-rate 3 , a shorter sequence length, and without the original optimizer and scheduler internal states (for a small amount of steps) is sub-optimal for the model.</p><p>Ablation: Pre-training Task 'Difficulty' We evaluate the pre-trained models (after convergence) on their specific tasks over the validation split of Wikipedia (to enable evaluating baselines such as BERT and ALBERT). <ref type="table" target="#tab_5">Table 3</ref> summarizes the accuracy and F1 of the models on the various tasks.</p><p>The results show that our objectives are generally harder than NSP (Next Sentence Prediction by <ref type="bibr" target="#b3">Devlin et al., 2019)</ref> and SOP (Sentence Order Prediction by <ref type="bibr">Lan et al., 2020)</ref>. In fact, NSP and SOP have been shown to not add any significant performance improvements in addition to <ref type="bibr">MLM (Liu et al., 2019)</ref>, and this corresponds to the model being able to perform this task extremely well (dev accuracy ? 94% with NSP and ? 97% with SOP) without learning any new semantics that may be useful for downstream tasks.</p><p>On the other hand, our pre-training objectives are "more challenging" than these previously proposed objectives due to the requirement of reasoning over multiple paragraphs and multiple documents, addressing same or different topics at the same time.</p><p>In fact, <ref type="table" target="#tab_5">Table 3</ref> shows that after convergence, our pre-trained model still finds it difficult to achieve a higher accuracy for our sentence level pre-training tasks. Empirically in <ref type="table" target="#tab_2">Table 1</ref>, we observed that pretraining with our objectives is able to rank the more relevant answers at the top, which we hypothesize is due to the model learning how to reason over multiple paragraphs and documents already while performing continuous pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have presented three sentencelevel pre-training objectives for transformers to incorporate paragraph and document-level semantics. Our objectives predict whether (i) two sequences are sentences extracted from the same paragraph, (ii) first sequence is a sentence extracted from the second, and (iii) two sequences are paragraphs belonging to the same document. We evaluate our pre-trained models for the task of AS2 on four datasets. Our results show that our pre-trained models outperform the baseline transformers such as RoBERTa and ELECTRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>We only consider English language datasets for our experiments in this paper. However we hypothesize that our pre-training objectives should provide similar performance improvements when extended to other languages with limited morphology, like English. The pre-training objectives proposed in our work are designed considering Answer Sentence Selection (AS2) as the target task, and can be extended for other tasks like Natural Language Inference, Question-Question Similarity, etc. in future work. The pre-training experiments in our paper require large amounts of GPU and compute resources (multiple NVIDIA A100 GPUs running for several days) to finish the model pre-training. This makes re-training models using our pre-training approaches computationally expensive using newer data. To mitigate this, we are releasing our code and pre-trained model checkpoints at https://github.com/ amazon-research/wqa-pretraining, which can directly be used by fine-tuning them on AS2 datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>A.1 Pre-training</p><p>For continued pre-training, we pre-process the English Wikipedia 4 , the BookCorpus 5 , OpenWebText (Gokaslan and Cohen, 2019) and the CC-News 6 datasets. We do not use the STORIES dataset as it is no longer available for research use 7 . We clean every dataset by removing headers, titles, tables and any HTML content. For every document, we keep paragraphs containing at least 60 characters and documents containing at least 200 characters. After cleaning, we obtain 5GB, 10GB, 34GB and 360GB of raw text from the BookCorpus, Wikipedia, OpenWebText and CC-News respectively. We split paragraph into lists of sentences using the blingfire tokenizer 8 . We present the details of our pre-training objectives in Section 4. We present the details on sampling lengths and number of negatives for each of the objectives below:</p><p>? Spans in Same Paragraph (SSP) We randomly sample the number of sentences in A in the interval [1, 3] and B in <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>. This is to keep the inputs to the model analogous to those in AS2 (shorter question text, followed by longer answer text). We sample up to 2 hard negatives from the same paragraph as A (if possible), and sample easy negatives from other documents so as to make the total number of negatives to be 4.</p><p>? Span in Paragraph (SP) We randomly sample the number of sentences in A ? P i in the interval <ref type="bibr">[1,</ref><ref type="bibr">3]</ref>. The number of sentences in the right part is given by the length of P i \ A (positive pair) or P j \ X j (negative pair). Similar to SSP, we sample up to 2 hard negatives from the same document (if possible), and sample easy negatives from other documents so as to make the total number to be 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Fine-tuning</head><p>Here we present statistics and links for downloading the AS2 datasets used: ASNQ 9 , Wik-iQA 10 , TREC-QA and WQA; to benchmark our pre-trained models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Setup</head><p>We experiment with the base architecture, which uses an hidden size of 768, 12 transformer layers, 12 attention heads and feed-forward size of 3072.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training</head><p>We perform continued pre-training starting from the publicly released checkpoints of RoBERTa-Base (Liu et al., 2019) and ELECTRA-Base <ref type="bibr" target="#b5">(Clark et al., 2020)</ref>. We optimize using Adam, which we instantiate with ? 1 = 0.9, ? 2 = 0.999 and = 10 ?8 . We use a triangular learning rate with 10k warmup steps. The peak learning rate is set to 1 * 10 ?4 . We apply a weight decay of 0.01, gradient clipping when values are larger than 1.0 and dropout ratio is set to 0.1. We set the batch size to 4096 examples for every combination of models and objectives. We truncate the input sequences to 128 tokens for SSP and to 256 tokens with SP and PSD. Finally, we perform 400k training steps with models using SSP and 200k steps with the other objectives: SP and PSD. The total amount of tokens seen in the continued pre-training is the same for all models and equal to ?210B. We combine the binary classification loss of SSP, SP and PSD with MLM for RoBERTa and with MLM (of the generator) and TD (token detection) for ELECTRA. For RoBERTa, we perform binary classification on the first <ref type="bibr">[CLS]</ref> token in addition to MLM. For ELECTRA, using the generator + discriminator architecture, we perform MLM on the generator; and token-detection along with binary classification on the discriminator using our pre-training objectives. Through experimentation, for RoBERTa, we use equal weights for MLM and our pre-training objectives. For ELECTRA, we combine MLM, TD and our pre-training objectives with the weights 1.0, 50.0 and 1.0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning</head><p>The evaluation of the models is performed on four different datasets for Answer Sentence Selection. We maintain the same hyperparameters used in pre-training apart from the learning rate, the number of warmup steps and the batch size. We do early stopping on the development set if the number of non-improving validations (patience) is higher than 5. For ASNQ, we found that using a very large batch size is beneficial, providing a higher accuracy. We use a batch size of 2048 examples on ASNQ for RoBERTa models and 1024 for ELECTRA models. The peak learning rate is set to 1 * 10 ?5 for all models, and the number of warmup steps to 1000. For WikiQA, TREC-QA and WQA, we select the best batch size out of {16, 32, 64} and learning rate out of {2 * 10 ?6 , 5 * 10 ?5 , 1 * 10 ?5 , 2 * 10 ?5 } using crossvalidation. We train the model for 6 epochs on ASNQ, and up to 40 epochs on WikiQA, TREC-QA, and WQA. The performance of practical AS2 systems is typically measured using Precision-at-1 P@1 <ref type="bibr">(Garg and Moschitti, 2021)</ref>. In addition to P@1, we also use Mean Average Precision (MAP) and Mean Reciprocal Recall (MRR) to evaluate the ranking of the set of candidates produced by the model.</p><p>We used metrics from Torchmetrics (Detlefsen et al., 2022) to compute MAP, MRR, Precision@1 and Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiments and Results</head><p>C.1 Ablation: MLM-only Pre-training <ref type="table">Table 5</ref> presents a more detailed comparison between models continuously pre-trained only with MLM and models using also the sentence-level classification loss functions we proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative Examples from AS2</head><p>We present some qualitative examples from the three public AS2 datasets. We highlight cases in which the baseline RoBERTa-Base model is unable to rank the correct answer in the top position, but where our model pretrained with SP is successful. The examples are provided in <ref type="table" target="#tab_11">Table 6</ref>. <ref type="table" target="#tab_2">Sampling   ASNQ  WikiQA  TREC-QA  WQA   P@1  MAP  MRR  P@1  MAP  MRR  P@1  MAP  MRR  P@1</ref> MAP MRR <ref type="table">Table 5</ref>: Results (with std. dev. across 5 runs in parentheses) of our pretrained transformer models when fine-tuned on AS2 datasets with MLM-only pre-training. SSP, SP and PSD refer to our pretraining objectives. Results on WQA are relative to RoBERTa baseline. We highlight in bold and underline results like in <ref type="table" target="#tab_2">Table 1</ref>. WikiQA Q: how are antibodies used in A1: Antibodies are secreted by a type of white blood cell called a plasma cell . A2: An antibody (Ab), also known as an immunoglobulin (Ig), is a large Y-shaped protein produced by B-cells that is used by the immune system to identify and neutralize foreign objects such as bacteria and viruses . A3: Using this binding mechanism, an antibody can tag a microbe or an infected cell for attack by other parts of the immune system, or can neutralize its target directly (for example, by blocking a part of a microbe that is essential for its invasion and survival). A4: Antibodies can occur in two physical forms, a soluble form that is secreted from the cell, and a membrane -bound form that is attached to the surface of a B cell and is referred to as the B cell receptor (BCR). A5: The BCR is only found on the surface of B cells and facilitates the activation of these cells and their subsequent differentiation into either antibody factories called plasma cells , or memory B cells that will survive in the body and remember that same antigen so the B cells can respond faster upon future exposure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model+ Data</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>transfer step on ASNQ (?20M labeled QA pairs). Note that we don't consider Ginzburg et al.; Caciularu et al.; Chang et al. as baselines as they are designed for document-matching and retrieval tasks, and Beltagy et al.; Zaheer et al. as they are used for long-context tasks like MR and summarization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>many players in football hall of fame A1: Two coaches ( Marv Levy , Bud Grant ) , one administrator ( Jim Finks ) , and five players ( Warren Moon , Fred Biletnikoff , John Henry Johnson , Don Maynard , Arnie Weinmeister ) who spent part of their careers in the Canadian Football League ( CFL ) have been inducted ; two of which have been inducted into the Canadian Football Hall of Fame : Warren Moon and Bud Grant. A2: As of 2018 , 318 individuals have been elected . A3: Six players or coaches who spent part of their careers in the short-lived United States Football League ( USFL ) have been inducted . A4: Current rules of the committee stipulate that between four and eight individuals are selected each year . A5: Fifteen inductees spent some of their playing career in the All -America Football Conference during the late 1940s .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(0.3) 68.1 (0.2) 74.5 (0.3) 82.9 (0.7) 88.7 (0.3) 89.9 (0.4) 88.5 (1.2) 89.3 (0.7) 93.6 (0.6) +0.2% +0.6% +0.3% (Ours) RoBERTa + SP 64.1 (0.2) 68.3 (0.1) 74.5 (0.2) 81.0 (0.8) 87.7 (0.3) 88.9 (0.4) 90.9 (2.6) 90.1 (0.8) 94.7 (1.3) +0.4% +0.7% +0.5% (Ours) RoBERTa + PSD 62.6 (0.4) 67.7 (0.2) 73.7 (0.3) 80.5 (1.6) 86.4 (1.1) 88.0 (1.0) 90.3 (1.3) 90.3 (0.5) 95.1 (0.7) +0.4% +0.7% +0.5% (Ours) RoBERTa + All 63.9 (0.4) 68.0 (0.1) 74.1 (0.2) 82.5 (0.9) 88.2 (0.4) 89.5 (0.4) 87.9 (1.2) 89.3 (0.7) 93.4 (0.6) +0.5% +0.8% +0.6% ELECTRA + SSP 65.3 (0.3) 69.7 (0.2) 75.7 (0.2)</figDesc><table><row><cell>Model</cell><cell></cell><cell>ASNQ</cell><cell></cell><cell></cell><cell>WikiQA</cell><cell cols="3">5 Experiments TREC-QA</cell><cell></cell><cell></cell><cell>WQA</cell></row><row><cell></cell><cell>P@1</cell><cell>MAP</cell><cell>MRR</cell><cell>P@1</cell><cell>MAP</cell><cell>MRR</cell><cell>P@1</cell><cell>MAP</cell><cell>MRR</cell><cell>P@1</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>RoBERTa-Base</cell><cell cols="3">61.8 (0.2) 66.9 (0.1) 73.1 (0.1)</cell><cell cols="6">5.1 Datasets 78.3 (2.8) 85.8 (1.3) 87.2 (1.3) 90.0 (1.9) 89.7 (0.7) 94.4 (1.1)</cell><cell></cell><cell>Baseline</cell></row><row><cell cols="2">(Ours) RoBERTa + SSP 64.1 TANDA RoBERTa -</cell><cell>-</cell><cell>-</cell><cell cols="9">Pre-training To eliminate any improvements stemming from the usage of more data, we per-form pre-training on the same corpora as RoBERTa: 83.0 (1.3) 88.5 (0.8) 89.9 (0.8) 89.7 (0.0) 90.1 (0.6) 94.1 (0.4) +0.5% +0.5% +0.5%</cell></row><row><cell>ELECTRA-Base (Ours)</cell><cell cols="3">62.4 (0.4) 67.5 (0.2) 73.6 (0.2)</cell><cell cols="9">77.1 (4.0) 85.0 (2.6) 86.5 (2.7) English Wikipedia, the BookCorpus, OpenWeb-90.3 (1.7) 89.9 (0.4) 94.0 (0.9) +1.0% +1.2% +0.9%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Text and CC-News. We perform continuous pre-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">training starting from RoBERTa (Liu et al., 2019)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">and ELECTRA (Clark et al., 2020) checkpoints,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">using a combination of our objectives with the orig-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">inal ones (MLM for RoBERTa and MLM + Token</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Detection for ELECTRA). Refer to Appendix A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">for complete details.</cell><cell></cell><cell></cell><cell></cell></row></table><note>AS2 Fine-tuning We consider three public and one industrial AS2 benchmark as fine-tuning datasets for AS2 (statistics presented in Ap- pendix A). We use standard evaluation metrics for AS2: Mean Average Precision (MAP), Mean Re- ciprocal Recall (MRR) and Precision@1 (P@1).? ASNQ is a large-scale AS2 dataset (Garg et al., 2020) with questions from Google search engine queries, and answer candidates extracted from a Wikipedia page. ASNQ is a modified version of the Natural Questions (NQ) (Kwiatkowski et al., 2019), obtained by labeling sentences from long answers that contain the short answer as positives and all others as negatives. We use the dev and test splits released by Soldaini and Moschitti 2 .? WikiQA is a popular AS2 dataset (Yang et al., 2015) where questions are derived from query logs of the Bing search engine, and the answer candi- dates are extracted from a Wikipedia page. This dataset has a subset of questions having no correct answers (all-) or having only correct answers (all+). We remove both the (all-) and (all+) questions for our experiments (standard "clean" setting). ? TREC-QA is a popular AS2 dataset (Wang et al., 2007) of factoid questions, extracted from the TREC-8 to TREC-13 QA tracks. The answer can- didates are sentences that contain one or more non- stopwords in common with the question, extracted from multiple documents. For the dev and test sets, we remove questions without answers, or having only correct or only incorrect answer candidates ("clean" setting (Shen et al., 2017b)). ? WQA A large scale industrial AS2 dataset con- taining non-representative de-identified user ques-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results (with std. dev. across 5 runs in parentheses) of our pretrained transformers when fine-tuned on AS2 datasets. SSP, SP, PSD denote our pretraining objectives, and 'All' denotes using SSP+SP+PSD together. TANDA uses additional labeled data as an intermediate transfer step. We underline statistically significant improvements over the baseline (T-test at a 95% confidence level). Results on WQA are relative to the RoBERTa baseline.</figDesc><table><row><cell>tions from Alexa virtual assistant. For every ques-</cell></row><row><cell>tion, ?15 answer candidates are collected from a</cell></row><row><cell>large web index of more than 100M documents us-</cell></row><row><cell>ing Elasticsearch. Results on WQA are presented</cell></row><row><cell>relative to the RoBERTa-Base baseline due to the</cell></row><row><cell>data being internal.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>P@1 of our pretrained models using SSP, SP and PSD objectives in addition to only MLM. We highlight in bold and underline results like inTable 1.</figDesc><table><row><cell cols="3">Model + Pre-training Objective Accuracy F1</cell></row><row><cell>RoBERTa-Base + SSP</cell><cell>91.8</cell><cell>83.1</cell></row><row><cell>ELECTRA-Base + SSP</cell><cell>90.4</cell><cell>79.9</cell></row><row><cell>RoBERTa-Base + SP</cell><cell>91.3</cell><cell>83.3</cell></row><row><cell>ELECTRA-Base + SP</cell><cell>89.9</cell><cell>80.1</cell></row><row><cell>RoBERTa-Base + PSD</cell><cell>83.5</cell><cell>61.4</cell></row><row><cell>ELECTRA-Base + PSD</cell><cell>82.3</cell><cell>57.1</cell></row><row><cell>BERT (Devlin et al., 2019) (NSP)</cell><cell>96.9</cell><cell>97.1</cell></row><row><cell>ALBERT (Lan et al., 2020) (SOP)</cell><cell>93.7</cell><cell>94.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of accuracy and F1-score of pre- training objectives on the pre-training validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Thuy Vu, and Alessandro Moschitti. 2020. Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7780-7788. Ivano Lauriola and Alessandro Moschitti. 2021. Answer sentence selection using local and global context in transformer models. ECIR. Xiangci Li, Gully A. Burns, and Nanyun Peng. 2020. A paragraph-level multi-task learning model for scientific fact-verification. CoRR, abs/2012.14500. Shuohang Wang and Jing Jiang. 2017. A compareaggregate model for matching text sequences. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Yi Yang, Scott Wen-tau Yih, and Chris Meek. 2015.</figDesc><table><row><cell>Siddhant Garg, Dvir Ginzburg, Itzik Malkiel, Oren Barkan, Avi Caciu-Wikiqa: A challenge dataset for open-domain ques-laru, and Noam Koenigstein. 2021. Self-supervised document similarity ranking via contextualized lan-guage models and hierarchical inference. In Find-ings of the Association for Computational Linguis-tics: ACL-IJCNLP 2021, pages 3088-3098, Online. tion answering. In Proceedings of the 2015 Con-ference on Empirical Methods in Natural Language Processing. ACL -Association for Computational Linguistics.</cell><cell>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap-</cell></row><row><cell>Association for Computational Linguistics. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-</cell><cell>proach.</cell></row><row><cell>John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep contrastive learning for unsupervised textual representations. In Proceed-bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. Xlnet: Generalized autoregressive pretraining for language understanding.</cell><cell>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. CoRR, abs/1908.10084.</cell></row><row><cell>ings of the 59th Annual Meeting of the Association Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, for Computational Linguistics and the 11th Interna-Trung Bui, and Kyomin Jung. 2019. A compare-tional Joint Conference on Natural Language Pro-aggregate model with latent clustering for answer se-cessing (Volume 1: Long Papers), pages 879-895, lection. Proceedings of the 28th ACM International Online. Association for Computational Linguistics. Conference on Information and Knowledge Manage-</cell><cell>Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text pairs with convolutional deep neural networks. Proceedings of the 38th Inter-national ACM SIGIR Conference on Research and</cell></row><row><cell>ment.</cell><cell>Development in Information Retrieval.</cell></row><row><cell>Aaron Gokaslan and Vanya Cohen. 2019. Open-</cell><cell></cell></row><row><cell>webtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus. Daphne Ippolito, David Grangier, Douglas Eck, and Chris Callison-Burch. 2020. Toward better story-lines with sentence-level language models. In Pro-Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago On-tanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33.</cell><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Gehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017a. Inter-weighted alignment network for sentence pair modeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process-ing, pages 1179-1189, Copenhagen, Denmark. As-sociation for Computational Linguistics.</cell></row><row><cell>ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 7472-7478, Online. Association for Computational Lin-guistics. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving pre-training by representing Xingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI-BERT: Document level pre-training of hierarchical bidirectional transformers for document summariza-tion. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5059-5069, Florence, Italy. Association for Computational Linguistics.</cell><cell>deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ-ation for Computational Linguistics. Gehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017b. Inter-weighted alignment network for sentence pair modeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process-ing, pages 1179-1189, Copenhagen, Denmark. As-sociation for Computational Linguistics.</cell></row><row><cell>and predicting spans. Transactions of the Associa-</cell><cell>Luca Di Liello, Siddhant Garg, Luca Soldaini, and Luca Soldaini and Alessandro Moschitti. 2020. The</cell></row><row><cell>tion for Computational Linguistics, 8:64-77.</cell><cell>Alessandro Moschitti. 2022. Paragraph-based trans-cascade transformer: an application for efficient an-</cell></row><row><cell></cell><cell>former pre-training for multi-sentence inference. In swer sentence selection. In Proceedings of the 58th</cell></row><row><cell>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-</cell><cell>Proceedings of the 2022 Conference of the North Annual Meeting of the Association for Computa-</cell></row><row><cell>field, Michael Collins, Ankur Parikh, Chris Al-</cell><cell>American Chapter of the Association for Computa-tional Linguistics, pages 5697-5708, Online. Asso-</cell></row><row><cell>berti, Danielle Epstein, Illia Polosukhin, Jacob De-</cell><cell>tional Linguistics: Human Language Technologies, ciation for Computational Linguistics.</cell></row><row><cell>vlin, Kenton Lee, Kristina Toutanova, Llion Jones,</cell><cell>pages 2521-2531, Seattle, United States. Associa-</cell></row><row><cell>Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,</cell><cell>tion for Computational Linguistics. Quan Hung Tran, Tuan Lai, Gholamreza Haffari, Ingrid</cell></row><row><cell>Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.</cell><cell>Zukerman, Trung Bui, and Hung Bui. 2018. The</cell></row><row><cell>Natural questions: A benchmark for question an-</cell><cell>Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. context-dependent additive recurrent neural net. In</cell></row><row><cell>swering research. Transactions of the Association</cell><cell>SimCSE: Simple contrastive learning of sentence Proceedings of the 2018 Conference of the North</cell></row><row><cell>for Computational Linguistics, 7:452-466.</cell><cell>embeddings. In Proceedings of the 2021 Conference American Chapter of the Association for Computa-</cell></row><row><cell></cell><cell>on Empirical Methods in Natural Language Process-tional Linguistics: Human Language Technologies,</cell></row><row><cell>Zhenzhong Lan, Mingda Chen, Sebastian Goodman,</cell><cell>ing, pages 6894-6910, Online and Punta Cana, Do-Volume 1 (Long Papers), pages 1274-1283, New</cell></row><row><cell>Kevin Gimpel, Piyush Sharma, and Radu Soricut.</cell><cell>minican Republic. Association for Computational Orleans, Louisiana. Association for Computational</cell></row><row><cell>2020. Albert: A lite bert for self-supervised learn-</cell><cell>Linguistics. Linguistics.</cell></row><row><cell>ing of language representations.</cell><cell></cell></row><row><cell></cell><cell>Siddhant Garg and Alessandro Moschitti. 2021. Will Mengqiu Wang, Noah A. Smith, and Teruko Mita-</cell></row><row><cell>Md Tahmid Rahman Laskar, Jimmy Xiangji Huang,</cell><cell>this question be answered? question filtering via mura. 2007. What is the Jeopardy model? a</cell></row><row><cell>and Enamul Hoque. 2020. Contextualized embed-</cell><cell>answer model distillation for efficient question an-quasi-synchronous grammar for QA. In Proceed-</cell></row><row><cell>dings based transformer encoder for sentence simi-</cell><cell>swering. In Proceedings of the 2021 Conference on ings of the 2007 Joint Conference on Empirical</cell></row><row><cell>larity modeling in answer selection task. In Proceed-</cell><cell>Empirical Methods in Natural Language Processing, Methods in Natural Language Processing and Com-</cell></row><row><cell>ings of the 12th Language Resources and Evaluation</cell><cell>pages 7329-7346, Online and Punta Cana, Domini-putational Natural Language Learning (EMNLP-</cell></row><row><cell>Conference, pages 5505-5514, Marseille, France.</cell><cell>can Republic. Association for Computational Lin-CoNLL), pages 22-32, Prague, Czech Republic. As-</cell></row><row><cell>European Language Resources Association.</cell><cell>guistics. sociation for Computational Linguistics.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>shows the number of unique questions and answer candidates for each dataset and for each split.</figDesc><table><row><cell>Dataset</cell><cell>Split</cell><cell># Q</cell><cell># C</cell><cell>Avg. # C/Q</cell></row><row><cell></cell><cell cols="3">Train 57,242 20,377,568</cell><cell>356.0</cell></row><row><cell>ASNQ</cell><cell>Dev</cell><cell>1,336</cell><cell>463,914</cell><cell>347.2</cell></row><row><cell></cell><cell>Test</cell><cell>1,336</cell><cell>466,148</cell><cell>348.9</cell></row><row><cell></cell><cell>Train</cell><cell>2,118</cell><cell>20,360</cell><cell>9.6</cell></row><row><cell>WikiQA</cell><cell>Dev</cell><cell>122</cell><cell>1,126</cell><cell>9.2</cell></row><row><cell></cell><cell>Test</cell><cell>237</cell><cell>2,341</cell><cell>9.9</cell></row><row><cell></cell><cell>Train</cell><cell>1,226</cell><cell>53,417</cell><cell>43.6</cell></row><row><cell>TREC-QA</cell><cell>Dev</cell><cell>69</cell><cell>1,343</cell><cell>19.5</cell></row><row><cell></cell><cell>Test</cell><cell>68</cell><cell>1,442</cell><cell>21.2</cell></row><row><cell></cell><cell>Train</cell><cell>9,984</cell><cell>149,513</cell><cell>15.0</cell></row><row><cell>WQA</cell><cell>Dev</cell><cell>5,000</cell><cell>74,805</cell><cell>15.0</cell></row><row><cell></cell><cell>Test</cell><cell>5,000</cell><cell>74,712</cell><cell>14.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Data Statistics for AS2 dataset. "Avg. # C/Q" is the average number of answer candidates per question.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Let 's now give a welcome to the Wiggles , a goofy new import from Australia . A2: The Wiggles are four effervescent performers from the Sydney area : Anthony Field , Murray Cook , Jeff Fatt and Greg Page . A3: In Australia , the Wiggles is like really huge . A4: His group had kids howling with joy with routines involving Dorothy the Dinosaur , Henry the Octopus and Wags the Dog . A5: While relatively new to the American scene , the Wiggles seem to be on to something , judging by kids ' reactions to the group 's belly-slapping shows .</figDesc><table><row><cell>TREC-QA</cell></row><row><cell>Q: Where is the group Wiggles from ?</cell></row><row><cell>A1:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Qualitative examples from AS2 datasets where the baseline RoBERTa-Base model is unable to rank a correct answer for the question at the top position, but our SP pre-trained model can (top ranked correct answer by SP). Here we present the top ranked answers {A1, . . . , A5} in the order given by the RoBERTa-Base model. For all these examples we highlight the top ranked answer by the baseline RoBERTa-Base model in red since it is incorrect, and any other correct answer in green.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will release code and pre-trained models at https: //github.com/amazon-research/wqa-pretraining</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/alexa/ wqa-cascade-transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The original models use a triangular learning-rate</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers and the ARR action-editor for their valuable suggestions. We would like to thank Thuy Vu for developing and sharing the WQA dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A compare-aggregate model with dynamic-clip attention for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matthew Peters, Arie Cattan, and Ido Dagan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Caciularu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2648" to="2662" />
		</imprint>
	</monogr>
	<note>CDLM: Cross-document language modeling</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language model pre-training for hierarchical document representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno>abs/1901.09128</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pre-training tasks for embedding-based large-scale retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Changsheng Quan, Maxim Grechkin, and William Falcon. 2022. Torchmetrics -measuring reproducibility in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicki</forename><surname>Skafte Detlefsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Schock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teddy</forename><surname>Koker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><forename type="middle">Di</forename><surname>Liello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stancl</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.04101</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page">4101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Ssp Data</surname></persName>
		</author>
		<idno>MLM-only) 63.4 (0.4) 67.1 (0.2) 73.8 (0.2) 76.7 (0.9) 84.5 (0.7) 85.8 (0.7) 87.4 (1.3) 88.8 (0.6) 93.1 (1.0) -0.6% -0.2% -0.3%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Sp Data</surname></persName>
		</author>
		<idno>1.0% -0.4% -0.6%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Psd Data</surname></persName>
		</author>
		<idno>MLM-only) 64.1 (0.5</idno>
		<imprint>
			<biblScope unit="volume">67</biblScope>
		</imprint>
	</monogr>
	<note>3 (0.2) 73.7 (0.2) 79.1 (1.6) 85.6 (1.4) 87.1 (1.2) 87.1 (2.8) 89.6 (1.0) 92.7 (1.3) -1.3% -0.3% -0.6%</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
