<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Bregman Learning Framework for Sparse Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-17">17 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bungert</surname></persName>
							<email>leon.bungert@hcm.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hausdorff Center for Mathematics</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<addrLine>Endenicher Allee 62, Villa Maria</addrLine>
									<postCode>53115</postCode>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Roith</surname></persName>
							<email>tim.roith@fau.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics Friedrich</orgName>
								<orgName type="institution">Alexander University</orgName>
								<address>
									<addrLine>Erlangen-N?rnberg Cauerstra?e 11</addrLine>
									<postCode>91058</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tenbrinck</surname></persName>
							<email>daniel.tenbrinck@fau.de</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics Friedrich</orgName>
								<orgName type="institution">Alexander University</orgName>
								<address>
									<addrLine>Erlangen-N?rnberg Cauerstra?e 11</addrLine>
									<postCode>91058</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Burger</surname></persName>
							<email>martin.burger@fau.de</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics Friedrich</orgName>
								<orgName type="institution">Alexander University</orgName>
								<address>
									<addrLine>Erlangen-N?rnberg Cauerstra?e 11</addrLine>
									<postCode>91058</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Bregman Learning Framework for Sparse Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-17">17 Feb 2022</date>
						</imprint>
					</monogr>
					<note>A Bregman Learning Framework Bungert et al</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bregman Iterations</term>
					<term>Sparse Neural Networks</term>
					<term>Sparsity</term>
					<term>Inverse Scale Space</term>
					<term>Optimization 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a learning framework based on stochastic Bregman iterations, also known as mirror descent, to train sparse neural networks with an inverse scale space approach. We derive a baseline algorithm called LinBreg, an accelerated version using momentum, and AdaBreg, which is a Bregmanized generalization of the Adam algorithm. In contrast to established methods for sparse training the proposed family of algorithms constitutes a regrowth strategy for neural networks that is solely optimization-based without additional heuristics. Our Bregman learning framework starts the training with very few initial parameters, successively adding only significant ones to obtain a sparse and expressive network. The proposed approach is extremely easy and efficient, yet supported by the rich mathematical theory of inverse scale space methods. We derive a statistically profound sparse parameter initialization strategy and provide a rigorous stochastic convergence analysis of the loss decay and additional convergence proofs in the convex regime. Using only 3.4% of the parameters of ResNet-18 we achieve 90.2% test accuracy on CIFAR-10, compared to 93.6% using the dense network. Our algorithm also unveils an autoencoder architecture for a denoising task. The proposed framework also has a huge potential for integrating sparse backpropagation and resource-friendly training.</p><p>( <ref type="formula">1.17)</ref> The Bregman distance can be interpreted as the distance between the linearization of J at ? and its graph and hence somewhat measures the degree of linearity of the functional. Note furthermore that the Bregman distance (1.17) is neither definite, symmetric nor fulfills the triangle inequality, hence it is not a metric. However, it fulfills the two distance axioms</p><p>(1.18)</p><p>By summing up two Bregman distances, one can also define the symmetric Bregman distance with respect to p ? ?J(?) and p ? ?J(?) as</p><p>Here, we suppress the dependency on p and p to simplify the notation. Last, we define the proximal operator of a convex, proper and lower semicontinuous functional J : ? ? (??, ?] as prox J (?) := arg min ???</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large and deep neural networks have shown astonishing results in challenging applications, ranging from real-time image classification in autonomous driving, over assisted diagnoses in healthcare, to surpassing human intelligence in highly complex games <ref type="bibr" target="#b0">(Amato et al., 2013;</ref><ref type="bibr" target="#b52">Rawat and Wang, 2017;</ref><ref type="bibr" target="#b58">Silver et al., 2016)</ref>. The main drawback of many of these architectures is that they require huge amounts of memory and can only be employed using specialised hardware, like GPGPUs and TPUs. This makes them inaccessible to normal users with only limited computational resources on their mobile devices or computers <ref type="bibr" target="#b31">(Hoefler et al., 2021)</ref>. Moreover, the carbon footprint of training large networks has become an issue of major concern recently (Dhar, 2020), hence calling for resource-efficient methods.</p><p>The success of large and deep neural networks is not surprising as it has been predicted by universal approximation theorems <ref type="bibr" target="#b15">(Cybenko, 1989;</ref><ref type="bibr" target="#b41">Lu et al., 2017)</ref>, promising a smaller error with increasing number of neurons and layers. Besides the increase in computational complexity, each neuron added to the network architecture also adds to the amount of free parameters and local optima of the loss.</p><p>Consequently, a significant branch of modern research aims for training "sparse neural networks", which has lead to different strategies, based on neglecting small parameters or such with little influence on the network output, see <ref type="bibr" target="#b31">Hoefler et al. (2021)</ref> for an extensive review. Apart from computational and resource efficiency, sparse training also sheds light on neural architecture design and might answer the question why certain architectures work better than others.</p><p>A popular approach for generating sparse neural networks are pruning techniques <ref type="bibr" target="#b36">(LeCun et al., 1990;</ref><ref type="bibr" target="#b28">Han et al., 2015)</ref>, which have been developed to sparsify a dense neural network during or after training by dropping dispensable neurons and connections. Another approach, which is based on the classical Lasso method from compressed sensing <ref type="bibr" target="#b60">(Tibshirani, 1996)</ref>, incorporates 1 regularization into the training problem, acting as convex relaxation of sparsity-enforcing 0 regularization. These endeavours are further supported by the recently stated "lottery ticket hypothesis" <ref type="bibr" target="#b23">(Frankle and Carbin, 2018)</ref>, which postulates that dense, feed-forward networks contain sub-networks with less neurons that, if trained in isolation, can achieve the same test accuracy as the original network.</p><p>An even more intriguing idea is "grow-and-prune" <ref type="bibr" target="#b16">(Dai et al., 2019)</ref>, which starts with a sparse network and augments it during training, while keeping it as sparse as possible. To this end new neurons are added, e.g., by splitting overloaded neurons into new specimen or using gradient-based indicators, while insignificant parameters are set to zero by thresholding.</p><p>Many of the established methods in the literature are bound to specific architectures, e.g., fully-connected feedforward layers <ref type="bibr" target="#b14">(Castellano et al., 1997;</ref><ref type="bibr" target="#b39">Liu et al., 2021)</ref>. In this paper we propose a more conceptual and optimization-based approach. The idea is to mathematically follow the intuition of starting with very few parameters and adding only necessary ones in an inverse scale space manner, see  <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration of our algorithm on a convolutional neural network. For this sake we propose a Bregman learning framework utilizing linearized Bregman iterations-originally introduced for compressed sensing by <ref type="bibr" target="#b65">Yin et al. (2008)</ref>-for training sparse neural networks.</p><p>Our main contributions are the following:</p><p>? We derive an extremely simple and efficient algorithm for training sparse neural networks, called LinBreg.</p><p>? We also propose a momentum-based acceleration and AdaBreg, which utilizes the Adam algorithm <ref type="bibr" target="#b34">(Kingma and Ba, 2014)</ref>.</p><p>? We perform a rigorous stochastic convergence analysis of LinBreg for strongly convex losses, in infinite dimensions, and without any smoothness assumptions on J.</p><p>? We propose a sparse initialization strategy for the network parameters.</p><p>? We show that our algorithms are effective for training sparse neural networks and show their potential for architecture design by unveiling a denoising autoencoder.</p><p>The structure of this paper is as follows: In Section 1.1 we explain our baseline algorithm LinBreg in a nutshell and in Section 1.2 we discuss related work. Sections 1.3 and 1.4 clarify notation and collect preliminaries on neural networks and convex analysis, the latter being important for the derivation and analysis of our algorithms. In Section 2 we explain how Bregman iterations can be incorporated into the training of sparse neural networks, derive and discuss variants of the proposed Bregman learning algorithm, including accelerations using momentum and Adam. We perform a mathematical analysis for stochastic linearized Bregman iterations in Section 3 and discuss conditions for convergence of the loss function and the parameters. In Section 4 we first discuss our statistical sparse initialization strategy and then evaluate our algorithms on benchmark data sets (MNIST, Fashion-MNIST, CIFAR-10) using feedforward, convolutional, and residual neural networks.</p><p>Algorithm 1: LinBreg, an inverse scale space algorithm for training sparse neural networks by successively adding weights whilst minimizing the loss. The functional J is sparsity promoting, e.g., the 1 -norm.</p><formula xml:id="formula_0">default: ? = 1 ? ? Section 4.1, v ? ?J(?) + 1 ? ? // initialize for epoch e = 1 to E do for minibatch B ? T do g ? ?L(?; B) // Backpropagation v ? v ? ? g // Gradient step ? ? prox ?J (?v) // Regularization</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Bregman Training Algorithm in a Nutshell</head><p>Algorithm 1 states our baseline algorithm LinBreg for training sparse neural networks with an inverse scale space approach. Mathematical tools and derivations of LinBreg and its variants LinBreg with momentum (Algorithm 2) and AdaBreg (Algorithm 3), a generalization of Adam <ref type="bibr" target="#b34">(Kingma and Ba, 2014)</ref>, are presented in Section 2; a convergence analysis is provided in Section 3. LinBreg can easily be applied to any neural network architecture f ? , parametrized with parameters ? ? ?, using a set of training data T , and an empirical loss function L(?; B), where B ? T is a batch of training data. LinBreg's most important ingredient is a sparsity enforcing functional J : ? ? (??, ?], which acts on groups of network parameters as, for instance, convolutional kernels, weight matrices, biases, etc. Following <ref type="bibr" target="#b57">Scardapane et al. (2017)</ref> and denoting the collection of all parameter groups for which sparsity is desired by G, two possible regularizers which induce sparsity or group sparsity, respectively, can be defined as</p><formula xml:id="formula_1">J(?) = ? g?G g 1 , the 1 -norm, (1.1) J(?) = ? g?G ? n g g 2 , the group 1,2 -norm. (1.2)</formula><p>Here ? &gt; 0 is a parameter controlling the regularization strength, n g denotes the number of elements in g, and the factor ? n g ensures a uniform weighting of all groups <ref type="bibr" target="#b57">(Scardapane et al., 2017)</ref>. LinBreg uses two variables v and ?, coupled through the condition that v ? ?J ? (?) is a subgradient of the elastic net regularization J ? (?) := J(?) + 1 2? ? 2 introduced by Zou and Hastie (2005) (see Sections 1.3 and 1.4 for definitions). The algorithm successively updates v with gradients of the loss and recovers sparse parameters ? by applying a proximal operator. For instance, if J(?) = ? ? 1 equals the 1 -norm, the proximal operator in Algorithm 1 coincides with the soft shrinkage operator:</p><formula xml:id="formula_2">prox ?J (?v) = ? shrink(v; ?) := ? sign(v) max(|v| ? ?, 0). (1.3)</formula><p>In this case only those parameters ? will be non-zero whose subgradients v have magnitude larger than the regularization parameter ?. Furthermore, ? &gt; 0 only steers the magnitude of the resulting weights and not their support. Furthermore, if J(?) = 0 then prox ?J (?v) = ?v and therefore Algorithm 1 coincides with stochastic gradient descent (SGD) with learning rate ?? . These two observations explain our default choice of ? = 1. In general, the proximal operators of the regularizers above can be efficiently evaluated since they admit similar closed form solutions based on soft thresholding. Hence, the computational complexity of LinBreg is dominated by the backpropagation and coincides with the complexity of vanilla stochastic gradient descent. However, note that our framework has great potential for complexity reduction via sparse backpropagation methods, cf. <ref type="bibr" target="#b18">Dettmers and Zettlemoyer (2019)</ref>.</p><p>The special feature which tells LinBreg apart from standard sparsity regularization <ref type="bibr" target="#b40">(Louizos et al., 2017;</ref><ref type="bibr" target="#b57">Scardapane et al., 2017;</ref><ref type="bibr" target="#b59">Srinivas et al., 2017)</ref> or pruning <ref type="bibr" target="#b36">(LeCun et al., 1990;</ref><ref type="bibr" target="#b28">Han et al., 2015)</ref> is its inverse scale space character. LinBreg is derived based on Bregman iterations, originally developed for scale space approaches in imaging <ref type="bibr" target="#b49">(Osher et al., 2005;</ref><ref type="bibr" target="#b9">Burger et al., 2006;</ref><ref type="bibr" target="#b65">Yin et al., 2008;</ref><ref type="bibr">Cai et al., 2009b,a;</ref><ref type="bibr" target="#b67">Zhang et al., 2011)</ref>. Instead of removing weights from a dense trained network, it starts from a very sparse initial set of parameters (see Section 4.1) and successively adds non-zero parameters whilst minimizing the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>Dense-to-Sparse Training A well-established approach for training sparse neural network consists in solving the regularized empirical risk minimization min ??? L(?; B) + J(?),</p><p>( <ref type="formula">1.4)</ref> where J is a (sparsity-promoting) non-smooth regularization functional. If J equals the 1 -norm this is referred to as Lasso <ref type="bibr" target="#b60">(Tibshirani, 1996)</ref> and was extended to Group Lasso for neural networks by <ref type="bibr" target="#b57">Scardapane et al. (2017)</ref> by using group norms. We refer to de Dios and Bruna (2020) for a mean-field analysis of this approach. The regularized risk minimization (1.4) is a special case of Dense-to-Sparse training. Even if the network parameters are initialized sparsely, any optimization method for (1.4) will instantaneously generate dense weights, which are subsequently sparsified. A different strategy for Dense-to-Sparse training is pruning <ref type="bibr" target="#b36">(LeCun et al., 1990;</ref><ref type="bibr" target="#b28">Han et al., 2015)</ref>, see also <ref type="bibr" target="#b68">Zhu and Gupta (2017)</ref>, which first trains a network and then removes parameters to create sparse weights. This procedure can also be applied alternatingly, which is referred to as iterative pruning <ref type="bibr" target="#b14">(Castellano et al., 1997)</ref>. The weight removal can be achieved based on different criteria, e.g., their magnitude or their influence on the network output.</p><p>Sparse-to-Sparse Training In contrast, Sparse-to-Sparse training aims to grow a neural network starting from a sparse initialization until it is sufficiently accurate. This is also the paradigm of our LinBreg algorithm, generating an inverse sparsity scale space. Other approaches from literature are grow-and-prune strategies <ref type="bibr" target="#b43">(Mocanu et al., 2018;</ref><ref type="bibr" target="#b18">Dettmers and Zettlemoyer, 2019;</ref><ref type="bibr" target="#b16">Dai et al., 2019;</ref><ref type="bibr" target="#b39">Liu et al., 2021;</ref><ref type="bibr" target="#b22">Evci et al., 2020)</ref> which, starting from sparse networks, successively add and remove neurons or connections while training the networks.</p><p>Proximal Gradient Descent A related approach to LinBreg is proximal gradient descent (ProxGD) for optimizing the regularized empirical risk minimization (1.4), which is an inherently non-smooth optimization problem due to the presence of the 1 -norm-type functional J. Therefore, proximal gradient descent alternates between a gradient step of the loss with a proximal step of the regularization:  <ref type="bibr" target="#b66">Yun et al. (2020)</ref>. It differs from Algorithm 1 by the lack of a subgradient variable and by using the learning rate ? within the proximal operator. These seemingly minor algorithmic differences cause major differences for the trained parameters. Indeed, the effect of J kicks in only after several iterations when the proximal operator has been applied sufficiently often to set some parameters to zero, as can be observed in <ref type="figure" target="#fig_2">Figure 2</ref> below. Furthermore, proximal gradient descent does not decrease the loss monotonously which we are able to prove for LinBreg.</p><formula xml:id="formula_3">g ? ?L(?; B) (1.5a) ? ? ? ? ? g (1.5b) ? ? prox ? J (?</formula><p>Bregman Iterations Bregman iterations and in particular linearized Bregman iterations have been introduced and thoroughly analyzed for sparse regularization approaches in imaging and compressed sensing (see, e.g., <ref type="bibr" target="#b49">Osher et al. (2005)</ref>  <ref type="bibr">et al. (2018)</ref>. Linearized Bregman iterations for non-convex problems, which appear in machine learning and imaging applications like blind deblurring, have first been analyzed by <ref type="bibr" target="#b2">Bachmayr and Burger (2009)</ref>; <ref type="bibr" target="#b7">Benning and Burger (2018)</ref>; <ref type="bibr" target="#b8">Benning et al. (2021)</ref>. <ref type="bibr" target="#b7">Benning and Burger (2018)</ref> also showed that linearized Bregman iterations for convex problems can be formulated as forward pass of a neural network. <ref type="bibr" target="#b8">Benning et al. (2021)</ref> applied them for training neural networks with low-rank weight matrices, using nuclear norm regularization.  suggested a split Bregman approach for training sparse neural networks and <ref type="bibr" target="#b25">Fu et al. (2019)</ref> provided a deterministic convergence result along the lines of <ref type="bibr" target="#b8">Benning et al. (2021)</ref>. A recent analysis of Bregman stochastic gradient descent, which is the same as linearized Bregman iterations, however using strong regularity assumptions on the involved functions, is done by <ref type="bibr" target="#b21">Dragomir et al. (2021)</ref>.</p><p>Mirror Descent As it turns out, linearized Bregman iterations are largely known under yet another name: mirror descent. This method was first proposed by <ref type="bibr" target="#b45">Nemirovskij and Yudin (1983)</ref> and related to Bregman distances by <ref type="bibr" target="#b5">Beck and Teboulle (2003)</ref>. <ref type="bibr" target="#b21">Dragomir et al. (2021)</ref> present a literature overview of stochastic mirror descent. Some months after the release of the preprint version of the present article, D' <ref type="bibr">Orazio et al. (2021)</ref> presented a convergence analysis of stochastic mirror descent a.k.a. Bregman iterations, using a weaker bounded variance condition for the stochastic gradients, albeit working in a smooth setting. In contrast, our analysis does not require any smoothness of J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Preliminaries on Neural Networks</head><p>We denote neural networks, which map from an input space X to an output space Y and have parameters in some parameter space ?, by</p><formula xml:id="formula_4">f ? : X ? Y, ? ? ?. (1.6)</formula><p>In principle X , Y, and ? can be infinite-dimensional and we only assume that ? is a Hilbert space, equipped with an inner product ? , ? and associated norm ? = ?, ? . Given a set of training pairs T ? X ? Y and a loss function : Y ? Y ? R we denote the empirical loss associated to the training data by</p><formula xml:id="formula_5">L(?) := 1 |T | (x,y)?T (f ? (x), y).</formula><p>(1.7)</p><p>The empirical risk minimization approach to finding optimal parameters ? ? ? of the neural network f ? then consists in solving min ??? L(?).</p><p>(1.8)</p><p>If one assumes that the training set T is sampled from some probability measure ? on the product space X ? Y, the empirical risk minimization is an approximation of the infeasible population risk minimization min ??? X ?Y (f ? (x), y) d?(x, y).</p><p>(1.9)</p><p>One typically samples batches B ? T from the training set and replaces L(?) by the empirical risk of the batch</p><formula xml:id="formula_6">L(?; B) := 1 |B| (x,y)?B (f ? (x), y), (1.10)</formula><p>which is utilized in stochastic gradient descent methods. For a feed-forward architecture with L ? N layers of sizes n l we split the variable ? into weights and biases W l ? R n l ,n l?1 , b l ? R n l for l ? {1, . . . , L}. In this case we have</p><formula xml:id="formula_7">f ? (x) = ? L ? ? ? ? ? ? 1 (x), (1.11)</formula><p>where the l-th layer for l ? {1, . . . , L} is given by</p><formula xml:id="formula_8">? l (z) := ? l (W l z + b l ). (1.12)</formula><p>Here ? l denote activation functions, as for instance ReLU, TanH, Sigmoid, etc., <ref type="bibr" target="#b27">(Goodfellow et al., 2016)</ref>. In this case, sparsity promoting regularizers are the 1norm or the group 1,2 -norm</p><formula xml:id="formula_9">J(?) = ? L l=1 W l 1,1 , (1.13) J(?) = ? L l=1 ? n l?1 W l 1,2 , (1.14)</formula><p>which induce sparsity of the weight matrices and of the non-zero rows of weight matrices, respectively. Here the scaling ? n l?1 weighs the influence of the l-th layer based on the number of incoming neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Preliminaries on Convex Analysis</head><p>In this section we introduce some essential concepts from convex analysis which we need to derive LinBreg and its variants and in order to make our argumentation more self-contained. For an overview of these topics we refer to <ref type="bibr" target="#b7">Benning and Burger (2018)</ref>; <ref type="bibr" target="#b54">Rockafellar (1997)</ref>; <ref type="bibr" target="#b3">Bauschke and Combettes (2011)</ref>.</p><formula xml:id="formula_10">A functional J : ? ? (??, ?] on a Hilbert space ? is called convex if J(?? + (1 ? ?)?) ? ?J(?) + (1 ? ?)J(?), ?? ? [0, 1], ?, ? ? ?. (1.15)</formula><p>We define the effective domain of J as dom(J) := {? ? ? : J(?) = ?} and call J proper if dom(J) = ?. Furthermore, J is called lower semicontinuous if J(u) ? lim inf n?? J(u n ) holds for all sequences (u n ) n?N ? ? converging to u. First, we define the subdifferential of a convex and proper functional J : ? ? (??, ?] at a point ? ? ? as</p><formula xml:id="formula_11">?J(?) := p ? ? : J(?) + p, ? ? ? ? J(?), ?? ? ? . (1.16)</formula><p>The subdifferential is a non-smooth generalization of the derivative and coincides with the classical gradient (or Fr?chet derivative) if J is differentiable. We denote dom(?J) := {? ? ? : ?J(?) = ?} and observe that dom(?J) ? dom(J). Next, we define the Bregman distance of two points ? ? dom(?J), ? ? ? with respect to a convex and proper functional J : ? ? (??, ?] as</p><formula xml:id="formula_12">1 2 ? ? ? 2 + J(?).</formula><p>(1.20)</p><p>Proximal operators are a key concept in non-smooth optimization since they can be used to replace gradient descent steps of non-smooth functionals, as done for instance in proximal gradient descent (1.5). Obviously, given some ? ? ? the proximal operator outputs a new element ? ? ? which has a smaller value of J whilst being close to the previous element ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Bregmanized training of Neural Networks</head><p>In this section we first give a short overview of inverse scale space flows which are the time-continuous analogue of our algorithms. Subsequently, we derive LinBreg (Algorithm 1) by passing from Bregman iterations to linearized Bregman iterations, which we then reformulate in a very easy and compact form. We then derive LinBreg with momentum (Algorithm 2) by discretizing a second-order in time inverse scale space flow and propose AdaBreg (Algorithm 3) as a generalization of the popular Adam algorithm (Kingma and Ba, 2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inverse Scale Space Flows (with Momentum)</head><p>In the following we discuss the inverse scale space flow, which arises as gradient flow of a loss functional L with respect to the Bregman distance (1.17). In particular, it couples the minimization of L with a simultaneous regularization through J. To give meaning to this, one considers the following implicit Euler scheme</p><formula xml:id="formula_13">? (k+1) = arg min ??? D p (k) J (?, ? (k) ) + ? (k) L(?), (2.1a) p (k+1) = p (k) ? ? (k) ?L(? (k+1) ) ? ?J(? (k+1) ) (2.1b)</formula><p>which is know as Bregman iteration. Here, ? (k) is the previous iterate with subgradient p (k) ? ?J(? (k) ), and ? (k) &gt; 0 is a sequence of time steps. Note that the subgradient update in the second line of (2.1) coincides with the optimality conditions of the first line. The time-continuous limit of (2.1) as</p><formula xml:id="formula_14">? (k) ? 0 is the inverse scale space flow ? t = ??L(? t ), p t ? ?J(? t ), (2.2)</formula><p>see <ref type="bibr" target="#b9">Burger et al. (2006</ref><ref type="bibr" target="#b10">Burger et al. ( , 2007</ref> for a rigorous derivation in the context of image denoising.</p><p>If J(?) = 1 2 ? 2 then ?J(?) = ? and (2.2) coincides with the standard gradient flow? Hence, the inverse scale space is a proper generalization of the gradient flow and allows for regularizing the path along which the loss is minimized using J (see <ref type="bibr" target="#b8">Benning et al. (2021)</ref>). For strictly convex loss functions this might seem pointless since they have a unique minimum anyways, however, for merely convex or even nonconvex losses the inverse scale space allows to 'select' (local) minima with desirable properties.</p><p>In this paper, we also propose an inertial version of (2.2) which depends on an inertial parameter ? ? 0 and takes the form</p><formula xml:id="formula_15">?p t +? t = ??L(? t ), p t ? ?J(? t ).</formula><p>(2.4)</p><p>One can introduce the momentum variable m t :=? t which solves the differential equation</p><formula xml:id="formula_16">?? t + m t = ??L(? t ).</formula><p>If one assumes m 0 = 0, this equation has the explicit solution</p><formula xml:id="formula_17">m t = ? t 0 exp s ? t ? ?L(? s )ds</formula><p>and hence the second-order in time equation (2.4) is equivalent to the gradient memory inverse scale space flow</p><formula xml:id="formula_18">? t = ? t 0 exp s?t ? ?L(? s )ds, p t ? ?J(? t ).</formula><p>(2.5) For a nice overview over the derivation of gradient flows with momentum we refer to <ref type="bibr" target="#b48">Orvieto et al. (2020)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">From Bregman to Linearized Bregman Iterations</head><p>The starting point for the derivation of Algorithm 1 is the Bregman iteration (2.1), which is the time discretization of the inverse scale space flow (2.2). Since the iterations (2.1) require the minimization of the loss in every iteration they are not feasible for large-scale neural networks. Therefore, we consider linearized Bregman iterations <ref type="bibr" target="#b13">(Cai et al., 2009b)</ref>, which linearize the loss function by</p><formula xml:id="formula_19">L(?) ? L(? (k) ) + g (k) , ? ? ? (k) , g (k) := ?L(? (k) ),</formula><p>and replace the energy J with the strongly convex elastic-net regularization</p><formula xml:id="formula_20">J ? (?) := J(?) + 1 2? ? 2 , ? ? (0, ?). (2.6)</formula><p>Omitting all terms which do not depend on ?, the first line of (2.1) then becomes</p><formula xml:id="formula_21">? (k+1) = arg min ??? ? (k) g (k) , ? + J ? (?) ? v (k) , ? = arg min ??? ? (k) g (k) , ? + J(?) + 1 2? ? 2 ? v (k) , ? = arg min ??? 1 2? ? ? ? v (k) ? ? (k) g (k) 2 + J(?) = prox ?J ? v (k) ? ? (k) g (k) . (2.7)</formula><p>The vector v (k) ? ?J ? (? (k) ) is a subgradient of the functional J ? in the previous iterate. Using the update</p><formula xml:id="formula_22">v (k+1) := v (k) ? ? (k) g (k)</formula><p>and combining this with (2.7) we obtain the compact update scheme</p><formula xml:id="formula_23">g (k) = ?L(? (k) ), (2.8a) v (k+1) = v (k) ? ? (k) g (k) , (2.8b) ? (k+1) = prox ?J ?v (k+1) . (2.8c)</formula><p>This iteration is an equivalent reformulation of linearized Bregman iterations <ref type="bibr" target="#b65">(Yin et al., 2008;</ref><ref type="bibr">Cai et al., 2009b,a;</ref><ref type="bibr" target="#b50">Osher et al., 2010;</ref><ref type="bibr" target="#b64">Yin, 2010;</ref><ref type="bibr" target="#b7">Benning and Burger, 2018)</ref>, which are usually expressed in a more complicated way. Furthermore, it coincides with the mirror descent algorithm <ref type="bibr" target="#b5">(Beck and Teboulle, 2003)</ref> applied to the functional J ? . Note that <ref type="bibr" target="#b64">Yin (2010)</ref> showed that for quadratic loss functions the elastic-net regularization parameter ? &gt; 0 has no influence on the asymptotics of linearized Bregman iterations, if chosen larger than a certain threshold. This effect is referred to as exact regularization <ref type="bibr" target="#b24">(Friedlander and Tseng, 2008)</ref>. The iteration scheme simply computes a gradient descent in the subgradient variable v and recovers the weights ? by evaluating the proximal operator of v. This makes it significantly cheaper than the original Bregman iterations (2.1) which require the minimization of the loss in every iteration.</p><p>Note that the last line in (2.8) is equivalent to v (k+1) satisfying the optimality condition v (k+1) ? ?J ? (? (k+1) ).</p><p>(2.9)</p><p>In particular, by letting ? (k) ? 0 the iteration (2.8) can be viewed as explicit Euler discretization of the inverse scale space flow (2.2) of the elastic-net regularized functional J ? :</p><formula xml:id="formula_24">v t = ??L(? t ), v t ? ?J ? (? t ).</formula><p>(2.10)</p><p>By embedding (2.8) into a stochastic batch gradient descent framework we obtain LinBreg from Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Linearized Bregman Iterations with Momentum</head><p>More generally we consider an inertial version of (2.10), which as in Section 2.1 is given by</p><formula xml:id="formula_25">?v t +v t = ??L(? t ), v t ? ?J ? (? t ).</formula><p>(2.11)</p><p>We discretize this equation in time by approximating the time derivatives as</p><formula xml:id="formula_26">v t ? v (k+1) ? 2v (k) + v (k?1) (? (k) ) 2 , v t ? v (k+1) ? v (k) ? (k) ,</formula><p>such that after some reformulation we obtain the iteration</p><formula xml:id="formula_27">v (k+1) = ? (k) + 2? ? (k) + ? v (k) ? ? ? (k) + ? v (k?1) ? (? (k) ) 2 ? (k) + ? ?L(? (k) ), (2.12a) ? (k+1) = prox ?J (?v (k+1) ). (2.12b)</formula><p>To see the relation to the gradient memory equation (2.5), derived in Section 2.1, we rewrite (2.12), using the new variables</p><formula xml:id="formula_28">m (k+1) := v (k) ? v (k+1) , ? (k) := ? ? (k) + ? ? [0, 1). (2.13)</formula><p>Plugging this into (2.12) yields the iteration</p><formula xml:id="formula_29">m (k+1) = ? (k) m (k) + (1 ? ? (k) )? (k) ?L(? (k) ), (2.14a) v (k+1) = v (k) ? m (k+1) , (2.14b) ? (k+1) = prox ?J (?v (k+1) ). (2.14c)</formula><p>Similar to before, embedding this into a stochastic batch gradient descent framework we obtain LinBreg with momentum from Algorithm 2. Note that, contrary to stochastic gradient descent with momentum <ref type="bibr" target="#b48">(Orvieto et al., 2020)</ref>, the momentum acts on the subgradients v and not on the parameters ?. Analogously, we propose AdaBreg in Algorithm 3, which is a generalization of the Adam algorithm <ref type="bibr" target="#b34">(Kingma and Ba, 2014)</ref>. Here, we also apply the bias correction steps on the subgradient v and reconstruct the parameters ? using the proximal operator of the regularizer J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis of Stochastic Linearized Bregman Iterations</head><p>In this section we provide a convergence analysis of stochastic linearized Bregman iterations. They are valid in a general sense and do not rely on L being an empirical loss or ? being weights of a neural network. Still we keep the notation fixed for clarity. All proofs can be found in the appendix.</p><p>We let (?, F, P) be a probability space, ? be a Hilbert space, L : ? ? R a Fr?chet differentiable loss function, and g : ? ? ? ? ? an unbiased estimator of ?L, meaning E [g(?; ?)] = ?L(?) for all ? ? ?. This and all other expected values are taken with respect to the random variable ? ? ? which, in our case, models Algorithm 2: LinBreg with Momentum, an acceleration of LinBreg using momentum-based gradient memory.</p><formula xml:id="formula_30">default: ? = 1, ? = 0.9 ? ? Section 4.1, v ? ?J(?) + 1 ? ?, m ? 0 // initialize for epoch e = 1 to E do for minibatch B ? T do g ? ?L(?; B) // Backpropagation m ? ? m + (1 ? ?)? g // Momentum update v ? v ? m // Momentum step ? ? prox ?J (?v) // Regularization</formula><p>Algorithm 3: AdaBreg, a Bregman version of the Adam algorithm which uses moment-based bias correction.</p><formula xml:id="formula_31">default: ? = 1, ? 1 = 0.9, ? 2 = 0.999, = 10 ?8 ? ? Section 4.1, v ? ?J(?) + 1 ? ?, m 1 ? 0, m 2 ? 0 // initialize for epoch e = 1 to E do for minibatch B ? T do k ? k + 1 g ? ?L(?; B) // Backpropagation m 1 ? ? 1 m 1 + (1 ? ? 1 ) g // First moment estimat? m 1 ? m 1 /(1 ? ? k 1 ) // Bias correction m 2 ? ? 2 m 2 + (1 ? ? 2 ) g 2 // Second raw moment estimat? m 2 ? m 2 /(1 ? ? k 2 ) // Bias correction v ? v ? ?m 1 /( ?m 2 + ) // Moment step ? ? prox ?J (?v)</formula><p>// Regularization the randomly drawn batch of training in data in (1.10). We study the stochastic linearized Bregman iterations draw ? (k) from ? using the law of P, (3.1a)</p><formula xml:id="formula_32">g (k) := g(? (k) ; ? (k) ), (3.1b) v (k+1) := v (k) ? ? (k) g (k) , (3.1c) ? (k+1) := prox ?J (?v (k+1) ). (3.1d)</formula><p>For our analysis we need some assumptions on the loss function L which are very common in the analysis of nonlinear optimization methods. Besides boundedness from below, we demand differentiability and Lipschitz-continuous gradients, which are standard assumptions in nonlinear optimization since they allow to prove sufficient decrease of the loss. We refer to <ref type="bibr" target="#b8">Benning et al. (2021)</ref> for an example of a neural network the associated loss of which satisfies the following assumption.</p><p>Assumption 1 (Loss function) We assume the following conditions on the loss function:</p><p>? The loss function L is bounded from below and without loss of generality we assume L ? 0.</p><p>? The function L is continuously differentiable.</p><p>? The gradient of the loss function ? ? ?L(?) is L-Lipschitz for L ? (0, ?):</p><formula xml:id="formula_33">?L(?) ? ?L(?) ? L ? ? ? , ??,? ? ?. (3.2)</formula><p>Remark 1 Note that the Lipschitz continuity of the gradient in particular implies the classical estimate <ref type="bibr" target="#b4">(Beck, 2017;</ref><ref type="bibr" target="#b3">Bauschke and Combettes, 2011</ref>)</p><formula xml:id="formula_34">L(?) ? L(?) + ?L(?),? ? ? + L 2 ? ? ? 2 , ??,? ? ?. (3.3)</formula><p>Furthermore, we need the following assumption, being of stochastic nature, which requires the gradient estimator to have uniformly bounded variance.</p><p>Assumption 2 (Bounded variance) There exists a constant ? &gt; 0 such that for any ? ? ? it holds</p><formula xml:id="formula_35">E g(?; ?) ? ?L(?) 2 ? ? 2 .</formula><p>(3.4) This is a standard assumption in the analysis of stochastic optimization methods and many authors actually demand the more restrictive condition of uniformly bounded stochastic gradients E g(?; ?) 2 ? C for all ? ? ?. Remarkably, both assumptions have been shown to be unnecessary for proving convergence of stochastic gradient descent of convex  and non-convex functions . Generalizing this to linearized Bregman iterations is however completely non-trivial which is why we stick to Assumption 2. We also state our assumptions on the regularizer J, which are extremely mild.</p><p>Assumption 3 (Regularizer) We assume that J : ? ? (??, ?] is a convex, proper, and lower semicontinuous functional on the Hilbert space ?.</p><p>All other assumptions will be stated when they are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decay of the Loss Function</head><p>We first analyze how the iteration (3.1) decreases the loss L. Such decrease properties of deterministic linearized Bregman iterations in a different formulation were already studied by <ref type="bibr" target="#b8">Benning et al. (2021)</ref>; <ref type="bibr" target="#b7">Benning and Burger (2018)</ref>. Note that for the loss decay we do not require any sort of convexity of L whatsoever, but merely Lipschitz continuity of the gradient, i.e., (3.3).</p><p>Theorem 2 (Loss decay) Assume that Assumptions 1-3 hold true, let ? &gt; 0, and let the step sizes satisfy ? (k) ? 2 ?L . Then there exist constants c, C &gt; 0 such that for every k ? N the iterates of (3.1) satisfy</p><formula xml:id="formula_36">E L(? (k+1) ) + 1 ? (k) E D sym J (? (k+1) , ? (k) ) + C 2?? (k) E ? (k+1) ? ? (k) 2 ? E L(? (k) ) + ? (k) ? ? 2 2c ,<label>(3.</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>Corollary 3 (Summability) Under the conditions of Theorem 2 and with the additional assumption that the step sizes are non-increasing and square-summable, meaning</p><formula xml:id="formula_37">? (k+1) ? ? (k) , ?k ? N, ? k=0 (? (k) ) 2 &lt; ?, it holds ? k=0 E D sym J (? (k+1) , ? (k) ) &lt; ?, ? k=0 E ? (k+1) ? ? (k) 2 &lt; ?.</formula><p>Remark 4 (Deterministic case) Note that in the deterministic setting with ? = 0 the statement of Theorem 2 coincides with <ref type="bibr" target="#b8">Benning et al. (2021)</ref>; <ref type="bibr" target="#b7">Benning and Burger (2018)</ref>. In particular, the loss decays monotonously and one gets stronger summability than in Corollary 3 since one does not have to multiply with ? (k) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convergence of the Iterates</head><p>In this section we establish two convergence results for the iterates of the stochastic linearized Bregman iterations (3.1). According to common practice and for self-containedness we restrict ourselves to strongly convex losses. Obviously, our results remain true for non-convex losses if one assumes convexity around local minima and applies our arguments locally. We note that one could also extend the deterministic convergence proof of <ref type="bibr" target="#b8">Benning et al. (2021)</ref>-which is based on the Kurdyka-Lojasiewicz (KL) inequality and works for non-convex losses-to the stochastic setting. However, this is beyond the scope of this paper and conveys less intuition than our proofs. For our first convergence result-asserting norm convergence of a subsequencewe need the condition on the loss function Assumption 1, the bounded variance condition from Assumption 2 and strong convexity of the loss L:</p><p>Assumption 4 (Strong convexity) The loss function ? ? L(?) is ?-strongly convex for ? ? (0, ?), meaning</p><formula xml:id="formula_38">L(?) ? L(?) + ?L(?),? ? ? + ? 2 ? ? ? 2 , ??,? ? ?. (3.6)</formula><p>Note that by virtue of (3.3) it holds ? ? L if the loss satisfies Assumptions 1 and 4. Our second convergence convergence result-asserting convergence in the Bregman distance of J ? which is a stronger topology than norm convergence-requires a stricter convexity condition, tailored to the Bregman geometry.</p><p>Assumption 5 (Strong Bregman convexity) The loss function ? ? L(?) satisfies</p><formula xml:id="formula_39">L(?) ? L(?) + ?L(?),? ? ? + ?D v J ? (?, ?), ??,? ? ?, v ? ?J ? (?), (3.7)</formula><p>where J ? for ? &gt; 0 is defined in (2.6). In particular, L satisfies Assumption 4 with ? = ?/?.</p><p>Remark 5 (The Bregman convexity assumption) Assumption 5 seems to be quite restrictive, however, in finite dimensions it is locally equivalent to Assumption 4, as we argue in the following. Note that it suffices if the assumptions above are satisfied in a vicinity of the (local) minimum to which the algorithm converges. For proving convergence we will use Assumption 5 with? = ? * , a (local) minimum, and ? close to ? * . Using Lemma 13 in the appendix the assumption can be rewritten as</p><formula xml:id="formula_40">L(? * ) ? L(?) + ?L(?), ? * ? ? + ? 2? ? * ? ? 2 + ?D p J (? * , ?), p ? ?J(?)</formula><p>and we will argue that for ? close to ? * the extra term vanishes, i.e. D p J (? * , ?) = 0. For this we have to show that p is not only a subgradient at ? but also at ? * : If p is a subgradient of J at both points, i.e., p ? ?J(?) ? ?J(? * ) we obtain that their Bregman distance is zero. This can be seen using the definition of the Bregman distance (1.17):</p><formula xml:id="formula_41">D p J (? * , ?) ? 0 D p J (?, ? * ) ? 0 =? 0 ? D p J (? * , ?) + D p J (?, ? * ) = 0.</formula><p>In finite dimensions and for J(?) = ? 1 = N i=1 |? i | equal to the 1 -norm we can use that p, ? = J(?) = N i=1 sign(? i )? i = N i=1 |? i | = J(?) and simplify the Bregman distance to</p><formula xml:id="formula_42">D p J (? * , ?) = J(? * ) ? p, ? * = N i=1 |? * i | ? sign(? i )? * i = N i=1 ? * i (sign(? * i ) ? sign(? i )) .</formula><p>Obviously, the terms in this sum where ? * i = 0 vanish anyways. Hence, the expression is zero whenever the non-zero entries of ? have the same sign as those of ? * which is the case if ??? * ? &lt; min {|? * i | : i ? {1, . . . , N }, ? * i = 0}. Since all norms are equivalent in finite dimensions, one obtains D p J (? * , ?) = 0 for ? ? ? * sufficiently small.</p><p>Hence, Assumption 5 is locally implied by Assumption 4 if J(?) = ? 1 .</p><p>We would like to remark that-even under the weaker Assumption 4-one needs some coupling of the loss L and the regularization functional J in order to obtain convergence to a critical point. <ref type="bibr" target="#b8">Benning et al. (2021)</ref> demand that a surrogate function involving both L and J admits the KL inequality and that the subgradients of J are bounded close to the minimum ? * of the loss. Indeed, in our theory using Assumption 4 it suffices to demand that J(? * ) &lt; ?. This assumption is weaker than assuming bounded subgradients but is nevertheless necessary as the following example taken from <ref type="bibr" target="#b8">Benning et al. (2021)</ref> shows.</p><p>Example 1 (Non-convergence to a critical point) Let L(?) = (? + 1) 2 for ? ? R and J(?) = ? [0,?) (?) be the characteristic function of the positive axis. Then for any initialization the linearized Bregman iterations (2.8) converge to ? = 0 which is no critical point of L. On the other hand, the only critical point ? * = ?1 clearly meets J(? * ) = ?.</p><p>Theorem 6 (Convergence in norm) Assume that Assumptions 1-4 hold true and let ? &gt; 0. Furthermore, assume that the step sizes ? <ref type="bibr">(k)</ref> are such that for all k ? N:</p><formula xml:id="formula_43">? (k) ? ? 2?L 2 , ? (k+1) ? ? (k) , ? k=0 (? (k) ) 2 &lt; ?, ? k=0 ? (k) = ?.</formula><p>The function L has a unique minimizer ? * and if J(? * ) &lt; ? the stochastic linearized Bregman iterations (3.1) satisfy the following:</p><formula xml:id="formula_44">? Letting d k := E D v (k) J ? (? * , ? (k) ) it holds d k+1 ? d k + ? 4 ? (k) E ? * ? ? (k+1) 2 ? ? 2 (? (k) ) 2 + E ? (k) ? ? (k+1) 2 .</formula><p>(3.8)</p><p>? The iterates possess a subsequence converging in the L 2 -sense of random variables:</p><formula xml:id="formula_45">lim j?? E ? * ? ? (k j ) 2 = 0. (3.9)</formula><p>Here, J ? is defined as in (2.6).</p><p>Remark 7 (Choice of step sizes) A possible step size which satisfies the conditions of Theorem 6 is given by</p><formula xml:id="formula_46">? (k) = c (k+1) p where 0 &lt; c &lt; ? ?L 2 and p ? ( 1 2 , 1].</formula><p>Remark 8 (Deterministic case) In the deterministic case ? = 0 inequality (3.8) even shows that the Bregman distances decrease along iterations. Furthermore, in this case it is not necessary that the step sizes are square-summable and nonincreasing since the term on the right hand side does not have to be summed.</p><p>In a finite dimensional setting and using 1 -regularization one can even show convergence of the whole sequence of Bregman distances.</p><p>Remark 9 With the help of Lemma 13 in the appendix, the quantity D v (k) J ? (? * , ? (k) ) which appears in the decay estimate (3.8) can be simplified as follows:</p><formula xml:id="formula_47">D v (k) J ? (? * , ? (k) ) = 1 2? ? * ? ? (k) 2 + D p (k) J (? * , ?) = 1 2? ? * ? ? (k) 2 + J(? * ) ? J(? (k) ) ? p (k) , ? * ? ? (k) , where p (k) := v (k) ? 1 ? ? (k) ? ?J(? (k)</formula><p>). In the case that J is absolutely 1-homogeneous, e.g., if J(?) = ? 1 equals the 1 -norm, this simplifies to</p><formula xml:id="formula_48">D v (k) J ? (? * , ? (k) ) = 1 2? ? * ? ? (k) 2 + J(? * ) ? p (k) , ? * ,</formula><p>where we used that for absolutely 1-homogeneous functionals it holds p, ? = J(?) for all ? ? ? and p ? ?J(?). Hence, it measures both the convergence of ? (k) to ? * in the norm and the convergence of the subgradients p (k) ? ?J(? (k) ) to a subgradient of J at ? * .</p><p>Corollary 10 (Convergence in finite dimensions) If the parameter space ? is finite dimensional and J equals the 1 -norm, under the conditions of Theorem 6 it even holds lim k?? d k = 0 which in particular implies lim k?? E ? * ? ? (k) 2 = 0.</p><p>Our second convergence theorem asserts convergence in the Bregman distance and gives quantitative estimates under Assumption 5, which is a stricter assumption than Assumption 4 and relates the loss function L with the regularizer; cf. <ref type="bibr" target="#b21">Dragomir et al. (2021)</ref> for a related approach working with C 2 functions. The theorem states that the Bregman distance to the minimizer of the loss can be made arbitrarily small using constant step sizes. For step sizes which go to zero and are not summable one obtains a quantitative convergence result.</p><p>Theorem 11 (Convergence in the Bregman distance) Assume that Assumptions 1-3 and 5 hold true and let ? &gt; 0. The function L has a unique minimizer ? * and if J(? * ) &lt; ? the stochastic linearized Bregman iterations (3.1) satisfy the following:</p><formula xml:id="formula_49">? Letting d k := E D v (k) J ? (? * , ? (k) ) it holds d k+1 ? 1 ? ? (k) ? 1 ? ? (k) 2? 2 L 2 ? d k + ?(? (k) ) 2 ? 2 .</formula><p>(3.10)</p><p>? For any ? &gt; 0 there exists ? &gt; 0 such that if ? (k) = ? for all k ? N then</p><formula xml:id="formula_50">lim sup k?? d k ? ?. (3.11) ? If ? (k) is such that lim k?? ? (k) = 0 and ? k=0 ? (k) = ? (3.12) then it holds lim k?? d k = 0. (3.13)</formula><p>Here, J ? is defined as in (2.6).</p><p>Corollary 12 (Convergence rate for diminishing step sizes) The error recursion (3.10) coincides with the one for stochastic gradient descent. In particular, for step sizes of the form ? (k) = c k with a suitably small constant c &gt; 0 one can prove with induction <ref type="bibr" target="#b44">(Nemirovski et al., 2009</ref>) that d k = C k for some C &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Numerical Experiments</head><p>In this section we perform an extensive evaluation of our algorithms focusing on different characteristics. First, we derive a sparse initialization strategy in Section 4.1, using similar statistical arguments as in the seminal works by <ref type="bibr" target="#b26">Glorot and Bengio (2010)</ref>; <ref type="bibr" target="#b29">He et al. (2015)</ref>. In Sections 4.2 and 4.3 we study the influence of hyperparameters and compare our family of algorithms with standard stochastic gradient descent (SGD) and the sparsity promoting proximal gradient descent method. In Sections 4.4 and 4.5 we demonstrate that our algorithms generate sparse and expressive networks for solving the classification task on Fashion-MNIST and CIFAR-10, for which we utilize state-of-the-art CNN and ResNet architectures. Finally, in Section 4.6 we show that, using row sparsity, our Bregman learning algorithm allows to discover a denoising autoencoder architecture, which shows the potential of the method for architecture design. Our code is available on GitHub at https://github.com/TimRoith/BregmanLearning and relies on PyTorch <ref type="bibr" target="#b51">Paszke et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initialization</head><p>Parameter initialization for neural networks has a crucial influence on the training process, see <ref type="bibr" target="#b26">Glorot and Bengio (2010)</ref>. In order to tackle the problem of vanishing and exploding gradients, standard methods consider the variance of the weights at initialization <ref type="bibr" target="#b26">(Glorot and Bengio, 2010;</ref><ref type="bibr" target="#b30">He et al., 2016)</ref>, assuming that for each l the entries W l i,j are i.i.d. with respect to a probability distribution. The intuition here is to preserve the variances over the forward and the backward pass similar to the variance of the respective input of the network, see <ref type="bibr">Glorot and Bengio (2010, Sec. 4.2)</ref>. If the distribution satisfies E W l = 0, this yields a condition of the form Var W l = ?(n l , n l?1 ) (4.1)</p><p>where the function ? depends on the activation function. For anti-symmetric activation functions with ? (0) = 1, as for instance a sigmoidal function, it was shown by <ref type="bibr" target="#b26">Glorot and Bengio (2010)</ref> that ?(n l , n l?1 ) = 2 n l ? n l?1 while for ReLU <ref type="bibr" target="#b30">He et al. (2016)</ref> suggest to use ?(n l , n l?1 ) = 2 n l or ?(n l , n l?1 ) = 2 n l?1 .</p><p>For our Bregman learning framework we have to adapt this argumentation, taking into account sparsity. For classical inverse scale space approaches and Bregman iterations of convex losses, as for instance used for image reconstruction <ref type="bibr" target="#b49">(Osher et al., 2005)</ref> and compressed sensing <ref type="bibr" target="#b65">(Yin et al., 2008;</ref><ref type="bibr" target="#b11">Burger et al., 2013;</ref><ref type="bibr" target="#b50">Osher et al., 2010;</ref><ref type="bibr" target="#b13">Cai et al., 2009b)</ref>, one would initialize all parameters equal to zero. However, for neural networks this yields an unbreakable symmetry of the network parameters, which makes training impossible, see, e.g., <ref type="bibr">Goodfellow et al. (2016, Ch. 6)</ref>. Instead, we employ an established approach for sparse neural networks (see, e.g., <ref type="bibr" target="#b39">Liu et al. (2021)</ref>; <ref type="bibr" target="#b18">Dettmers and Zettlemoyer (2019)</ref>; Martens <ref type="formula">(2010)</ref>) which masks the initial parameters, i.e.,</p><formula xml:id="formula_51">W l :=W l M l .</formula><p>Here, the mask M l ? R n l ,n l?1 is chosen randomly such that each entry is distributed according to the Bernoulli distribution with a parameter r ? [0, 1], i.e.,</p><formula xml:id="formula_52">M l i,j ? B(r).</formula><p>The parameter r coincides with the expected percentage of non-zero weights</p><formula xml:id="formula_53">N(W l ) := W l 0 n l ? n l?1 = 1 ? S(W l ), (4.2)</formula><p>where S(W l ) denotes the sparsity. In the following we derive a strategy to initializ? W l . ChoosingW l and M l independent and using E W l = 0 standard variance calculus implies</p><formula xml:id="formula_54">Var W l = Var W l M l = E M l 2 Var W l + E W l 2 Var M l =0 +Var M l Var W l = E M l 2 + Var M l Var W l = E (M l ) 2 Var W l = r Var W l</formula><p>and thus deriving from (4.1) we obtain the condition</p><formula xml:id="formula_55">Var W l = 1 r ?(n l , n l?1 ). (4.3)</formula><p>Instead of having linear feedforward layers with corresponding weight matrices, the neural network architecture at hand might consist of other groups of parameters which one would like to keep sparse, e.g., using the group sparsity regularization (1.2). For example, in a convolutional neural network one might be interested in having only a few number of non-zero convolution kernels in order to obtain compact feature representations of the input. Similarly, for standard feedforward architectures sparsity of rows of the weight matrices yields compact networks with a small number of active neurons. In such cases one can apply the same arguments as above and initialize single elements g ? G of a parameter group G as non-zero with probability r ? [0, 1] and with respect to a variance condition similar to (4.3):</p><p>g =g ? m, m ? B(r), (4.4)</p><formula xml:id="formula_56">Var [g] = 1 r ?(g) (4.5)</formula><p>Note that these arguments are only valid in a linear regime around the initialization, see <ref type="bibr" target="#b26">Glorot and Bengio (2010)</ref> for details. Hence, if one initializes with sparse parameters but optimizes with very weak regularization, e.g., by using vanilla SGD or choosing ? in (1.1) or (1.2) very small, the assumption is violated since the sparse initial parameters are rapidly filled during the first training steps. The biases are initialized non-sparse and the precise strategy depends on the activation function. We would like to emphasize that initializing biases with zero is not a good idea in the context of sparse neural networks. In this case, the neuron activations would be equal for all "dead neurons" whose incoming weights are zero, which would then yield an unbreakable symmetry. For ReLU we initialize biases with positive random values to ensure a flow of information and to break symmetry also for dead neurons, which is similar to the strategy proposed by <ref type="bibr">Goodfellow et al. (2016, Ch. 6)</ref>. For other activation functions ? which meet ?(x) = 0 if and only if x = 0, e.g., TanH, Sigmoid, or Leaky ReLU, biases can be initialized with random numbers of arbitrary sign. We start by comparing the proposed LinBreg Algorithm 1 with vanilla stochastic gradient descent (SGD) without sparsity regularization and with the Lasso-based approach from <ref type="bibr" target="#b57">Scardapane et al. (2017)</ref>, for which we compute solutions to the sparsity-regularized risk minimization problem (1.4) using the proximal gradient descent algorithm (ProxGD) from (1.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison of Algorithms</head><p>We consider the classification task on the MNIST dataset (LeCun and Cortes, 2010) for studying the impact of the hyperparameters of these methods. The set consists of 60, 000 images of handwritten digits which we split into 55, 000 images used for the training and 5, 000 images used for a validation process during training.</p><p>We train a fully connected net with ReLU activations and two hidden layers (200 and 80 neurons), and use the 1 -regularization from (1.13),</p><formula xml:id="formula_57">J(?) = ? L l=1 W l 1,1</formula><p>In <ref type="figure" target="#fig_2">Figure 2</ref> we compare the training results of vanilla SGD, the proposed LinBreg, and the ProxGD algorithm. Following the strategy introduced in Section 4.1 we initialize the weights with 1% non-zero entries, i.e., r = 0.01. The learning rate is chosen as ? = 0.1 and is multiplied by a factor of 0.5 whenever the validation accuracy stagnates. For a fair comparison the training is executed for three different fixed random seeds, and the plot visualizes mean and standard deviation of the three runs, respectively. We show the training and validation accuracies, the 1 -norm, and the overall percentage of non-zero weights. Note that for the validation accuracies we do not show standard deviations for the sake of clarity.</p><p>While SGD without sparsity regularization instantaneously destroys sparsity, LinBreg exhibits the expected inverse scale space behaviour, where the number of non-zero weights gradually grows during training, and the train accuracy increases monotonously. This is suggested by Theorem 2, even though our experimental setup, in particular the non-smooth ReLU activation functions, is not covered by the theoretical framework which require at least L-smoothness of the loss functions.</p><p>In contrast, ProxGD shows no monotonicity of training accuracy or sparsity and the validation accuracies oscillate heavily. Instead, it adds a lot of non-zero weights in the beginning and then gradually reduces them. Obviously, the regularized empirical risk minimization (1.4) implies a trade-off between training accuracy and sparsity which depends on ?. For LinBreg this trade-off is neither predicted by theory nor observed numerically. Here, the regularization parameter only induces a trade-off between validation accuracy and sparsity, which is to be expected.</p><p>LinBreg (blue curves) can generate networks whose validation accuracy equals the one of a full network and use only 80% of the weights. For the largest regularization parameter (magenta curves) LinBreg uses only 10% of the weights and still does not drop more than half a percentage point in validation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Accelerated Bregman Algorithms</head><p>We use the same setup as in the previous section to compare LinBreg with its momentum-based acceleration and AdaBreg (see Algorithms 1-3). Using the regularization parameter ? = 10 ?1 from the previous section (see the magenta curves), <ref type="figure" target="#fig_3">Figure 3</ref> shows the validation accuracy of the different networks trained with LinBreg, LinBreg with momentum, and AdaBreg. For comparison we visualize again the results of vanilla SGD as gray curve. It is obvious that all three proposed algorithms generate very accurate networks using approximately 10% of the weights. As expected, the accelerated versions increase both the validation accuracy and the number of non-zero parameters faster than the baseline algorithm LinBreg. While after 100 epochs LinBreg and its momentum version have slightly lower validation accuracies than the non-sparse networks generated by SGD, AdaBreg outperforms the other algorithms including SGD in terms of validation accuracy while maintaining a high degree of sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sparsity for Convolutional Neural Networks (CNNs)</head><p>In this example we apply our algorithms to a convolutional neural network of the form 5 ? 5 conv, 64</p><p>Maxpool/2 ?? 5 ? 5 conv, 64</p><p>Maxpool/2 ?? fc 1024 ?? fc 128 ?? fc 10 with ReLU activations to solve the classification task on Fashion-MNIST. We run experiments both for sparsity regularization utilizing the 1 -norm (1.1) and for a combination of the 1 -norm on the linear layers and the group 1,2 -norm (1.2) on the convolutional kernels. This way, we aim to obtain compressed network architectures with only few active convolutional filters. The inverse scale space character of our algorithms is visualized in <ref type="figure" target="#fig_0">Figure 1</ref> from the beginning of the paper which shows the 64 feature maps of an input image, generated by the first convolutional layer of the network after 0, 5, 20, and 100 epochs of LinBreg with group sparsity. One can observe that gradually more kernels are added until iteration 20, from where on the number of kernels stays fixed and the kernels themselves are optimized. <ref type="table" target="#tab_2">Tables 1 and 2</ref> shows the test and training accuracies as well as sparsity levels. For plain sparsity regularization we only show the total sparsity level of all network parameters whereas for the group sparsity regularization we show the sparsity of the linear layers and the relative number of non-zero convolutional kernels.</p><p>A convolutional layer for a input z ? R c l?1 ,n l?1 ,m l?1 is given as</p><formula xml:id="formula_58">? l j (z) = b j + c l?1 i=1 K l i,j * z i,? ,</formula><p>where K l i,j ? R k,k denote kernel matrices with corresponding biases b j ? R n l ,m l for in-channels i ? {1, . . . , c l?1 } and out-channels j ? {1, . . . , c l }. Therefore, we denote by</p><formula xml:id="formula_59">N conv := l?Iconv #{K l i,j : K l i,j = 0} l?Iconv c l ? c l?1</formula><p>the percentage of non-zero kernels of the whole net where I conv denotes the index set of the convolutional layers. Analogously, using a similar term as in (4.2) we denote by N linear := l?I linear W l 0 l?I linear n l ? n l?1 the percentage of weights used in the linear layers. Finally, we define N total := N conv + N linear . We compare our algorithms LinBreg (with momentum) and AdaBreg against vanilla training without sparsity, iterative pruning <ref type="bibr" target="#b28">(Han et al., 2015)</ref>, and the Group Lasso approach from <ref type="bibr" target="#b57">Scardapane et al. (2017)</ref>, and train all networks to a comparable sparsity level, given in brackets. The pruning scheme is taken from <ref type="bibr" target="#b28">Han et al. (2015)</ref>, where in each step a certain amount of weights is pruned, followed by a retraining step. For our experiment the amount of weights pruned in each iteration was chosen, so that a specified target sparsity is met. For the Group Lasso approach, which is based on the regularized risk minimization (1.4), we use two different optimizers. First, we apply SGD applied to the (1.4) and apply thresholding afterwards to obtain sparse weights, which is the standard approach in the community (cf. <ref type="bibr" target="#b57">Scardapane et al. (2017)</ref>). Second, we apply proximal gradient descent (1.5) to (1.4) which yields sparse solutions without need for thresholding. Our Bregman algorithms were initialized with 1% non-zero parameters, following the strategy from Section 4.1, all other algorithms were initialized non-sparse using standard techniques <ref type="bibr" target="#b26">(Glorot and Bengio, 2010;</ref><ref type="bibr" target="#b29">He et al., 2015)</ref>. For all algorithms we tuned the hyperparameters (e.g., pruning rate, regularization parameter for Group Lasso and Bregman) in order to achieve comparable sparsity levels.</p><p>Note that it is non-trivial to compare different algorithms for sparse training since they optimize different objectives, and both the sparsity, the train, and the test accuracy of the resulting networks matter. Therefore, we show results whose sparsity levels and accuracies are in similar ranges. <ref type="table">Table 1</ref> shows that all algorithms manage to compute very sparse networks with ca. 2% drop in test accuracy on Fasion-MNIST, compared to vanilla dense training with Adam. Note that we optimized the hyperparameters (regularization and thresholding parameters) of all algorithms for optimal performance on a validation set, subject to having comparable sparsity levels. Our algorithms LinBreg and AdaBreg yield sparser networks with the same accuracies as Pruning and Lasso.</p><p>Similar observations are true for <ref type="table" target="#tab_2">Table 2</ref> where we used group sparsity regularization on the convolutional kernels. Here all algorithms apart from pruning yield similar results, whereas pruning exhibits a significantly worse test accuracy despite using a larger number of non-zero parameters. The combination of SGD-optimized Group Lasso with subsequent thresholding yields the best test accuracy using a moderate sparsity level.</p><p>As mentioned above the Lasso and Group Lasso results using SGD underwent an additional thresholding step after training in order to generate sparse solutions. Obviously, one could also do this with the results of ProxGD and Bregman which would further improve their sparsity levels. However, in this experiment we refrain from doing so in order not to change the nature of the algorithms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Residual Neural Networks (ResNets)</head><p>In this experiment we trained a ResNet-18 architecture for classification on CIFAR-10, enforcing sparsity through the 1 -norm (1.1) and comparing different strategies, as before. <ref type="table">Table 3</ref> shows the resulting sparsity levels of the total number of parameters and the percentage of non-zero convolutional kernels as well as the train and test accuracies. Note that even though we used the standard 1 regularization (1.1) and no group sparsity, the trained networks exhibit large percentages of zero-kernels. For comparison we also show the unregularized vanilla results using SGD with momentum and Adam, which both use 100% of the parameters. The LinBreg result with thresholding shows that one can train a very sparse network using only 3.4% of all parameters with 3.4% drop in test accuracy. With AdaBreg we obtain a sparsity level of 14.7%, resulting in a drop of only 1.3%. The combination of Adam-optimized Lasso with subsequent thresholding yields a 3% sparsity with a drop of 3.6% in test accuracy, which is the sparsest result in this comparison.  <ref type="table">Table 3</ref>: Sparsity levels and accuracies on the CIFAR-10 data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Towards Architecture Design: Unveiling an Autoencoder</head><p>In this final experiment we investigate the potential of our Bregman training framework for architecture design, which refers to letting the network learn its own architecture. <ref type="bibr" target="#b31">Hoefler et al. (2021)</ref> identified this as one of the main potentials of sparse training. The inverse scale space character of our approach turns out to be promising for starting with very sparse networks and letting the network choose its own architecture by enforcing, e.g., row sparsity of weight matrices. The simplest yet widely used non-trivial architecture one might hope to train is an autoencoder, as used, e.g., for denoising images. To this end, we utilize the MNIST data set and train a fully connected feedforward network with five hidden layers, all having the same dimension as the input layer, to denoise MNIST images. Similar to the experiments in Section 4.2 we split the dataset in 55,000 images used for training and 5,000 images to evaluate the performance. We enforce row sparsity by using the regular-izer (1.14), which in this context is equivalent to having few active neurons in the network. <ref type="figure" target="#fig_5">Figure 4</ref> shows the number of active neurons in the network at different stages of the training process using LinBreg. Here darker colors indicate more iterations. We initialized around 1% of all rows non-zero, which corresponds to 8 neurons per layer being initially active. Our algorithm successively adds neurons and converges to an autoencoder-like structure, where the number of neurons decreases until the middle layer and then increases again. Note the network developed this structure "on its own" and that we did not artificially generate this result by using different regularization strengths for the different layers. In <ref type="figure" target="#fig_6">Figure 5</ref> we additionally show the denoising performance of the trained network on some images from the MNIST test set. The network was trained for 100 iterations, using a regularization value of ? = 0.07 in (1.14) and employing a standard MSE loss. We evaluate the test performance using the established structural similarity index measure (SSIM) <ref type="bibr" target="#b62">(Wang et al., 2004)</ref>, which assigns values close to 1 to pairs of images that are perceptually similar. Averaging this value over the whole test set we report a value of SSIM ? 0.93. For comparison, we also trained a network with 100 iterations of standard SGD which yields no sparsity and a value of SSIM ? 0.89.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we proposed an inverse scale space approach for training sparse neural networks based on linearized Bregman iterations. We introduced LinBreg as baseline algorithm for our learning framework and also discuss two variants using momentum and Adam. The effect of incorporating Bregman iterations into neural network training was investigated in numerical experiments on benchmark data sets. Our observations showed that the proposed method is able to train very sparse and accurate neural networks in an inverse scale space manner without using additional heuristics. Furthermore, we gave a glimpse of its applicability for discovering suitable network architectures for a given application task, e.g., an autoencoder architecture for image denoising. We mathematically supported our findings by performing a stochastic convergence analysis of the loss decay, and we proved convergence of the parameters in the case of convexity. The proposed Bregman learning framework has a lot of potential for training sparse neural networks, and there are still a few open research questions (see also <ref type="bibr" target="#b31">Hoefler et al. (2021)</ref>) which we would like to emphasize in the following.</p><p>First, we would like to use the inverse scale space character of the proposed Bregman learning algorithms in combination with sparse backpropagation for resourcefriendly training, hence improving the carbon footprint of training <ref type="bibr" target="#b1">(Anthony et al., 2020)</ref>. This is a non-trivial endeavour for the following reason: A-priori it is not clear which weights are worth updating since estimating the magnitude of the gradient with respect to these weights already requires evaluating the backpropagation. A possible way to achieve this consists in performing a Bregman step to obtain a sparse support of the weights, performing several masked backpropagation steps to optimize the weights in these positions, and alternate this procedure.</p><p>Second, our experiment from Section 4.6, where our algorithm discovered a denoising autoencoder, suggests that our method has great potential for general ar-chitecture design tasks. Using suitable sparsity regularization, e.g., on residual connections and rows of the weight matrices, one can investigate whether networks learn to form a U-net <ref type="bibr" target="#b55">(Ronneberger et al., 2015)</ref> structure for the solution of inverse problems.</p><p>On the analysis side, it is worth investigating the convergence of LinBreg in the fully non-convex setting based on the Kurdyka-Lojasiewicz inequality and to extend these results to our accelerated algorithms LinBreg with momentum and AdaBreg. Furthermore, it will be interesting to remove the bounded variance condition from Assumption 2, which is known to be possible for stochastic gradient descent if the batch losses satisfy (3.3), see . Finally, the characterizing the limit point of LinBreg for non-strongly convex losses as minimizer which minimizes the Bregman distance to the initialization will be worthwhile.</p><formula xml:id="formula_60">= g (k) , ? (k+1) ? ? (k) ) + ?L(? (k) ) ? g (k) , ? (k+1) ? ? (k) + L 2 ? (k+1) ? ? (k) 2 = ? 1 ? (k) v (k+1) ? v (k) , ? (k+1) ? ? (k) + ?L(? (k) ) ? g (k) , ? (k+1) ? ? (k) + L 2 ? (k+1) ? ? (k) 2 = ? 1 ? (k) D sym J (? (k+1) , ? (k) ) ? 1 ?? (k) ? (k+1) ? ? (k) 2 + ?L(? (k) ) ? g (k) , ? (k+1) ? ? (k) + L 2 ? (k+1) ? ? (k) 2 .</formula><p>Reordering and using the Cauchy-Schwarz inequality yields L(? (k+1) ) ? L(? (k) ) + 1 ? (k) D sym J (? (k+1) , ? (k) )+ 2 ? L?? (k) 2?? (k) ? (k+1) ? ? (k) 2 ? ?L(? (k) ) ? g (k) ? (k+1) ? ? (k) .</p><p>Taking expectations and using Young's inequality gives for any c &gt; 0 E L(? (k+1) ) ? E L(? (k) ) + 1 ? (k) E D sym J (? (k+1) , ? (k) )</p><formula xml:id="formula_61">+ 2 ? L?? (k) 2?? (k) E ? (k+1) ? ? (k) 2 ? ? (k) ? ? 2 2c + c 2?? (k) E ? (k+1) ? ? (k) 2 .</formula><p>If c is sufficiently small and ? (k) &lt; 2 L? , we can absorb the last term into the left hand side and obtain</p><formula xml:id="formula_62">E L(? (k+1) ) ? E L(? (k) ) + 1 ? (k) E D sym J (? (k+1) , ? (k) ) + C 2?? (k) E ? (k+1) ? ? (k) 2 ? ? (k) ? ? 2 2c ,</formula><p>where C &gt; 0 is a suitable constant. This shows (3.5).</p><p>Proof [Proof of Corollary 3] Using the assumptions on ? (k) , we can multiply (3.5) with ? (k) and sum up the resulting inequality to obtain Lemma 14 Denoting d k := E D v (k) J ? (? * , ? (k) ) the iteration (3.1) fulfills:</p><formula xml:id="formula_63">d k+1 ? d k = ?E D v (k) J ? (? (k+1) , ? (k) ) + ? (k) E g (k) , ? * ? ? (k+1) , (B.1) d k+1 ? d k = E D v (k+1) J ? (? (k) , ? (k+1) ) + ? (k) E ?L(? (k) ), ? * ? ? (k) . (B.2)</formula><p>Proof We compute using the update in (3.1)</p><formula xml:id="formula_64">D v (k+1) J ? (? * , ? (k+1) ) ? D v (k) J ? (? * , ? (k) ) = J ? (? (k) ) ? J ? (? (k+1) ) ? v (k+1) , ? * ? ? (k+1) + v (k) , ? * ? ? (k) = ? J ? (? (k+1) ) ? J ? (? (k) ) ? v (k) , ? (k+1) ? ? (k) ? v (k) , ? (k+1) ? ? (k) ? v (k+1) , ? * ? ? (k+1) + v (k) , ? * ? ? (k) = ?D v (k) J ? (? (k+1) , ? (k) ) + v (k) ? v (k+1) , ? * ? ? (k+1) = ?D v (k) J ? (? (k+1) , ? (k) ) + ? (k) g (k) , ? * ? ? (k+1) .</formula><p>For the second equation we similarly compute</p><formula xml:id="formula_65">D v (k+1) J ? (? * , ? (k+1) ) ? D v (k) J ? (? * , ? (k) ) = J ? (? (k) ) ? J ? (? (k+1) ) ? v (k+1) , ? * ? ? (k+1) + v (k) , ? * ? ? (k) = J ? (? (k) ) ? J ? (? (k+1) ) ? v (k+1) , ? * ? ? (k+1) + v (k+1) , ? * ? ? (k) + ? (k) g (k) , ? * ? ? (k) = J ? (? (k) ) ? J ? (? (k+1) ) ? v (k+1) , ? (k) ? ? (k+1) + ? (k) g (k) , ? * ? ? (k) = D v (k+1)</formula><p>J ? (? (k) , ? (k+1) ) + ? (k) g (k) , ? * ? ? (k) .</p><p>Taking expectations and using that g (k) and ? * ? ? (k) are stochastically independent to replace g (k) with ?L(? (k) ) inside the expectation concludes the proof. Note that this argument does not apply to the first equality since ? (k+1) is not stochastically independent of ? (k) . Now we prove Theorem 6 by showing that the Bregman distance to the minimizer of the loss and that the iterates converge in norm to the minimizer. Proof [Proof of Theorem 6] Using (3.2), Assumption 4, and Young's inequality we obtain for any c &gt; 0 ?L(? (k) ), ? * ? ? (k+1) = ?L(? (k+1) ), ? * ? ? (k+1) + ?L(? (k) ) ? ?L(? (k+1) ), ? * ? ? (k+1) ? L(? * ) ? L(? (k+1) ) ? ? 2 ? * ? ? (k+1) 2 + L 2 2c ? (k) ? ? (k+1) 2 + c 2 ? * ? ? (k+1) 2 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Inverse scale space character of LinBreg visualized through feature maps of a convolutional neural network. Descriptive kernels are gradually added in the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>; Yin et al. (2008); Bachmayr and Burger (2009); Cai et al. (2009b,a); Yin (2010); Burger et al. (2007, 2013)). More recent applications of Bregman type methods in the context of image restoration are Benfenati and Ruggiero (2013); Jia et al. (2016); Li</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of vanilla SGD (black solid line), LinBreg (colored solid lines), and ProxGD (colored dotted lines) for different regularization parameters on MNIST. The curves show the averaged accuracies on train and validation sets,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of LinBreg (with momentum), AdaBreg, and vanilla SGD. The networks generated by AdaBreg are sparse and generalize better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Architecture design for denoising: LinBreg automatically unveils an autoencoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The denoising performance of the trained autoencoder on the test set with an average SSIM value of ? 0.93.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Group sparsity levels and accuracies on the Fashion-MNIST data set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = ??L(? t ).(2.3)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">-norms, and non-zero entries over three runs. The shaded area visualizes the standard deviation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the European Union's Horizon 2020 research and innovation programme under the Marie Sk lodowska-Curie grant agreement No. 777826 (NoMADS) and by the German Ministry of Science and Technology (BMBF) under grant agreement No. 05M2020 (DELETO). Additionally we thank for the financial support by the Cluster of Excellence "Engineering of Advanced Materials" (EAM) and the "Competence Unit for Scientific Computing" (CSC) at the University of Erlangen-N?rnberg (FAU). LB acknowledges support by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -GZ 2047/1, Projekt-ID 390685813.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In all proofs we will use the abbreviation g (k) := g(? <ref type="bibr">(k)</ref> ; ? (k) ) as in (3.1).</p><p>Appendix A. Proofs from Section 3.1</p><p>Proof [Proof of Theorem 2] Using (3.3) one obtains</p><p>Hence, since E ? * ? ? (k j ) 2 converges to zero according to Theorem 6, the same is true for the sequence d k j . Furthermore, we have proved that d k+1 ? d k ? c k , where c k is a non-negative and summable sequence. This also implies d m ? d k + ? j=k c k for every m &gt; k.</p><p>Since c k is summable and d k j converges to zero there exists k ? N and l ? N such that</p><p>Hence, we obtain for any m &gt; k l</p><p>Since ? &gt; 0 was arbitrary, this implies that d m ? 0 as m ? ?.</p><p>Finally we prove Theorem 11 based on Assumption 5, which asserts convergence in the Bregman distance.</p><p>If ? k = ? for all k ? N is constant and we choose C = ? ?? 2 ? , we obtain</p><p>where we passed to the positive part x + := max(x, 0). Iterating this inequality yields</p><p>Demanding ? &lt; ?? ?? 2 ? 1 ? we finally obtain lim sup k?? d k ? ?.</p><p>Item 3: We use the reformulation (B.3) with C = ? &gt; 0. Since ? (k) converges to zero the second term is non-positive for k ? N sufficiently large and we obtain</p><p>Iterating this inequality yields</p><p>If k ? N is sufficiently large, then ? (l)? &lt; 1 for all l ? k and the product satisfies</p><p>where we used log(1?x) ? ?x for all x &lt; 1 and k ? (k) = ?. Since ? was arbitrary we obtain the assertion.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Artificial neural networks in medical diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pe?a-M?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Va?hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hampl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Havel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Carbontracker: Tracking and predicting the carbon footprint of training deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F W</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kanding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Selvan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03051</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Iterative total variation schemes for nonlinear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bachmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">105004</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convex analysis and monotone operator theory in hilbert spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bauschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Combettes</surname></persName>
		</author>
		<idno>978-3-319-48311-5</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">First-Order Methods in Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611974997</idno>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mirror descent and nonlinear projected subgradient methods for convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="167" to="175" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inexact bregman iteration with an application to poisson data reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benfenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ruggiero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">65016</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modern regularization methods for inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="111" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Choose your path wisely: gradient descent in a bregman distance framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Betcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-B</forename><surname>Sch?nlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="814" to="843" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonlinear inverse scale space methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="179" to="212" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Scherzer</surname></persName>
		</author>
		<title level="m">Inverse total variation flow. Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="366" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An adaptive inverse scale space method for compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">281</biblScope>
			<biblScope unit="page" from="269" to="299" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convergence of the linearized bregman iteration for 1 -norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">268</biblScope>
			<biblScope unit="page" from="2127" to="2136" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linearized bregman iterations for compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">267</biblScope>
			<biblScope unit="page" from="1515" to="1536" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An iterative pruning algorithm for feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="519" to="531" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nest: A neural network synthesis tool based on a grow-and-prune paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1487" to="1497" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Dios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10225</idno>
		<title level="m">On sparsity in overparametrised shallow relu networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<title level="m">Sparse networks from scratch: Faster training without losing performance</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The carbon impact of artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Mach Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="423" to="428" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Stochastic mirror descent: Convergence analysis and adaptive variants via the mirror stochastic polyak stepsize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Orazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast stochastic bregman gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-A</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Even</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hendrikx</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09813</idno>
	</analytic>
	<monogr>
		<title level="m">Sharp analysis and variance reduction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="2943" to="2952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exact regularization of convex programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Friedlander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1326" to="1350" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring structural sparsity of deep networks via inverse scale spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09449</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02626</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peste</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00554</idno>
		<title level="m">Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Split lbi: An iterative regularization path with structural sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3377" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An image restoration model combining mixed l 1 /l 2 fidelity terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="461" to="473" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent for nonconvex learning without bounded gradient assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4394" to="4400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive fractional-order total variation image restoration with split bregman iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISA transactions</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="210" to="222" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R R</forename><surname>Matavalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2589" to="2604" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<title level="m">Learning sparse neural networks through l 0 regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02540</idno>
		<title level="m">The expressive power of neural networks: A view from the width</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning via hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Problem complexity and method efficiency in optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovskij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SGD and hogwild! Convergence without the bounded gradients assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richtarik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takac</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stochastic proximal gradient descent with acceleration techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nitanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1574" to="1582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The role of memory in stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An iterative regularization method for total variation-based image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="489" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast linearized bregman iteration for compressive sensing and sparse denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="111" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for image classification: A comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2352" to="2449" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1153" to="1161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex analysis</title>
		<meeting><address><addrLine>Princeton, N.J,</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">9781400873173</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>V?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.5074</idno>
		<title level="m">Convergence of stochastic proximal gradient algorithm</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Group sparse regularization for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page" from="81" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Training sparse neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">The convergence of the stochastic gradient descent (sgd): a selfcontained proof</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turinici</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14350</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Member</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Proxsgd: Training structured neural networks under regularization and constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatzimichailidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Van Sloun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatzinotas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analysis and generalizations of the linearized bregman method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="856" to="877" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bregman iterative algorithms for 1 -minimization with applications to compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="168" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A general family of stochastic proximal gradient methods for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07484</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A unified primal-dual algorithm framework based on bregman iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="46" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Journal of the royal statistical society: series B (statistical methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
	<note>Regularization and variable selection via the elastic net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
