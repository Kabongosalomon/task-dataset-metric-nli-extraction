<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runfa</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence (THUAI)</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent AI lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangmin</forename><surname>Guo</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence (THUAI)</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence (THUAI)</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent AI lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Institute for AI Industry Research (AIR)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) self-training and adversarial training on target domain (c) change of target features (predictions from source discriminator) (b) change of target pseudo label (predictions from source classifier) source feature extractor discriminator ? !"# # source classifier ? $% source classifier target pseudo label Target Imaget arget features self-training adversarial training ?10000 ? 10000 Figure 1. The conventional training paradigm and change of predictions over training iterations. We sampled images from target domain, Cityscapes [9]</p><p>, and plot the L1-distance between the predictions over images on iteration t and next iteration t + 1 as the lines. The standard deviation of the distances are indicated by the corresponding shadow areas around the lines. See ? 3.3 for details. In both (b) and (c) diagrams, higher value means more unsmooth learning dynamics, thus more high-frequency components, since the model always significantly change its prediction iteration by iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>After the great success of Vision Transformer variants (ViTs) in computer vision, it has also demonstrated great potentials in domain adaptive semantic segmentation. Unfortunately, straightforwardly applying local ViTs in domain adaptive semantic segmentation does not bring in expected improvement. We find that the pitfall of local ViTs is due to the severe high-frequency components gen-*</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. The conventional training paradigm and change of predictions over training iterations. We sampled images from target domain, Cityscapes <ref type="bibr" target="#b8">[9]</ref>, and plot the L1-distance between the predictions over images on iteration t and next iteration t + 1 as the lines. The standard deviation of the distances are indicated by the corresponding shadow areas around the lines. See ? 3.3 for details. In both (b) and (c) diagrams, higher value means more unsmooth learning dynamics, thus more high-frequency components, since the model always significantly change its prediction iteration by iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>After the great success of Vision Transformer variants (ViTs) in computer vision, it has also demonstrated great potentials in domain adaptive semantic segmentation. Unfortunately, straightforwardly applying local ViTs in domain adaptive semantic segmentation does not bring in expected improvement. We find that the pitfall of local ViTs is due to the severe high-frequency components gen-erated during both the pseudo-label construction and features alignment for target domains. These high-frequency components make the training of local ViTs very unsmooth and hurts their transferability. In this paper, we introduce a low-pass filtering mechanism, momentum network, to smooth the learning dynamics of target domain features and pseudo labels. Furthermore, we propose a dynamic of discrepancy measurement to align the distributions in the source and target domains via dynamic weights to evaluate the importance of the samples. After tackling above issues, extensive experiments on sim2real benchmarks show that the proposed method outperforms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55]</ref>, as one of the most central tasks in computer vision, aims to label the semantic content of an image pixel by pixel. It requires dense annotations of training images and becomes more challenging when it is difficult to obtain these dense labels in practice. A commonly recognised solution is the domain adaptive semantic segmentation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, which usually first trains models on abundantly labeled images simulated by virtual environment, then carries out the simulation-to-reality (sim2real) domain adaptation (DA) to adapt the model on real-world images.</p><p>Given the past success of Convolutional Neural Networks (CNNs) on many computer vision (CV) tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>, plenty of works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref> resort to CNNs as the semantic segmentation function f ? . Although these conventional CNN-based backbones obtained decent performance on various benchmarks, recent works (e.g. DRT <ref type="bibr" target="#b25">[26]</ref>) show that they are still unable to tackle domain conflicts, and suffering from performance bottleneck. To make further improvement, a natural alternative backbone is the Transformer model proposed by <ref type="bibr" target="#b45">[46]</ref> which has shown to be more powerful than CNNs in various areas: i) natural language processing (NLP) tasks, e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>; ii) CV tasks, e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53]</ref>. More importantly, existing works such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">50]</ref> have also shown the great potential of the Vision Transformer variants (ViTs) for semantic segmentation. Inspired by the above success of ViTs, we propose a domain adaptive semantic segmentation framework based on local ViTs without incurring substantial training efforts in this work. We employ local ViTs as backbone and adapt a simple semi-supervised approach, self-training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref> along with adversarial training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref> widely-used in DA <ref type="figure" target="#fig_12">(Figure 1(a)</ref>). Since there is no ground-truth for target domain, in self-training, target pseudo labels are crucial for the classifier to make dense prediction, and in adversarial training, target features are essential for the discriminator to align features distribution. Based on the above basic framework, our contribution is two-fold as follows.</p><p>As far as we know, this is the first work that leverages local ViTs as backbone and explores its potential on domain adaptive semantic segmentation. Unfortunately, through our experiments, we find that direct self-training along with adversarial training upon ViT segmentation network does not bring in expected improvement, as shown in <ref type="table">Table 2</ref>. Our first contribution is the discovery of the high-frequency component problem of local ViTs in pseudo-label generation and feature alignment for target domain. The statistics about the change of predictions over training iterations are illustrated in Figure1. By comparing the change of predictions from different backbones, we can see that the predictions on target domain of local ViTs (Swin-S in our specific case) vibrate more drastically than CNNs (ResNet-101 in our specific case), as indicated by the larger changes of predictions over iterations.</p><p>Our second contribution is a solution to tackle the above issues. We formalize the self-training and adversarial training as knowledge distillation <ref type="bibr" target="#b18">[19]</ref> and features alignment respectively, i.e. the network from source domain is the teacher network to provide pseudo labels and features for target domain. Following the findings from <ref type="bibr" target="#b36">[37]</ref> that lowpass filtering the predictions from teacher can improve the quality of supervision, we also introduce a low-pass filtering into features and pseudo labels for target domain. However, their original implementation maintains a look-up table which stores predictions for all training example, which is infeasible in semantic segmentation as it requires huge storage overhead for all pixels. Therefore, we propose an indirect low-pass filter, the momentum network, to smooth the learning dynamics for target domain. Specifically, the feature extractor and classifier for target domain is updated with momentum copy from source domain, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. Notably, jointly smoothing the pseudo labels and features is essential. After applying momentum network, the change of predictions over training iterations is indeed significantly suppressed in <ref type="figure">Figure 1</ref>.</p><p>To further smooth the learning dynamics and suppress noise, we propose a dynamic adversarial training strategy, dynamic of discrepancy measurement. In particular, the alignment is guided by a score function w(x) that adaptively balances the importance of each sample during the adaptation process. Through experiments, we find that this strategy can further improve the performance of ViTs-based backbone, whereas it doesn't work with CNN-based backbones. We argue that the high-frequency component problem is specific for ViTs but not CNNs, which is supported by both the smaller changes of predictions from CNNs illustrated in <ref type="figure">Figure 1</ref>(b)(c) and the different feature representations learnt by CNNs as illustrated in <ref type="figure" target="#fig_5">Figure 4</ref>.</p><p>In summary, we propose a novel framework based on local ViTs dubbed TransDA, via a simple yet effective training paradigm. Exploring this new baseline shows that it's an urge to develop new methods based on a new backbone. After solving the high-frequency component issues during adaptation, our experiments on two standard sim2real benchmarks, GTA5 <ref type="bibr" target="#b37">[38]</ref>?Cityscapes <ref type="bibr" target="#b8">[9]</ref> and SYNTHIA <ref type="bibr" target="#b38">[39]</ref>?Cityscapes, demonstrate that TransDA achieves attractive and robust performance in terms of best and averaged mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain adaptation. Domain adaptation deals with the scenario where a labeled source domain and an unlabeled target domain are provided, and certain shift in distribution exists in between <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. To bridge the gap, a line of work leverages the idea of adversarial learning <ref type="bibr" target="#b12">[13]</ref>, i.e. train the target feature extractor with a domain classifier under an adversarial schedule <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref>. UniDA <ref type="bibr" target="#b51">[52]</ref> proposes a weighting mechanism to identify private classes. In contrast to the universal DA , we adopt this similar method to dynamically adjust the pixel-by-pixel adversarial weight for closed set DA and semantic segmentation. DRT <ref type="bibr" target="#b25">[26]</ref> abandons conventional CNNs backbone and present dynamic transfer to address domain conflicts, where the model parameters are adapted to sample. It inspires us to explore a new backbone in domain adaptive semantic segmentation.</p><p>Domain adaptive semantic segmentation. Large process has been made in semantic segmentation with the paradigm of domain adaptation. Many works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref> utilize standard adversarial learning to enhance the performance. Among them, AdaptSeg <ref type="bibr" target="#b42">[43]</ref>, which is the first work to replace VGG with ResNet backbone in this task. Besieds, <ref type="bibr" target="#b40">[41]</ref> assigns sample-wise weight scores for cross-entropy loss to suppress the negative effect in the transfer process. In contrast, our dynamic discrepancy is designed for adversarial loss. Besides, self-training is another technique, which generates pseudo labels for making use of the unlabeled data. For instance, Seg-Uncertainty <ref type="bibr" target="#b55">[56]</ref> predicts the confidence for the pseudo labels; ProDA <ref type="bibr" target="#b53">[54]</ref> relies on prototypes to further correct the pseudo labels during training in an online manner; MetaCorrection <ref type="bibr" target="#b14">[15]</ref> depicts the noise distribution of pseudo labels via meta-learning; SAC <ref type="bibr" target="#b0">[1]</ref> proposes an augmentation consistency approach trained on co-evolving pseudo labels. Akin to the momentum network in SAC, our momentum network requires smooth pseudo labels and features for target domain at the same time, without incurring substantial effort, e.g., thresholds and focal loss, importance sampling, etc. Self-training is widely used in conjunction with adversarial training in recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref>. In this paper, we successfully explore the potential of this kind of joint training paradigm for local ViTs in domain adaptation.</p><p>Other related works. Recently, Transformer <ref type="bibr" target="#b45">[46]</ref> and dynamic networks <ref type="bibr" target="#b16">[17]</ref> become more popular in deep learning. Deeper discussion about the relationship between Transformer and dynamic networks can be found in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Pioneered by ViT <ref type="bibr" target="#b9">[10]</ref>, many Transformer-based vision backbones <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref> have been proposed and achieved superior performance over a wide range of CV tasks. Particularly, by introducing shifted windows which capture local information to the self-attention module, Swin-Transformer <ref type="bibr" target="#b27">[28]</ref> became a representative of the local ViTs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">45]</ref>. Despite the prevalence of Transformers in CV tasks, there is still limited attempt of them in domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TransDA</head><p>In this section, we first illustrate the overall architecture of our method, then provide more details about each component, i.e., local ViT segmentation network, momentum network, and dynamic of discrepancy measurement, in the corresponding subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General Formulation</head><p>In domain adaptive semantic segmentation, we have a source domain</p><formula xml:id="formula_0">D s = {x s i , y s i } N s i=1 that consists of N s la- beled images and a target domain D t = {x t i } N t i=1 of N t unlabeled images.</formula><p>Let Y s and Y t denote the label spaces of the source and target spaces, respectively. For simplicity, the superscripts s and t denote the source and target domains, respectively. We use Y c = Y s ? Y t to denote the common label set shared by both domains, and Y sp = Y s \Y c and Y tp = Y t \Y c to represent the source and target private parts, respectively, following the setting of UniDA <ref type="bibr" target="#b51">[52]</ref>. We denote K c the number of classes in Y c . In what follows, we utilize the symbols x(i, j) (resp. y(i, j)) to denote each pixel of the input image (resp. the output dense predictions) at grid (i, j), and x (resp. y(x)) to represent arbitrary pixel by omitting the pixel index.</p><p>Following <ref type="bibr" target="#b15">[16]</ref> and attention <ref type="bibr" target="#b45">[46]</ref> definition, ?(?) varies with the input. From this, we can define the segmentation network using local ViTs as f ?(?) . Based on local ViTs, the goal of domain adaptive semantic segmentation is to learn a segmentation network f t ?(?) with parameter function ?(?) for target domain that can be generalized from source domain f s ?(?) . This implies We decompose it into two sub-goals: i) fitting the source-domain data, and ii) aligning the distributions of features between the source and target domains. The formal definition is given:</p><p>Definition 1 (Transformer Domain Adaptive Semantic Segmentation) Given the denotations defined before, the segmentation network f ?(?) is learned by</p><formula xml:id="formula_1">min ?(?) E x s ,y s ?D s x t ?D t [H(f s ?(x s ;? s ) (x s ), y s ) + H(f s ?(x t ;? s ) (x t ),? t ) + Dis(f s ?(x s ;? s ) (x s ), f t ?(x t ;? t ) (x t ))], s. t. ? t ? ? s , y t ? f t ?(x t ;? t ) (x t ),<label>(1)</label></formula><p>where, ?(?) is a function of the input sample, E[?] computes the expectation, H(?) can usually be implemented as the cross-entropy loss, and Dis(?) measures the discrepancy between domains. The pseudo labels? t is constructed from the most probable class predicted by f t ?(?) , which is directly  In practice, H(?) and Dis(?) do not share the exact function f ?(?) in Eq. 1; for example, as what we do in our implementation later, the outputs by f ?(?) are treated as the hidden features and will be stacked with different layers before the inputs for H(?) and Dis(?). Here, for brevity, we omit this kind of nonessential difference, and use Eq. 1 in its current form.</p><p>As discussed later in Sec 3.3, to smooth the learning dynamics on target domain, we propose a momentum network to work with ViTs, i.e., Momentum Transformer. Moreover, the discrepancy loss Dis(?) penalizes all input pairs as a whole, which will compromise the discrimination between the samples of different classes in either the source or target domain. Therefore, we correlate both the segmentation network and the discrepancy measurement with the input samples. The formal formulation of the problem is given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 (Momentum Transformer Domain Adaptive</head><p>Semantic Segmentation) Given the denotations defined before, the segmentation network f ?(?) and the weight w are learned by</p><formula xml:id="formula_2">min ?(?),w(?,?) E x s ,y s ?D s x t ?D t [H(f s ?(x s ;? s ) (x s ), y s ) + H(f s ?(x t ;? s ) (x t ),? t ) + Dis w(x s ,x t ) (f s ?(x s ;? s ) (x s ), f t ?(x t ;? t ) (x t ))], s. t. ? t ? m? t + (1 ? m)? s , y t ? f t ?(x t ;? t ) (x t ),<label>(2)</label></formula><p>Here, ?(?) is a function of the input sample, and Dis(?) is correlated with the weight w(?, ?). The pseudo labels? t is constructed from the most probable class predicted by f t ?(?) , which is updated with momentum copy from f s ?(?) where m ? [0, 1) is a momentum coefficient.</p><p>In the following subsections, we will introduce the details of the local ViT segmentation network f s ?(?) , the momentum network f t ?(?) and the dynamic of discrepancy measurement Dis w(x s ,x t ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local ViT Segmentation Network for Domain Adaptation</head><p>In this paper, our segmentation network f ?(?) via a typical Local ViT model -Swin Transformer for Semantic Segmentaion <ref type="bibr" target="#b27">[28]</ref>. It includes a feature extractor F and a classifier C that will be used for the formulation of the loss H(?) in Eq. 2. The segmentation categorical prediction is then:</p><formula xml:id="formula_3">p s = [p s 1 , . . . , p s K c ] = C s (F s (x s )) = f s ?(x s ) (x s )<label>(3)</label></formula><p>With the above function, the objective H(?) in Eq. 2 can be formally defined:</p><formula xml:id="formula_4">L source ce = ?E D s [ K c k=1 y s k log (p s k )],<label>(4)</label></formula><p>where p s k and y s k denote the predicted outputs and groundtruth, respectively, for the k-th class.</p><p>Similarly, on target domain, we can get the construct the pseudo labels? t based on f t ?(?) . We first obtain the categorical prediction on target domain: Then, the target pseudo labels is defined as a one-hot vector:</p><formula xml:id="formula_5">p t = [p t 1 , . . . ,p t K c ] = C t (F t (x t )) = f t ?(x t ) (x t ). (5) feature extractor discri- minator ? !"# # s classifier ? %&amp;</formula><formula xml:id="formula_6">y k = 1, if k = arg max k p t k 0, otherwise.<label>(6)</label></formula><p>To align the source and target domain and improve the discriminabilty of the latent space, we follow the common practice, i.e., self-training f s ?(?) with both labeled source samples and pseudo-labeled target samples. Following the above notations, that said we need to train F s and C s by L target ce . In this way, the networks F s and C s are bootstrapped by learning from pseudo labels that only get update till convergence, and then the updated labels are employed for the next training round. We obtain the segmentation categorical prediction on target domain by f s ?(x t ) as follows:</p><formula xml:id="formula_7">p t = [p t 1 , . . . , p t K c ] = C s (F s (x t )) = f s ?(x t ) (x t )<label>(7)</label></formula><p>With the pseudo labels, the loss functions of segmentation in the target domain can then be defined as:</p><formula xml:id="formula_8">L target ce = ?E D t [ K c k=1? t k log p t k ],<label>(8)</label></formula><p>where p t k and? t k denote the predicted outputs and groundtruth, respectively, for the k-th class.</p><p>Traditional implementation by using adversarial training has been proven effective <ref type="bibr" target="#b20">[21]</ref>. Particularly, it aligns features by using a binary domain discriminator D <ref type="bibr" target="#b12">[13]</ref> to model the marginal distribution P(d|F(x)) by a 1dimensional vector. In contrast to perform the classagnostic adversarial training, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47]</ref> uses a class-level domain discriminator to explicitly models the conditional distribution P(d, k|F(x)), according to P(d|F(x)) = K c k=1 P(d, k|F(x)), to minimize the discrepancy between domains for each class separately. We will introduce the details of adversarial loss L adv in ? 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Momentum Network for Target Domain</head><p>On target domain, we cannot directly train the segmentation network f t ?(?) (constituted by feature extractor F t and classifier C t ), since there is no ground-truth. A na?ve solution is to use pseudo-labels and features generated by the source domain segmentation network f s ?(?) to train f t ?(?) in self-training and adversarial training. As shown in <ref type="figure" target="#fig_12">Figure 1(a)</ref>, target pseudo labels C s (F s (x t )) are crucial for the classifier to make dense prediction, and target features F s (x t ) are essential for the discriminator to align features distribution.</p><p>However, through experiments, we find that such training schema on target domain is very unstable. To further inspect target pseudo labels C s (F s (x t )) and target features F s (x t ), we sample 8 images from Cityscapes train set <ref type="bibr" target="#b8">[9]</ref>, and track how drastically D(F s (x t )) and C s (F s (x t )) change over time. To be more specific, let's denote the output of these two functions at iteration i as</p><formula xml:id="formula_9">D i (F s i (x t )) and C s i (F s i (x t )). Then, we track both D i+1 (F s i+1 (x t )) ? D i (F s i (x t )) 1 and C s i+1 (F s i+1 (x t )) ? C s i (F s i (x t )) 1 .</formula><p>To make sure this phenomenon is consistent, we use 5 random seeds to run the experiment, and the average and standard deviation of these distance sums are given in <ref type="figure">Figure 1</ref> (b) and (c) respectively. As shown in <ref type="figure">Figure 1</ref>, by comparing the change of predictions under different backbones, we can see that the outputs of local ViTs (Swin-S) vibrate more drastically than conventional CNNs (ResNet-101), as indicated by the larger changes of L1-distance over iterations. We refer this problem as the high-frequency component problem in the predictions of local ViTs (see Supplementary Material for details), and we argue that this problem causes that f s ?(?) provides poor quality of supervision for f t ?(?) . Formally, we can also formulate the learning of f t ?(?) following the knowledge distillation <ref type="bibr" target="#b18">[19]</ref> framework, where f s ?(?) is the teacher model and f t ?(?) is the student model. Then, our above phenomenon is essentially the same to the   <ref type="bibr" target="#b32">[33]</ref>. For a clear illustration, we only show two categories, i.e., red for source person, orange for source rider, green for target person, and blue for target rider. Abbreviation: (S)elf (T)raining, (A)dversarial (T)raining. Data Augmentation is the default option and all ST+AT variants employ dynamic discrepancy and momentum network "zig-zag" learning dynamics found in <ref type="bibr" target="#b36">[37]</ref>. To be more specific, the authors find that change of teacher's predictions is very unsmooth, thus generates massive high-frequency components during its supervised learning. Thus, they introduced a low-pass filter to smooth the predictions from teacher, and it turned out this can indeed improve the quality of supervision. Although the teacher model and student model in the domain adaptation task are from two different domains, in contrast to single domain in <ref type="bibr" target="#b36">[37]</ref>, we argue that the above issues can also be tackled by a low-pass filter. However, the original implementation in <ref type="bibr" target="#b36">[37]</ref>, i.e., a lookup table which stores predictions for all training example, is very infeasible in semantic segmentation as it requires huge storage overhead for all pixels. With further analysis, the drastic change of predictions actually indicate that the parameters are changing drastically. Thus, we argue that momentum network can also be used to smooth the parameters of F t and C t in order to smooth the supervision provided by f s ?(?) illustrated in the above paragraph. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, MoPL only smooths the target pseudo label supervision C s (F s (x t )) for source classifier C s and feature extractor F s , MoFA only smooths the target feature supervision F s (x t ) for discriminator D, and MoPL+MoFA jointly smooth the C s (F s (x t )) and F s (x t ). As shown in <ref type="table" target="#tab_0">Table 1</ref>, we find that MoPL+MoFA is the only way for local ViT model to succeed in this task, revealing that jointly smoothing the pseudo labels and features is essential. Concretely, the feature extractor F t and classifier C t on target domain are updated by a momentum copy from source domain f s ?(?) in Eqn. <ref type="bibr" target="#b1">(2)</ref>. Note that only the parameters ? s (of is the momentum network of f s ?(?) , the gradient cannot directly update f t ?(?) . As shown in ? 4.4, we find that a relatively large momentum (e.g., m = 0.999, our default) works much better than a smaller value (e.g., m = 0.9), suggesting that a slowly evolving f t ?(?) can effectively smooth the target pseudo labels and features <ref type="figure">(Figure 1(b)</ref> and (c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic of Discrepancy Measurement</head><p>This subsection presents how to realize the dynamic of discrepancy measurement Dis w(x s ,x t ) in Eq. 2. To avoid the exacerbation by the noise and errors, we propose to use dynamic weighting w to suppress the influence of false samples in the transfer process. Concretely, the larger w is, the more reliable transferability it is. For binary domain discriminator, we realize the dynamic discrepancy Dis w(x s ,x t ) by the weighted adversarial loss below:</p><formula xml:id="formula_10">L adv = ?E D s [w s (x s ) log D(F s (x s )))] ? E D t [w t (x t ) log(1 ? D(F t (x t )))],<label>(9)</label></formula><p>where the dynamic weighting w(x s , x t ) is implemented as w s (x s ) for the source input and w t (x t ) for the target input. For class-level domain discriminator, please refer to Supplementary Material. Similiar to UniDA <ref type="bibr" target="#b51">[52]</ref>, we use a non-adversarial domain similarity network S to obtain the domain similarity to the source domain. The objective of S is to predict the samples from source domain as 1 and those from target domain as 0. The loss function of domain similarity is given as follows:</p><formula xml:id="formula_11">L sim = ?E D s log S(F s (x s )) ? E D t log(1 ? S(F t (x t ))).</formula><p>(10) Here, S(?) ? [0, 1] can be seen as the quantification for the domain similarity of each sample. For a source sample, smaller score means that it is more similar to the target domain; for a target sample, vice versa. Therefore, we can hypothesize that</p><formula xml:id="formula_12">E x?D Y sp S(F s (x Y sp )) &gt; E x?D Y c S(F(x Y c )) &gt; E x?D Y tp S(F t (x Y tp )).</formula><p>We now depict how to obtain the weights w(x s ) and w(x t ) in Eq. 9. Given each input x, its domain similarity S(x) and segmentation categorical prediction p(x) over the label set Y c , we dynamically compute w s (x) and w t (x) by: </p><formula xml:id="formula_13">w s (x s ) = E(p s ) log |K c | ? S(F s (x s )), w t (x t ) = S(F t (x t )) ? E(p t ) log |K c | .<label>(11)</label></formula><p>Note that the entropy E(p) is normalized by its maximum value (log |K c |) so that it is restricted into [0, 1] and comparable to the domain similarity S(F(x)). The entropy E(p(x)) can quantify the uncertainty, and smaller entropy means more confident prediction. Thus, we can</p><formula xml:id="formula_14">assume that E x?D Y tp E(p Y tp ) &gt; E x?D Y c E(p Y c ) &gt; E x?D Y sp E(p Y sp ).</formula><p>Besides, the weights w are min-max normalized into interval [0, 1]. The motivation is that for any source sample, it should be weighted more, if there is larger uncertainty within the prediction and smaller similarity to the source domain. For the target sample, vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Full Training Procedure</head><p>By assembling the above modules, we propose a simple training schedule for TransDA to joint self-training along with adversarial training. The training process is proceeded in terms of curriculum-like learning.</p><p>Warm-up Phase: We load and fine-tune the Swin model in terms of cross-entropy (CE) loss (Eq. 4) in source domain, the domain similarity loss (Eq. 10)and the weighted adversarial loss (Eq. 9). The full objective of warm-up phase: min</p><formula xml:id="formula_15">F,C,S max D L source ce + L sim ? L adv .<label>(12)</label></formula><p>Train Phase: We further transfer knowledge from the warm-up phase segmentation network to a pretrained student network (i.e., Swin pretrained on ImageNet), by the self-training paradigm <ref type="bibr" target="#b53">[54]</ref>. The teacher model generates one-hot pseudo labels? to teach the student network via a cross-entropy loss. It can be carried out for several rounds, and for each round, the student model is always initialized as the pretrained model in order to escape from the local optima in the last round. We conduct self-training in terms of cross-entropy (CE) loss (Eq. 4 and 8), the domain similarity loss (Eq. 10)and the weighted adversarial loss (Eq. 9). The full objective of start phase is as follows:</p><formula xml:id="formula_16">min F,C,S max D L source ce + L target ce + L sim ? L adv .<label>(13)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>Datasets. We evaluate the performance of our methods on the challenging domain adaptive semantic segmentation task. The source domain contains two synthetic datasets, GTA5 <ref type="bibr" target="#b37">[38]</ref> and SYNTHIA <ref type="bibr" target="#b38">[39]</ref> and the target domain is a real dataset: Cityscapes <ref type="bibr" target="#b8">[9]</ref>. We conduct experiments on two domain adaption flows: GTA5?Cityscapes and SYNTHIA?Cityscapes. To be specific, GTA5 and SYN-THIA share 19 and 16 common categories with Cityscapes, repectively. On SYNTHIA?Cityscapes, following <ref type="bibr" target="#b31">[32]</ref>, we consider two different testing protocols: applying all 16 common categories or just a subset consisting of 13 categories for evaluations. Note that we train the model on the  Metrics. Following the common protocol in this area, we use PASCAL VOC Intersection-over-Union (IoU) <ref type="bibr" target="#b10">[11]</ref> as the evaluation metric. Notably, previous methods benchmark the performance by only reporting the best numbers without reflecting the averaged scores and standard deviations. In our experiments, we recommend to show the robustness by further reporting the averaged scores and standard deviations. We report the best value in <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> and report the averaged scores and standard deviations in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study: smoothing matters</head><p>To validate the effectiveness of our proposed key techniques:momentum network and dynamic of discrepancy measurement, we conduct the ablation study under different backbones on the GTA5?Cityscapes. We report the results in <ref type="table" target="#tab_0">Table 1</ref> and have the following observations:</p><p>? As can be seen in the third row of results, the gain of backbone replacement is +5.1 (53.9-48.8) when employing just the almost ubiquitous self-training, which validates the cross-domain transferability of local ViT backbone. However, by addressing the high-frequency components problem in local ViT, our proposed method can further boost the performance +5.4 (59.3-53.9). This additional improvement can be viewed as significant since it is much larger than that of AdaptSeg <ref type="bibr" target="#b42">[43]</ref> (+3.1), which is the first work to replace VGG with ResNet in this task.</p><p>? We also observe different trends for different models under the same setting, such as row 2 and 4 in binary discriminator and row 2,4 and 6 in class discriminator. It implies the inherent difference between two backbones, which justifies the motivation of our model design.</p><p>? On Swin-S, the proposed momentum network dramatically improves the performance (+5.0=58.9-53.9 on binary discriminator). It indicates that Swin-S suffers from the severe high-frequency component problem both in target pseudo labels and features, it is vital to smooth both at the same time.</p><p>? For ResNet-101, smoothing pseudo labels can also improve its performance, while smoothing both pseudo la-   <ref type="table">Table 3</ref>. Comparisons with state-of-the-art methods on GTA5?Cityscapes   <ref type="table">Table 4</ref>. Comparisons with state-of-the-art methods on SYNTHIA?Cityscapes. mIoU and mIoU* denote the scores across 16 and 13 categories respectively bels and target features or applying dynamic discrepancy will degenerate its performance. This may due to that the target features of ResNet-101 are already smooth enough (shown in <ref type="figure">Figure 1</ref>) and the different feature representations learnt by CNNs (illustrated in <ref type="figure" target="#fig_5">Figure 4</ref>). Adding the additional smoothing would damage its target features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study: foundation components</head><p>As the first work to build a new baseline based on a local ViT backbone, it is essential to ablate the foundation components to provide the basic insight.</p><p>As shown in <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_7">Figure 5</ref>, we explore the potential of leveraging local ViT backbones via a classical training paradigm. We observe: i) Besides in fully supervised learning, data augmentation techniques also play an vital role in local ViT backbones for domain adaptation; ii) Adversarial training can further improve the transferability, but the effect is not as obvious as using self-training alone; iii) If they are adopted jointly, a bottleneck for further improvement is encountered. However, with our proposed smoothing method, the potential of the model under this classical training paradigm is fully released (+5.4=59.3-53.9).</p><p>To better develop intuition, we visualize the learned features for TransDA in <ref type="figure" target="#fig_5">Figure 4</ref>. We find that: i) By using ResNet-101, the predictions of different classes are mixed together in both domains, showing the weak ability in semantic distinction; ii) When applying self-training upon Swin-S, the samples of different classes are well separated, which is probably owing to the better expressivity of local ViT. Meanwhile, particularly for the class "person", the source domain and target domain overlap with each other by a large percentage, showing the better generalization ability across domains; iii) After applying adversarial training upon Swin-S, the shape of the feature space is closer to a sphere. At the same time, the samples of both domains are well separated across different categories.</p><p>Compared with conventional CNNs, the representations of local ViTs are significantly different. Taking the interesting inspiration from <ref type="bibr" target="#b15">[16]</ref> that the attentions used in local ViTs are indeed equivalent to depth-wise CNNs with dynamic weight, local ViTs acquire the dynamic nature of f ?(?) . Joining the study by DRT <ref type="bibr" target="#b25">[26]</ref>, the dynamic nature may break down domain barriers better. Based on the above observations and discussions, it reveals that it's an urge to explore the original techniques in this field to develop new methods based on this new backbone to break through the performance ceiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study: momentum coefficient</head><p>The It performs reasonably well when m is 0.99 and 0.999, showing that a smooth (i.e., relatively large momentum) F t and C t is beneficial. When m is too large (e.g., 0.9999), due to over-smoothing, it will lead to under-fitting in insufficient training schedule; When m is too small (e.g., 0 and 0.9), the performance drops considerably. Thus, the momentum should conform to the learning dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Benchmarks</head><p>We comprehensively compare our proposed method with the recently leading approaches in the two domain adaptation scenarios: GTA5 ? Cityscapes in <ref type="table">Table 3</ref> and SYN-THIA ? Cityscapes in <ref type="table">Table 4</ref>. From <ref type="table">Table 3</ref>, we have the following observations: Our TransDA-S arrives at the best mIoU score as 60.6, outperforming all existing methods by a large margin. Even looking at the adaptation gain, our TransDA achieves +21.4(=60. <ref type="bibr">6-39.2)</ref> which is still better than +20.7(=57. <ref type="bibr">5-36.8)</ref> by the state-of-the-art method ProDA <ref type="bibr" target="#b53">[54]</ref>. From <ref type="table">Table 4</ref>, we observe that naively replacing the backbone with Swin-S even suffers a small small performance degradation -0.9(=37. <ref type="bibr">7-38.6)</ref>. It implies that generalization is instead compromised by naively replacing the backbone. On the other hand, our TransDA still consistently surpasses all compared methods. Specially, looking at the adaptation gain, our TransDA achieves +24.5(=62.2-37.7) with a larger margin than +23.4(=62.0-38.6) by the ProDA <ref type="bibr" target="#b53">[54]</ref>, which can be viewed as significant considering that the better performance is harder to optimize. Such adaption gain verifies the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present TransDA, a novel domain adaptive semantic segmentation based on local ViTs. Our contri-bution is the discovery of the severe high-frequency component problem of local ViTs on pseudo-label generation and feature alignment for target domain. To tackle the above issues, we propose the momentum network and dynamic of discrepancy measurement, to smooth the learning dynamics for target domain. Compared to these long-explored methods on conventional backbones, TransDA can unleash the great potential of traditional methods without incurring substantial technical complexity, leading to a new SOTA baseline based on ViTs in domain adaptive semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Toy Example to Illustrate High-Frequency Components Problem</head><p>In <ref type="figure">Figure 1(b)</ref> and (c) of our main body, we track both</p><formula xml:id="formula_17">D i+1 (F s i+1 (x t )) ? D i (F s i (x t )) 1 and C s i+1 (F s i+1 (x t )) ? C s i (F s i (x t )) 1 . Denotes D i (F s i (x t )) or C s i (F s i (x t ))</formula><p>as p i . Then we refer to  <ref type="bibr" target="#b13">(14)</ref> as change of predictions in local ViTs (Swin-S) for domain adaptive semantic segmentation, where S = 5 denotes the number of seeds, N = 10 denotes the number of images, W and H denotes the size of predictions, K = 19 denotes the number of categories.</p><formula xml:id="formula_18">p i+1 ? p i 1 = 1 S * N</formula><p>However, since the above p i is too complicated to visualize, in the following of this subsection, we instead use a toy example for visualisation and illustration about the so called high-frequency component problem which implies that large variance in model's prediction in this section. Suppose the number of categories is K = 5. To control the variance degree of model's predictions over time, we synthesize the prediction sequences with three Gaussian processes whose means are ? 1 = ? 2 = ? 3 = 0 ? R 5 and variances are ? 1 = 0.5, ? 2 = 1.0, ? 3 = 10.0. To make the following easier, let's denote these three Gaussian processes as 'small', 'medium', 'large' respectively. We then sample predictions over T = 20 iterations time for each of them, and then apply a softmax function along categorical dimension on these predictions to convert them to categorical distributions which are denoted as p s i , p m i , p l i , ?i ? T respectively. The probability sequences of all categories are then plotted in <ref type="figure" target="#fig_12">Figure 6a</ref>, 6b, and 6c.</p><p>Similar to Eq. 14, we refer to p i+1 ? p <ref type="figure">Figure 6d</ref>, 6e, and 6f. It is straightforward to see that the 'large' process produce higher values since its predictions change more drastically.</p><formula xml:id="formula_19">i 1 = K k p k i+1 ? p k i as change of predictions in our toy example. We plot p s i+1 ? p s i 1 , p m i+1 ? p m i 1 , and p l i+1 ? p l i 1 , ?i ? T in</formula><p>We then can do a Discrete Fourier Transformation (DFT) on the series of p s t , p m i , and p l i , ?i ? T , and the results are shown in <ref type="figure">Figure 6g</ref>, 6h, and 6i. From <ref type="figure">Figure 6i</ref>, it can be straightforwardly seen that the amplitude of the high frequencies (e.g., 4 and -4 on the horizontal axis) are higher when the change of prediction are higher in <ref type="figure">Figure 6f</ref>. That said, the existence of more high-frequency components implies more drastic change of model's predictions. Therefore, we refer it as the "high-frequency component" problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. More Discussions about the High-Frequency Components Problem</head><p>More Observations of the Problem. In Supplementary Material, we further explore the high-frequency component problem on source domain. We randomly sample 8 images from source domain, GTA5 <ref type="bibr" target="#b37">[38]</ref> and target domain, Cityscapes train set <ref type="bibr" target="#b8">[9]</ref>, respectively and use S = 5 random seeds to run the experiment. First, we conduct research on Swin-S ViT and observe:</p><p>? In <ref type="figure" target="#fig_12">Figure 7(a)</ref>, without smoothing, the change of predictions C s (F s (x t )) from classifier for target domain is large than C s (F s (x s )) for source domain, but the opposite in D(F s (x t )) and D(F s (x s )) from discriminator. This may due to that in adversarial training, the features of target domain F s (x t ) encoding by the source feature extractor are difficult to deceive the discriminator, but the features of source domain F s (x s ) can easily deceive the discriminator during the training process.</p><p>? In <ref type="figure" target="#fig_13">Figure 7</ref>(b), after applying momentum networks F t and C t for target domain, the change of predictions C t (F t (x t )) from classifier for target domain is significantly reduced, and the change of predictions D(F s (x s )) and D(F t (x t )) from discriminator are also both pulled down, which might because the features of source domain is indirectly smoothed due to feature alignment.</p><p>? For the high-frequency components problem on source domain, we only consider smoothing the source domain features for feature alignment, due to that source domain has ground-truth supervision signals. In <ref type="figure" target="#fig_13">Figure 7</ref>(c), after applying momentum network F t for source domain, the change of predictions D(F t (x s )) and D(F t (x t )) from discriminator are both pulled down further. However, this caused the change of predictions C t (F t (x t )) from classifier for target domain to oscillate violently. The performance of this variant on the task of GTA5 ? Cityscapes (57.8 ?0.8 ) shows that smoothing the source domain features for feature alignment will cause performance degradation.</p><p>Secondly, we also conduct research on ResNet-101, and observe:</p><p>? In contrast to <ref type="figure" target="#fig_13">Figure 7</ref>, we can find the change of predictions is very small in <ref type="figure" target="#fig_14">Figure 8</ref>, which implies that the high-frequency component problem is specific for ViTs but not CNNs.</p><p>? In <ref type="figure" target="#fig_14">Figure 8</ref> ? In <ref type="figure" target="#fig_14">Figure 8(c)</ref>, smoothing the source domain features for feature alignment is basically useless. Compared with <ref type="figure" target="#fig_13">Figure 7</ref>(c), the change of predictions C t (F t (x t )) from classifier is also not affected.  <ref type="figure">Figure 6</ref>. Illustration of High-Frequency Component Problem.</p><formula xml:id="formula_20">(d) p s i+1 ? p s i 1 (e) p m i+1 ? p m i 1 (f) p l i+1 ? p l i 1 (g) DFT of 'small' (h) DFT of 'medium' (i) DFT of 'large'</formula><p>Impact of the Issue. The impact of high-frequency components in the predictions of the classifier is reflected in generating target pseudo labels after each training round, which is not conducive to self-training in the next training round. In contrast, the impact of high-frequency components in feature encoding is mainly reflected in features alignment after each training iteration, which is not conducive to adversarial training in the next training iteration. The period and phase of impact are different, target features affect each iteration in the current round, but tar-get pseudo labels affect each iteration in the next round. It is worth noting that adversarial training itself is unstable, let alone to align the poor target domain distribution produced by ViT, it will be even more unstable. So smoothing could be a requirement for ViT with adversarial training in various tasks. Different from <ref type="bibr" target="#b36">[37]</ref>, our work discusses the importance of features in adversarial training for domain adaptation, while <ref type="bibr" target="#b36">[37]</ref> emphasizes the importance of labels in supervised training.  <ref type="table" target="#tab_0">Table 1</ref> In this section, we provide the detailed definitions about the discrepancy measurements Dis(?) that we compared in the ablation study: smoothing matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Details of Discrepancy Measurements in</head><p>For binary domain discriminator, the binary adversarial loss is defined as follows:</p><formula xml:id="formula_21">L bin adv = ?E D s [log D(F s (x s )))] ? E D t [log(1 ? D(F t (x t )))].<label>(15)</label></formula><p>We realize the dynamic discrepancy Dis w(x s ,x t ) by the weighted binary adversarial loss below:</p><formula xml:id="formula_22">L wbin adv = ?E D s [w s (x s ) log D(F s (x s )))] ? E D t [w t (x t ) log(1 ? D(F t (x t )))],<label>(16)</label></formula><p>where the dynamic weighting w(x s , x t ) refers to w s (x s ) on the source input and w t (x t ) on the target input. For class domain discriminator, the class-level adversarial loss is:</p><formula xml:id="formula_23">L cls adv = ?E D s [ K c k=1 p s k log D k (F s (x s ))] ? E D t [ K c k=1p t k log(1 ? D k (F t (x t )))],<label>(17)</label></formula><p>where D k refers to the k-th output channel of the discriminator, and the prediction p k from the classifier is adopted to balance the class-level importance. We implement the dynamic discrepancy Dis w(x s ,x t ) as a weighted class-level adversarial loss defined as below:</p><formula xml:id="formula_24">L wcls adv = ?E D s [w s (x s ) K c k=1 p s k log D k (F s (x s ))] ? E D t [w t (x t ) K c k=1p t k log(1 ? D k (F t (x t )))].<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Averaged Scores and Standard Deviations on Benchmarks</head><p>We report the mean mIoU with standard deviations to demonstrate the robustness of TransDA in the two domain adaptation scenarios: GTA5 ? Cityscapes in <ref type="table">Table 5</ref> and SYNTHIA ? Cityscapes in <ref type="table">Table 6</ref>. The low standard deviations demonstrate the stability of our method. Even looking in the sense of averaged mIoU, our TransDA is still surpassing or matching the best score by the recently leading approaches in these two domain adaptation scenarios.</p><p>Besides, we can find that the generalization performance can be significantly improved as the scale of model parameters increases (+5.8=44. <ref type="bibr">1-38.4</ref> in GTA5? Cityscapes and +7.5=43.7-36.2 in SYNTHIA? Cityscapes). However, the gains of the adaptation are obviously limited For discrepancy measurement, we find that adversarial training based on binary discriminator is more stable than class discriminator. Although the per-class distribution alignment is in a more fine-grained manner <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47]</ref>,the problem is that such alignment heavily depends on the class supervision from the source and target domain. Therefore, as the scale of the model parameters increases, the class-level method based on the Swin-B backbone can better predict the self-supervised signal, resulting in comparable performance (see last row in <ref type="table">Table 5</ref> and <ref type="table">Table 6</ref>), which bodes well for greater potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Results on the Official Cityscapes Benchmark Server</head><p>In addition, the current benchmarks of domain adaptive semantic segmentation also have some inherent problems, as discussed in <ref type="bibr" target="#b0">[1]</ref>'s supplement material.</p><p>In fact, Cityscapes does not provide ground truth labeling for the test set, it instead provides a labeled validation set. The previous methods all verify the final model on the validation set, because the benchmarking process on the validation set does not have clear and strict standards, which is in discord with the established best practice on Cityscapes test set <ref type="bibr" target="#b8">[9]</ref>, in particular. The new practice evaluation is proposed and the results on Cityscapes test set are reported in <ref type="bibr" target="#b0">[1]</ref> supplement material, which encourages researchers to report the results on Cityscapes test set. The holdout test set for testing the final segmentation accuracy after adaptation becomes Cityscapes test, with the results obtained via submitting the predicted segmentation masks to the official Cityscapes benchmark server 1 . Owing to the regulated access to the test set, they believe this setting to offer more transparency and fairness to the benchmarking process.</p><p>However, Cityscapes Benchmark websites has a restricted access to the test annotation (e.g., limited number of submissions per time window and user), which limits researchers to consult the test set for verifying a number of model with different random seed. For this reason, we recommend that, for a fair comparison, use the validation set to select the best model and submit it for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Datasets</head><p>In this section, we provide more details about the datasets used in our experiments.</p><p>Cityscapes <ref type="bibr" target="#b8">[9]</ref> contains 2975 real images in the training set, and the original image size is 2048 ? 1024 pixels resolution. We utilize three data augmentation techniques to enhance the training stability, including random horizontal flipping, random re-scaling 1024 ? 512 pixels within ratio range [0.5, 2.0], color jittering with brightness, contrast, saturation, and hue.</p><p>The two domain adaptive semantic segmentation tasks 4 https://www.cityscapes-dataset.com/anonymous-results/?id= a805e7844fc51c7c7333e28a05529cbd19d23db058b5eff438819edfb623813b 5 https://www.cityscapes-dataset.com/anonymous-results/?id= 102ba9c29c1fe0aa94c7ebd824b4cc80c6573ee80fe57db0cbe32222b1330e0f are GTA5?Cityscapes and SYNTHIA?Cityscapes. We use only the semantic classes shared with the simulation dataset for training and testing, and merge other categories as an "ignore" class. During the inference, the "ignore" class area will be skipped. Following the previous protocol <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref>, we use 2975 images in the training set as the unlabeled target domain training set and evaluate the proposed model on 500 images in the validation set due to that its test set does not provide ground truth labeling.</p><p>GTA5 <ref type="bibr" target="#b37">[38]</ref> contains 24966 simulation images, and the original image size is 1914 ? 1052 pixels resolution. We utilize three data augmentation techniques to enhance the training stability, including random horizontal flipping, random re-scaling 1280 ? 720 pixels within ratio range [0.5, 2.0], color jittering with brightness, contrast, saturation, and hue. Moreover, we employ only the 19 semantic classes shared with the real-world city street Cityscapes dataset. Similarly, the other classes are all labelled as "ignore", and skipped during the inference.</p><p>Synthia-Rand-Cityscapes <ref type="bibr" target="#b38">[39]</ref> contains 9400 simulation images, the original image size is 1280 ? 760 pixels resolution. We utilize three data augmentation techniques to enhance the training stability, including random horizontal flipping, random re-scaling 1280 ? 760 pixels within ratio range [0.5, 2.0], color jittering with brightness, contrast, saturation, and hue. Similarly, only 16 semantic classes shared with the real-world city street Cityscapes dataset are used for training. Similarly, the other classes are all labelled as "ignore", and skipped during the inference. However, this dataset usually selects the following two evaluation settings: performed on 16 classes or a subset of 13 classes. Here we follow the protocol in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47</ref>] to train the model on the whole set and test it on both settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Further Implementation Details</head><p>Networks and Optimizers. As for the feature extractor F, we initially load the Swin-S model pre-trained on ImageNet-1K for TransDA-S, which has the similar model size and computation complexity to ResNet-101, and load the Swin-B model pre-trained on ImageNet-22K with the input size of 224 ? 224 for TransDA-B, which is similar to ViT-B/DeiT-B in size. The classifier C is implemented by UPerNet <ref type="bibr" target="#b48">[49]</ref> with deep supervision from FCN <ref type="bibr" target="#b28">[29]</ref>. Following <ref type="bibr" target="#b27">[28]</ref>, we use the AdamW <ref type="bibr" target="#b29">[30]</ref> optimizer for the training of F as well as the classifier C. We set the initial learning rate as 6?10 ?5 , weight decay as 0.01, and employ a scheduler with linear learning rate decay along with a linear warm-up over 1500 iterations. As for both the discriminator D and similarity network S, we follow the settings from FADA <ref type="bibr" target="#b46">[47]</ref> and adopt a simple structure network consisting of 3 convolution layers. The Adam <ref type="bibr" target="#b23">[24]</ref> optimizer is used for the training of D and S, where the initial learning rate is 1 ? 10 ?4 , and the scheduler uses the 'poly' learning rate decay with power 0.9.</p><p>Reported Metrics. For each experiments, we run each variant over five times with different random seeds unless otherwise stated. We use the single-scale test at the inference stage. Follow the common practice of previous works, we report the best mIoU for a fair comparison. Meanwhile, we also report the mean mIoU with standard deviations to demonstrate the robustness of TransDA. Since there are no ground-truth labels available for target domain in domain adaptive semantic segmentation, the optimal model cannot be picked out in practice. Hence, we strongly prefer to show the robustness of our method by the averaged scores and standard deviations.</p><p>Training Schedule. Our models are trained on 4 Tesla V100 GPUs with 2 images per GPU per iteration, and the training process consists of one round of warm-up phase and three rounds of train phase, where each round lasts over 10k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8. Broader Impact</head><p>Our research can help reduce burden of collecting largescale supervised data in many real-world applications of semantic segmentation by transferring knowledge from models trained on large labeled datasets to specific unlabeled datasets, especially sim2real scenario. The positive impact that our work could have on society is to make technology more accessible for institutions and individuals that do not have rich resources for annotating newly collected datasets. Besides, in this work, we put forward suggestions for the benchmarking of domain adaptive semantic segmentation, such as providing averaged score and standard deviation, and finally submitting the official Cityscapes benchmark score to standardize and promote this community. We hope our research can also facilitate the thinking about the peculiarity of ViTs, and how it is potentially applicable to refresh the conventional transfer framework in the research fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) self-training and adversarial training on target domain (c) change of target features (predictions from source discriminator) (b) change of target pseudo label (predictions from source classifier)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? !"# : Adversarial Loss ? $% : Cross-entropy Loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Momentum Transformer Domain Adaptive Semantic Segmentation copied from f s ?(?) where ? s are the parameters of f s ?(?) and ? t are those of f t ?(?) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Smooth different supervisions for target domain in self-training and adversarial training. Mo: Momentum Network, PL: for pseudo labels in self-training, FA: for features in adversarial training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Swin-S + ST (d) TransDA (Swin-S + ST + AT) (b) ResNet101 + ST + AT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>The visualization of feature space, where we map features to 2D space with UMAP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>F s and C s ) are updated by the gradient of L source ce , L target ce and L adv . Meanwhile, since f t ?(?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Visualizations of domain adaptive semantic segmentation based on Swin ViT at the inference stage. Abbreviation: (S)elf (T)raining, (A)dversarial (T)raining. * denotes employing our proposed smoothing method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>n,w,h,k i+1 ? p s,n,w,h,k i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) and (b), the situation is basically the same as that ofFigure 7(a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( a )</head><label>a</label><figDesc>Predictions of 'small' over time (b) Predictions of 'medium' over time (c) Predictions of 'large' over time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 .</head><label>7</label><figDesc>High-frequency components problem in Swin-S ViT on source domain and target domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 .</head><label>8</label><figDesc>High-frequency components problem in ResNet-101 on source domain and target domain. (+2.5=61.8-59.3 in GTA5? Cityscapes and +3.9=65.4-61.5 in SYNTHIA? Cityscapes), considering that the better performance is harder to optimize.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>adversarial</cell><cell>dynamic discrepancy</cell><cell>MoPL</cell><cell>MoFA</cell><cell cols="2">ResNet-101(43M) mIoU gain</cell><cell>Swin-S(50M) mIoU gain</cell></row><row><cell>w/o</cell><cell></cell><cell></cell><cell></cell><cell>46.7 ?0.8</cell><cell>-</cell><cell>52.5 ?5.1</cell><cell>-</cell></row><row><cell>discriminator</cell><cell></cell><cell></cell><cell></cell><cell>46.3 ?0.7</cell><cell>? 0.4</cell><cell>47.4 ?0.2</cell><cell>? 5.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.8 ?0.5</cell><cell>? 2.1</cell><cell>53.9 ?2.3</cell><cell>? 1.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>49.0 ?0.3</cell><cell>? 2.3</cell><cell>51.8 ?1.3</cell><cell>? 0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.9 ?0.5</cell><cell>? 0.8</cell><cell>46.8 ?1.7</cell><cell>? 5.7</cell></row><row><cell>binary</cell><cell></cell><cell></cell><cell></cell><cell>46.4 ?0.4</cell><cell>? 0.3</cell><cell>58.9 ?0.5</cell><cell>? 6.4</cell></row><row><cell>discriminator</cell><cell></cell><cell></cell><cell></cell><cell>47.9 ?0.3</cell><cell>? 1.2</cell><cell>54.5 ?1.3</cell><cell>? 1.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.6 ?0.2</cell><cell>? 1.9</cell><cell>52.6 ?0.1</cell><cell>? 0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.3 ?0.9</cell><cell>? 0.4</cell><cell>49.2 ?3.2</cell><cell>? 3.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.7 ?0.7</cell><cell>? 0.0</cell><cell>59.3 ?0.7</cell><cell>? 6.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>47.9 ?1.1</cell><cell>? 1.2</cell><cell>52.7 ?1.1</cell><cell>? 0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>49.6 ?0.5</cell><cell>? 2.9</cell><cell>49.3 ?1.1</cell><cell>? 3.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.3 ?1.5</cell><cell>? 1.4</cell><cell>49.0 ?4.6</cell><cell>? 3.5</cell></row><row><cell>class</cell><cell></cell><cell></cell><cell></cell><cell>46.4 ?0.5</cell><cell>? 0.3</cell><cell>57.2 ?0.8</cell><cell>? 4.7</cell></row><row><cell>discriminator</cell><cell></cell><cell></cell><cell></cell><cell>47.4 ?0.9</cell><cell>? 0.7</cell><cell>55.5 ?1.6</cell><cell>? 3.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.5 ?0.5</cell><cell>? 1.8</cell><cell>49.4 ?1.4</cell><cell>? 3.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.2 ?1.3</cell><cell>? 1.5</cell><cell>48.4 ?3.4</cell><cell>? 4.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.8 ?0.3</cell><cell>? 0.1</cell><cell>58.2 ?0.9</cell><cell>? 5.7</cell></row></table><note>. Smoothing matters on different backbones. All variants employ self-training on GTA5?Cityscapes. MoPL: Momentum Network for target pseudo labels, MoFA: Momentum Network for target features in adversarial training</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>16.8 21.3 31.4 11.2 83.0 22.0 78.0 54.4 33.8 73.9 12.7 30.7 13.7 28.1 19.7 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5</figDesc><table><row><cell cols="2">Backbone: ResNet-101 (43M)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o Adaptation [47]</cell><cell>65.0</cell><cell>16.1</cell><cell>68.7</cell><cell cols="2">18.6 36.8</cell></row><row><cell>AdaptSeg [43]</cell><cell>86.5</cell><cell>25.9</cell><cell>79.8</cell><cell cols="2">22.1 41.4</cell></row><row><cell cols="2">Seg-Uncertainty [56] 90.4</cell><cell>31.2</cell><cell>85.1</cell><cell>36.9 25.6 37.5 48.8 48.5 85.3 34.8 81.1 64.4 36.8 86.3 34.9 52.2 1.7 29.0 44.6</cell><cell>50.3</cell></row><row><cell cols="2">MetaCorrection [15] 92.8</cell><cell>58.1</cell><cell>86.2</cell><cell>39.7 33.1 36.3 42.0 38.6 85.5 37.8 87.6 62.8 31.7 84.8 35.7 50.3 2.0 36.8 48.0</cell><cell>52.1</cell></row><row><cell>SAC [1]</cell><cell>90.4</cell><cell>53.9</cell><cell>86.6</cell><cell>42.4 27.3 45.1 48.5 42.7 87.4 40.1 86.1 67.5 29.7 88.5 49.1 54.6 9.8 26.6 45.3</cell><cell>53.8</cell></row><row><cell>Coarse-to-Fine [32]</cell><cell>92.5</cell><cell>58.3</cell><cell>86.5</cell><cell>27.4 28.8 38.1 46.7 42.5 85.4 38.4 91.8 66.4 37.0 87.8 40.7 52.4 44.6 41.7 59.0</cell><cell>56.1</cell></row><row><cell>ProDA [54]</cell><cell>87.8</cell><cell>56.0</cell><cell>79.7</cell><cell>46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4</cell><cell>57.5</cell></row><row><cell cols="2">Backbone: Swin-S ViT (50M)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o Adaptation</cell><cell>55.9</cell><cell>21.8</cell><cell>63.1</cell><cell>14.0 22.0 27.2 46.8 17.4 83.3 32.8 86.1 62.2 28.7 43.8 32.2 36.9 1.1 34.8 35.5</cell><cell>39.2</cell></row><row><cell>TransDA-S</cell><cell>92.9</cell><cell>59.1</cell><cell>88.2</cell><cell>42.5 32.0 47.6 57.6 39.2 89.6 42.0 94.1 74.3 45.3 91.4 54.0 58.0 44.4 48.3 51.4</cell><cell>60.6</cell></row><row><cell cols="2">Backbone: Swin-B ViT (88M)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o Adaptation</cell><cell>63.3</cell><cell>28.6</cell><cell>68.3</cell><cell>16.8 23.4 37.8 51.0 34.3 83.8 42.1 85.7 68.5 25.4 83.5 36.3 17.7 2.9 36.1 42.3</cell><cell>44.6</cell></row><row><cell>TransDA-B</cell><cell>94.7</cell><cell>64.2</cell><cell>89.2</cell><cell>48.1 45.8 50.1 60.2 40.8 90.4 50.2 93.7 76.7 47.6 92.5 56.8 60.1 47.6 49.6 55.4</cell><cell>63.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table below</head><label>below</label><figDesc>?1.3 51.6 ?2.3 56.5 ?2.0 59.3 ?0.7 53.1 ?0.4</figDesc><table><row><cell></cell><cell></cell><cell cols="4">shows TransDA-S mIoU on</cell></row><row><cell cols="6">GTA5?Cityscapes with different momentum values</cell></row><row><cell cols="2">(m in Eqn.(2)):</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>momentum m</cell><cell>0</cell><cell>0.9</cell><cell>0.99</cell><cell>0.999</cell><cell>0.9999</cell></row><row><cell>mIoU</cell><cell>54.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Adaptation 51.43.3 22.83.8 55.85.2 14.52.1 17.13.5 26.83.7 42.83.7 19.61.6 83.60.7 32.41.5 80.96.1 63.20.9 28.12.5 66.916.3 25.47.6 26.210.6 3.13.1 32.91.9 33.45.3 38.30.8 TransDA-S 90.32.9 55.84.1 85.92.3 41.51.7 26.82.9 46.81.1 57.20.2 40.11.6 90.10.3 46.32.5 93.80.2 74.30.3 44.21.2 91.40.2 49.52.9 56.34.2 38.610.6 44.64.8 52.40.9 59.30.7 Adaptation 72.67.0 23.63.7 72.33.5 16.93.3 24.12.5 36.64.2 49.41.2 30.64.8 82.91.5 34.15.7 84.42.8 68.61.4 29.12.8 74.79.3 31.13.7 20.06.1 8.85.9 37.62.0 40.11.6 44.10.5 TransDA-B 91.70.8 55.82.0 89.10.2 46.70.7 39.24.9 49.80.7 60.30.3 44.51.1 90.70.1 51.40.5 93.90.1 75.20.2 45.50.2 92.40.2 50.83.8 51.45.6 48.33.2 43.57.6 54.13.6 61.80.9 TransDA-B wcls 92.71.1 58.54.0 89.20.2 48.91.1 42.43.4 50.10.4 60.30.3 43.91.8 90.50.2 50.71.0 93.70.2 75.70.7 46.60.9 92.10.3 53.53.3 57.44.2 29.311.9 45.64.1 55.22.6 61.91.2 The averaged scores and standard deviations on GTA5?Cityscapes. wcls denotes employing the weighted class-level adversarial loss in Eq. 18 Adaptation 25.24.5 24.52.2 39.62.4 4.00.4 0.10.0 26.51.6 32.40.7 15.60.1 79.93.1 67.92.7 55.53.6 6.11.5 73.83.5 30.02.4 9.51.3 11.23.0 31.40.8 36.21.0 TransDA-S 82.11.8 43.62.3 83.22.7 18.94.5 1.10.4 49.32.4 56.41.9 29.93.8 89.90.5 92.51.4 65.81.7 20.43.5 91.00.2 57.25.5 41.00.7 48.32.4 54.40.7 61.50.4 Adaptation 43.610.7 30.34.6 56.42.9 11.14.6 0.30.1 36.82.3 34.33.3 20.82.4 82.21.9 78.23.0 63.60.7 13.11.5 67.715.8 36.23.6 20.52.3 21.01.3 38.50.5 43.71.1 TransDA-B 83.72.5 43.63.2 86.50.6 27.33.2 1.20.3 54.30.7 61.00.3 36.41.9 90.10.5 92.41.7 71.10.6 27.42.9 92.40.3 66.44.0 49.61.6 50.10.6 58.30.5 65.40.5 TransDA-B wcls 84.63.3 46.05.1 85.81.3 25.44.1 1.30.3 54.60.6 61.10.5 38.14.4 90.40.2 93.50.5 71.10.9 26.21.4 91.12.0 64.93.1 47.62.2 50.11.2 58.30.6 65.40.6The averaged scores and standard deviations on SYNTHIA?Cityscapes. mIoU and mIoU* denote the scores across 16 and 13 categories respectively. wcls denotes employing the weighted class-level adversarial loss in Eq. 18</figDesc><table><row><cell cols="2">The official Cityscapes benchmark server provides</cell></row><row><cell cols="2">anonymous link to our results on Cityscapes test set</cell></row><row><cell>for referencing in blind paper submissions.</cell><cell>Ta-</cell></row><row><cell cols="2">ble 7 shows the "Best" results on Cityscapes test for</cell></row><row><cell cols="2">GTA5?Cityscapes (TransDA-S 2 and TransDA-B 3 ) and</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.cityscapes-dataset.com 2 https://www.cityscapes-dataset.com/anonymous-results/?id= b6ea23f38aa214510e3b69e2780af1e02d44f9d3c741e05842ac0756fae5dc4e 3 https://www.cityscapes-dataset.com/anonymous-results/?id= 8dadcafb1549b277f5477246bde011c1ede443a84eecbf64ea261dfcbeec4d4f</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
	<note>Cotraining for domain adaptation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selfensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS 2021, 2021. 3</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sotr: Segmenting objects with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dantong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metacorrection: Domain-aware meta loss correction for unsupervised domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Demystifying local vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04263</idno>
	</analytic>
	<monogr>
		<title level="m">Sparse connectivity, weight sharing, and dynamic weight</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04906</idno>
		<title level="m">Dynamic neural networks: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<editor>Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova</editor>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12975" to="12984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic transfer for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coarseto-fine domain adaptive semantic segmentation with photometric alignment and category-center regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Better supervisory signals by observing learning paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangmin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Not all areas are equal: Transfer learning for semantic segmentation via hierarchical region selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4360" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Universal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
