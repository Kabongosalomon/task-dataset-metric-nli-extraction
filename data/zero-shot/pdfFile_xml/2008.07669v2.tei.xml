<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HiPPO: Recurrent Memory with Optimal Polynomial Projections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-26">October 26, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<email>albertgu@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<email>trid@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University at Buffalo</orgName>
								<address>
									<region>SUNY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HiPPO: Recurrent Memory with Optimal Polynomial Projections</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-26">October 26, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy. * Equal contribution. Order determined by coin flip.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling and learning from sequential data is a fundamental problem in modern machine learning, underlying tasks such as language modeling, speech recognition, video processing, and reinforcement learning. A core aspect of modeling long-term and complex temporal dependencies is memory, or storing and incorporating information from previous time steps. The challenge is learning a representation of the entire cumulative history using bounded storage, which must be updated online as more data is received.</p><p>One established approach is to model a state that evolves over time as it incorporates more information. The deep learning instantiation of this approach is the recurrent neural network (RNN), which is known to suffer from a limited memory horizon <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b56">56]</ref> (e.g., the "vanishing gradients" problem). Although various heuristics have been proposed to overcome this, such as gates in the successful LSTM and GRU <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">34]</ref>, or higher-order frequencies in the recent Fourier Recurrent Unit <ref type="bibr" target="#b79">[79]</ref> and Legendre Memory Unit (LMU) <ref type="bibr" target="#b71">[71]</ref>, a unified understanding of memory remains a challenge. Furthermore, existing methods generally require priors on the sequence length or timescale and are ineffective outside this range <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b71">71]</ref>; this can be problematic in settings with distribution shift (e.g. arising from different instrument sampling rates in medical data <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b63">63]</ref>). Finally, many of them lack theoretical guarantees on how well they capture long-term dependencies, such as gradient bounds. To design a better memory representation, we would ideally (i) have a unified view of these existing methods, (ii) be able to address dependencies of any length without priors on the timescale, and (iii) have a rigorous theoretical understanding of their memory mechanism.</p><p>Our insight is to phrase memory as a technical problem of online function approximation where a function f (t) : R + ? R is summarized by storing its optimal coefficients in terms of some basis functions. This approximation is evaluated with respect to a measure that specifies the importance of each time in the past. Given this function approximation formulation, orthogonal polynomials (OPs) emerge as a natural basis since their optimal coefficients can be expressed in closed form <ref type="bibr" target="#b13">[14]</ref>. With their rich and well-studied history <ref type="bibr" target="#b65">[65]</ref>, along with their widespread use in approximation theory <ref type="bibr" target="#b68">[68]</ref> and signal processing <ref type="bibr" target="#b57">[57]</ref>, OPs bring a library of techniques to this memory representation problem. We formalize a framework, HiPPO (high-order polynomial projection operators), which produces operators that project arbitrary functions onto the space of orthogonal polynomials with respect to a given measure. This general framework allows us to analyze several families of measures, where this operator, as a closed-form ODE or linear recurrence, allows fast incremental updating of the optimal polynomial approximation as the input function is revealed through time.</p><p>By posing a formal optimization problem underlying recurrent sequence models, the HiPPO framework (Section 2) generalizes and explains previous methods, unlocks new methods appropriate for sequential data at different timescales, and comes with several theoretical guarantees. (i) For example, with a short derivation we exactly recover as a special case the LMU <ref type="bibr" target="#b71">[71]</ref> (Section 2.3), which proposes an update rule that projects onto fixed-length sliding windows through time. <ref type="bibr" target="#b0">1</ref> HiPPO also sheds new light on classic techniques such as the gating mechanism of LSTMs and GRUs, which arise in one extreme using only low-order degrees in the approximation (Section 2.5). (ii) By choosing more suitable measures, HiPPO yields a novel mechanism (Scaled Legendre, or LegS) that always takes into account the function's full history instead of a sliding window. This flexibility removes the need for hyperparameters or priors on the sequence length, allowing LegS to generalize to different input timescales. (iii) The connections to dynamical systems and approximation theory allows us to show several theoretical benefits of HiPPO-LegS: invariance to input timescale, asymptotically more efficient updates, and bounds on gradient flow and approximation error (Section 3).</p><p>We integrate the HiPPO memory mechanisms into RNNs, and empirically show that they outperform baselines on standard tasks used to benchmark long-term dependencies. On the permuted MNIST dataset, our hyperparameter-free HiPPO-LegS method achieves a new state-of-the-art accuracy of 98.3%, beating the previous RNN SoTA by over 1 point and even outperforming models with global context such as transformers (Section 4.1). Next, we demonstrate the timescale robustness of HiPPO-LegS on a novel trajectory classification task, where it is able to generalize to unseen timescales and handle missing data whereas RNN and neural ODE baselines fail (Section 4.2). Finally, we validate HiPPO's theory, including computational efficiency and scalability, allowing fast and accurate online function reconstruction over millions of time steps (Section 4.3). Code for reproducing our experiments is available at https://github.com/HazyResearch/hippo-code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The HiPPO Framework: High-order Polynomial Projection Operators</head><p>We motivate the problem of online function approximation with projections as an approach to learning memory representations (Section 2.1). Section 2.2 describes the general HiPPO framework to derive memory updates, including a precise definition of the technical problem we introduce, and an overview of our approach to solving it. Section 2.3 instantiates the framework to recover the LMU and yield new memory updates (e.g. HiPPO-LagT), demonstrating the generality of the HiPPO framework. Section 2.4 discusses how to convert the main continuous-time results into practical discrete versions. Finally in Section 2.5 we show how gating in RNNs is an instance of HiPPO memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">HiPPO Problem Setup</head><p>Given an input function f (t) ? R on t ? 0, many problems require operating on the cumulative history f ?t := f (x) | x?t at every time t ? 0, in order to understand the inputs seen so far and make future predictions.</p><p>Since the space of functions is intractably large, the history cannot be perfectly memorized and must be compressed; we propose the general approach of projecting it onto a subspace of bounded dimension. Thus, our goal is to maintain (online) this compressed representation of the history. In order to specify this problem fully, we require two ingredients: a way to quantify the approximation, and a suitable subspace.</p><p>Function Approximation with respect to a Measure. Assessing the quality of an approximation requires defining a distance in function space. Any probability measure ? on [0, ?) equips the space of square integrable functions with inner product f, g ? = ? 0 f (x)g(x) d?(x), inducing a Hilbert space structure H ? and corresponding norm f L2(?) = f, f</p><formula xml:id="formula_0">1/2 ? .</formula><p>Polynomial Basis Expansion. Any N -dimensional subspace G of this function space is a suitable candidate for the approximation. The parameter N corresponds to the order of the approximation, or the size of the compression; the projected history can be represented by the N coefficients of its expansion in any basis of G. For the remainder of this paper, we use the polynomials as a natural basis, so that G is the set of polynomials of degree less than N . We note that the polynomial basis is very general; for example, the Fourier basis sin(nx), cos(nx) can be seen as polynomials on the unit circle (e 2?ix ) n (cf. Appendix D.4). In Appendix C, we additionally formalize a more general framework that allows different bases other than polynomials by tilting the measure with another function.</p><p>Online Approximation. Since we care about approximating f ?t for every time t, we also let the measure vary through time. For every t, let ? (t) be a measure supported on (??, t] (since f ?t is only defined up to time t). Overall, we seek some g (t) ? G that minimizes f ?t ? g (t)</p><p>L2(? (t) ) . Intuitively, the measure ? controls the importance of various parts of the input domain, and the basis defines the allowable approximations. The challenge is how to solve the optimization problem in closed form given ? <ref type="bibr">(t)</ref> , and how these coefficients can be maintained online as t ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">General HiPPO framework</head><p>We provide a brief overview of the main ideas behind solving this problem, which provides a surprisingly simple and general strategy for many measure families ? (t) . This framework builds upon a rich history of the well-studied orthogonal polynomials and related transforms in the signal processing literature. Our formal abstraction (Definition 1) departs from prior work on sliding transforms in several ways, which we discuss in detail in Appendix A.1. For example, our concept of the time-varying measure allows choosing ? (t) more appropriately, which will lead to solutions with qualitatively different behavior. Appendix C contains the full details and formalisms of our framework.</p><p>Calculating the projection through continuous dynamics. As mentioned, the approximated function can be represented by the N coefficients of its expansion in any basis; the first key step is to choose a suitable basis {g n } n&lt;N of G. Leveraging classic techniques from approximation theory, a natural basis is the set of orthogonal polynomials for the measure ? (t) , which forms an orthogonal basis of the subspace. Then the coefficients of the optimal basis expansion are simply c (t) n := f ?t , g n ? (t) . The second key idea is to differentiate this projection in t, where differentiating through the integral (from the inner product f ?t , g n ? (t) ) will often lead to a self-similar relation allowing d dt c n (t) to be expressed in terms of (c k (t)) k?[N ] and f (t). Thus the coefficients c(t) ? R N should evolve as an ODE, with dynamics determined by f (t).</p><p>The HiPPO abstraction: online function approximation. Definition 1. Given a time-varying measure family ? (t) supported on (??, t], an N -dimensional subspace G of polynomials, and a continuous function f : R ?0 ? R, HiPPO defines a projection operator proj t and a coefficient extraction operator coef t at every time t, with the following properties: (1) proj t takes the function f restricted up to time t, f ?t := f (x) | x?t , and maps it to a polynomial g (t) ? G, that minimizes the approximation error f ?t ? g (t) L2(? (t) ) . (2) coef t : G ? R N maps the polynomial g (t) to the coefficients c(t) ? R N of the basis of orthogonal polynomials defined with respect to the measure ? (t) .</p><formula xml:id="formula_1">(t) 0 ! " Time ($ ! ) ($ " ) (t) ($ ! ) ($ " ) 0 1 3 2 4 5 6 " &amp; ' ( ) Time ( ! ) = 0.1 ?1.1 3.7 2.5 ( " ) = 1.5 2.9 ?0.3 2.0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous-time HiPPO ODE</head><formula xml:id="formula_2">= + ( ) Discrete-time HiPPO Recurrence !"# = ! ! + ! ! proj t coef t discretize (1)<label>(2)</label></formula><p>(3) (4) <ref type="figure">Figure 1</ref>: Illustration of the HiPPO framework. (1) For any function f , (2) at every time t there is an optimal projection g (t) of f onto the space of polynomials, with respect to a measure ? (t) weighing the past. The composition coef ? proj is called hippo, which is an operator mapping a function f : R ?0 ? R to the optimal projection coefficients c :</p><formula xml:id="formula_3">R ?0 ? R N , i.e. (hippo(f ))(t) = coef t (proj t (f )).</formula><p>For each t, the problem of optimal projection proj t (f ) is well-defined by the above inner products, but this is intractable to compute naively. Our derivations (Appendix D) will show that the coefficient function</p><formula xml:id="formula_4">c(t) = coef t (proj t (f )) has the form of an ODE satisfying d dt c(t) = A(t)c(t) + B(t)f (t) for some A(t) ? R N ?N , B(t) ? R N ?1 .</formula><p>Thus our results show how to tractably obtain c (t) online by solving an ODE, or more concretely by running a discrete recurrence. When discretized, HiPPO takes in a sequence of real values and produces a sequence of N -dimensional vectors. <ref type="figure">Figure 1</ref> illustrates the overall framework when we use uniform measures. Next, we give our main results showing hippo for several concrete instantiations of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">High Order Projection: Measure Families and HiPPO ODEs</head><p>Our main theoretical results are instantiations of HiPPO for various measure families ? (t) . We provide two examples of natural sliding window measures and the corresponding projection operators. The unified perspective on memory mechanisms allows us to derive these closed-form solutions with the same strategy, provided in Appendices D.1,D.2. The first explains the core Legendre Memory Unit (LMU) <ref type="bibr" target="#b71">[71]</ref> update in a principled way and characterizes its limitations, while the other is novel, demonstrating the generality of the HiPPO framework. Appendix D contrasts the tradeoffs of these measures ( <ref type="figure" target="#fig_10">Fig. 5</ref>), contains proofs of their derivations, and derives additional HiPPO formulas for other bases such as Fourier (recovering the Fourier Recurrent Unit <ref type="bibr" target="#b79">[79]</ref>) and Chebyshev.</p><p>The translated Legendre (LegT) measures assign uniform weight to the most recent history [t ? ?, t]. There is a hyperparameter ? representing the length of the sliding window, or the length of history that is being summarized. The translated Laguerre (LagT) measures instead use the exponentially decaying measure, assigning more importance to recent history.</p><p>LegT : </p><formula xml:id="formula_5">? (t) (x) = 1 ? I [t??,t] (x) LagT : ? (t) (x) = e ?(t?x) I (??,t] (x) = e x?t if x ? t 0 if x &gt; t<label>Theorem</label></formula><formula xml:id="formula_6">c(t) = ?Ac(t) + Bf (t), where A ? R N ?N , B ? R N ?1 :</formula><p>LegT:</p><formula xml:id="formula_7">A nk = 1 ? (?1) n?k (2n + 1) if n ? k 2n + 1 if n ? k , Bn = 1 ? (2n + 1)(?1) n (1)</formula><p>LagT:</p><formula xml:id="formula_8">A nk = 1 if n ? k 0 if n &lt; k , Bn = 1 (2)</formula><p>Equation <ref type="formula">(1)</ref> proves the LMU update [71, equation <ref type="formula">(1)</ref>]. Additionally, our derivation (Appendix D.1) shows that outside of the projections, there is another source of approximation. This sliding window update rule requires access to f (t ? ?), which is no longer available; it instead assumes that the current coefficients c(t) are an accurate enough model of the function f (x) x?t that f (t ? ?) can be recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">HiPPO recurrences: from Continuous to Discrete Time with ODE Discretization</head><p>Since actual data is inherently discrete (e.g. sequences and time series), we discuss how the HiPPO projection operators can be discretized using standard techniques, so that the continuous-time HiPPO ODEs become discrete-time linear recurrences. In the continuous case, these operators consume an input function f (t) and produce an output function c(t). The discrete time case (i) consumes an input sequence (f k ) k?N , (ii) implicitly defines a function f (t) where f (k ? ?t) = f k for some step size ?t, (iii) produces a function c(t) through the ODE dynamics, and (iv) discretizes back to an output sequence c k := c(k ? ?t).</p><p>The basic method of discretizating an ODE d dt c(t) = u(t, c(t), f (t)) chooses a step size ?t and performs the discrete updates c(t + ?t) = c(t) + ?t ? u(t, c(t), f (t)). <ref type="bibr" target="#b1">2</ref> In general, this process is sensitive to the discretization step size hyperparameter ?t.</p><p>Finally, we note that this provides a way to seamlessly handle timestamped data, even with missing values: the difference between timestamps indicates the (adaptive) ?t to use in discretization <ref type="bibr" target="#b12">[13]</ref>. Appendix B.3 contains a full discussion of discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Low Order Projection: Memory Mechanisms of Gated RNNs</head><p>As a special case, we consider what happens if we do not incorporate higher-order polynomials in the projection problem. Specifically, if N = 1, then the discretized version of HiPPO-LagT (2) becomes c(t + ?t) = c(t) + ?t(?Ac(t) + Bf (t)) = (1 ? ?t)c(t) + ?tf (t), since A = B = 1. If the inputs f (t) can depend on the hidden state c(t) and the discretization step size ?t is chosen adaptively (as a function of input f (t) and state c(t)), as in RNNs, then this becomes exactly a gated RNN. For instance, by stacking multiple units in parallel and choosing a specific update function, we obtain the GRU update cell as a special case. <ref type="bibr" target="#b2">3</ref> In contrast to HiPPO which uses one hidden feature and projects it onto high order polynomials, these models use many hidden features but only project them with degree 1. This view sheds light on these classic techniques by showing how they can be derived from first principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HiPPO-LegS: Scaled Measures for Timescale Robustness</head><p>Exposing the tight connection between online function approximation and memory allows us to produce memory mechanisms with better theoretical properties, simply by choosing the measure appropriately. Although sliding windows are common in signal processing (Appendix A.1), a more intuitive approach for memory should scale the window over time to avoid forgetting.</p><p>Our novel scaled Legendre measure (LegS) assigns uniform weight to all history [0, t]: <ref type="figure" target="#fig_10">Fig. 5</ref>  </p><formula xml:id="formula_9">? (t) = 1 t I [0,t] . App D,</formula><formula xml:id="formula_10">d dt c(t) = ? 1 t Ac(t) + 1 t Bf (t) (3) c k+1 = 1 ? A k c k + 1 k Bf k (4) A nk = ? ? ? ? ? (2n + 1) 1/2 (2k + 1) 1/2 if n &gt; k n + 1 if n = k 0</formula><p>if n &lt; k , Bn = (2n + 1) <ref type="bibr">1 2</ref> We show that HiPPO-LegS enjoys favorable theoretical properties: it is invariant to input timescale, is fast to compute, and has bounded gradients and approximation error. All proofs are in Appendix E.</p><p>Timescale robustness. As the window size of LegS is adaptive, projection onto this measure is intuitively robust to timescales. Formally, the HiPPO-LegS operator is timescale-equivariant: dilating the input f does not change the approximation coefficients. Informally, this is reflected by HiPPO-LegS having no timescale hyperparameters; in particular, the discrete recurrence (4) is invariant to the discretization step size. <ref type="bibr" target="#b3">4</ref> By contrast, LegT has a hyperparameter ? for the window size, and both LegT and LagT have a step size hyperparameter ?t in the discrete time case. This hyperparameter is important in practice; Section 2.5 showed that ?t relates to the gates of RNNs, which are known to be sensitive to their parameterization <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b66">66]</ref>. We empirically demonstrate the benefits of timescale robustness in Section 4.2.</p><p>Computational efficiency. In order to compute a single step of the discrete HiPPO update, the main operation is multiplication by the (discretized) square matrix A. More general discretization specifically requires fast multiplication for any matrix of the form I + ?t ? A and (I ? ?t ? A) ?1 for arbitrary step sizes ?t. Although this is generically a O(N 2 ) operation, LegS operators use a fixed A matrix with special structure that turns out to have fast multiplication algorithms for any discretization. 5  Gradient flow. Much effort has been spent to alleviate the vanishing gradient problem in RNNs <ref type="bibr" target="#b56">[56]</ref>, where backpropagation-based learning is hindered by gradient magnitudes decaying exponentially in time. As LegS is designed for memory, it avoids the vanishing gradient issue.</p><p>Proposition 5. For any times t 0 &lt; t 1 , the gradient norm of HiPPO-LegS operator for the output at time t 1 with respect to input at time t 0 is ?c(t1) ?f (t0) = ? (1/t 1 ).</p><p>Approximation error bounds. The error rate of LegS decreases with the smoothness of the input.</p><p>Proposition 6. Let f : R + ? R be a differentiable function, and let g (t) = proj t (f ) be its projection at time t by HiPPO-LegS with maximum polynomial degree</p><formula xml:id="formula_11">N ? 1. If f is L-Lipschitz then f ?t ? g (t) = O(tL/ ? N ). If f has order-k bounded derivatives then f ?t ? g (t) = O(t k N ?k+1/2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Validation</head><p>The HiPPO dynamics are simple recurrences that can be easily incorporated into various models. We validate three claims that suggest that when incorporated into a simple RNN, these methods-especially HiPPO-LegS-yield a recurrent architecture with improved memory capability. In Section 4.1, the HiPPO-LegS RNN outperforms other RNN approaches in benchmark long-term dependency tasks for RNNs. Section 4.2 shows that HiPPO-LegS RNN is much more robust to timescale shifts compared to other RNN and neural ODE models. Section 4.3 validates the distinct theoretical advantages of the HiPPO-LegS memory mechanism, allowing fast and accurate online function reconstruction over millions of time steps. Experiment details and additional results are described in Appendix F.</p><p>Model Architectures. We first describe briefly how HiPPO memory updates can be incorporated into a simple neural network architecture, yielding a simple RNN model reminiscent of the classic LSTM. Given inputs x t or features thereof f t = u(x t ) in any model, the HiPPO framework can be used to memorize the history of features f t . Thus, given any RNN update function h t = ? (h t?1 , x t ), we simply replace h t?1 with a projected version of the entire history of h, as described in <ref type="figure" target="#fig_4">Figure 2</ref>. The output of each cell is h t , which can be passed through any downstream module (e.g. a classification head trained with cross-entropy) to produce predictions.</p><p>We map the vector h t?1 to 1D with a learned encoding before passing to hippo (full architecture in App. F.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Long-range Memory Benchmark Tasks</head><p>Models and Baselines. We consider all of the HiPPO methods (LegT, LagT, and LegS). As we show that many different update dynamics seem to lead to LTI systems that give sensible results (Section 2.3), we additionally consider the Rand baseline that uses random A and B matrices (normalized appropriately) in its updates, to confirm that the precise derived dynamics are important. LegT additionally considers an additional hyperparameter ?, which should be set to the timescale of the data if known a priori; to show the effect of the timescale, we set it to the ideal value as well as values that are too large and small. The MGU is a minimal gated architecture, equivalent to a GRU without the reset gate. The HiPPO architecture we use is simply the MGU with an additional hippo intermediate layer.</p><p>We also compare to several RNN baselines designed for long-term dependencies, including the LSTM <ref type="bibr" target="#b34">[34]</ref>, GRU <ref type="bibr" target="#b16">[17]</ref>, expRNN <ref type="bibr" target="#b48">[48]</ref>, and LMU <ref type="bibr" target="#b71">[71]</ref>. <ref type="bibr" target="#b5">6</ref> All methods have the same hidden size in our experiments. In particular, for simplicity and to reduce hyperparameters, HiPPO variants tie the memory size N to the hidden state dimension d, so that all methods and baselines have a comparable number of hidden units and parameters. A more detailed comparison of model architectures is in Appendix F.1.</p><p>Sequential Image Classification on Permuted MNIST. The permuted MNIST (pMNIST) task feeds inputs to a model pixel-by-pixel in the order of a fixed permutation. The model must process the entire image sequentially -with non-local structure -before outputting a classification label, requiring learning long-term dependencies. <ref type="table" target="#tab_3">Table 1</ref> shows the validation accuracy on the pMNIST task for the instantiations of our framework and baselines. We highlight that LegS has the best performance of all models. While LegT is close at the optimal hyperparameter ?, its performance can fall off drastically for a mis-specified window length. LagT also performs well at its best hyperparameter ?t. <ref type="table" target="#tab_3">Table 1</ref> also compares test accuracy of our methods against reported results from the literature, where the LMU was the state-of-the-art for recurrent models. In addition to RNN-based baselines, other sequence models have been evaluated on this dataset, despite being against the spirit of the task because they have global receptive field instead of being strictly sequential. With a test accuracy of 98.3%, HiPPO-LegS sets a true state-of-the-art accuracy on the permuted MNIST dataset.   <ref type="bibr" target="#b49">[49]</ref> 96.0 URLSTM <ref type="bibr" target="#b31">[31]</ref> 96.96 LMU <ref type="bibr" target="#b71">[71]</ref> 97.15</p><p>Transformer <ref type="bibr" target="#b69">[69]</ref> 97.9 TCN <ref type="bibr" target="#b4">[5]</ref> 97.2 TrellisNet <ref type="bibr" target="#b5">[6]</ref> 98.13 Copying task. This standard RNN task <ref type="bibr" target="#b2">[3]</ref> directly tests memorization, where models must regurgitate a sequence of tokens seen at the beginning of the sequence. It is well-known that standard models such as LSTMs struggle to solve this task. Appendix F shows the loss for the Copying task with length L = 200. Our proposed update LegS solves the task almost perfectly, while LegT is very sensitive to the window length hyperparameter. As expected, most baselines make little progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Timescale Robustness of HiPPO-LegS</head><p>Timescale priors. Sequence models generally benefit from priors on the timescale, which take the form of additional hyperparameters in standard models. Examples include the "forget bias" of LSTMs which needs to be modified to address long-term dependencies <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b66">66]</ref>, or the discretization step size ?t of HiPPO-Lag and HiPPO-LegT (Section 2.4). The experiments in Section 4.1 confirm their importance. <ref type="figure">Fig. 7</ref> (Appendix) and <ref type="table" target="#tab_3">Table 1</ref> ablate these hyperparameters, showing that for example the sliding window length ? must be set correctly for LegT. Additional ablations for other hyperparameters are in Appendix F.</p><p>Distribution shift in trajectory classification. Recent trends in ML have stressed the importance of understanding robustness under distribution shift, when training and testing distributions are not i.i.d. For time series data, for example, models may be trained on EEG data from one hospital, but deployed at another using instruments with different sampling rates <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b63">63]</ref>; or a time series may involve the same trajectory evolving at different speeds. Following Kidger et al. <ref type="bibr" target="#b40">[40]</ref>, we consider the Character Trajectories dataset <ref type="bibr" target="#b3">[4]</ref>, where the goal is to classify a character from a sequence of pen stroke measurements, collected from one user at a fixed sampling rate. To emulate timescale shift (e.g. testing on another user with slower handwriting), we consider two standard time series generation processes: (1) In the setting of sampling an underlying sequence at a fixed rate, we change the test sampling rate; crucially, the sequences are variable length so the models are unable to detect the sampling rate of the data. (2) In the setting of irregular-sampled (or missing) data with timestamps, we scale the test timestamps.</p><p>Recall that the HiPPO framework models the underlying data as a continuous function and interacts with discrete input only through the discretization. Thus, it seamlessly handles missing or irregularly-sampled data by simply evolving according to the given discretization step sizes (details in Appendix B.3). Combined with LegS timescale invariance (Prop. 3), we expect HiPPO-LegS to work automatically in all these settings. We note that the setting of missing data is a topic of independent interest and we compare against SOTA methods, including the GRU-D <ref type="bibr" target="#b10">[11]</ref> which learns a decay between observations, and neural ODE methods which models segments between observations with an ODE. <ref type="table" target="#tab_4">Table 2</ref> validates that standard models can go catastrophically wrong when tested on sequences at different timescales than expected. Though all methods achieve near-perfect accuracy (? 95%) without distribution shift, aside from HiPPO-LegS, no method is able to generalize to unseen timescales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Theoretical Validation and Scalability</head><p>We empirically show that HiPPO-LegS can scale to capture dependencies across millions of time steps, and its memory updates are computationally efficient (processing up to 470,000 time steps/s).</p><p>Long-range function approximation. We test the ability of different memory mechanisms in approximating an input function, as described in the problem setup in Section 2.1. The model only consists of the memory update (Section 3) and not the additional RNN architecture. We choose random samples from a continuous-time band-limited white noise process, with length 10 6 . The model is to traverse the input sequence, and then asked to reconstruct the input, while maintaining no more than 256 units in memory ( <ref type="figure" target="#fig_1">Fig. 3</ref>). This is a difficult task; the LSTM fails with even sequences of length 1000 (MSE ? 0.25). As shown in <ref type="table" target="#tab_6">Table 3</ref>, both the LMU and HiPPO-LegS are able to accurately reconstruct the input function, validating that HiPPO can solve the function approximation problem even for very long sequences. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates the function and its approximations, with HiPPO-LegS almost matching the input function while LSTM unable to do so.</p><p>Speed. HiPPO-LegS operator is computationally efficient both in theory (Section 3) and in practice. We implement the fast update in C++ with Pytorch binding and show in <ref type="table" target="#tab_6">Table 3</ref> that it can perform 470,000 time step updates per second on a single CPU core, 10x faster than the LSTM and LMU. 7    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Additional Experiments</head><p>We validate that the HiPPO memory updates also perform well on more generic sequence prediction tasks not exclusively focused on memory. Full results and details for these tasks are in Appendix F.</p><p>Sentiment classification task on the IMDB movie review dataset. Our RNNs with HiPPO memory updates perform on par with the LSTM, while other long-range memory approaches such as expRNN perform poorly on this more generic task (Appendix F.6).</p><p>Mackey spin glass prediction. This physical simulation task tests the ability to model chaotic dynamical systems. HiPPO-LegS outperforms the LSTM, LMU, and the best hybrid LSTM+LMU model from <ref type="bibr" target="#b71">[71]</ref>, reducing normalized MSE by 30% (Appendix F.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We address the fundamental problem of memory in sequential data by proposing a framework (HiPPO) that poses the abstraction of optimal function approximation with respect to time-varying measures. In addition to unifying and explaining existing memory approaches, HiPPO unlocks a new method (HiPPO-LegS) that takes a first step toward timescale robustness and can efficiently handle dependencies across millions of time steps. We anticipate that the study of this core problem will be useful in improving a variety of sequence models, and are excited about future work on integrating our memory mechanisms with other models in addition to RNNs. We hope to realize the benefits of long-range memory on large-scale tasks such as speech recognition, video processing, and reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>Our work touches on a variety of topics and related work, which we explore in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Signal Processing and Orthogonal Polynomials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Sliding transforms</head><p>The technical contributions in this work build on a rich history of approximation theory in signal processing. Our main framework -orthogonalizing functions with respect to time-varying measures (Section 2) -are related to "online" versions of classical signal processing transforms. In short, these methods compute specific transforms on sliding windows of discrete sequences. Concretely, they calculate c n,k =</p><formula xml:id="formula_12">N ?1 i=0 f k+i ?(i, n)</formula><p>given signal (f k ), where {?(i, n)} is a discrete orthogonal transform. Our technical problem differs in several key aspects:</p><p>Specific discrete transforms Examples of sliding transforms considered in the literature include the sliding DFT <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>, sliding DCT <ref type="bibr" target="#b43">[43]</ref>, sliding discrete (Walsh-)Hadamard transform <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b75">75]</ref>, Haar <ref type="bibr" target="#b51">[51]</ref>, sliding discrete Hartley transform <ref type="bibr" target="#b44">[44]</ref>, and sliding discrete Chebyshev moments <ref type="bibr" target="#b11">[12]</ref>. While each of these address a specific transform, we present a general approach (Section 2) that addresses several transforms at once. Furthermore, we are unaware of sliding transform algorithms for the OPs we consider here, in particular the Legendre and Laguerre polynomials. Our derivations in Appendix D cover Legendre, (generalized) Laguerre, Fourier, and Chebyshev continuous sliding transforms.</p><p>Fixed-length sliding windows All mentioned works operate in the sliding window setting, where a fixedsize context window on the discrete signal is taken into account. Our measure-based abstraction for approximation allows considering a new type of scaled measure where the window size increases over time, leading to methods with qualitatively different theoretical (Section 3) and empirical properties (Section 4.2). We are not aware of any previous works addressing this scaled setting.</p><p>Discrete vs. continuous time Even in the fixed-length sliding window case, our solutions to the "translated measure" problems (e.g., HiPPO-LegT Appendix D.1) solve a continuous-time sliding window problem on an underlying continuous signal, then discretize.</p><p>On the other hand, the sliding transform problems calculate transforms directly on a discrete stream. Discrete transforms are equivalent to calculating projection coefficients on a measure (equation <ref type="formula" target="#formula_21">(18)</ref>) by Gaussian quadrature, which assumes the discrete input is subsampled from a signal at the quadrature nodes <ref type="bibr" target="#b13">[14]</ref>. However, since these nodes are non-uniformly spaced in general, the sliding discrete transform is not consistent with a discretization of an underlying continuous signal.</p><p>Thus, our main abstraction (Definition 1) has a fundamentally different interpretation than standard transforms, and our approach of first calculating the dynamics of the underlying continuous-time problem (e.g. equation <ref type="formula" target="#formula_2">(20)</ref>) is correspondingly new.</p><p>We remark that our novel scaled measures are fundamentally difficult to address with a standard discrete-time based approach. These discrete sliding methods require a fixed-size context in order to have consistent transform sizes, while the scaled measure would require solving transforms with an increasing number of input points over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 OPs in ML</head><p>More broadly, orthogonal polynomials and orthogonal polynomial transforms have recently found applications in various facets of machine learning. For example, Dao et al. <ref type="bibr" target="#b18">[19]</ref> leverage the connection between orthogonal polynomials and quadrature to derive rules for computing kernel features in machine learning. More directly, <ref type="bibr" target="#b67">[67]</ref> apply parametrized families of structured matrices directly inspired by orthogonal polynomial transforms ( <ref type="bibr" target="#b21">[22]</ref>) as layers in neural networks. Some particular families of orthogonal polynomials such as the Chebyshev polynomials have desirable approximation properties that find many well-known classical uses in numerical analysis and optimization. More recently, they have been applied to ML models such as graph convolutional neural networks <ref type="bibr" target="#b23">[24]</ref>, and generalizations such as Gegenbauer and Jacobi polynomials have been used to analyze optimization dynamics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b76">76]</ref>. Generalization of orthogonal polynomials and Fourier transform, expressed as products of butterfly matrices, have found applications in automatic algorithm design <ref type="bibr" target="#b19">[20]</ref>, model compression <ref type="bibr" target="#b0">[1]</ref>, and replacing hand-crafted preprocessing in speech recognition <ref type="bibr" target="#b20">[21]</ref>. Orthogonal polynomials are known to have various efficiency results <ref type="bibr" target="#b21">[22]</ref>, and we conjecture that Proposition 4 on the efficiency of HiPPO methods can be extended to arbitrary measures besides the ones considered in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Memory in Machine Learning</head><p>Memory in sequence models Sequential or temporal data in areas such as language, reinforcement learning, and continual learning can involve increasingly long dependencies. However, direct parametric modeling cannot handle inputs of unknown and potentially unbounded lengths. Many modern solutions such as attention <ref type="bibr" target="#b70">[70]</ref> and dilated convolutions <ref type="bibr" target="#b4">[5]</ref>, are functions on finite windows, thus sidestepping the need for an explicit memory representation. While this suffices for certain tasks, these approaches can only process a finite context window instead of an entire sequence. Naively increasing the window length poses significant compute and memory challenges. This has spurred various approaches to extend this fixed context window subjected to compute and storage constraints <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b74">74]</ref>. We instead focus on the core problem of online processing and memorization of continuous and discrete signals, and anticipate that the study of this foundational problem will be useful in improving a variety of models.</p><p>Recurrent memory Recurrent neural networks are a natural tool for modeling sequential data online, with the appealing property of having unbounded context; in other words they can summarize history indefinitely. However, due to difficulties in the optimization process (vanishing/exploding gradients <ref type="bibr" target="#b56">[56]</ref>), particular care must be paid to endow them with longer memory. The ubiquitous LSTM <ref type="bibr" target="#b34">[34]</ref> and simplifications such as the GRU <ref type="bibr" target="#b16">[17]</ref> control the update with gates to smooth the optimization process. With more careful parametrization, the addition of gates alone make RNNs significantly more robust and able to address long-term dependencies <ref type="bibr" target="#b31">[31]</ref>. Tallec and Ollivier <ref type="bibr" target="#b66">[66]</ref> show that gates are in fact fundamental for recurrent dynamics by allowing time dilations. Many other approaches to endowing RNNs with better memory exist, such as noise injection <ref type="bibr" target="#b32">[32]</ref> or non-saturating gates <ref type="bibr" target="#b8">[9]</ref>, which can suffer from instability issues. A long line of work controls the spectrum of the recurrent updates with (nearly-) orthogonal matrices to control gradients <ref type="bibr" target="#b2">[3]</ref>, but have been found to be less robust across different tasks <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Directly related methods</head><p>LMU The main result of the Legendre Memory Unit <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73]</ref> is a direct instantiation of our framework using the LegT measure (Section 2.3). The original LMU is motivated by neurobiological advances and approaches the problem from the opposite direction as us: it considers approximating spiking neurons in the frequency domain, while we directly solve an interpretable optimization problem in the time domain. More specifically, they consider time-lagged linear time invariant (LTI) dynamical systems and approximate the dynamics with Pad? approximants; Voelker et al. <ref type="bibr" target="#b71">[71]</ref> observes that the result also has an interpretation in terms of Legendre polynomials, but not that it is the optimal solution to a natural projection problem. This approach involves heavier machinery, and we were not able to find a complete proof of the update mechanism <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73]</ref>.</p><p>In contrast, our approach directly poses the relevant online signal approximation problem, which ties to orthogonal polynomial families and leads to simple derivations of several related memory mechanisms (Appendix D). Our interpretation in time rather than frequency space, and associated derivation (Appendix D.1) for the LegT measure, reveals a different set of approximations stemming from the sliding window, which is confirmed empirically (Appendix F.8).</p><p>As the motivations of our work are substantially different from Voelker et al. <ref type="bibr" target="#b71">[71]</ref>, yet finds the same memory mechanism in a special case, we highlight the potential connection between these sequence models and biological nervous systems as an area of exploration for future work, such as alternative interpretations of our methods in the frequency domain.</p><p>We remark that the term LMU in fact refers to a specific recurrent neural network architecture, which interleaves the projection operator with other specific neural network components. By contrast, we use HiPPO to refer to the projection operator in isolation (Theorem 1), which is a function-to-function or sequence-to-sequence operator independent of model. HiPPO is integrated into an RNN architecture in Section 4, with slight improvements to the LMU architecture, as ablated in Appendices F.2 and F.3. As a standalone module, HiPPO can be used as a layer in other types of models.</p><p>Fourier Recurrent Unit The Fourier Recurrent Unit (FRU) <ref type="bibr" target="#b79">[79]</ref> uses Fourier basis (cosine and sine) to express the input signal, motivated by the discrete Fourier transform. In particular, each recurrent unit computes the discrete Fourier transform of the input signal for a randomly chosen frequency. It is not clear how discrete transform with respect to other bases (e.g., Legendre, Laguerre, Chebyshev) can in turn yield similar memory mechanisms. We show that FRU is also an instantiation of the HiPPO framework (Appendix D.4), where the Fourier basis can be viewed as orthogonal polynomials z n on the unit circle {z : |z| = 1}.</p><p>Zhang et al. <ref type="bibr" target="#b79">[79]</ref> prove that if a timescale hyperparameter is chosen appropriately, FRU has bounded gradients, thus avoiding vanishing and exploding gradients. This essentially follows from the fact that</p><formula xml:id="formula_13">(1 ? ?t) T = ?(1) if the discretization step size ?t = ?( 1 T ) is chosen, if the time horizon T is known (cf. Appendices B.</formula><p>3 and E). It is easily shown that this property is not intrinsic to the FRU but to sliding window methods, and is shared by all of our translated measure HiPPO methods (all but HiPPO-LegS in Appendix D). We show the stronger property that HiPPO-LegS, which uses scaling rather than sliding windows, also enjoys bounded gradient guarantees, without needing a well-specified timescale hyperparameter (Proposition 5).</p><p>Neural ODEs HiPPO produces linear ODEs that describe the dynamics of the coefficients. Recent work has also incorporated ODEs into machine learning models. Chen et al. <ref type="bibr" target="#b12">[13]</ref> introduce neural ODEs, employing general nonlinear ODEs parameterized by neural networks in the context of normalizing flows and time series modeling. Neural ODEs have shown promising results in modeling irregularly sampled time series <ref type="bibr" target="#b40">[40]</ref>, especially when combined with RNNs <ref type="bibr" target="#b61">[61]</ref>. Though neural ODEs are expressive <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b78">78]</ref>, due to their complex parameterization, they often suffer from slow training <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b58">58]</ref> because of their need for more complicated ODE solvers. On the other hand, HiPPO ODEs are linear and are fast to solve with classical discretization techniques in linear systems, such as Euler method, Bilinear method, and Zero-Order Hold (ZOH) <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Technical Preliminaries</head><p>We collect here some technical background that will be used in presenting the general HiPPO framework and in deriving specific HiPPO update rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Orthogonal Polynomials</head><p>Orthogonal polynomials are a standard tool for working with function spaces <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b65">65]</ref>. Every measure ? induces a unique (up to a scalar) sequence of orthogonal polynomials (OPs) P 0 (x), P 1 (x), . . . satisfying deg(P i ) = i and P i , P j ? := P i (x)P j (x) d?(x) = 0 for all i = j. This is the sequence found by orthogonalizing the monomial basis {x i } with Gram-Schmidt with respect to ?, ? ? . The fact that OPs form an orthogonal basis is useful because the optimal polynomial g of degree deg(g) &lt; N that approximates a function f is then given by</p><formula xml:id="formula_14">N ?1 i=0 c i P i (x)/ P i 2 ? where c i = f, P i ? = f (x)P i (x) d?(x).</formula><p>Classical OPs families comprise Jacobi (which include Legendre and Chebyshev polynomials as special cases), Laguerre, and Hermite polynomials. The Fourier basis can also be interpreted as OPs on the unit circle in the complex plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Properties of Legendre Polynomials</head><p>Legendre polynomials Under the usual definition of the canonical Legendre polynomial P n , they are orthogonal with respect to the measure ? leg = 1 [?1,1] :</p><formula xml:id="formula_15">2n + 1 2 1 ?1 P n (x)P m (x) dx = ? nm<label>(5)</label></formula><p>Also, they satisfy P n (1) = 1 P n (?1) = (?1) n .</p><p>Shifted and Scaled Legendre polynomials We will also consider scaling the Legendre polynomials to be orthogonal on the interval [0, t]. A change of variables on (5) yields</p><formula xml:id="formula_16">(2n + 1) t 0 P n 2x t ? 1 P m 2x t ? 1 1 t dx = (2n + 1) P n 2x t ? 1 P m 2x t ? 1 ? leg 2x t ? 1 1 t dx = 2n + 1 2 P n (x)P m (x)? leg (x) dx = ? nm .</formula><p>Therefore, with respect to the measure ? t = 1 [0,t] /t (which is a probability measure for all t), the normalized orthogonal polynomials are</p><formula xml:id="formula_17">(2n + 1) 1/2 P n 2x t ? 1 .</formula><p>Similarly, the basis</p><formula xml:id="formula_18">(2n + 1) 1/2 P n 2 x ? t ? + 1</formula><p>is orthonormal for the uniform measure 1 ? I [t??,t] . In general, the orthonormal basis for any uniform measure consists of (2n + 1) 1 2 times the corresponding linearly shifted version of P n .</p><p>Derivatives of Legendre polynomials We note the following recurrence relations on Legendre polynomials ([2, Chapter 12]):</p><p>(2n + 1)P n = P n+1 ? P n?1 P n+1 = (n + 1)P n + xP n</p><p>The first equation yields</p><formula xml:id="formula_19">P n+1 = (2n + 1)P n + (2n ? 3)P n?2 + . . . ,<label>(6)</label></formula><p>where the sum stops at P 0 or P 1 . These equations directly imply P n = (2n ? 1)P n?1 + (2n ? 5)P n?3 + . . .</p><p>and (x + 1)P n (x) = P n+1 + P n ? (n + 1)P n = nP n + (2n ? 1)P n?1 + (2n ? 3)P n?2 + . . . .</p><p>These will be used in the derivations of the HiPPO-LegT and HiPPO-LegS updates, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Properties of Laguerre Polynomials</head><p>The standard Laguerre polynomials L n (x) are defined to be orthogonal with respect to the weight function e ?x supported on [0, ?), while the generalized Laguerre polynomials (also called associated Laguerre polynomials) L (?)</p><p>n are defined to be orthogonal with respect to the weight function x ? e ?x also supported on [0, ?):</p><formula xml:id="formula_22">? 0 x ? e ?x L (?) n (x)L (?) m (x) dx = (n + ?)! n! ? n,m .<label>(9)</label></formula><p>Also, they satisfy</p><formula xml:id="formula_23">L (?) n (0) = n + ? n = ?(n + ? + 1) ?(n + 1)?(? + 1)</formula><p>.</p><p>The standard Laguerre polynomials correspond to the case of ? = 0 of generalized Laguerre polynomials.</p><p>Derivatives of generalized Laguerre polynomials We note the following recurrence relations on generalized Laguerre polynomials ([2, Chapter 13.2]):</p><formula xml:id="formula_25">d dx L (?) n (x) = ?L (?+1) n?1 (x) L (?+1) n (x) = n i=0 L (?) i (x).</formula><p>These equations imply</p><formula xml:id="formula_26">d dt L (?) n (x) = ?L (?) 0 (x) ? L (?) 1 (x) ? ? ? ? ? L (?) n?1 (x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3 Properties of Chebyshev polynomials</head><p>Let T n be the classical Chebyshev polynomials (of the first kind), defined to be orthogonal with respect to the weight function (1 ? x 2 ) 1/2 supported on [?1, 1], and let p n be the normalized version of T n (i.e, with norm 1):</p><formula xml:id="formula_27">? cheb = (1 ? x 2 ) ?1/2 I (?1,1) , p n (x) = 2 ? T n (x) for n ? 1, p 0 (x) = 1 ? ? .</formula><p>Note that ? cheb is not normalized (it integrates to ?).</p><p>Derivatives of Chebyshev polynomials The chebyshev polynomials satisfy</p><formula xml:id="formula_28">2T n (x) = 1 n + 1 d dx T n+1 (x) ? 1 n ? 1 d dx T n?1 (x) n = 2, 3, . . . .</formula><p>By telescoping this series, we obtain 1 n T n = 2(T n?1 + T n?3 + ? ? ? + T 2 ) + T 0 n odd 2(T n?1 + T n?3 + ? ? ? + T 1 ) n even .</p><p>Translated Chebyshev polynomials We will also consider shifting and scaling the Chebyshev polynomials to be orthogonal on the interval [t ? ?, t] for fixed length ?.</p><p>The normalized (probability) measure is</p><formula xml:id="formula_30">?(t, x) = 2 ?? ? cheb 2(x ? t) ? + 1 = 1 ?? x ? t ? + 1 ?1/2 ? x ? t ? ?1/2 I (t??,t) .</formula><p>The orthonormal polynomial basis is</p><formula xml:id="formula_31">p n (t, x) = ? ?p n 2(x ? t) ? + 1 .</formula><p>In terms of the original Chebyshev polynomials, these are</p><formula xml:id="formula_32">p n (t, x) = ? 2T n 2(x ? t) ? + 1 for n ? 1, p (t) 0 = T 0 2(x ? t) ? + 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Leibniz Integral Rule</head><p>As part of our standard strategy for deriving HiPPO update rules (Appendix C), we will differentiate through integrals with changing limits. For example, we may wish to differentiate with respect to t the expression</p><formula xml:id="formula_33">f (t, x)?(t, x) dx = t 0 f (t,</formula><p>x) 1 t dx when analyzing the scaled Legendre (LegS) measure. Differentiating through such integrals can be formalized by the Leibniz integral rule, the basic version of which states that ? ?t</p><formula xml:id="formula_34">?(t) ?(t) f (x, t) dx = ?(t) ?(t) ? ?t f (x, t) dx ? ? (t)f (?(t), t) + ? (t)f (?(t), t).</formula><p>We elide over the formalisms in our derivations (Appendix D) and instead use the following trick. We replace integrand limits with an indicator function; and using the Dirac delta function ? when differentiating (i.e., using the formalism of distributional derivatives). For example, the above formula can be derived succinctly with this trick:</p><formula xml:id="formula_35">? ?t ?(t) ?(t) f (x, t) dx = ? ?t f (x, t)I [?(t),?(t)] (x) dx = ? ?t f (x, t)I [?(t),?(t)] (x) dx + f (x, t) ? ?t I [?(t),?(t)] (x) dx = ? ?t f (x, t)I [?(t),?(t)] (x) dx + f (x, t)(? (t)? ?(t) (x) ? ? (t)? ?(t) )(x) dx = ?(t) ?(t) ? ?t f (x, t) dx ? ? (t)f (?(t), t) + ? (t)f (?(t), t).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 ODE Discretization</head><p>In our framework, time series inputs will be modeled with a continuous function and then discretized. Here we provide some background on ODE discretization methods, including a new discretization that applies to a specific type of ODE that our new method encounters. The general formulation of an ODE is d dt c(t) = f (t, c(t)). We will also focus on the linear time-invariant ODE of the form d dt c(t) = Ac(t) + Bf (t) for some input function f (t), as a special case. The general methodology for discretizing the ODE, for step size ?t, is to rewrite the ODE as</p><formula xml:id="formula_36">c(t + ?t) ? c(t) = t+?t t f (s, c(s)) ds,<label>(12)</label></formula><p>then approximate the RHS integral. Many ODE discretization methods corresponds to different ways to approximate the RHS integral:</p><p>Euler (aka forward Euler). To approximate the RHS of equation <ref type="formula" target="#formula_2">(12)</ref>, keep the left endpoint ?tf (t, c(t)).</p><p>For the linear ODE, we get: c(t + ?t) = (I + ?tA)c(t) + ?tBf (t).</p><p>Backward Euler. To approximate the RHS of equation <ref type="formula" target="#formula_2">(12)</ref>, keep the right endpoint ?tf (t + ?t, c(t + ?t)).</p><p>For the linear ODE, we get the linear equation and the update:</p><formula xml:id="formula_37">c(t + ?t) ? ?tAc(t + ?t) = c(t) + ?tBf (t) c(t + ?t) = (I ? ?tA) ?1 c(t) + ?t(I ? ?tA) ?1 Bf (t).</formula><p>Bilinear (aka Trapezoid rule, aka Tustin's method). To approximate the RHS of equation <ref type="formula" target="#formula_2">(12)</ref>, average the endpoints ?t f (t,c(t))+f (t+?t,c(t+?t))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>. For the linear ODE, again we get a linear equation and the update:</p><formula xml:id="formula_38">c(t + ?t) ? ?t 2 Ac(t + ?t) = (I + ?t/2A)c(t) + ?tBf (t) c(t + ?t) = (I ? ?t/2A) ?1 (I + ?t/2A)c(t) + ?t(I ? ?t/2A) ?1 Bf (t).</formula><p>Generalized Bilinear Transformation (GBT). This method <ref type="bibr" target="#b77">[77]</ref> approximates the RHS of equation <ref type="formula" target="#formula_2">(12)</ref> by taking a weighted average of the endpoints ?t[(1 ? ?)f (t, c(t)) + ?f (t + ?t, c(t + ?t))], for some parameter ? ? [0, 1]. For the linear ODE, again we get a linear equation and the update:</p><formula xml:id="formula_39">c(t + ?t) ? ?t?Ac(t + ?t) = (I + ?t(1 ? ?)A)c(t) + ?tBf (t) c(t + ?t) = (I ? ?t?A) ?1 (I + ?t(1 ? ?)A)c(t) + ?t(I ? ?t?A) ?1 Bf (t).<label>(13)</label></formula><p>GBT generalizes the three methods mentioned above: forward Euler corresponds to ? = 0, backward Euler to ? = 1, and bilinear to ? = 1/2. We also note another method called Zero-order Hold (ZOH) <ref type="bibr" target="#b22">[23]</ref> that specializes to linear ODEs. The RHS of equation <ref type="formula" target="#formula_2">(12)</ref> is calculated in closed-form assuming constant input f between t and t + ?t. This HiPPO-LegS invariance to discretization step size. In the case of HiPPO-LegS, we have a linear ODE of the form d dt c(t) = 1 t Ac(t) + 1 t Bf (t). Adapting the GBT discretization (which generalizes forward/backward Euler and bilinear) to this linear ODE, we obtain:</p><formula xml:id="formula_40">c(t + ?t) ? ?t? 1 t + ?t Ac(t + ?t) = I + ?t(1 ? ?) 1 t A c(t) + ?t 1 t Bf (t) c(t + ?t) = I ? ?t t + ?t ?A ?1 I + ?t t (1 ? ?)A c(t) + ?t t I ? ?t t + ?t ?A ?1 Bf (t).</formula><p>We highlight that this system is invariant to the discretization step size ?t. Indeed, if c (k) := c(k?t) and f k := f (k?t) then we have the recurrence</p><formula xml:id="formula_41">c (k+1) = I ? 1 k + 1 ?A ?1 I + 1 k (1 ? ?)A c (k) + 1 k I ? 1 k + 1 ?A ?1 Bf k ,</formula><p>which does not depend on ?t.</p><p>Ablation: comparison between different discretization methods To understand the impact of approximation error in discretization, in <ref type="figure" target="#fig_2">Fig. 4</ref>, we show the absolute error for the HiPPO-LegS updates in function approximation (Appendix F.8) for different discretization methods: forward Euler, backward Euler, and bilinear. The bilinear method generally provide sufficiently accurate approximation. We will use bilinear as the discretization method for the LegS updates for the experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C General HiPPO Framework</head><p>We present the general HiPPO framework, as described in Section 2, in more details. We also generalize it to include bases other than polynomials.</p><p>Given a time-varying measure family ? (t) supported on (??, t], a sequence of basis functions G = span{g (t) n } n?[N ] , and a continuous function f : R ?0 ? R, HiPPO defines an operator that maps f to the optimal projection coefficients c : R ?0 ? R N , such that</p><formula xml:id="formula_42">g (t) := argmin g?G f ?t ? g ? (t) ,</formula><p>and</p><formula xml:id="formula_43">g (t) = N ?1 n=0 c n (t)g (t) n .</formula><p>The first step refers to the proj t operator and the second the coef t operator in Definition 1.</p><p>We focus on the case where the coefficients c(t) has the form of a linear ODE satisfying d dt c(t) =</p><formula xml:id="formula_44">A(t)c(t) + B(t)f (t) for some A(t) ? R N ?N , B(t) ? R N ?1 .</formula><p>We first describe the parameters of the hippo operator (a measure and basis) in more detail in Appendix C.1. We define the projection proj t and coefficient coef t operators in Appendix C.2. Then we give a general strategy to calculate these coefficients c(t), by deriving a differential equation that governs the coefficient dynamics (Appendix C.3). Finally we discuss how to turn the continuous hippo operator into a discrete one that can be applied to sequence data (Appendix C.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Measure and Basis</head><p>We describe and motivate the ingredients of HiPPO in more detail here. Recall that the high level goal is online function approximation; this requires both a set of valid approximations and a notion of approximation quality.</p><p>Approximation Measures At every t, the approximation quality is defined with respect to a measure ? (t) supported on (??, t]. We seek some polynomial g (t) of degree at most N ? 1 that minimizes the error f x?t ? g (t) L2(? (t) ) . Intuitively, this measure ? (t) governs how much to weigh every time in the past. For simplicity, we assume that the measures ? (t) are sufficiently smooth across their domain as well as in time; in particular, they have densities ?(t, x) := d? (t) d? (x) with respect to the Lebesgue measure d?(x) := dx such that ? is C 1 almost everywhere. Thus integrating against d? (t) (x) can be rewritten as integrating against ?(t, x) dx.</p><p>We also assume for simplicity that the measures ? (t) are normalized to be probability measures; arbitrary scaling does not affect the optimal projection.</p><p>Orthogonal polynomial basis Let {P n } n?N denote a sequence of orthogonal polynomials with respect to some base measure ?. Similarly define {P (t) n } n?N to be a sequence of orthogonal polynomials with respect to the time-varying measure ? (t) . Let p (t) n be the normalized version of P (t) n (i.e., have norm 1), and define p n (t, x) = p (t) n (x).</p><p>Note that the P (t) n are not required to be normalized, while the p (t) n are. Tilted measure and basis Our goal is simply to store a compressed representation of functions, which can use any basis, not necessarily OPs. For any scaling function</p><formula xml:id="formula_46">?(t, x) = ? (t) (x),<label>(15)</label></formula><p>the functions p n (x)?(x) are orthogonal with respect to the density ?/? 2 at every time t. Thus, we can choose this alternative basis and measure to perform the projections.</p><p>To formalize this tilting with ?, define ? (t) to be the normalized measure with density proportional to ? (t) /(? (t) ) 2 .</p><p>We will calculate the normalized measure and the orthonormal basis for it. Let</p><formula xml:id="formula_47">?(t) = ? ? 2 = ? (t) (x) (? (t) (x)) 2 dx<label>(16)</label></formula><p>be the normalization constant, so that ? (t) has density</p><formula xml:id="formula_48">? (t) ?(t)(? (t) ) 2 . If ?(t, x) = 1 (no tilting), this constant is ?(t) = 1.</formula><p>In general, we assume that ? is constant for all t; if not, it can be folded into ? directly.</p><p>Next, note that (dropping the dependence on x inside the integral for shorthand)</p><formula xml:id="formula_49">?(t) 1 2 p (t) n ? (t) 2 ? (t) = ?(t) 1 2 p (t) n ? (t) 2 ? (t) ?(t)(? (t) ) 2 = (p (t) n ) 2 ? (t) = p (t) n 2 ? (t) = 1.</formula><p>Thus we define the orthogonal basis for ? (t)</p><formula xml:id="formula_50">g (t) n = ? n ?(t) 1 2 p (t) n ? (t) , n ? N.<label>(17)</label></formula><p>We let each element of the basis be scaled by a ? n scalar, for reasons discussed soon, since arbitrary scaling does not change orthogonality:</p><formula xml:id="formula_51">g (t) n , g (t) m ? (t) = ? 2 n ? n,m</formula><p>Note that when ? n = ?1, the basis {g (t) n } is an orthonormal basis with respect to the measure ? (t) , at every time t. Notationally, let g n (t, x) := g (t)</p><p>n (x) as usual. We will only use this tilting in the case of Laguerre (Appendix D.2 and Chebyshev (Appendix D.5). Note that in the case ? = 1 (i.e., no tilting), we also have ? = 1 and g n = ? n p n (for all t, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 The Projection and Coefficients</head><p>Given a choice of measures and basis functions, we next see how the coefficients c(t) can be computed.</p><p>Input: Function We are given a C 1 -smooth function f : [0, ?) ? R which is seen online, for which we wish to maintain a compressed representation of its history f (x) ?t = f (x) x?t at every time t.</p><p>Output: Approximation Coefficients The function f can be approximated by storing its coefficients with respect to the basis {g n } n&lt;N . For example, in the case of no tilting ? = 1, this encodes the optimal polynomial approximation of f of degree less than N . In particular, at time t we wish to represent f ?t as a linear combination of polynomials g (t) n . Since the g (t)</p><p>n are orthogonal with respect to the Hilbert space defined by ?, ? ? (t) , it suffices to calculate coefficients</p><formula xml:id="formula_52">c n (t) = f ?t , g (t) n ? (t) = f g (t) n ? (t) ?(t)(? (t) ) 2 = ?(t) ? 1 2 ? n f p (t) n ? (t) ? (t) .<label>(18)</label></formula><p>Reconstruction At any time t, f ?t can be explicitly reconstructed as</p><formula xml:id="formula_53">f ?t ? g (t) := N ?1 n=0 f ?t , g (t) n ? (t) g (t) n g (t) n 2 ? (t) = N ?1 n=0 ? ?2 n c n (t)g (t) n = N ?1 n=0 ? ?1 n ? 1 2 c n (t)p (t) n ? (t) .<label>(19)</label></formula><p>Equation <ref type="formula" target="#formula_22">(19)</ref> is the proj t operator; given the measure and basis parameters, it defines the optimal approximation of f ?t .</p><p>The coef t operator simply extracts the vector of coefficients c(t) = (c n (t)) n?[N ] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Coefficient Dynamics: the hippo Operator</head><p>For the purposes of end-to-end models consuming an input function f (t), the coefficients c(t) are enough to encode information about the history of f and allow online predictions. Therefore, defining c(t) to be the vector of c n (t) from equation <ref type="formula" target="#formula_21">(18)</ref>, our focus will be on how to calculate the function c : R ?0 ? R N from the input function f : R ?0 ? R.</p><p>In our framework, we will compute these coefficients over time by viewing them as a dynamical system. Differentiating <ref type="bibr" target="#b17">(18)</ref>,</p><formula xml:id="formula_54">d dt c n (t) = ?(t) ? 1 2 ? n f (x) ? ?t p n (t, x) ? ? (t, x) dx + f (x) ? ? 1 2 ? n p n (t, x) ? ?t ? ? (t, x) dx.<label>(20)</label></formula><p>Here we have made use of the assumption that ? is constant for all t. Let c(t) ? R N ?1 denote the vector of all coefficients (c n (t)) 0?n&lt;N .</p><p>The key idea is that if ? ?t P n and ? ?t ? ? have closed forms that can be related back to the polynomials P k , then an ordinary differential equation can be written for c(t). This allows these coefficients c(t) and hence the optimal polynomial approximation to be computed online. Since d dt P (t) n is a polynomial (in x) of degree n ? 1, it can be written as linear combinations of P 0 , . . . , P n?1 , so the first term in Eq. (20) is a linear combination of c 0 , . . . , c n?1 . For many weight functions ?, we can find scaling function ? such that ? ?t ? ? can also be written in terms of ? ? itself, and thus in those cases the second term of Eq. (20) is also a linear combination of c 0 , . . . , c N ?1 and the input f . Thus this often yields a closed-form linear ODE for c(t).</p><p>Normalized dynamics Our purpose of defining the free parameters ? n was threefold.</p><p>1. First, note that the orthonormal basis is not unique, up to a ?1 factor per element.</p><p>2. Second, choosing ? n can help simplify the derivations.</p><p>3. Third, although choosing ? n = ?1 will be our default, since projecting onto an orthonormal basis is most sensible, the LMU <ref type="bibr" target="#b71">[71]</ref> used a different scaling. Appendix D.1 will recover the LMU by choosing different ? n for the LegT measure.</p><p>Suppose that equation <ref type="bibr" target="#b19">(20)</ref> reduced to dynamics of the form</p><formula xml:id="formula_55">d dt c(t) = ?A(t)c(t) + B(t)f (t).</formula><p>Then, letting ? = diag n?[N ] {? n },</p><formula xml:id="formula_56">d dt ? ?1 c(t) = ?? ?1 A(t)?? ?1 c(t) + ? ?1 B(t)f (t).</formula><p>Therefore, if we reparameterize the coefficients (? ?1 c(t) ? c(t)) then the normalized coefficients projected onto the orthonormal basis satisfy dynamics and associated reconstruction</p><formula xml:id="formula_57">d dt c(t) = ?(? ?1 A(t)?)c(t) + (? ?1 B(t))f (t) (21) f ?t ? g (t) = N ?1 n=0 ? 1 2 c n (t)p (t) n ? (t)<label>(22)</label></formula><p>These are the hippo and proj t operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Discretization</head><p>As defined here, hippo is a map on continuous functions. However, as hippo defines a closed-form ODE of the coefficient dynamics, standard ODE discretization methods (Appendix B.3) can be applied to turn this into discrete memory updates. Thus we overload these operators, i.e. hippo either defines an ODE of the form</p><formula xml:id="formula_58">d dt c(t) = A(t)c(t) + B(t)f (t)</formula><p>or a recurrence</p><formula xml:id="formula_59">c t = A t c t?1 + B t f t ,</formula><p>whichever is clear from context. Appendix F.5 validates the framework by applying <ref type="bibr" target="#b19">(20)</ref> and <ref type="bibr" target="#b18">(19)</ref> to approximate a synthetic function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Derivations of HiPPO Projection Operators</head><p>We derive the memory updates associated with the translated Legendre (LegT) and translated Laguerre (LagT) measures as presented in Section 2.3, along with the scaling Legendre (LegS) measure (Section 3).</p><p>To show the generality of the framework, we also derive memory updates with Fourier basis (recovering the Fourier Recurrent Unit <ref type="bibr" target="#b79">[79]</ref>) and with Chebyshev basis. The majority of the work has already been accomplished by setting up the projection framework, and the proof simply requires following the technical outline laid out in Appendix C. In particular, the definition of the coefficients <ref type="bibr" target="#b17">(18)</ref> and reconstruction <ref type="bibr" target="#b18">(19)</ref> does not change, and we only consider how to calculate the coefficients dynamics <ref type="bibr" target="#b19">(20)</ref>.</p><p>For each case, we follow the general steps:</p><p>Measure and Basis define the measure ? (t) or weight ?(t, x) and basis functions p n (t, x),</p><p>Derivatives compute the derivatives of the measure and basis functions, Coefficient Dynamics plug them into the coefficient dynamics (equation <ref type="formula" target="#formula_2">(20)</ref>) to derive the ODE that describes how to compute the coefficients c(t),</p><p>Reconstruction provide the complete formula to reconstruct an approximation to the function f ?t , which is the optimal projection under this measure and basis.</p><p>The derivations in Appendices D.1 and D.2 prove Theorem 1, and the derivations in Appendix D.3 prove Theorem 2. Appendices D.4 and D.5 show additional results for Fourier-based bases. <ref type="figure" target="#fig_10">Figure 5</ref> illustrates the overall framework when we use Legendre and Laguerre polynomials as the basis, contrasting our main families of time-varying measures ? (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Derivation for Translated Legendre (HiPPO-LegT)</head><p>This measure fixes a window length ? and slides it across time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure and Basis</head><p>We use a uniform weight function supported on the interval [t??, t] and pick Legendre polynomials P n (x), translated from [?1, 1] to [t ? ?, t], as basis functions:</p><formula xml:id="formula_60">?(t, x) = 1 ? I [t??,t] p n (t, x) = (2n + 1) 1/2 P n 2(x ? t) ? + 1 g n (t, x) = ? n p n (t, x).</formula><p>Here, we have used no tilting so ? = 1 and ? = 1 (equations <ref type="bibr" target="#b14">(15)</ref> and <ref type="formula" target="#formula_19">(16)</ref>). We leave ? n unspecified for now. At the endpoints, these basis functions satisfy g n (t, t) = ? n (2n + 1) g n (t, t ? ?) = ? n (?1) n (2n + 1) Derivatives The derivative of the measure is</p><formula xml:id="formula_61">? ?t ?(t, x) = 1 ? ? t ? 1 ? ? t?? .</formula><p>The derivative of Legendre polynomials can be expressed as linear combinations of other Legendre polynomials (cf. Appendix B.1.1).</p><p>? ?t g n (t, x) = ? n (2n + 1)</p><formula xml:id="formula_62">1 2 ? ?2 ? P n 2(x ? t) ? + 1 = ? n (2n + 1) 1 2 ?2 ? (2n ? 1)P n?1 2(x ? t) ? + 1 + (2n ? 5)P n?3 2(x ? t) ? + 1 + . . . = ?? n (2n + 1) 1 2 2 ? ? ?1 n?1 (2n ? 1) 1 2 g n?1 (t, x) + ? ?1 n?3 (2n ? 3) 1 2 g n?3 (t, x) + . . . .</formula><p>We have used equation <ref type="formula" target="#formula_20">(7)</ref> here.</p><p>Sliding Approximation As a special case for the LegT measure, we need to consider an approximation due to the nature of the sliding window measure. When analyzing d dt c(t) in the next section, we will need to use the value f (t ? ?). However, at time t this input is no longer available. Instead, we need to rely on our compressed representation of the function: by the reconstruction equation <ref type="bibr" target="#b18">(19)</ref>, if the approximation is succeeding so far, we should have</p><formula xml:id="formula_63">f ?t (x) ? N ?1 k=0 ? ?1 k c k (t)(2k + 1) 1 2 P k 2(x ? t) ? + 1 f (t ? ?) ? N ?1 k=0 ? ?1 k c k (t)(2k + 1) 1 2 (?1) k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coefficient Dynamics</head><p>We are ready to derive the coefficient dynamics. Plugging the derivatives of this measure and basis into equation <ref type="formula" target="#formula_2">(20)</ref> gives</p><formula xml:id="formula_64">d dt c n (t) = f (x) ? ?t g n (t, x) ?(t, x) dx + f (x)g n (t, x) ? ?t ?(t, x) dx = ?? n (2n + 1) 1 2 2 ? ? ?1 n?1 (2n ? 1) 1 2 c n?1 (t) + ? ?1 n?3 (2n ? 5) 1 2 c n?3 (t) + . . . + 1 ? f (t)g n (t, t) ? 1 ? f (t ? ?)g n (t, t ? ?) ? ? ? n ? (2n + 1) 1 2 ? 2 (2n ? 1) 1 2 c n?1 (t) ? n?1 + (2n ? 5) 1 2 c n?3 (t) ? n?3 + . . . + (2n + 1) 1 2 ? n ? f (t) ? (2n + 1) 1 2 ? n ? (?1) n N ?1 k=0 (2k + 1) 1 2 c k (t) ? k (?1) k = ? ? n ? (2n + 1) 1 2 ? 2 (2n ? 1) 1 2 c n?1 (t) ? n?1 + (2n ? 5) 1 2 c n?3 (t) ? n?3 + . . . ? (2n + 1) 1 2 ? n ? N ?1 k=0 (?1) n?k (2k + 1) 1 2 c k (t) ? k + (2n + 1) 1 2 ? n ? f (t) = ? ? n ? (2n + 1) 1 2 N ?1 k=0 M nk (2k + 1) 1 2 c k (t) ? k + (2n + 1) 1 2 ? n ? f (t),</formula><p>where</p><formula xml:id="formula_65">M nk = 1 if k ? n (?1) n?k if k ? n .</formula><p>Now we consider two instantiations for ? n . The first one is the more natural ? n = 1, which turns g n into an orthonormal basis. We then get</p><formula xml:id="formula_66">d dt c(t) = ? 1 ? Ac(t) + 1 ? Bf (t)</formula><p>A nk = (2n + 1)</p><formula xml:id="formula_67">1 2 (2k + 1) 1 2 1 if k ? n (?1) n?k if k ? n B n = (2n + 1) 1 2 .</formula><p>The second case takes ? n = (2n + 1)</p><formula xml:id="formula_68">1 2 (?1) n . This yields d dt c(t) = ? 1 ? Ac(t) + 1 ? Bf (t) A nk = (2n + 1) (?1) n?k if k ? n 1 if k ? n B n = (2n + 1)(?1) n .</formula><p>This is exactly the LMU update equation.</p><p>Reconstruction By equation <ref type="formula" target="#formula_22">(19)</ref>, at every time t we have</p><formula xml:id="formula_69">f (x) ? g (t) (x) = n ? ?1 n c n (t)(2n + 1) 1 2 P n 2(x ? t) ? + 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Derivation for Translated Laguerre (HiPPO-LagT)</head><p>We consider measures based on the generalized Laguerre polynomials. For a fixed ? ? R, these polynomials L (?) (t ? x) are orthogonal with respect to the measure x ? e ?x on [0, ?) (cf. Appendix B.1.2). This derivation will involve tilting the measure governed by another parameter ?. The result in Theorem 1 for HiPPO-LagT is for the case ? = 0, ? = 1, corresponding to the basic Laguerre polynomials and no tilting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure and Basis</head><p>We flip and translate the generalized Laguerre weight function and polynomials from [0, ?) to (??, t]. The normalization is found using equation <ref type="bibr" target="#b8">(9)</ref>.</p><formula xml:id="formula_70">?(t, x) = (t ? x) ? e x?t if x ? t 0 if x &gt; t = (t ? x) ? e ?(t?x) I (??,t] p n (t, x) = ?(n + 1) 1 2 ?(n + ? + 1) 1 2 L (?) n (t ? x)</formula><p>Tilted Measure We choose the following tilting ?</p><formula xml:id="formula_71">?(t, x) = (t ? x) ? exp ? 1 ? ? 2 (t ? x) I (??,t]</formula><p>for some fixed ? ? R. The normalization is (constant across all t)</p><formula xml:id="formula_72">? = ? ? 2 = (t ? x) ?? e ??(t?x) I (??,t] dx = ?(1 ? ?)? ??1 ,</formula><p>so the tilted measure has density</p><formula xml:id="formula_73">?(t) ?1 ? (t) (? (t) ) 2 = ?(1 ? ?) ?1 ? 1?? (t ? x) ?? exp (??(t ? x)) I (??,t] .</formula><p>We choose ? n = ?(n + ? + 1)</p><formula xml:id="formula_74">1 2 ?(n + 1) 1 2</formula><p>to be the norm of the generalized Laguerre polynomial L (?) n , so that ? n p</p><formula xml:id="formula_75">(t) n = L (?)</formula><p>n (t ? x), and (following equation <ref type="formula" target="#formula_20">(17)</ref>) the basis for ? (t) is</p><formula xml:id="formula_76">g (t) n = ? n ? 1 2 p (t) n ? (t) = ? 1 2 ? (t) L (?) n (t ? x)<label>(23)</label></formula><p>Derivatives We first calculate the density ratio</p><formula xml:id="formula_77">? ? (t, x) = exp ? 1 + ? 2 (t ? x) I (??,t] .</formula><p>and its derivative</p><formula xml:id="formula_78">? ?t ? ? (t, x) = ? 1 + ? 2 ? ? (t, x) + exp ? 1 + ? 2 (t ? x) ? t .</formula><p>The derivative of Laguerre polynomials can be expressed as linear combinations of other Laguerre polynomials (cf. Appendix B.1.2).</p><formula xml:id="formula_79">? ?t ? n p n (t, x) = ? ?t L (?) n (t ? x) = ?L (?) 0 (t ? x) ? ? ? ? ? L (?) n?1 (t ? x) = ?? 0 p 0 (t, x) ? ? ? ? ? ? n?1 p n?1 (t, x)</formula><p>Coefficient Dynamics Plugging these derivatives into equation <ref type="bibr" target="#b19">(20)</ref> (obtained from differentiating the coefficient equation <ref type="formula" target="#formula_21">(18)</ref>), where we suppress the dependence on x for convenience:</p><formula xml:id="formula_80">d dt c n (t) = ? ? 1 2 f ? ? ?t ? n p (t) n ? (t) ? (t) + f ? ? ? 1 2 ? n p (t) n ? ?t ? (t) ? (t) = ? n?1 k=0 f ? ? ? 1 2 ? k p (t) k ? (t) ? (t) (? (t) ) 2 ? 1 + ? 2 f ? ? ? 1 2 ? n p (t) n ? (t) ? (t) + f (t) ? ? ? 1 2 L (?) n (0) = ? n?1 k=0 c k (t) ? 1 + ? 2 c n (t) + ?(1 ? ?) ? 1 2 ? 1?? 2 n + ? n f (t).</formula><p>We then get</p><formula xml:id="formula_81">d dt c(t) = ?Ac(t) + Bf (t) A = ? ? ? ? ? 1+? 2 0 . . . 0 1 1+? 2 . . . 0 . . . . . . 1 1 . . . 1+? 2 ? ? ? ? ? B = ? ? 1 2 ? ? ? ? ? 0 . . . N ?1+? N ?1 ? ? ?<label>(24)</label></formula><p>Reconstruction By equation <ref type="formula" target="#formula_22">(19)</ref>, at every time t, for x ? t,</p><formula xml:id="formula_82">f (x) ? g (t) (x) = N ?1 n=0 ? ?1 n ? 1 2 c n (t)p (t) n ? (t) = n n! (n + ?)! ? 1 2 c n (t) ? L (?) n (t ? x) ? (t ? x) ? e ( ??1 2 )(t?x) .</formula><p>Normalized Dynamics Finally, following equations <ref type="formula" target="#formula_2">(21)</ref> and <ref type="formula" target="#formula_2">(22)</ref> to convert these to dynamics on the orthonormal basis of the normalized (probability) measure ? (t) leads to the following hippo operator</p><formula xml:id="formula_83">d dt c(t) = ?Ac(t) + Bf (t) A = ?? ?1 ? ? ? ? ? 1+? 2 0 . . . 0 1 1+? 2 . . . 0 . . . . . . 1 1 . . . 1+? 2 ? ? ? ? ? ? B = ?(1 ? ?) ? 1 2 ? 1?? 2 ? ? ?1 ? ? ? ? 0 . . . N ?1+? N ?1 ? ? ? ? = diag n?[N ]</formula><p>?(n + ? + 1)</p><formula xml:id="formula_84">1 2 ?(n + 1) 1 2<label>(25)</label></formula><p>and correspondingly a proj t operator:</p><formula xml:id="formula_85">f (x) ? g (t) (x) = ?(1 ? ?) 1 2 ? ? 1?? 2 n c n (t) ? ?(n + 1) 1 2 ?(n + ? + 1) 1 2 ? L (?) n (t ? x) ? (t ? x) ? e ( ??1 2 )(t?x) .<label>(26)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Derivation for Scaled Legendre (HiPPO-LegS)</head><p>As discussed in Section 3, the scaled Legendre is our only method that uses a measure with varying width.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure and Basis</head><p>We instantiate the framework in the case</p><formula xml:id="formula_86">?(t, x) = 1 t I [0,t]<label>(27)</label></formula><p>g n (t, x) = p n (t, x) = (2n + 1)</p><formula xml:id="formula_87">1 2 P n 2x t ? 1<label>(28)</label></formula><p>Here, P n are the basic Legendre polynomials (Appendix B.1.1). We use no tilting, i.e. ?(t, x) = 1, ?(t) = 1, and ? n = 1 so that the functions g n (t, x) are an orthonormal basis.</p><p>Derivatives We first differentiate the measure and basis:</p><formula xml:id="formula_88">? ?t ?(t, ?) = ?t ?2 I [0,t] + t ?1 ? t = t ?1 (??(t) + ? t ) ? ?t g n (t, x) = ?(2n + 1) 1 2 2xt ?2 P n 2x t ? 1 = ?(2n + 1) 1 2 t ?1 2x t ? 1 + 1 P n 2x t ? 1 .</formula><p>Now define z = 2x t ? 1 for shorthand and apply the properties of derivatives of Legendre polynomials (equation <ref type="formula" target="#formula_21">(8)</ref>). ? ?t g n (t, x) = ?(2n + 1)</p><formula xml:id="formula_89">1 2 t ?1 (z + 1)P n (z) = ?(2n + 1) 1 2 t ?1 [nP n (z) + (2n ? 1)P n?1 (z) + (2n ? 3)P n?2 (z) + . . . ] = ?t ?1 (2n + 1) 1 2 n(2n + 1) ? 1 2 g n (t, x) + (2n ? 1) 1 2 g n?1 (t, x) + (2n ? 3) 1 2 g n?2 (t, x) + . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coefficient Dynamics</head><p>Plugging these into <ref type="bibr" target="#b19">(20)</ref>, we obtain</p><formula xml:id="formula_90">d dt c n (t) = f (x) ? ?t g n (t, x) ?(t, x) dx + f (x)g n (t, x) ? ?t ?(t, x) dx = ?t ?1 (2n + 1) 1 2 n(2n + 1) ? 1 2 c n (t) + (2n ? 1) 1 2 c n?1 (t) + (2n ? 3) 1 2 c n?2 (t) + . . . ? t ?1 c n (t) + t ?1 f (t)g n (t, t) = ?t ?1 (2n + 1) 1 2 (n + 1)(2n + 1) ? 1 2 c n (t) + (2n ? 1) 1 2 c n?1 (t) + (2n ? 3) 1 2 c n?2 (t) + . . . + t ?1 (2n + 1) 1 2 f (t)</formula><p>where we have used g n (t, t) = (2n + 1) 1 2 P n (1) = (2n + 1) 1 2 . Vectorizing this yields equation <ref type="formula">(3)</ref>:</p><formula xml:id="formula_91">d dt c(t) = ? 1 t Ac(t) + 1 t Bf (t)<label>(29)</label></formula><formula xml:id="formula_92">A nk = ? ? ? ? ? (2n + 1) 1/2 (2k + 1) 1/2 if n &gt; k n + 1 if n = k 0 if n &lt; k , B n = (2n + 1) 1 2</formula><p>Alternatively, we can write this as</p><formula xml:id="formula_93">d dt c(t) = ?t ?1 D M D ?1 c(t) + 1f (t) ,<label>(30)</label></formula><p>where D := diag (2n + 1) </p><formula xml:id="formula_94">1 3 5 7 . . . N ? ? ? ? ? ? ? ? ? , that is, M nk = ? ? ? ? ? 2k + 1 if k &lt; n k + 1 if k = n 0 if k &gt; n</formula><p>Equation <ref type="formula" target="#formula_2">(29)</ref> is a linear dynamical system, except dilated by a time-varying factor t ?1 , which arises from the scaled measure.</p><p>Reconstruction By equation <ref type="bibr" target="#b18">(19)</ref>, at every time t we have f (x) ? g (t) (x) = n c n (t)g n (t, x). = n c n (t)(2n + 1)</p><formula xml:id="formula_95">1 2 P n 2x t ? 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Derivation for Fourier Bases</head><p>In the remainder of Appendix D, we consider some additional bases which are analyzable under the HiPPO framework. These use measures and bases related to various forms of the Fourier transform.</p><p>Reconstruction At every time step t, we have</p><formula xml:id="formula_96">f (x) ? Ac(t) + 1 ? Bf (t) A = 4 ? ? ? ? ? ? ? 0 . . . 2 ? 1 2 0 0 2 0 . . . 2 ? 1 2 ? 3 0 3 0 . . . . . . ? ? ? ? ? ? ? B = 2 3/2 ? ? ? ? ? ? ? ? 1 ? 2 ? 2 ? 2 . . . ? ? ? ? ? ? ? Reconstruction In the interval (t ? ?, t), f (x) ? N ?1 n=0 c n (t)p n (t, x)?(t, x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E HiPPO-LegS Theoretical Properties</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Timescale equivariance</head><p>Proof of Proposition 3. Letf (t) = f (?t). Let c = proj f andc = projf . By the HiPPO equation <ref type="bibr" target="#b17">(18)</ref> update and the basis instantiation for LegS (equation <ref type="formula" target="#formula_2">(28)</ref>),</p><formula xml:id="formula_97">c n (t) = f , g (t) n ? (t) = f (t)(2n + 1) 1 2 P n 2 x t ? 1 1 t I [0,1] x t dx = f (?t)(2n + 1) 1 2 P n 2 x t ? 1 1 t I [0,1] x t dx = f (?t)(2n + 1) 1 2 P n 2 x ?t ? 1 1 ?t I [0,1] x ?t dx = c n (?t).</formula><p>The second-to-last equality uses the change of variables x ? x ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Speed</head><p>In this section we work out the fast update rules according to the forward Euler, backward Euler, bilinear, or generalized bilinear transform discretizations (cf. Appendix B.3). Recall that we must be able to perform matrix-vector multiplication by I + ?A and (I ? ?A) ?1 where ? is some multiple of the step size ?t (equation <ref type="formula" target="#formula_39">(13)</ref>). It is easily seen that the LegS update rule involves a matrix A of the following form (Theorem 2):</p><formula xml:id="formula_98">A = D 1 (L + D 0 )D 2 ,</formula><p>where L is the all 1 lower triangular matrix and D 0 , D 1 , D 2 are diagonal. Clearly, I + ?A is efficient (only requiring O(N ) operations), as it only involves matrix-vector multiplication by diagonals D 0 , D 1 , D 2 , or multiplication by L which is the cumsum operation. Now we consider multiplication by the inverse (I + ?A) ?1 (the minus sign can be absorbed into ?). Write</p><formula xml:id="formula_99">(I + ?D 1 (L + D 0 )D 2 ) ?1 = D 1 (D ?1 1 D ?1 2 + ?(L + D 0 ))D 2 ?1 = ? ?1 D ?1 2 ? ?1 D ?1 1 D ?1 2 + D 0 + L ?1 D ?1 1</formula><p>Since diagonal multiplication is efficient, the crucial operation is inversion multiplication by a matrix of the form L + D.</p><p>Consider solving the equation (L + D)x = y. This implies x 0 + ? ? ? + x k?1 = y k ? (1 + d k )x k . The solution is</p><formula xml:id="formula_100">x 0 = y 0 1 + d 0 x k = y k ? x 0 ? ? ? ? ? x k?1 1 + d k Define s k = x 0 + ? ? ? + x k . Then s k = s k?1 + x k = s k?1 + y k ? s k?1 1 + d k = y k + d k s k?1 1 + d k = d k 1 + d k s k?1 + y k 1 + d k .</formula><p>Finally, consider how to calculate a recurrence of the following form efficiently.</p><formula xml:id="formula_101">x 0 = ? 0 , x k = ? k x k?1 + ? k .</formula><p>This update rule can also be written</p><formula xml:id="formula_102">x k ? k . . . ? 1 = x k?1 ? k?1 . . . ? 1 + ? k ? k . . . ? 1 .</formula><p>Evidently x can be computed in a vectorized way as</p><formula xml:id="formula_103">x = cumsum(?/cumprod(?)) ? cumprod(?).</formula><p>This is an O(N ) computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Gradient Norms</head><p>We analyze the discrete time case under the Euler discretization <ref type="figure" target="#fig_1">(Appendix B.3)</ref>, where the HiPPO-LegS recurrent update is equation <ref type="formula">(4)</ref>, restated here for convenience:</p><formula xml:id="formula_104">c k+1 = 1 ? A k c k + 1 k Bf k .</formula><p>These gradient asymptotics hold under other discretizations. We will show that Proposition 7. For any times k &lt; , the gradient norm of the HiPPO-LegS operator for the output at time + 1 with respect to input at time k is ?c +1 ?f k = ? (1/ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>We take N to be a constant. Without loss of generality assume k &gt; 2, as the gradient change for a single initial step is bounded. By unrolling the recurrence (4), the dependence of c +1 on c k and f k , . . . , f can be made explicit:</p><formula xml:id="formula_105">c +1 = I ? A . . . I ? A k c k + I ? A . . . I ? A k + 1 B k f k + I ? A . . . I ? A k + 2 B k + 1 f k+1 . . . + I ? A B ? 1 f ?1 + B f . Therefore ?c +1 ?f k = I ? A . . . I ? A k + 1 B k .</formula><p>Notice that A has distinct eigenvalues 1, 2, . . . , N , since those are the elements of its diagonal and A is triangular (Theorem 2). Thus the matrices I ? A , . . . , I ? A k+1 are diagonalizable with a common change of basis. The gradient then has the form P DP ?1 B for some invertible matrix P and some diagonal matrix D. Its norm is therefore bounded from below (up to constant) by the smallest singular value of P and P ?1 B , both of which are nonzero constants, and the largest diagonal entry of D. It thus suffices to bound this largest diagonal entry of D, which is the largest eigenvalue of this product,</p><formula xml:id="formula_106">? = 1 ? 1 . . . 1 ? 1 k + 1 1 k .</formula><p>The problem reduces to showing that ? = ?(1/l). We will use the following facts about the function log 1 ? 1 x . First, it is an increasing function, so</p><formula xml:id="formula_107">log 1 ? 1 x ? x x?1 log 1 ? 1 ? d?.</formula><p>Second, its antiderivative is</p><formula xml:id="formula_108">log 1 ? 1 x = log(x ? 1) ? log(x) = (x ? 1) log(x ? 1) ? x log(x) = x log 1 ? 1 x ? log(x ? 1).</formula><p>Therefore, we have</p><formula xml:id="formula_109">log 1 ? 1 . . . 1 ? 1 k + 1 = i=k+1 log(1 ? 1 i ) ? i=k+1 i i?1 log 1 ? 1 x dx = k log 1 ? 1 x dx = [(x ? 1) log(x ? 1) ? x log(x)] k = log 1 ? 1 ? log( ? 1) ? k log 1 ? 1 k ? log(k ? 1) .</formula><p>Finally, note that x log 1 ? 1 x is an increasing function, and bounded from above since it is negative, so it is ?(1) (this can also be seen from its Taylor expansion). Thus we have</p><formula xml:id="formula_110">log ? ? ?(1) ? log( ? 1) + log(k ? 1) ? log(k),</formula><p>Furthermore, all inequalities are asymptotically tight, so that ? = ?(1/ ) as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Function Approximation Error</head><p>Proof of Proposition 6. Fix a time t. HiPPO-LegS uses the measure ?(t, x) = 1 t I [0,t] and the polynomial basis p n (t, x) = (2n + 1)</p><formula xml:id="formula_111">1 2 P n 2x t ? 1 . Let c n (t) = f ?t , p (t)</formula><p>n ? (t) for n = 0, 1, . . . . Then the projection g (t) is obtained by linear combinations of the basis functions, with c n (t) as coefficients:</p><formula xml:id="formula_112">g (t) = N ?1 n=0 c n (t)p (t) n .</formula><p>Since p (t) n forms an orthonormal basis of the Hilbert space defined by the inner product ?, ? ? (t) <ref type="bibr" target="#b13">[14]</ref>, by Parseval's identity,</p><formula xml:id="formula_113">f ?t ? g (t) 2 ? (t) = ? n=N c 2 n (t).</formula><p>To bound the error f ?t ? g (t) ? (t) , it suffices to bound the sum of the squares of the high-order coefficients c n (t) for n = N, N + 1, . . . . We will bound each coefficient by integration by parts.</p><p>We first simplify the expression for c n (t). For any n ? 1, we have c n (t) = f ?t , p (t) n ? (t)</p><formula xml:id="formula_114">= 1 t (2n + 1) 1 2 t 0 f (x)P n 2x t ? 1 dx = (2n + 1) 1 2 2 1 ?1 f 1 + x 2 t P n (x) dx (change of variable x ? 1 + x 2 t). 1 ?1 |P n+1 (x) ? P n?1 (x)| dx 2 ? t 2 L 2 1 16 ? 1 2n + 1 ? 2 1 ?1 (P n+1 (x) ? P n?1 (x)) 2 dx (Cauchy-Schwarz) = t 2 L 2 1 8 1 2n + 1 1 ?1 P 2 n+1 (x) dx + 1 ?1 P 2 n?1 (x) dx (P n+1 and P n?1 are orthogonal) = t 2 L 2 1 8 1 2n + 1 2 2n + 3 + 2 2n ? 1 = O(1)t 2 L 2 1 n 2 .</formula><p>Summing for all n ? N yields:</p><formula xml:id="formula_115">f ?t ? g (t) 2 ? (t) = ? n=N c 2 n (t) = O(1)t 2 L 2 ? n=N 1 n 2 = O(1)t 2 L 2 1 N .</formula><p>We then obtain that f ?t ? g (t) ? (t) = O(tL/ ? N ) as claimed. Now supposed that f has k derivatives and the k-th derivative is bounded. The argument is similar to the one above where we integrate by parts k times. We sketch this argument here.</p><p>Take k to be a constant, and let n ? k. Applying integration by parts k times, noting that all the boundary terms are zero, gives: c n (t) = O(1)(2n + 1)</p><formula xml:id="formula_116">1 2 t k 1 ?1 f (k) 1 + x 2 t q k (x) dx, where q k (x) is a polynomial such that d k dx k q k (x) = P n (x). Then since f (k) is bounded, |c n (t)| = O(1)(2n + 1) 1 2 1 ?1 |q k (x)| dx, and so c 2 n (t) = O(1)t 2k (2n + 1) 1 ?1 |q k (x)| dx 2 = O(1)t 2k (2n + 1) 1 ?1 q 2 k (x) dx (Cauchy-Schwarz).</formula><p>It remains to bound 1 ?1 q 2 k (x) dx. Using the fact that d dx P n (x) = 1 2n+1 (P n+1 (x) ? P n?1 (x)) repeatedly, we have:</p><formula xml:id="formula_117">q 1 = 1 2n + 1 (P n+1 ? P n?1 ) = 1 n + O(1) ? 1 2 (P n+1 ? P n?1 )</formula><p>q 2 = 1 (n + O(1)) 2 1 2 2 (P n+2 ? P n ? P n + P n?2 ) = 1 (n + O(1)) 2 1 2 2 (P n+2 ? 2P n + P n?2 )</p><formula xml:id="formula_118">q 3 = 1 (n + O(1)) 3 1 2 3 (P n+3 ? P n+1 ? 2P n+1 + 2P n?1 + P n?1 ? P n?3 ) = 1 (n + O(1)) 3 1 2 3 (P n+3 ? 3P n+1 + 3P n?1 ? P n?3 ) . . .</formula><p>In general, when we expand out </p><formula xml:id="formula_119">q 2 k (x) dx = 1 (n + O(1)) 2k+1 1 2 k 2k k .</formula><p>By Stirling's approximation, 2k</p><formula xml:id="formula_120">k = O(1)4 k , so 1 ?1 q 2 k (x) dx = O(1)2 k (n+O(1)) 2k+1 .</formula><p>Noting that k is a constant, plugging this into the bound for c 2 n (t):</p><formula xml:id="formula_121">c 2 n (t) = O(1)t 2k (2n + 1) O(1)2 k (n + O(1)) 2k+1 = O(1)t 2k 1 n 2k .</formula><p>Summing for all n ? N yields:</p><formula xml:id="formula_122">f ?t ? g (t) 2 ? (t) = ? n=N c 2 n (t) = O(1)t 2k ? n=N 1 n 2k = O(1)t 2k 1 N 2k?1 .</formula><p>We then obtain that f ?t ? g (t)</p><formula xml:id="formula_123">? (t) = O(t k N ?k+1/2 ) as claimed.</formula><p>Remark. The approximation error of Legendre polynomials reduces to how fast the Legendre coefficients decay, subjected to the smoothness assumption of the input function. This result is analogous to the classical result in Fourier analysis, where the n-th Fourier coefficients decay as O(n ?k ) if the input function has order-k bounded derivatives <ref type="bibr" target="#b45">[45]</ref>. That result is also proved by integration by parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experiment Details and Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Model Architecture Details</head><p>Given inputs x t or features thereof f (x t ) in any model, the HiPPO framework can be used to memorize the history of features f t through time. As the discretized HiPPO dynamics form a linear recurrent update similar in style to RNNs (e.g., Theorem 2), we focus on these models in our experiments.</p><p>Thus, given any RNN update function h t = ? (h t?1 , x t ), we simply replace the previous hidden state with a projected version of its entire history. Equations <ref type="bibr" target="#b31">(31)</ref> lists the explicit update equations and <ref type="figure" target="#fig_16">Figure 6</ref> illustrates the model. In our experiments, we choose a basic gated RNN update <ref type="figure">x)</ref>).</p><formula xml:id="formula_124">? (h, x) = (1 ? g(h, x)) ? h + g(h, x) ? tanh(L ? (h, x)), g(h, x) = ?(L g (h,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods and Baselines</head><p>We consider the following instantiations of our framework HiPPO. HiPPO-LegT, LagT, and LegS, use the translated Legendre, and tilted Laguerre, and scaled Legendre measure families with update dynamics (1), <ref type="bibr" target="#b1">(2)</ref>, and (3). As mentioned, LegT has an additional hyperparameter ?, which should be set to the timescale of the data if known a priori. We attempt to set it equal to its ideal value (the length of the sequences) in every task, and also consider ? values that are too large and small to illustrate the effect of this hyperparameter.</p><p>Our derivations in Appendices D.1 to D.5 show that there is a large variety of update equations that can arise from the HiPPO framework-for example, the tilted generalized Laguerre polynomials lead to an entire family governed by two free parameters (Appendix D.2)-many of which lead to linear dynamics of the form d dt c(t) = ?Ac(t) + Bf (t) for various A, B. Given that many different update dynamics lead to such dynamical systems that give sensible results, we additionally consider the HiPPO-Rand baseline that uses random A and B matrices (normalized appropriately) in its dynamics.</p><p>We additionally compare against the following standard RNN baselines. The RNN is a vanilla RNN. The MGU is a minimal gated architecture, equivalent to a GRU without the reset gate. The HiPPO architecture we use is simply the MGU with an additional hippo intermediate layer. The LSTM is the most well-known  and popular RNN architecture, which is a more sophisticated gated RNN. The expRNN <ref type="bibr" target="#b48">[48]</ref> is the state-ofthe-art representative of the orthogonal RNN family of models designed for long-term dependencies <ref type="bibr" target="#b2">[3]</ref>. The LMU is the exact same model as in Voelker et al. <ref type="bibr" target="#b71">[71]</ref>; it is equivalent to HiPPO-LegT with a different RNN architecture.</p><formula xml:id="formula_125">hippo ! ! ? ! ! ? !"# !"# ! ! ! h t ? R d = ? (h t?1 , [c t?1 , x t ]) f t ? R 1 = L f (h t ) c t ? R N = hippo t (f ) = A t c t?1 + B t f t (31)</formula><p>All methods have the same hidden size in our experiments. In particular, for simplicity and to reduce hyperparameters, HiPPO variants tie the memory size N to the hidden state dimension d. The hyperparameter N and d is also referred to as the number of hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model and Architecture Comparisons</head><p>The model <ref type="bibr" target="#b31">(31)</ref> we use is a simple RNN that bears similarity to the classical LSTM and the original LMU cell. In comparison to the LSTM, HiPPO can be seen as a variant where the memory m t plays the role of the LSTM's hidden state and h t plays the role of the LSTM's gated cell state, with equal dimensionalities. HiPPO updates m t using the fixed A transition matrix instead of a learned matrix, and also lacks "input" and "output" gates, so for a given hidden size, it requires about half the parameters.</p><p>The LMU is a version of the HiPPO-LegT cell with an additional hidden-to-hidden transition matrix and memory-to-memory transition vector instead of the gate g, leaving it with approximately the same number of trainable parameters.</p><p>Training Details Unless stated otherwise, all methods use the Adam optimizer <ref type="bibr" target="#b41">[41]</ref> with learning rate frozen to 0.001, which has been a robust default for RNN based models <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b71">71]</ref>.</p><p>All experiments use PyTorch 1.5 and are run on a Nvidia P100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Permuted MNIST</head><p>Task The input to the sequential MNIST (sMNIST) task <ref type="bibr" target="#b47">[47]</ref> is an MNIST source image, flattened in row-major order into a single sequence of length 784. The goal of the model is to process the entire image sequentially before outputting a classification label, requiring learning long-term dependencies. A variant of this, the permuted MNIST (pMNIST) task, applies a fixed permutation to every image, breaking locality and further straining a model's capacity for long-term dependencies. Models are trained using the cross-entropy loss. We use the standard train-test split (60,000 examples for training and 10,000 for testing), and further split the training set with 10% to be used as validation set. <ref type="table" target="#tab_3">Table 1 is duplicated here in Tables 4 and 5</ref>, with more complete baselines and hyperparameter ablations. <ref type="table" target="#tab_7">Table 4</ref> consists of our implementations of various baselines related to our method, described in Appendix F.1. Each method was ran for 3 seeds, and the maximum average validation accuracy is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Ablations</head><p>All methods used the same hidden size of 512; we found that this gave better performance than 256, and further increasing it did not improve more. All methods were trained for 50 epochs with a batch size of 100. <ref type="table" target="#tab_8">Table 5</ref> directly shows the reported test accuracy of various methods on this data (Middle and Bottom). <ref type="table" target="#tab_8">Table 5</ref> (Top) reports the test accuracy of various instantations of our methods. We additionally include our reproduction of the LMU, which achieved better results than reported in Voelker et al. <ref type="bibr" target="#b71">[71]</ref> (possibly due to a larger hidden size). We note that all of our HiPPO methods are competitive; each of them (HiPPO-LegT, HiPPO-LagT, HiPPO-LegS) achieves state-of-the-art among previous recurrent sequence models. Note that differences between our HiPPO-LegT and LMU numbers in <ref type="table" target="#tab_8">Table 5</ref> (Top) stem primarily from the architecture difference (Appendix F.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State of the Art</head><p>Timescale Hyperparameters <ref type="table" target="#tab_7">Table 4</ref> also shows ablations for the HiPPO-LegT and HiPPO-LagT timescale hyperparameters. HiPPO-LagT sweeps the discretization step size ?t (Section 2.4 and Appendix B.3). For LegT, we set ?t = 1.0 without loss of generality, as only the ratio of ? to ?t matters. These timescale hyperparameters are important for these methods. Previous works have shown that the equivalent of ?t in standard RNNs, i.e. the gates of LSTMs and GRUs (Section 2.4), can also drastically affect their performance <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b66">66]</ref>. For example, the only difference between the URLSTM and LSTM in <ref type="table" target="#tab_8">Table 5</ref> is a reparametrization of the gates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Copying</head><p>Task In the Copying task <ref type="bibr" target="#b2">[3]</ref>, the input is a sequence of L + 20 digits where the first 10 tokens (a 0 , a 1 , . . . , a 9 ) are randomly chosen from {1, . . . , 8}, the middle N tokens are set to 0, and the last ten tokens are 9. The goal of the recurrent model is to output (a 0 , . . . , a 9 ) in order on the last 10 time steps, whenever the cue token 9 is presented. Models are trained using the cross-entropy loss; the random guessing baseline has loss log(8) ? 2.08. We use length L = 200. The training and testing examples are generated in the same way. Our motivation of studying the Copying task is that standard models such as the LSTM struggle to solve it. We note that the Copying task is much harder than other memory benchmarks such as the Adding task <ref type="bibr" target="#b2">[3]</ref>, and we do not consider those. 96.96 IndRNN <ref type="bibr" target="#b49">[49]</ref> 96.0 Dilated RNN <ref type="bibr" target="#b9">[10]</ref> 96.1 r-LSTM <ref type="bibr" target="#b69">[69]</ref> 95.2 LSTM <ref type="bibr" target="#b31">[31]</ref> 95.11</p><p>TrellisNet <ref type="bibr" target="#b5">[6]</ref> 98.13 Temporal ConvNet <ref type="bibr" target="#b4">[5]</ref> 97.2 Transformer <ref type="bibr" target="#b69">[69]</ref> 97.9</p><p>Results The HiPPO-LegS method solves this task the fastest. The LegT method also solves this task quickly, only if the parameter ? is initialized to the correct value of 200. Mis-specifying this timescale hyperparameter to ? = 20 or ? = 2000 drastically slows down the convergence of HiPPO-LegT. The LMU (at optimal parameter ? = 200) solves this task at comparable speed; like in Appendix F.2, differences between HiPPO-LegT (? = 200) and LMU here arise from the minor architecture difference in Appendix F.1. The HiPPO-Rand baseline (denoted "random LTI" system here) does much worse than the updates with the dynamics derived from our framework, highlighting the importance of the precise dynamics (in contrast to just the architecture).</p><p>Standard methods such as the RNN and LSTM are also nearly stuck at baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Trajectory Classification</head><p>Dataset The Character Trajectories dataset <ref type="bibr" target="#b3">[4]</ref> from the UCI machine learning repository <ref type="bibr" target="#b24">[25]</ref> consists of pen tip trajectories recorded from writing individual characters. The trajectories were captured at 200Hz and data was normalized and smoothed. Input is 3-dimensional (x and y positions, and pen tip force), and there are 20 possible outputs (number of classes). Models are trained using the cross-entropy loss. The dataset contains 2858 time series. The length of the sequences is variable, ranging up to 182. We use a train-val-test split of 70%-15%-15%.</p><p>Methods RNN baselines include the LSTM <ref type="bibr" target="#b34">[34]</ref>, GRU <ref type="bibr" target="#b16">[17]</ref>, and LMU <ref type="bibr" target="#b71">[71]</ref>. Our implementations of these used 256 hidden units each.</p><p>The GRU-D <ref type="bibr" target="#b10">[11]</ref> is a method for handling missing values in time series that computes a decay between observations. The ODE-RNN <ref type="bibr" target="#b61">[61]</ref> and Neural CDE (NCDE) <ref type="bibr" target="#b40">[40]</ref> baselines are state-of-the-art neural ODE methods, also designed to handle irregularly-sampled time series. Our GRU-D, ODE-RNN, and Neural CDE baselines used code from Kidger et al. <ref type="bibr" target="#b40">[40]</ref>, inheriting the hyperparameters for those methods.</p><p>All methods trained for 100 epochs.</p><p>Timescale mis-specification The goal of this experiment is to investigate the performance of models when the timescale is mis-specified between train and evaluation time, leading to distribution shift. We considered the following two standard types of time series:</p><p>1. Sequences sampled at a fixed rate <ref type="figure">Figure 7</ref>: Loss on the Copying task. HiPPO methods are the only to fully solve the task. The hyperparameter-free LegS update is best, while methods with timescale parameters (e.g. LegT) do not solve the task if mis-specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Irregularly-sampled time series (i.e., missing values) with timestamps</head><p>Timescale shift is emulated in the corresponding ways, which can be interpreted as different sampling rates or trajectory speeds.</p><p>1. Either the train or evaluation sequences are downsampled by a factor of 2 2. The train or evaluation timestamps are halved. <ref type="bibr" target="#b7">8</ref> The first scenario in each corresponds to the original sequence being sampled at 100Hz instead of 200Hz; alternatively, it is equivalent to the writer drawing twice as fast. Thus, these scenarios correspond to a train ? evaluation timescale shift of 100Hz ? 200Hz and 200Hz ? 100Hz respectively.</p><p>Note that models are unable to obviously tell that there is timescale shift. For example, in the first scenario, shorter or longer sequences can be attributed to the variability of sequence lengths in the original dataset. In the second scenario, the timestamps have different distributions, but this can correspond to different rates of missing data, which the baselines for irregularly-sampled data are able to address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Online Function Approximation and Speed Benchmark</head><p>Task The task is to reconstruct an input function (as a discrete sequence) based on some hidden state produced after the model has traversed the input function. This is the same problem setup as in Section 2.1; the online approximation and reconstruction details are in Appendix C. The input function is randomly sampled from a continuous-time band-limited white noise process, with length 10 6 . The sampling step size is ?t = 10 ?4 , and the signal band limit is 1Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>We compare HiPPO-LegS, LMU, and LSTM. The HiPPO-LegS and LMU model only consists of the memory update and not the additional RNN architecture. The function is reconstructed from the coefficients using the formula in Appendix D, so no training is required. For LSTM, we use a linear decoder to reconstruct the function from the LSTM hidden states and cell states, trained on a collection of 100 sequences. All models use N = 256 hidden units. The LSTM uses the L2 loss. The HiPPO methods including LMU follow the fixed dynamics of Theorem 1 and Theorem 2.</p><p>Speed benchmark We measure the inference time of HiPPO-LegS, LMU, and LSTM, in single-threaded mode on a server Intel Xeon CPU E5-2690 v4 at 2.60GHz.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.6 Sentiment Classification on the IMDB Movie Review Dataset</head><p>Dataset The IMDB movie review dataset <ref type="bibr" target="#b50">[50]</ref> is a standard binary sentiment classification task containing 25000 train and test sequences, with sequence lengths ranging from hundreds to thousands of steps. The task is to classify the sentiment of each movie review into either positive or negative. We use 10% of the standard training set as validation set.</p><p>Methods RNN baselines include the LSTM <ref type="bibr" target="#b34">[34]</ref>, vanilla RNN, LMU <ref type="bibr" target="#b71">[71]</ref>, and expRNN <ref type="bibr" target="#b48">[48]</ref>. Our implementations of these used 256 hidden units each.</p><p>Result As shown in <ref type="table" target="#tab_10">Table 6</ref>, our HiPPO-RNNs have similar and consistent performance, on par or better than LSTM. Other long-range memory RNN approaches that constrains the expressivity of the network (e.g. expRNN) performs worse on this more generic task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.7 Mackey Glass prediction</head><p>The Mackey-Glass data <ref type="bibr" target="#b52">[52]</ref> is a time series prediction task for modeling chaotic dynamical systems. We build on the implementation of Voelker et al. <ref type="bibr" target="#b71">[71]</ref>. The data is a sequence of one-dimensional observations, and models are tasked with predicting 15 time steps into the future. The models are 4-layer stacked recurrent neural networks, trained with the mean squared error (MSE) loss. Voelker et al. <ref type="bibr" target="#b71">[71]</ref> additionally consider a hybrid model with alternating LSTM and LMU layers, which improved on either by itself. We did not try this approach with our method HiPPO-LegS such as combining it with the LSTM or other HiPPO methods, but such ideas could further improve our performance. As a baseline method, the identity function does not simulate the dynamics, and simply guesses that the future time step is equal to the current input. <ref type="figure" target="#fig_17">Fig. 8</ref> plots the training and validation mean squared errors (MSE) of these methods. <ref type="table">The table reports</ref> final normalized root mean squared errors (NRMSE)</p><formula xml:id="formula_126">E[(Y ?? ) 2 ] E[Y 2 ]</formula><p>between the targets Y and predictions? .</p><p>HiPPO-LegS outperforms the LSTM, LMU, and the best hybrid LSTM+LMU model from <ref type="bibr" target="#b68">[68]</ref>, reducing normalized MSE by over 30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.8 Additional Analysis and Ablations of HiPPO</head><p>To further analyze the tradeoffs of the memory updates derived from our framework, in <ref type="figure" target="#fig_19">Fig. 9</ref> we plot a simple input function f (x) = 1/4 sin x + 1/2 sin(x/3) + sin(x/7) to be approximated. The function is subsampled on the range x ? [0, 100], creating a sequence of length 1000. This function is simpler than the functions sampled from white noise signals described in Appendix F.5. Given this function, we use the same methodology as in Appendix F.5 for processing the function online and then reconstructing it at the end.  In <ref type="figure" target="#fig_19">Figure 9</ref>(a, b), we plot the true function f , and its absolute approximation error based on LegT, LagT, and LegS. LegS has the lowest approximation error, while LegT and LagT are similar and slightly worse than LegS. Next, we analyze some qualitative behaviors.</p><p>LegT Window Length In <ref type="figure" target="#fig_19">Figure 9</ref>(c), shows that the approximation error of LegT is sensitive to the hyperparameter ?, the length of the window. Specifying ? to be even slightly too small (by 0.5% relative to the total sequence length) causes huge errors in approximation. This is expected by the HiPPO framework, as the final measure ? (t) is not supported everywhere, so the projection problem does not care that the reconstructed function is highly inaccurate near x = 0.</p><p>Generalized LagT Family Our LagT method actually comprises a family of related transforms, governed by two parameters ?, ? specifying the original measure and the tilting (Appendix D.2). <ref type="figure" target="#fig_21">Fig. 10</ref> shows the error as these parameters change. <ref type="figure" target="#fig_21">Fig. 10(a)</ref> shows that small ? generally performs better. <ref type="figure" target="#fig_21">Fig. 10(b, c)</ref> show that the reconstruction is unstable for larger ?, but small values of ? work well. More detailed theoretical analysis explaining these tradeoffs would be an interesting question to analyze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LegS vs. LegT</head><p>In comparison to LegT, LegS does not need any hyperparameters governing the timescale. However, suppose that the LegT ? window size was chosen perfectly to match the length of the sequence; that is, ? = T where T is the final time range. Note that at the end of consuming the input function (time t = T ), the measures ? (t) for LegS and LegT are both equal to 1 T I [0,T ] (Sections 2.3 and 3). Therefore, the approximation proj T (f ) is specifying the same function for both LegS and LegT at time t = T . The sole difference is that LegT has an additional approximation term for f (t ? ?) while calculating the update at every time t (see Appendix D.1), due to the nature of the sliding rather than scaling window.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(3) For an appropriately chosen basis, the corresponding coefficients c(t) ? R N representing a compression of the history of f satisfy linear dynamics. (4) Discretizing the dynamics yields an efficient closed-form recurrence for online compression of time series (f k ) k?N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proposition 3 .</head><label>3</label><figDesc>For any scalar ? &gt; 0, if h(t) = f (?t), then hippo(h)(t) = hippo(f )(?t).In other words, if ? : t ? ?t is any dilation function, then hippo(f ? ?) = hippo(f ) ? ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 4 .</head><label>4</label><figDesc>Under any generalized bilinear transform discretization (cf. Appendix B.3), each step of the HiPPO-LegS recurrence in equation (4) can be computed in O(N ) operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Section 4 .</head><label>4</label><figDesc>3 validates the efficiency of HiPPO layers in practice, where unrolling the discretized versions of Theorem 2 is 10x faster than standard matrix multiplication as done in standard RNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?Figure 2 :</head><label>2</label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b J Q N X J R k e S x a 7 e M I D 0 Z 9 D m 2 f U I 8 = " &gt; A A A C S X i c b V D L S s N A F J 2 0 P u t b l 2 6 C R X B V E 6 n o U h S K O x V s F Z o g k 8 m N H Z y Z h J k b p Y T 8 g l v 9 J 7 / A z 3 A n r p z W L L T 1 w M D h n H O 5 d 0 6 U C W 7 Q 8 9 6 d W n 1 m d m 5 + Y b G x t L y y u r a + s d k z a a 4 Z d F k q U n 0 b U Q O C K + g i R w G 3 m Q Y q I w E 3 0 c P Z y L 9 5 B G 1 4 q q 5 x m E E o 6 b 3 i C W c U R 1 K A N L 9 b b 3 o t b w x 3 m v g V a Z I K l 3 c b z n 4 Q p y y X o J A J a k z f 9 z I M C 6 q R M w F l I 8 g N Z J Q 9 0 H v o W 6 q o B B M W 4 2 N L d 9 c q s Z u k 2 j 6 F 7 l j 9 P V F Q a c x Q R j Y p K Q 7 M p D c S / / P 6 O S b H Y c F V l i M o 9 r M o y Y W L q T v 6 u R t z D Q z F 0 B L K N L e 3 u m x A N W V o + 2 k E C p 5 Y K i V V c R H I K C n 7 f m i J 3 R I l R d M v y 7 + J T l m Z U d G Z 9 P D C m k 8 8 B u Q i h u K i L G 3 F / m S h 0 6 R 3 0 P L b r c O r d v P k t C p 7 g W y T H b J H f H J E T s g 5 u S R d w s i A P J M X 8 u q 8 O R / O p / P 1 E 6 0 5 1 c w W + Y N a / R s D 2 b O 0 &lt; / l a t e x i t &gt; HiPPO incorporated into a simple RNN model. hippo is the HiPPO memory operator which projects the history of the ft features depending on the chosen measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Input function and its reconstructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>yields the update c(t + ?t) = e ?tA c(t) + ?t ? =0 e ? A d? Bf (t).If A is invertible, this can be simplified as c(t + ?t) = e ?tA c(t) + A ?1 (e ?tA ? I)Bf (t).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Absolute error for different discretization methods. Forward and backward Euler are generally not very accurate, while bilinear yields more accurate approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of HiPPO measures. At time t0, the history of a function f (x) x?t 0 is summarized by polynomial approximation with respect to the measure ? (t 0 ) (blue), and similarly for time t1 (purple). (Left) The Translated Legendre measure (LegT) assigns weight in the window [t ? ?, t]. For small t, ? (t) is supported on a region x &lt; 0 where f is not defined. When t is large, the measure is not supported near 0, causing the projection of f to forget the beginning of the function. (Middle) The Translated Laguerre (LagT) measure decays the past exponentially. It does not forget, but also assigns weight on x &lt; 0. (Right) The Scaled Legendre measure (LegS) weights the entire history [0, t] uniformly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>1 2 N ? 1 n=0, 1</head><label>1211</label><figDesc>is the all ones vector, and the state matrix M is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 ? 1 1 ? 1 1 ?</head><label>11111</label><figDesc>q 2 k (x) dx, since the P m 's are orthogonal, we get k + 1 terms of the form P 2 m (x) dx for k different values of m in the range [n ? k, n + k], and l goes from 0 to k. For each m,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>?</head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b J Q N X J R k e S x a 7 e M I D 0 Z 9 D m 2 f U I 8 = " &gt; A A A C S X i c b V D L S s N A F J 2 0 P u t b l 2 6 C R X B V E 6 n o U h S K O x V s F Z o g k 8 m N H Z y Z h J k b p Y T 8 g l v 9 J 7 / A z 3 A n r p z W L L T 1 w M D h n H O 5 d 0 6 U C W 7 Q 8 9 6 d W n 1 m d m 5 + Y b G x t L y y u r a + s d k z a a 4 Z d F k q U n 0 b U Q O C K + g i R w G 3 m Q Y q I w E 3 0 c P Z y L 9 5 B G 1 4 q q 5 x m E E o 6 b 3 i C W c U R 1 K A N L 9 b b 3 o t b w x 3 m v g V a Z I K l 3 c b z n 4 Q p y y X o J A J a k z f 9 z I M C 6 q R M w F l I 8 g N Z J Q 9 0 H v o W 6 q o B B M W 4 2 N L d 9 c q s Z u k 2 j 6 F 7 l j 9 P V F Q a c x Q R j Y p K Q 7 M p D c S / / P 6 O S b H Y c F V l i M o 9 r M o y Y W L q T v 6 u R t z D Q z F 0 B L K N L e 3 u m x A N W V o + 2 k E C p 5 Y K i V V c R H I K C n 7 f m i J 3 R I l R d M v y 7 + J T l m Z U d G Z 9 P D C m k 8 8 B u Q i h u K i L G 3 F / m S h 0 6 R 3 0 P L b r c O r d v P k t C p 7 g W y T H b J H f H J E T s g 5 u S R d w s i A P J M X 8 u q 8 O R / O p / P 1 E 6 0 5 1 c w W + Y N a / R s D 2 b O 0 &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 6 :</head><label>6</label><figDesc>The simple RNN model we use HiPPO with, and associated update equations. L is a parametrized linear function, ? is any RNN update function, and [?] denotes concatenation. hippo is the HiPPO memory operator which orthogonalizes the history of the ft features up to time t. At, Bt are fixed matrices depending on the chosen measure . N and d represent the approximation order and hidden state size, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 :</head><label>8</label><figDesc>Mackey-Glass predictions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Error for different ?'s in LegT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 9 :</head><label>9</label><figDesc>Function approximation comparison between LegT, LagT, and LegS. LegS has the lowest approximation error. LegT error is sensitive to the choice of window length ?, especially if ? is smaller than the length of the true function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Generalized Laguerre family, fixed ? = 0 and large ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 10 :</head><label>10</label><figDesc>Function approximation comparison between different instantiations of the generalized tilted Laguerre family (Appendix D.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>compares LegS, LegT, and LagT visually, showing the advantages of the scaled measure. Simply by specifying the desired measure, specializing the HiPPO framework (Sections 2.2, 2.4) yields a new memory mechanism (proof in Appendix D.3). The continuous-(3) and discrete-(4) time dynamics for HiPPO-LegS are:</figDesc><table><row><cell>Theorem 2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: (Left) pMNIST validation, average over 3 seeds. Top: Our</cell></row><row><cell>methods. Bottom: RNN baselines. (Right) Reported test accuracies</cell></row><row><cell>from previous works. Top: Our methods. Middle: Recurrent models.</cell></row><row><cell>Bottom: Non-recurrent models requiring global receptive field.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Test set accuracy on Character Trajectory classification on out-of-distribution timescales.ModelLSTM GRU GRU-D ODE-RNN NCDE LMU HiPPO-LegS</figDesc><table><row><cell>100Hz ? 200Hz</cell><cell>31.9</cell><cell>25.4</cell><cell>23.1</cell><cell>41.8</cell><cell>44.7</cell><cell>6.0</cell><cell>88.8</cell></row><row><cell>200Hz ? 100Hz</cell><cell>28.2</cell><cell>64.6</cell><cell>25.5</cell><cell>31.5</cell><cell>11.3</cell><cell>13.1</cell><cell>90.1</cell></row><row><cell>Missing values upsample</cell><cell>24.4</cell><cell>28.2</cell><cell>5.5</cell><cell>4.3</cell><cell>63.9</cell><cell>39.3</cell><cell>94.5</cell></row><row><cell>Missing values downsample</cell><cell>34.9</cell><cell>27.3</cell><cell>7.7</cell><cell>7.7</cell><cell>69.7</cell><cell>67.8</cell><cell>94.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Function approximation error after 1 million time steps, with 256 hidden units.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Our methods and related baselines. Permuted MNIST (pMNIST) validation scores. (Top): Our methods. (Bottom): Recurrent baselines.</figDesc><table><row><cell>Method</cell><cell>Validation accuracy (%)</cell></row><row><cell>HiPPO-LegS</cell><cell>98.34</cell></row><row><cell>HiPPO-LagT ?t = 1.0</cell><cell>98.15</cell></row><row><cell>HiPPO-LegT ? = 200</cell><cell>98.00</cell></row><row><cell>HiPPO-LegT ? = 2000</cell><cell>97.90</cell></row><row><cell>HiPPO-LagT ?t = 0.1</cell><cell>96.44</cell></row><row><cell>HiPPO-LegT ? = 20</cell><cell>91.75</cell></row><row><cell cols="2">HiPPO-LagT ?t = 0.01 90.71</cell></row><row><cell>HiPPO-Rand</cell><cell>69.93</cell></row><row><cell>LMU</cell><cell>97.08</cell></row><row><cell>ExpRNN</cell><cell>94.67</cell></row><row><cell>GRU</cell><cell>93.04</cell></row><row><cell>LSTM</cell><cell>92.54</cell></row><row><cell>MGU</cell><cell>89.37</cell></row><row><cell>RNN</cell><cell>52.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison to prior methods for pixel-by-pixel image classification. Reported test accuracies from previous works on pixel-by-pixel image classification benchmarks. Top: Our methods. Middle: Recurrent baselines and variants. Bottom: Non-recurrent sequence models with global receptive field.</figDesc><table><row><cell>Model</cell><cell>Test accuracy (%)</cell></row><row><cell>HiPPO-LegS</cell><cell>98.3</cell></row><row><cell>HiPPO-Laguerre</cell><cell>98.24</cell></row><row><cell>HiPPO-LegT</cell><cell>98.03</cell></row><row><cell>LMU (ours)</cell><cell>97.29</cell></row><row><cell cols="2">URLSTM + Zoneout [46] 97.58</cell></row><row><cell>LMU [71]</cell><cell>97.15</cell></row><row><cell>URLSTM [31]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>IMDB test accuracy, averaged over 3 seeds. Top: Our methods. Bottom: Recurrent baselines.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The LMU was originally motivated by spiking neural networks in modeling biological nervous systems; its derivation is not self-contained but a sketch can be pieced together from<ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is known as the Euler method, used for illustration here; our experiments use the more numerically stable Bilinear and ZOH methods. Appendix B.3 provides a self-contained overview of our full discretization framework.<ref type="bibr" target="#b2">3</ref> The LSTM cell update is similar, with a parameterization known as "tied" gates<ref type="bibr" target="#b30">[30]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4"><ref type="bibr" target="#b3">(4)</ref> uses the Euler method for illustration; HiPPO-LegS is invariant to other discretizations (Appendix B.3).<ref type="bibr" target="#b4">5</ref> It is known that large families of structured matrices related to orthogonal polynomials are efficient<ref type="bibr" target="#b21">[22]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In our experiments, LMU refers to the architecture in<ref type="bibr" target="#b71">[71]</ref> while LegT uses the one described inFig. 2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The LMU is only known to be fast with the simple forward Euler discretization<ref type="bibr" target="#b71">[71]</ref>, but not with more sophisticated methods such as bilinear and ZOH that are required to reduce numerical errors for this task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Instead of the train timestamps being halved, equivalently the evaluation timestamps can be doubled.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Avner May, Mayee Chen, Dan Fu, Aditya Grover, and Daniel L?vy for their helpful feedback. We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M), FA86501827865 (SDH), and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Stanford HAI AWS cloud credit, Swiss Re, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government. Atri Rudra's research is supported by NSF grant CCF-1763481.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As P n (x) = 1 2n+1 d dx (P n+1 (x) ? P n?1 (x)) (cf. Appendix B.1.1), integration by parts yields: c n (t) = (2n + 1)</p><p>Notice that the boundary term is zero, since P n+1 (1) = P n?1 (1) = 1 and P n+1 (?1) = P n?1 (?1) = ?1 (either both 1 or both ?1 depending on whether n is odd or even). Hence:</p><p>Now suppose that f is L-Lipschitz, which implies that |f | ? L. Then c 2 n (t) ? t 2 L 2 1 16 ? 1 2n + 1</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Butterfly transform: An efficient FFT based neural architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keivan</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mathematical methods for physicists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arfken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Elsevier Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The UEA multivariate time series classification archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Southam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00075</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerated gossip in networks of given dimension using Jacobi polynomial iterations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Berthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Gaillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematics of Data Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="47" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Chebyshev and Fourier spectral methods. Courier Corporation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards non-saturating recurrent units for modelling long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3280" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast computation of sliding discrete Tchebichef moments and its application in duplicated regions detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beijing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gouenou</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Louis</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5424" to="5436" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An introduction to orthogonal polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chihara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
	<note>ISBN 9780486479293</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gaussian quadrature for kernel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher M De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6107" to="6117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning fast algorithms for linear transforms using butterfly factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaleidoscope: An efficient, learnable representation for all structured linear maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nimit</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Blonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A two-pronged progress in structured dense matrix vector multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Christopher De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Puttagunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1060" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Linear systems: A state variable approach with numerical implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Decarlo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Accurate, guaranteed stable, sliding discrete Fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Duda</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>DSP tips &amp; tricks</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="124" to="127" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Augmented neural ODEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3134" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized sliding FFT and its application to implementation of block LMS adaptive filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrouz</forename><surname>Farhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Boroujeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Gazor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="532" to="538" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How to train your neural ODE: the world of Jacobian and kinetic regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levon</forename><surname>Nurbekyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">M</forename><surname>Oberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving the gating mechanism of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Noisy activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3059" to="3068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent orthogonal networks and long-memory tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A first course in the numerical analysis of differential equations. Number 44</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arieh</forename><surname>Iserles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The sliding DFT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An update to the sliding DFT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="111" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">5667</biblScope>
			<biblScope unit="page" from="78" to="80" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast algorithms for the computation of sliding discrete sinusoidal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1704" to="1710" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast algorithms for the computation of sliding discrete Hartley transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2937" to="2944" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Fourier analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>William K?rner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?nos</forename><surname>Kram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01305</idno>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio, Aaron Courville, and Chris Pal. Zoneout: Regularizing RNNs by randomly preserving hidden activations</title>
		<meeting><address><addrLine>Nicolas Ballas, Nan Rosemary Ke</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mart?nez-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (IndRNN): Building a longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient computation of the running discrete Haar transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosendo</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Macias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gomez Exposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on power delivery</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="504" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Oscillation and chaos in physiological control systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">4300</biblScope>
			<biblScope unit="page" from="287" to="289" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelangelo</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Asama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08063</idno>
		<title level="m">Stable neural flows</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An efficient recursive algorithm and an explicit formula for calculating update vectors of running Walsh-Hadamard transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barzan</forename><surname>Mozafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savoji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Symposium on Signal Processing and Its Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast algorithm for Walsh Hadamard transform on sliding windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kuen</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="165" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Digital signal processing: principles algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SNODE: Spectral discretization of neural ODEs for system identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Quaglino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gallieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutn?k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Weak supervision as an efficient approach for automated seizure detection in electroencephalography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lee-Messer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The Temple University hospital seizure detection corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinit</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Von Weltin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Riley</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meysam</forename><surname>Golmohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iyad</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Picone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Orthogonal Polynomials. Number v.23 in American Mathematical Society colloquium publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szeg?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">9780821889527</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Can recurrent neural networks warp time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning compressed transforms with low displacement rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9052" to="9060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Approximation theory and approximation practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lloyd N Trefethen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>SIAM</publisher>
			<biblScope unit="volume">164</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning longer-term dependencies in RNNs with auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15544" to="15553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Improving spiking dynamical networks: Accurate delays, higherorder synapses, and time cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="609" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Dynamical systems in spiking neuromorphic hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">Russell</forename><surname>Voelker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Sliding conjugate symmetric sequency-ordered complex Hadamard transform: fast algorithm and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lotfi</forename><surname>Senhadji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1321" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A mean field theory of batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Performance recovery in digital implementation of analogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2207" to="2223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Approximation capabilities of neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Unterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Arodz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning long term dependencies via Fourier recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Following Appendix B.1.3, we choose the following measure and orthonormal basis polynomials</title>
		<imprint/>
	</monogr>
	<note>Measure and Basis The basic Chebyshev measure is ? cheb = (1 ? x 2 ) ?1/2 on (?1, 1). in terms of the Chebyshev polynomials of the first kind T n</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
