<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangkai</forename><surname>Jiao</surname></persName>
							<email>jiaofangkai@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from overfitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform selfsupervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Logical reasoning has long been recognized as one key critical thinking ability of human being. Until very recently, some pioneer researchers have crystallized this for the NLP community, and built several public challenging benchmarks, such as ReColor <ref type="bibr" target="#b31">(Yu et al., 2020)</ref> and LogiQA <ref type="bibr" target="#b16">(Liu et al., 2020)</ref>. Logical reasoning 2 requires to correctly infer the semantic relations with respect to the constituents among different sentences. A typical formulation of logical reasoning is illustrated <ref type="figure">Figure 1</ref>: An instance of logical reasoning from the Re-Clor dataset. To infer the right answer, we should uncover the underlying logical structure, as shown in the bottom. (x) represents the logical variable (e.g., entity or phrase) and r j denotes the relation (e.g., predicate) between two logical variables.r j is the passive relation of r j .</p><p>in <ref type="figure">Figure 1</ref>, namely, a real-world examination instance from ReClor. As can be seen, to find the correct answer for the given question, one needs to extract the logical structures residing in a pair of each option and the whole context, and justify its reasonableness.</p><p>As a matter of fact, logical reasoning is still at its initial stage, thence, existing studies are somewhat rare in literature. Some efforts have been devoted to designing specific model architectures or integrating symbolic logic as the hints attached to the potential logical structure. For instance,  and <ref type="bibr" target="#b19">Ouyang et al. (2021)</ref> first constructed a graph of different constituents and then performed implicit reasoning with graph neural networks (GNNs). <ref type="bibr" target="#b29">Wang et al. (2021)</ref> proposed LReasoner, a unified context extension and data augmentation framework based on the parsed logical expressions.</p><p>These approaches have achieved some progress on benchmark datasets. However, though equipped with pre-trained language models, they still suffer from problems like overfitting and poor generalization. We attribute these drawbacks to the difficulty of building a model aware of the logical relations beneath natural language, which is revealed from two sides: 1) the high sparsity of the existing datasets, and 2) the goal of general pre-training, i.e., masked language modeling <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, which however, deviates largely from that of the logical reasoning. To tackle this issue, we aim to build a bridge between logical reasoning and self-supervised pre-training, and accordingly inherit the strong generalization power from pre-trained language models.</p><p>Our proposed method is inspired by the recent progress of contrastive learning based pre-training. It mainly consists of two novel components: metapath guided data construction and counterfactual data augmentation. Both components are leveraged to perform automatic instance composition from unlabeled corpus (e.g., Wikipedia) for contrastive learning. Regarding the first component, we propose to employ the meta-path to define a symbolic form of logical structure. The intuition behind this is that the logical structure can be expressed as a reasoning path composed of a series of relation triplets, and a meta-path inherently offers such a means of consistency <ref type="bibr" target="#b20">(Liu et al., 2021)</ref>. Specifically, given an arbitrary document and a pair of entities in it, we try to find a positive instance pair in the document according to the logical structure. And the negative ones can thus be generated by modifying the relations involved in the structure, which explicitly break the logical consistency. Nevertheless, the contrastive learning often fails when models easily locate trivial solutions <ref type="bibr" target="#b13">(Lai et al., 2021)</ref>. In this context, the pre-trained language model may exclude the negative options through their conflicts with the world knowledge. To eliminate this information shortcut, in our second novel component, we devise a strong counterfactual data augmentation <ref type="bibr" target="#b34">(Zeng et al., 2020b)</ref> strategy. By mixing counterfactual data during pre-training, of which the positive instance pair is also against the world knowledge, this component shows more ad-vantage in reasoning over logical relations.</p><p>We integrate this method with both AL-BERT <ref type="bibr" target="#b14">(Lan et al., 2020)</ref> and RoBERTa  3 for further pre-training, and then fine-tune them on two downstream logical reasoning benchmarks, i.e., ReClor and LogiQA. The experimental results demonstrate that our method can outperform all the existing strong baselines, yet without any augmentation from the original training data. Besides, the ablation studies also show the effectiveness of the two essential strategies in our method. The contribution of this paper is summarized as follows:</p><p>1. We propose MERIt, a MEta-path guided contrastive learning method for logical Reason-Ing of text, to reduce the heavy reliance on annotated data. To the best of our knowledge, we are the first to explore self-supervised pretraining for logical reasoning.</p><p>2. We successfully employ the meta-path strategy to mine the potential logical structure in raw text. It is able to automatically generate negative candidates for contrastive learning via logical relation editing.</p><p>3. We propose a simple yet effective counterfactual data augmentation method to eliminate the information shortcut during pre-training.</p><p>4. We evaluate our method on two logical reasoning tasks, LogiQA and ReClor. The experimental results show that our method achieves the new state-of-the-art performance on two benchmark datasets.</p><p>2 Related Work 2.1 Self-Supervised Pre-training</p><p>With the success of language modeling based pretraining <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>, designing self-supervised pretext tasks to facilitate specific downstream ones has been extensively studied thus far. For example, <ref type="bibr" target="#b8">Guu et al. (2020)</ref> proposed to train the retriever jointly with the encoder via retrieval enhanced masked language modeling for open-domain question answering. <ref type="bibr" target="#b11">Jiao et al. (2021)</ref> devised a retrieval-based pre-training approach to bridge the gap between language modeling and machine reading comprehension by enhancing the evidence extraction ability. <ref type="bibr" target="#b3">Deng et al. (2021)</ref> proposed ReasonBERT to facilitate complex reasoning over multiple and hybrid contexts. The model is pre-trained on automatically constructed query-evidence pairs, which involve different types of corpora and long-range relations. In addition, contrastive learning <ref type="bibr" target="#b9">(Hadsell et al., 2006)</ref> contributes to a strong toolkit to implement self-supervised pre-training. The key to contrastive learning is to build efficacious positive and negative counterparts. For example, <ref type="bibr" target="#b5">Gao et al. (2021)</ref> leveraged Dropout <ref type="bibr" target="#b22">(Srivastava et al., 2014)</ref> to build positive pairs from the same sentence while keeping the semantics untouched. Other sentences in the same mini-batch serve as negative candidates to obtain better sentence embeddings. ERICA <ref type="bibr" target="#b20">(Qin et al., 2021)</ref> is a knowledge enhanced language model pre-trained through entity and relation discrimination, where the negative candidates are sampled from the pre-defined dictionaries. Nevertheless, directly employing these contrastive learning approaches to logical reasoning is arduous. One possible reason to this is the absence of distant labels or strong assumptions to group the naturally occurring text by its logical structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Logical Reasoning</head><p>Logical reasoning has attracted increasing research attention recently. Devising specific model architectures and integrating symbolic logic have been proved to be two effective solutions. For example,  and <ref type="bibr" target="#b19">Ouyang et al. (2021)</ref> proposed to extract the basic units for logical reasoning, e.g., the elementary discourse or fact units, and then employed GNNs to model possible relationships. The graph structure of constituents can be viewed as a form of prior knowledge pertaining to logical relations. Differently, <ref type="bibr" target="#b0">Betz (2020)</ref> and <ref type="bibr" target="#b2">Clark et al. (2020)</ref> used synthetically generated datasets to prove that the Transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> or pre-trained GPT-2 is able to perform complex reasoning, motivating following researchers to introduce symbolic rules into neural models. For example, <ref type="bibr" target="#b29">Wang et al. (2021)</ref> developed a context extension and data augmentation framework, which is based on the extracted logical expressions. Superior performance over its contenders can be observed on the ReClor dataset.</p><p>In this paper, we propose a self-supervised contrastive learning approach to enhance the logical reasoning ability of neural models. Orthogonal to existing methods, our approach is endowed with two intriguing merits: 1) it shows strong advan-tage in utilizing the unlabeled text data, and 2) the symbolic logic is seamlessly introduced into neural models via the guidance of meta-path for automatic data construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contrastive Learning</head><p>Contrastive Learning (CL) aims to learn recognizable representations by pulling the semantically similar examples close and pushing apart the dissimilar ones <ref type="bibr" target="#b9">(Hadsell et al., 2006)</ref>. Given an instance x, a semantically similar example x + , and a set of dissimilar examples X ? to x, the objective of CL can be formulated as:</p><formula xml:id="formula_0">L CL = L(x, x + , X ? ) = ? log exp f (x, x + ) x ?X ? ?{x + } exp f (x, x )<label>(1)</label></formula><p>where f is the model to be optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Symbolic Logical Reasoning</head><p>As shown in <ref type="figure">Figure 1</ref>, given a context containing a series of logical variables {v 1 , v 2 , ? ? ? , v n }, and the relations between them, the logical reasoning objective is to judge whether a triplet v i , r i,j , v j in language, where r i,j is the relation between v i and v j , can be inferred from the context through a reasoning path:</p><formula xml:id="formula_1">v i , r i,j , v j ? (v i r i,i+1 ?? v i+1 ? ? ? r j?1,j ?? v j ). (2)</formula><p>The equation is also referred to symbolic logic rules <ref type="bibr" target="#b2">(Clark et al., 2020;</ref><ref type="bibr" target="#b20">Liu et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Meta-Path</head><p>Given an entity-level knowledge graph, where the nodes refer to entities and edges are the relations among them, the meta-path connecting two target entities e i , e j can be given as,</p><formula xml:id="formula_2">e i r i,i+1 ?? e i+1 r i+1,i+2 ?? ? ? ? e j?1 r j?1,j ?? e j ,<label>(3)</label></formula><p>where r i,j denotes the relation between entities e i and e j . The meta-path in the entity-level knowledge graph are often employed as a particular data structure expressing the relation between two indirectly connected entities <ref type="bibr" target="#b33">(Zeng et al., 2020a;</ref><ref type="bibr" target="#b29">Xu et al., 2021)</ref>. The screenplay was written by Mirror Mask, from a story by Gaiman and Stephanie Leonidas. Positive Data Pair = 1 , 5 ? 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Graph Construction (b) Meta-Path Guided Positive Instance Construction (c) Negative Candidate Generation (d) Counterfactual Data Augmentation</head><p>Context-oriented</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(e) Objectives of Contrastive Learning</head><p>Option-oriented <ref type="figure">Figure 2</ref>: The overall framework of our proposed method. (a) A document D from Wikipedia and the corresponding entity-level graph construction. The sentences in black will be extracted as the context input for (b). (b) Given two target entities e 1 , e 5 , the possible answers A + and the meta-path are firstly extracted. The context sentences S connecting the entities in the meta-path, and the answers in A, are leveraged to yield positive instance pairs. (c) Given a sentence z with alternative relations, the relation modification for negative context sentence and option construction is implemented through entity replacement. The top operation is performed for negative options while the bottom one is to facilitate negative contexts. (d) The counterfactual sentences are generated by entity replacement to eliminate the information shortcut during pre-training. (e) The generated positive and negative samples are used for contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this paper, we study the problem of logical reasoning on the task of multiple choice question answering (MCQA). Specifically, given a passage P , a question Q and a set of K options O = {O 1 , ? ? ? , O K }, the goal is to select the correct option O y , where y ? [1, K]. Notably, to tackle this task, we devise a novel pre-training method equipped with contrastive learning, where the abundant knowledge contained in the largescale Wikipedia documents is explored. We then transfer the learned knowledge to the downstream logical reasoning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">From Logical Reasoning to Meta-Path</head><p>In a sense, in MCQA for logical reasoning, both the given context (i.e., passage and question) and options express certain relations between different logical variables <ref type="figure">(Figure 1)</ref>. Go a step further, following Equation 2, the relation triplet contained in the correct option should be deduced from the given context through a reasoning path, while that in the wrong options should not. In other words, the context is logically consistent with the correct option only.</p><p>In light of this, the training instances for our contrastive learning based pre-training should be in the form of a context-option pair, where the context consists of multiple sentences and expresses the relations between the included constituents, while the option should illustrate the potential relations between parts of the constituents. Nevertheless, it is non-trivial to derive such instance pairs from large-scale unlabeled corpus like Wikipedia due to the redundant constituents, e.g., nouns and predicates. In order to address it, we propose to take the entities contained in unlabeled text as logical variables, and Equation 2 can be transformed as:</p><formula xml:id="formula_3">e i , r i,j , e j ? (e i r i,i+1 ?? e i+1 ? ? ? r j?1,j ?? e j ). (4)</formula><p>As can be seen, the right part above is indeed a meta-path connecting e i , e j as formulated in Equation 3, indicating an indirect relation between e i , e j through intermediary entities and relations. In order to aid the logical consistency conditioned on entities to be established, we posit an assumption that under the same context (in the same passage), the definite relation between a pair of en-tities can be inferred from the contextual indirect one, or at least not logically contradict to it. Taking the passage in <ref type="figure">Figure 2</ref> as an example, it can be concluded from the sentences s 1 and s 5 that, the director McKean has cooperated with Stephanie Leonidas. Therefore, the logic is consistent between {s 1 , s 5 } and s 3 . This can be viewed as a weaker constraint than the original one in Equation 2 for logical consistency, yet it can be further enhanced by constructing negative candidates violating logics.</p><p>Motivated by this, given an arbitrary document D = {s 1 , ? ? ? , s m }, where s i is the i-th sentence, we can first build an entity-level graph, denoted as G = (V, E), where V is the set of entities contained in D and E denotes the set of relations between entities. Notably, to comprehensively capture the relations among entities, we take into account both the external relation from the knowledge graph and the intra-sentence relation. As illustrated in <ref type="figure">Figure 2</ref> (a), there will be an intra-sentence relation between two entities if they are mentioned in a common sentence. Thereafter, we can derive the pre-training instance pairs according to the meta-paths extracted from the graph, which will be detailed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Meta-Path Guided Positive Instance Construction</head><p>As defined in Equation 4, in the positive instances, the answer should contain a relation triplet that is logically consistent with the given context. Since we take the intra-sentence relationship into consideration, given a pair of entities contained in the document, we first collect the sentences mentioning both of them as the set of answer candidates. Accordingly, we then try to find a meta-path connecting the entity pair and hence derive the corresponding logically consistent context. In particular, as shown in <ref type="figure">Figure 2</ref> (b), given an entity pair e i , e j , we denote the collected answer candidates as A + , and then we use Depth-First Search <ref type="bibr" target="#b24">(Tarjan, 1972)</ref> to find a meta-path linking them on G, following Equation 3. Thereafter, the context sentences S corresponding to the answer candidates in A + are derived by retrieving those sentences undertaking the intra-sentence relations during the search algorithm. Finally, for each answer candidate a ? A + , the pair (S, a) is treated as a positive context-answer pair to facilitate our contrastive learning. The details of positive instance generation algorithm are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Negative Instance Generation</head><p>In order to obtain the negative instances (i.e., negative context-option pairs) where the option is not logically consistent with the context, the most straightforward way is to randomly sample the sentences from different documents. However, this approach could lead to trivial solutions by simply checking whether the entities involved in each option are the same as those in the given context. In the light of this, we resort to directly breaking the logical consistency of the positive instance pair by modifying the relation rather than the entities in the context or the option, to derive the negative instance pair.</p><p>In particular, given a positive instance pair (S, a), we devise two negative instance generation methods: the context-oriented and the optionoriented method, focusing on generating negative pairs by modifying the relations involved in the context S and answer a of the positive pair, respectively. Considering that the relation is difficult to be extracted, especially the intra-sentence relation, we propose to implement this reversely via the entity replacement. In particular, for the optionoriented method, suppose that e i , e j is the target entity pair for retrieving the answer a, we first randomly sample a sentence z that contains at least one different entity pair e a , e b from e i , e j as the relation provider. We then obtain the negative option by replacing the entities e a and e b in z with e i and e j , respectively. The operation is equivalent to replacing the relation contained in a with that in z. Formally, we denote the operation as</p><formula xml:id="formula_4">a ? = Relation_Replace(z ? a).</formula><p>Pertaining to the context-oriented negative instance generation method, we first randomly sample a sentence s i ? S, and then conduct the modification process as follows,</p><formula xml:id="formula_5">s ? i = Relation_Replace(z ? s i ),</formula><p>where the entity pair to be replaced in s i should be contained in the meta-path corresponding to the target entity pair e i , e j . Accordingly, the negative context can be written as S ? = S \ {s i } ? {s ? i }. <ref type="figure">Figure 2 (c)</ref> illustrates the above operations on both the answer and context sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Counterfactual Data Augmentation</head><p>According to <ref type="bibr" target="#b12">Ko et al. (2020)</ref>; <ref type="bibr" target="#b6">Guo et al. (2019)</ref>; <ref type="bibr" target="#b13">Lai et al. (2021)</ref>; <ref type="bibr" target="#b7">Guo et al. (2022)</ref>, the neural models are adept at finding a trivial solution through the illusory statistical information in datasets to make correct predictions, which often leads to inferior generalization. In fact, this issue can also occur in our scenario. In particular, since the correct answer is from a natural sentence and describes a real world fact, while the negative option is synthesized by entity replacement, which may conflict with the commonsense knowledge. As a result, the pretrained language model tends to identify the correct option directly by judging its factuality rather than the logical consistency with the given context. For example, as shown in <ref type="figure">Figure 2 (d) (left)</ref>, the language model deems a as correct, simply due to that the other synthetic option a ? conflicts with the world knowledge.</p><p>To overcome this problem, we develop a simple yet effective counterfactual data augmentation method to further improve the capability of logical reasoning <ref type="bibr" target="#b34">(Zeng et al., 2020b)</ref>. Specifically, given the entities P that are involved in the metapath, we randomly select some entities from P and replace their occurrences in the context and the answer of the positive instance pair (S, a) with the entities extracted from other documents. In this manner, the positive instance also contradicts to the world knowledge. Notably, considering that the positive and negative instance pairs should keep the same set of entities, we also conduct the same replacement for a ? or S ? , if they mention the selected entities. As illustrated in <ref type="figure">Figure 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Contrastive Learning based Pre-training</head><p>As discussed in previous subsection, there are two contrastive learning schemes: option-oriented CL and context-oriented CL. Let A ? be the set of all constructed negative options with respect to the correct option a. The option-oriented CL can be formulated as:</p><formula xml:id="formula_6">L OCL = L(S, a, A ? ).<label>(5)</label></formula><p>In addition, given C ? as the set of all generated negative contexts corresponding to S, the objective of context-oriented CL can be written as:</p><formula xml:id="formula_7">L CCL = L(a, S, C ? ).<label>(6)</label></formula><p>To avoid the catastrophic forgetting problem, we also add the MLM objective during pre-training and the final loss is:</p><formula xml:id="formula_8">L = L OCL + L CCL + L MLM .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Fine-tuning</head><p>During the fine-tuning stage, to approach the task of MCQA, we adopt the following loss function:</p><formula xml:id="formula_9">L QA = ? log exp f (P, Q, O y ) i exp f (P, Q, O i ) ,<label>(8)</label></formula><p>where O y is the ground-truth option for the question Q, given the passage P . <ref type="figure" target="#fig_2">Figure 3</ref> shows the overall training scheme of our method. f is the model to be optimized, ?, ? 0 , ? 1 and ? are parameters of different modules. During pre-training, we use a 2-layer MLP as the output layer. The parameters of the output layer are denoted as ? 0 , and ? represents the pre-trained Transformer parameters. As for the fine-tuning stage, we employ two schemes. For simple fine-tuning, we follow <ref type="bibr" target="#b4">Devlin et al. (2019)</ref> to add another 2layer MLP with randomly initialized parameters ? 1 on the top of the pre-trained Transformer. In addition, to fully take advantage the knowledge acquired during pre-training stage, we choose to directly fine-tune the pre-trained output layer with optimizing both ? and ? 0 . In order to address the discrepancy that the question is absent during pretraining, the prompt-tuning technique <ref type="bibr" target="#b15">(Lester et al., 2021)</ref> is employed. Specifically, some learnable embeddings with randomly initialized parameters ? are appended to the input to transform the question in downstream tasks into declarative constraint. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Baseline</head><p>We evaluated our method on two challenging logical reasoning benchmarks, i.e., LogiQA and Re-Clor, with several strong baselines, including the pre-trained language models, DAGN , Focal Reasoner <ref type="bibr" target="#b19">(Ouyang et al., 2021)</ref> and LReasoner . For more details, please refer to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Detail</head><p>We further pre-trained RoBERTa and ALBERT on Wikipedia for another 500 and 100 steps, respectively, and the batch size for pre-training is set to 4,096. All experiments conducted on downstream tasks are repeated for 5 times with different random seeds. The knowledge graph we used for constructing training data is provided by <ref type="bibr" target="#b20">Qin et al. (2021)</ref>. More implementation details can be found in Appendix C.</p><p>6 Result and Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overall Results</head><p>The overall results on ReClor and LogiQA are shown in <ref type="table" target="#tab_1">Table 1</ref>. It can be observed that 1) MERIt outperforms all the strong baselines using the same backbone with significant improvements. Besides, our method achieves the new state-of-theart performance on both datasets. 2) Our method leads to drastic contribution to the original models without further pre-training, i.e., RoBERTa and ALBERT, and the prompt-tuning further enhances our model with a significant performance margin, which both demonstrate the potential of our pretraining method. 3) MERIt achieves better performance on the more difficult split of ReClor (Test-H), indicating that our pre-training method is less affected by the statistical shortcut <ref type="bibr" target="#b31">(Yu et al., 2020)</ref>. 4) MERIt + Prompt does not benefit from the framework of LReasoner significantly. This is probably because the basic knowledge about logic rules has been covered in our method. 5) We also report the best result on the test set on LogiQA and ReClor for fair comparison with the published results of LReasoner. It can be observed that in terms of the best accuracy on the test set, our model still outperforms LReasoner consistently based on both RoBERTa and ALBERT. <ref type="table">Table 2</ref> shows the results of our ablation studies. To observe the impacts brought by the meta-path strategy, we built a baseline model without the metapath strategy by randomly selecting the sentences in a passage to form the context-answer pairs. From this table we can conclude that: 1) the model without counterfactual data augmentation (-DA) has a severe performance degradation. It suggests that the counterfactual data is essential for MERIt to conduct logical reasoning. As for the  ratio of original data to the counterfactual one, on test set, we found that 1:3 (+ DA 3 ) leads to better performance using prompt tuning while 1:2 (+ DA 2 ) obtains the best performance using simple fine-tuning. 2) The model without the guidance of meta-path (-Meta-Path) demonstrates a much worse performance than MERIt, indicating that the meta-path strategy plays an important role by discovering the potential logic structure. 3) Considering the results of models without the objectives of option-oriented CL and context-oriented CL, it can be seen that both contrastive learning schemes are beneficial for logical reasoning. In addition, the context-oriented CL is more effective than optionoriented CL. One possible reason to this is that the context-oriented CL is more diverse in format since each sentence can be disturbed while the optionoriented CL will make the model pay more attention to the option, leading to a worse generalization during fine-tuning. <ref type="figure" target="#fig_3">Figure 4</ref> shows the accuracy on the test set and test-H set of ReClor with respect to different amount of training data. We reported the average results of MERIt + Prompt, LReasoenr and RoBERTa. It can be observed that: 1) With the scale of training data becoming larger, the performance of all models achieves improvements. 2) MERIt + Prompt shows Pre-training Steps <ref type="figure">Figure 5</ref>: The prompt-tuning results on ReClor using the models pre-trained with different steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance with Limited Training Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Test RoBERTa 84.9 84.2 MERIt 85.9 85.5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of Pre-training Steps</head><p>In order to explore the effects of pre-training steps, we fine-tuned the models pre-trained for different steps on ReClor and the results are shown in <ref type="figure">Figure 5</ref>. From the histogram we can find that our method achieves the best performance on dev set at 500 steps. Besides, the model pre-trained with 100 steps (using only around 410k samples) has achieved comparable performance with the best one, indicating that our method is very competitive with few training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Performance on DREAM</head><p>We also evaluated our method on another benchmark requiring complex reasoning abilities, DREAM <ref type="bibr" target="#b23">(Sun et al., 2019)</ref>, to verify its generalization ability to different tasks. As shown in <ref type="table" target="#tab_3">Table 3</ref>, our method can also make significant improvements compared with RoBERTa, demonstrating the generalization ability of our method.   <ref type="table" target="#tab_5">Table 4</ref> shows the results of DeBERTa-v2-xlarge and DeBERTa-v2-xxlarge on ReClor, which validate that our method can be scaled to stronger pre-trained language models with significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Results of DeBERTa</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we present MERIt, a meta-path guided contrastive learning method to facilitate logical reasoning via self-supervised pre-training. MERIt is built upon the meta-path strategy for automatic data construction and the counterfactual data augmentation to eliminate the information shortcut during pre-training. With the evaluation on two logical reasoning benchmarks, our method has obtained significant improvements over strong baselines relying on task-specific model architecture or augmentation of original dataset. Pertaining to the further work, we plan to strengthen our method from both data construction and model architecture design angles. More challenging instances are expected to be constructed if multiple meta-paths can be considered at the same time. Besides, leveraging GNNs may bring better interpretability and generalization since the graph structure can be integrated into both pre-training and fine-tuning stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DFS-based Algorithm for Meta-Path Extraction</head><p>Algorithm 1 The DFS algorithm to obtain the meta-paths.</p><p>Input: The graph G = (E, V); The sentences of the document D = {s 1 , ? ? ? , s m }; The entity set of the i-th sentence V i ; Output: P, S, and A + ; 1: for each (e i , e j ) ? V ? V and i = j do 2:</p><formula xml:id="formula_10">A + = {s k |e i ? V k , e j ? V k }; 3: D = D \ A + ; 4: cond, P, S ? DFS(e i , {e i }, ?, e j , G, D ); 5:</formula><p>if cond is TRUE and A + is not ? then ReClor <ref type="bibr" target="#b31">(Yu et al., 2020)</ref> is extracted from logical reasoning questions of standardized graduate admission examinations. The held-out test set is further divided into EASY and HARD subsets, denoted as test-E and test-H, respectively. The instances in test-E are biased and can be solved even without knowing contexts and questions by neu-ral models. A leaderboard 4 is also host for public evaluation.</p><p>LogiQA <ref type="bibr" target="#b16">(Liu et al., 2020)</ref> consists of 8,678 multiple-choice questions collected from National Civil Servants Examinations of China and are manually translated into English by experts. The dataset is randomly split into train/dev/test sets with 7,376/651/651 samples, respectively. LogiQA contains various logical reasoning types, e.g., categorical reasoning and sufficient conditional reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Baseline</head><p>DAGN ) is a discourse-aware graph network that reasons on the discourse structure of texts. It is based on elementary discourse units and discourse relations. DAGN (Aug) is a variant that augments the graph features. Focal Reasoner <ref type="bibr" target="#b19">(Ouyang et al., 2021</ref>) is a factdriven logical reasoning model, which builds supergraphs on the top of fact units as the basis for logical reasoning. It captures both global connections between facts and the local concepts or actions inside the fact. LReasoner ) includes a context extension framework and a data augmentation algorithm, which are all conducted based on the extracted logical expressions. This method has achieved new state-of-the-art performance on Re-Clor recently.</p><p>Besides, we also compare the performance with the directly fine-tuned large pre-trained language models, including RoBERTa and ALBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Detail</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Data Construction</head><p>During the data construction process, we have employed two tricks to improve the complexity of the pretext task:</p><p>1. For the sentence z as the relation provider for negative instance construction, the sentences from the document are primarily to be considered because they share the same entities with the context or describe the same topic. This can also be viewed as a trick to avoid trivial solution by checking whether the samples come from the same domain. Another problem is that if z comes from the same document, taking the option-oriented method as  example, the replacement may not work if e i = e a and e j = e b . To address it, we will change the order of the entities to be replaced, i.e., swapping the mentions of e i and e j .</p><p>2. Similarly, for counterfactual data augmentation, supposing the extracted meth-path of a training instance connects an entity pair e i , e j , e i and e j are always considered to be replaced for generating counterfactual data. And thus the sets of answer candidates A + constructed from other documents, where the corresponding meta-paths also link e i , e j , can be employed as negative candidates directly. The motivation of the trick is to avoid modifications on the original texts as many as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Pre-training Setting</head><p>We employed the model implementation of Transformer from Huggingface <ref type="bibr">(Wolf et al., 2020)</ref> and pytorch 5 framework. The corpus for pre-training is generated from the dataset provided by <ref type="bibr" target="#b20">Qin et al. (2021)</ref>  <ref type="bibr">6</ref> , which includes the pre-processed passages from Wikipedia and the recognized entities with their distantly annotated relations. The generated corpus contains one million samples and each sample has 3 negative options.</p><p>During pre-training, we adopted the LAMB <ref type="bibr" target="#b30">(You et al., 2020)</ref> optimizer, warming up the learning rate to the peak and then linearly decaying it. It takes 32 hours on 4 RTX 2080Ti GPUs for RoBERTa pre-training and 3 days on 2 TeslaT4 GPUs for ALBERT pre-training. Other hyperparameters for pre-training are reported in <ref type="table" target="#tab_7">Table 5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Hyper-parameters for Fine-tuning</head><p>The random seeds we utilized for repeated experiments are 42, 43, 44, 45 and 4321. The hyperparameters for fine-tuning are shown in <ref type="table" target="#tab_11">Table 7</ref>. <ref type="figure" target="#fig_5">Figure 6</ref> shows the constructed examples for contrastive learning as well as the corresponding counterfactual examples. <ref type="table" target="#tab_9">Table 6</ref> shows the results of linear probing on Re-Clor, where we used a single linear layer as the output layer and only fine-tuned its parameters. As shown in the table, MERIt (100 steps) and MERIt (ALBERT) outperform RoBERTa and AL-BERT on both dev and test set, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Case Study for Generated Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Results for Linear Probing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F A Different View from Contrastive Graph Representation Learning</head><p>To understand why the pre-training approach can promote logical reasoning, we provide a different view from the contrastive learning for graphs. Following <ref type="bibr" target="#b21">Qiu et al. (2020)</ref>, x and x + in Equation 1 are different sub-graphs extracted from the same graph through random walk with restart <ref type="bibr" target="#b25">(Tong et al., 2006</ref>) while x ? is sub-graph sampled from a different graph. To avoid the trivial solution by simply checking whether the node indices of two subgraphs match, they also developed an anonymization operation by relabeling the nodes of each subgraph. In fact, our proposed method can be taken as a special case of graph contrastive learning. Firstly, the context and answer based on the meta-path can be viewed as sub-graphs of G. In particular, the answer is the sub-graph with only two nodes (the two entities connected by the meta-path). Secondly, the entity replacement for negative candi-dates construction and counterfactual data generation play similar roles with the anonymization operation. Both of them aim at guiding the model focus on the logical/graph structure. The only assumption our approach built upon is that inferring the consistency defined in Equation 4 is in demand of logical reasoning, which has already been explored in many studies for document-level relation extraction <ref type="bibr" target="#b32">(Zeng et al., 2021</ref><ref type="bibr" target="#b33">(Zeng et al., , 2020a</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALBERT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(d) (right), a counterfactual instance can be generated by replacing Mirror Mask and Stephanie Leonidas in a and a ? with [ENT A] and [ENT B], where [ENT A] and [ENT B] are arbitrary entities. Ultimately, the key to infer the correct answer lies in the accurate inference of the logical relation between entities [ENT A] and [ENT B] implied in each context-option pair. We provide more cases of the constructed data and their corresponding counterfactual samples in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The overall training scheme of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Results on the test set (left) and the test-H set (right) of ReClor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>DFS(e i , P , S , e d , G = (E, V), D ) each (e j , s k ) ? V ? D and (e i , e j ) ? E, e j ? V k do 16: G = (E, V \ {e j }); DFS(e j , P , S , e d , G , D );</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Two cases of the generated and the counterfactual examples. The target entities used for extracting meta-path are colored in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>)", McKean ( 2 )'s first feature film as director, premiered at ? in January 2005. ( ) The screenplay was written by Neil Gaiman ( 3 ), from a story by Gaiman and McKean. ( 3 ) A children's fantasy ?, "Mirror Mask" was produced by Jim Henson Studios ( 4 ) and stars a British cast Stephanie Leonidas ( 5 ), ? and Gina McKee ( 6 ). ( 4 ) Before "Mirror Mask", McKean directed a number of ?. ( 5 ) McKean has directed "The Gospel of Us ( 7 )", ?. A new feature film, "Luna", written and directed by McKean and starring Stephanie Leonidas, ..., debuted at ?.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">A children's fantasy ?, "Mirror</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mask" was produced by ? and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">stars a British cast Stephanie</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Leonidas, ?, and Gina McKee.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Answer: = 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>The screenplay was written by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Relation Provider z</cell><cell>Neil Gaiman, from a story by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gaiman and McKean.</cell></row><row><cell>7</cell><cell>6</cell><cell></cell><cell>Entity</cell><cell cols="2">A new feature film, "Luna", ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">and directed by McKean and</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Intra-Sentence</cell><cell cols="2">starring Stephanie Leonidas ?.</cell></row><row><cell>5</cell><cell></cell><cell>4</cell><cell>Relation</cell><cell cols="2">Context Sentence 5</cell></row><row><cell>2</cell><cell>1</cell><cell>3</cell><cell>External Relation</cell><cell cols="2">Negative Context ? = 1 , 5 ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">A children's fantasy which ?, "Mirror</cell><cell>A children's fantasy which ?,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mask" was produced by ? and stars a</cell><cell>"[ENT A]" was produced by ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">British cast Stephanie Leonidas, ?,</cell><cell>and stars a British cast [ENT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>and Gina McKee.</cell><cell>B], ?, and Gina McKee.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>The screenplay was written by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[ENT A], from a story by Gaiman</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and [ENT B].</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, ?</cell><cell>, ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, ?</cell><cell>? ,</cell></row></table><note>? ( ) "Mirror Mask ( 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The overall results on ReClor and LogiQA. We adopt the accuracy as the evaluation metric and all the baselines are based on RoBERTa except specific statement. For each model we repeated training for 5 times using different random seeds and reported the average results.</figDesc><table><row><cell>Model / Dataset</cell><cell>Dev</cell><cell>Test</cell><cell>ReClor Test-E</cell><cell>Test-H</cell><cell>Dev</cell><cell>LogiQA</cell><cell>Test</cell></row><row><cell>RoBERTa</cell><cell>62.6</cell><cell>55.6</cell><cell>75.5</cell><cell>40.0</cell><cell>35.0</cell><cell></cell><cell>35.3</cell></row><row><cell>DAGN</cell><cell>65.2</cell><cell>58.2</cell><cell>76.1</cell><cell>44.1</cell><cell>35.5</cell><cell></cell><cell>38.7</cell></row><row><cell>DAGN (Aug)</cell><cell>65.8</cell><cell>58.3</cell><cell>75.9</cell><cell>44.5</cell><cell>36.9</cell><cell></cell><cell>39.3</cell></row><row><cell>LReasoner (RoBERTa)  ?</cell><cell>64.7</cell><cell>58.3</cell><cell>77.6</cell><cell>43.1</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Focal Reasoner</cell><cell>66.8</cell><cell>58.9</cell><cell>77.1</cell><cell>44.6</cell><cell>41.0</cell><cell></cell><cell>40.3</cell></row><row><cell>MERIt</cell><cell>66.8</cell><cell>59.6</cell><cell>78.1</cell><cell>45.2</cell><cell>40.0</cell><cell></cell><cell>38.9</cell></row><row><cell>MERIt + LReasoner</cell><cell>67.4</cell><cell>60.4</cell><cell>78.5</cell><cell>46.2</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>MERIt + Prompt</cell><cell>69.4</cell><cell>61.6</cell><cell>79.3</cell><cell>47.8</cell><cell>39.9</cell><cell></cell><cell>40.7</cell></row><row><cell>MERIt + Prompt + LReasoner</cell><cell>67.3</cell><cell>61.4</cell><cell>79.8</cell><cell>46.9</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>ALBERT</cell><cell>69.1</cell><cell>66.5</cell><cell>76.7</cell><cell>58.4</cell><cell>38.9</cell><cell></cell><cell>37.6</cell></row><row><cell>MERIt (ALBERT)</cell><cell>74.2</cell><cell>70.1</cell><cell>81.6</cell><cell>61.0</cell><cell>43.7</cell><cell></cell><cell>42.5</cell></row><row><cell>MERIt (ALBERT) + Prompt</cell><cell>74.7</cell><cell>70.5</cell><cell>82.5</cell><cell>61.1</cell><cell>46.1</cell><cell></cell><cell>41.7</cell></row><row><cell>max</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LReasoner (RoBERTa)</cell><cell>66.2</cell><cell>62.4</cell><cell>81.4</cell><cell>47.5</cell><cell>38.1</cell><cell></cell><cell>40.6</cell></row><row><cell>MERIt</cell><cell>67.8</cell><cell>60.7</cell><cell>79.6</cell><cell>45.9</cell><cell>42.4</cell><cell></cell><cell>41.5</cell></row><row><cell>MERIt + Prompt</cell><cell>70.2</cell><cell>62.6</cell><cell>80.5</cell><cell>48.5</cell><cell>39.5</cell><cell></cell><cell>42.4</cell></row><row><cell>LReasoner (ALBERT)</cell><cell>73.2</cell><cell>70.7</cell><cell>81.1</cell><cell>62.5</cell><cell>41.6</cell><cell></cell><cell>41.2</cell></row><row><cell>MERIt (ALBERT)</cell><cell>73.2</cell><cell>71.1</cell><cell>83.6</cell><cell>61.3</cell><cell>43.9</cell><cell></cell><cell>45.3</cell></row><row><cell>MERIt (ALBERT) + Prompt</cell><cell>75.0</cell><cell>72.2</cell><cell>82.5</cell><cell>64.1</cell><cell>45.8</cell><cell></cell><cell>43.8</cell></row></table><note>? : The results are reproduced by ourselves. max: The results of the model achieving the best accuracy on the test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Performance comparisons on ReClor between different variants of MERIt. DA means data augmentation and DA N refers to 1:N ratio of the original data to the augmented data. P. is short for Prompt Tuning.</figDesc><table><row><cell>Model</cell><cell>Dev Dev (P.) Test Test (P.)</cell></row><row><cell>MERIt</cell><cell>66.8 69.4 59.6 61.6</cell></row><row><cell>-DA</cell><cell>63.0 64.5 57.9 59.8</cell></row><row><cell>+ DA 2</cell><cell>65.3 67.8 60.2 61.3</cell></row><row><cell>+ DA 3</cell><cell>66.2 68.0 59.3 61.9</cell></row><row><cell cols="2">-Option-oriented CL 63.8 65.4 58.9 61.5</cell></row><row><cell cols="2">-Context-oriented CL 64.0 66.5 58.8 60.2</cell></row><row><cell>-Meta-Path</cell><cell>64.8 65.1 58.0 60.8</cell></row><row><cell>Table 2: Data</cell><cell></cell></row><row><cell>Ratio of Training Data</cell><cell>Ratio of Training Data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: The accuracy of different models on DREAM</cell></row><row><cell>dataset.</cell></row><row><cell>better performance under low resource, especially</cell></row><row><cell>on test-H. Our method trained on 40% data has</cell></row><row><cell>achieved comparable performance with RoBERTa.</cell></row><row><cell>In addition, on test-H, our method outperforms</cell></row><row><cell>RoBERTa and LReasoner trained on full dataset us-</cell></row><row><cell>ing only 20% and 40% training data, respectively,</cell></row><row><cell>evidently demonstrating the generalization capa-</cell></row><row><cell>bility of our method. 3) Further improvements to</cell></row><row><cell>LReasoner become insignificant when consuming</cell></row><row><cell>more training data. This suggests that the basic</cell></row><row><cell>logic rules can be easily fitted.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on ReClor with DeBERTa as the backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters for ALBERT and RoBERTa during pre-training, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Model Dev Test Test-E Test-H RoBERTa 35.8 35.7 44.5 28.8 MERIt (500 steps) 39.0 35.2 41.8 30.0 100 steps 37.5 38.1 47.5 30.6</figDesc><table><row><cell>200 steps</cell><cell>38.1 38.0 47.3 30.7</cell></row><row><cell>300 steps</cell><cell>37.4 36.4 43.6 30.7</cell></row><row><cell>400 steps</cell><cell>38.5 35.9 42.5 30.7</cell></row><row><cell>ALBERT</cell><cell>43.6 40.2 46.6 35.2</cell></row><row><cell cols="2">MERIt (ALBERT) 46.3 44.6 51.8 38.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results of Linear Probing on ReClor.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters for fine-tuning on ReClor and LogiQA. ?: Fine-Tuning. ?: Prompt Tuning.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In this paper, we refer ALBERT-xxlarge and RoBERTalarge to ALBERT and RoBERTa for simplicity, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://eval.ai/web/challenges/ challenge-page/503/leaderboard/1347.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://pytorch.org. 6 https://github.com/thunlp/ERICA.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We sincerely appreciate the valuable comments from all the reviewers to help us make the paper polished. We also greatly thank to Liqiang Jing and Harry Cheng for their kind suggestions. This work is supported by the National Natural Science Foundation of China, No.:U1936203; the Shandong Provincial Natural Science Foundation, No.:ZR2019JQ23; and Young creative team in universities of Shandong Province, No.:2020KJN012.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Critical thinking for language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Betz</surname></persName>
		</author>
		<idno>abs/2009.07185</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<title level="m">NeurIPS</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transformers as soft reasoners over language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/537</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3882" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ReasonBERT: Pre-trained to reason with distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Lees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<editor>EMNLP. ACL</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<editor>EMNLP. ACL</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantifying and alleviating the language prior problem in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Loss re-scaling VQA: revisiting the language prior problem from a classimbalance view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="227" to="238" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">REALM: retrievalaugmented language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/2002.08909</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.100</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DAGN: discourse-aware graph network for logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.467</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5848" to="5855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rept: Bridging language models and machine reading comprehension via retrieval-based pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangkai</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL-IJCNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="150" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look at the first sentence: Position bias in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miyoung</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.84</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1109" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why machine reading comprehension models learn shortcuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.85</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL/IJCNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="989" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<editor>EMNLP. ACL</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LogiQA: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/501</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3622" to="3628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Martin Ringsquandl, Rime Raissouni, and Volker Tresp. 2021. Neural multi-hop reasoning with logical rules on biomedical knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Joblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">12731</biblScope>
			<biblScope unit="page" from="375" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fact-driven logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siru</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno>abs/2105.10334</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ERICA: improving entity and relation understanding for pre-trained language models via contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.260</idno>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3350" to="3363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GCC: graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403168</idno>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DREAM: A challenge dataset and models for dialogue-based reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Depth-first search and linear graph algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Endre Tarjan</surname></persName>
		</author>
		<idno type="DOI">10.1137/0201010</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast random walk with restart and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2006.70</idno>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Logic-driven context extension and data augmentation for logical reasoning of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5642" to="5650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP: System Demonstrations</title>
		<editor>Quentin Lhoest, and Alexander M. Rush. 2020.</editor>
		<imprint>
			<publisher>ACL</publisher>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.144</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL/IJCNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1653" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reclor: A reading comprehension dataset requiring logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SIRE: separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.47</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL/IJCNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="524" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.127</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1630" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Counterfactual generator: A weaklysupervised method for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangji</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.590</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7270" to="7280" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
