<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scene Text Recognition with Permuted Autoregressive Sequence Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darwin</forename><surname>Bautista</surname></persName>
							<email>darwin.bautista@eee.upd.edu.ph</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronics Engineering Institute</orgName>
								<orgName type="institution">University of the Philippines</orgName>
								<address>
									<settlement>Diliman</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowel</forename><surname>Atienza</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronics Engineering Institute</orgName>
								<orgName type="institution">University of the Philippines</orgName>
								<address>
									<settlement>Diliman</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scene Text Recognition with Permuted Autoregressive Sequence Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>scene text recognition</term>
					<term>permutation language modeling</term>
					<term>au- toregressive modeling</term>
					<term>cross-modal attention</term>
					<term>transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Context-aware STR methods typically use internal autoregressive (AR) language models (LM). Inherent limitations of AR models motivated two-stage methods which employ an external LM. The conditional independence of the external LM on the input image may cause it to erroneously rectify correct predictions, leading to significant inefficiencies. Our method, PARSeq, learns an ensemble of internal AR LMs with shared weights using Permutation Language Modeling. It unifies context-free non-AR and context-aware AR inference, and iterative refinement using bidirectional context. Using synthetic training data, PARSeq achieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and more challenging datasets. It establishes new SOTA results (96.0% accuracy) when trained on real data. PARSeq is optimal on accuracy vs parameter count, FLOPS, and latency because of its simple, unified structure and parallel token processing. Due to its extensive use of attention, it is robust on arbitrarily-oriented text which is common in real-world images. Code, pretrained weights, and data are available at: https://github.com/baudm/parseq. T t=1 P (y t |x). This enables the external LM, P (y) =</p><p>T t=1 P (y t |y ? =t ) in ABINet [24] for example, to use bidirectional context since all characters are available at once. The LM functions as a spell checker and rectifies the initial prediction, producing a</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machines read text in natural scenes by first detecting text regions, then recognizing text in those regions. The task of recognizing text from the cropped regions is called Scene Text Recognition (STR). STR enables reading of road signs, billboards, paper bills, product labels, logos, printed shirts, etc. It has practical applications in self-driving cars, augmented reality, retail, education, and devices for the visually-impaired, among others. In contrast to Optical Character Recognition (OCR) in documents where the text attributes are more uniform, STR has to deal with varying font styles, orientations, text shapes, illumination, amount of occlusion, and inconsistent sensor conditions. Images captured in natural environments could also be noisy, blurry, or distorted. In essence, STR is an important but very challenging problem.</p><p>STR is mainly a vision task, but in cases where parts of the text are impossible to read, e.g. due to an occluder, the image features alone will not be enough to make accurate inferences. In such cases, language semantics is typically used to aid the recognition process. Context-aware STR methods incorporate semantic priors from a word representation model <ref type="bibr">[56]</ref> or dictionary <ref type="bibr">[53]</ref>, or learned from data <ref type="bibr" target="#b17">[60,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">58,</ref><ref type="bibr">38,</ref><ref type="bibr">80,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b18">61,</ref><ref type="bibr" target="#b9">10]</ref> using sequence modeling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">69]</ref>.</p><p>Sequence modeling has the advantage of learning end-to-end trainable language models (LM). STR methods with internal LMs jointly process image features and language context. They are trained by enforcing an autoregressive (AR) constraint on the language context where future tokens are conditioned on past tokens but not the other way around, resulting in the model P (y|x) = T t=1 P (y t |y &lt;t , x) where y is the T -length text label of the image x. AR models have two inherent limitations arising from this constraint. First, the model is able to learn the token dependencies in one direction only-usually the left-to-right (LTR) direction. This unidirectionality causes AR models to be biased towards a single reading direction, resulting in spurious addition of suffixes or direction-dependent predictions (illustrated in Appendix A). Second, during inference, the AR model can only be used to output tokens serially in the same direction used for training. This is called next-token or monotonic AR decoding. uses a combination of context-free vision and context-aware language models. The language model functions as a spell checker but is prone to erroneous rectification of correct initial predictions due to its conditional independence on the image features. (b) Our proposed method performs both initial decoding and iterative refinement by jointly processing image and context features, resulting in a single holistic output. This eschews the need for separate language and fusion models resulting in a more efficient and robust STR method</p><p>To address these limitations, prior works have combined left-to-right and right-to-left (RTL) AR models <ref type="bibr" target="#b18">[61,</ref><ref type="bibr" target="#b9">10]</ref>, or opted for a two-stage approach using an ensemble of a context-free STR model with a standalone or external LM <ref type="bibr">[80,</ref><ref type="bibr">24]</ref>. A combined LTR and RTL AR model still suffers from unidirectional context but works around it by performing two separate decoding streamsone for each direction-then choosing the prediction with the higher likelihood. Naturally, this results in increased decoding time and complexity. Meanwhile, two-stage ensemble approaches like in <ref type="figure" target="#fig_0">Figure 1a</ref> obtain their initial predictions using parallel non-AR decoding. The initial context-less prediction is decoded directly from the image using the context-free model P (y|x) = context-based output. The conditional independence of the LM from the input image may cause it to erroneously rectify correct predictions if they appear misspelled, or if a similar word with a higher likelihood exists. This is evident in the low word accuracy of the LM in SRN (27.6%) and in ABINet (41.9%) when used as a spell checker <ref type="bibr">[24]</ref>. Hence, a separate fusion layer is used to combine the features from the initial prediction and the LM prediction to get the final output. A closer look at the LM of ABINet (Appendix B) reveals that it is inefficient for STR. It is underutilized relative to its parameter count, and it exhibits dismal word accuracy despite using a significant chunk of the overall compute requirements of the full ABINet model.</p><p>In sequence model literature, there has been recent interest in generalized models of sequence generation. Various neural sequence models, such as AR and refinement-based non-AR, were shown to be special cases in the generalized framework proposed by Mansimov et al . <ref type="bibr">[47]</ref>. This result posits that the same generalization can be done in STR models, unifying context-free and contextaware STR. While the advantages of this unification are not apparent, we shall show later that such a generalized model enables the use of an internal LM while maintaining the refinement capabilities of an external LM.</p><p>Permutation Language Modeling (PLM) was originally proposed for largescale language pretraining <ref type="bibr">[79]</ref>, but recent works <ref type="bibr" target="#b23">[66,</ref><ref type="bibr">55]</ref> have adapted it for learning Transformer-based generalized sequence models capable of different decoding schemes. In this work, we adapt PLM for STR. PLM can be considered a generalization of AR modeling, and a PLM-trained model can be seen as an ensemble of AR models with shared architecture and weights <ref type="bibr" target="#b25">[68]</ref>. With the use of attention masks for dynamically specifying token dependencies, such a model, illustrated in <ref type="figure">Figure 2</ref>, can learn and use conditional character probabilities given an arbitrary subset of the input context, enabling monotonic AR decoding, parallel non-AR decoding, and even iterative refinement. <ref type="figure">Fig. 2</ref>. Illustration of NAR and iterative refinement (cloze) models in relation to an ensemble of AR models for an image x with a three-element text label y. Four different factorizations of P (y|x) (out of six possible) are shown, with each one determined by the factorization order shown in the subscript In summary, state-of-the-art (SOTA) STR methods <ref type="bibr">[80,</ref><ref type="bibr">24]</ref> opted for a twostage ensemble approach in order to use bidirectional language context. The low word accuracy of their external LMs, despite increased training and runtime requirements, highlights the need for a more efficient approach. To this end, we propose a permuted autoregressive sequence (PARSeq) model for STR. Trained with PLM, PARSeq is a unified STR model with a simple structure, but is capable of both context-free and context-aware inference, as well as iterative refinement using bidirectional (cloze) context. PARSeq achieves SOTA results on the STR benchmarks for both synthetic and real training data <ref type="table" target="#tab_6">(Table 6</ref>) across all character sets <ref type="table" target="#tab_4">(Table 4</ref>), while being optimal in its use of parameters, FLOPS, and runtime ( <ref type="figure" target="#fig_3">Figure 5</ref>). For a more comprehensive comparison, we also benchmark on larger and more difficult real datasets which contain occluded and arbitrarily-oriented text <ref type="figure">(Figure 4b</ref>). PARSeq likewise achieves SOTA results in these datasets <ref type="table" target="#tab_5">(Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The recent surveys of Long et al .</p><p>[45] and Chen et al . <ref type="bibr" target="#b12">[13]</ref> provide comprehensive discussions on different approaches in STR. In this section, we focus on the use of language semantics in STR.</p><p>Context-free STR methods directly predict the characters from image features. The output characters are conditionally-independent of each other. The most prominent approaches are CTC-based [27] methods <ref type="bibr" target="#b16">[59,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b29">72,</ref><ref type="bibr" target="#b10">11]</ref>, with a few using different approaches such as self-attention [23] for pooling features into character positions <ref type="bibr" target="#b1">[2]</ref>, or casting STR as a multi-instance classification problem <ref type="bibr">[30,</ref><ref type="bibr" target="#b11">12]</ref>. Ensemble methods [80,24] use an attention mechanism <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">69]</ref> to produce the initial context-less predictions. Since context-free methods rely solely on the image features for prediction, they are less robust against corruptions like occluded or incomplete characters. This limitation motivated the use of language semantics for making the recognition model more robust.</p><p>Context-aware STR methods typically use semantics learned from data to aid in recognition. Most approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b17">60,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">61]</ref> use RNNs with attention <ref type="bibr" target="#b5">[6]</ref> or Transformers <ref type="bibr" target="#b15">[58,</ref><ref type="bibr">38,</ref><ref type="bibr" target="#b9">10]</ref> to learn internal LMs using the standard AR training. These methods are limited to monotonic AR decoding. Ensemble methods [80,24] use bidirectional context via an external LM for prediction refinement. The conditional independence of the external LM on image features makes it prone to erroneous rectification, limiting usefulness while incurring significant overhead. VisionLAN [75] learns semantics by selectively masking image features of individual characters during training, akin to denoising autoencoders and Masked Language Modeling (MLM) <ref type="bibr">[21]</ref>. In contrast to prior work, PARSeq learns an internal LM using PLM instead of the standard AR modeling. It supports flexible decoding by using a parameterization which decouples the target decoding position from the input context, similar to the query stream of two-stream attention [79]. Unlike ABINet [24] which uses the cloze context for both training and inference, PARSeq uses it for iterative refinement only. Moreover, as said earlier, the refinement model of ABINet is conditionally independent of the in-put image, while PARSeq considers both input image and language context in the refinement process.</p><p>Generation from Sequence Models can be categorized into two contrasting schemes: autoregressive (one token at a time) and non-autoregressive (all tokens predicted at once). Mansimov et al . <ref type="bibr">[47]</ref> proposed a generalized framework for sequence generation which unifies the said schemes. BANG [55] adapted two-stream attention [79] for use with MLM, in contrast to our use of PLM. PMLM [40] is trained using a generalization of MLM where the masking ratio is stochastic. A variant which uses a uniform prior was shown to be equivalent to a PLM-trained model. Closest to our work is Tian et al . <ref type="bibr" target="#b23">[66]</ref> which adapts the two-stream attention parameterization [79] to decoders by interspersing the content and query streams from different layers. In contrast, our decoder does not use self-attention and does not intersperse the two streams. This allows our single layer decoder to use the query stream only, and avoid the overhead of the unused content stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Permuted Autoregressive Sequence Models</head><p>In this section, we first present the Transformer-based model architecture of PARSeq. Next, we discuss how to train it using Permutation Language Modeling. Lastly, we show how to use the trained model for inference by discussing the different decoding schemes and the iterative refinement procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>Multi-head Attention (MHA) <ref type="bibr" target="#b26">[69]</ref> is extensively used by PARSeq. We denote it as M HA(q, k, v, m), where q, k, and v refer to the required parameters query, key, and value, while m refers to the optional attention mask. We provide the background material on MHA in Appendix C.</p><p>PARSeq follows an encoder-decoder architecture, shown in <ref type="figure" target="#fig_1">Figure 3</ref>, commonly used in sequence modeling tasks. The encoder has 12 layers while the decoder is only a single layer. This deep-shallow configuration [33] is a deliberate design choice which minimizes the overall computational requirements of the model while having a negligible impact in performance. Details in Appendix D.</p><p>ViT Encoder. Vision Transformer (ViT) [23] is the direct extension of the Transformer to images. A ViT layer contains one MHA module used for selfattention, i.e. q = k = v. The encoder is a 12-layer ViT without the classification head and the [CLS] token. An image x ? R W ?H?C , with width W , height H, and number of channels C, is tokenized by evenly dividing it into p w ?p h patches, flattening each patch, then linearly projecting them into d modeldimensional tokens using a patch embedding matrix W p ? R pwp h C?d model , resulting in (W H)/(p w p h ) tokens. Learned position embeddings of equal dimension are added to the tokens prior to being processed by the first ViT layer. In contrast to the standard ViT, all output tokens z are used as input to the decoder:</p><formula xml:id="formula_0">z = Enc(x) ? R W H pw p h ?d model (1)</formula><p>Visio-lingual Decoder. The decoder follows the same architecture as the pre-LayerN orm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">74]</ref> Transformer decoder but uses twice the number of attention heads, i.e. nhead = d model /32. It has three required inputs consisting of position, context, and image tokens, and an optional attention mask.</p><p>In the following equations, we omit LayerN orm and Dropout for brevity. The first MHA module is used for context-position attention:</p><formula xml:id="formula_1">h c = p + M HA(p, c, c, m) ? R (T +1)?d model<label>(2)</label></formula><p>where T is the context length, p ? R (T +1)?d model are the position tokens, c ? R (T +1)?d model are the context embeddings with positional information, and m ? R (T +1)?(T +1) is the optional attention mask. Note that the use of special delimiter tokens ([B] or [E]) increases the total sequence length to T + 1.</p><p>The position tokens encode the target position to be predicted, each one having a direct correspondence to a specific position in the output. This parameterization is similar to the query stream of two-stream attention <ref type="bibr">[79]</ref>. It decouples the context from the target position, allowing the model to learn from PLM. Without the position tokens, i.e. if the context tokens are used as queries themselves like in standard Transformers, the model will not learn anything meaningful from PLM and will simply function like a standard AR model.</p><p>The supplied mask varies depending on how the model is used. During training, masks are generated from random permutations (Section 3.2). At inference (Section 3.3), it could be a standard left-to-right lookahead mask (AR decoding), a cloze mask (iterative refinement), or no mask at all (NAR decoding).</p><p>The second MHA is used for image-position attention:</p><formula xml:id="formula_2">h i = h c + M HA(h c , z, z) ? R (T +1)?d model<label>(3)</label></formula><p>where no attention mask is used. The last decoder hidden state is the output of the MLP,</p><formula xml:id="formula_3">h dec = h i + M LP (h i ) ? R (T +1)?d model . Finally, the output logits are y = Linear(h dec ) ? R (T +1)?(S+1)</formula><p>where S is the size of the character set (charset) used for training. The additional character pertains to the [E] token (which marks the end of the sequence). In summary, given an attention mask m, the decoder is a function which takes the form:</p><formula xml:id="formula_4">y = Dec(z, p, c, m) ? R (T +1)?(S+1)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Permutation Language Modeling</head><p>Given an image x, we want to maximize the likelihood of its text label y = [y 1 , y 2 , . . . , y T ] under the set of model parameters ?. In standard AR modeling, the likelihood is factorized using the chain rule according to the canonical ordering, [1, 2, . . . , T ], resulting in the model log p(y|x) = T t=1 log p ? (y t |y &lt;t , x). However, Transformers process all tokens in parallel, allowing the output tokens to access or be conditionally-dependent on all the input tokens. In order to have a valid AR model, past tokens cannot have access to future tokens. The AR property is enforced in Transformers with the use of attention masks. For example, a standard AR model for a three-element sequence y will have the attention mask shown in <ref type="table" target="#tab_0">Table 1a</ref>.</p><p>The key idea behind PLM is to train on all T ! factorizations of the likelihood:</p><formula xml:id="formula_5">log p(y|x) = E z?Z T T t=1 log p ? (y zt |y z&lt;t , x)<label>(5)</label></formula><p>where Z T denotes the set of all possible permutations of the index sequence [1, 2, . . . , T ], and z t and z &lt;t denote the t-th element and the first t ? 1 elements, respectively, of a permutation z ? Z T . Each permutation z specifies an ordering which corresponds to a distinct factorization of the likelihood. To implement PLM in Transformers, we do not need to actually permute the text label y. Rather, we craft the attention mask to enforce the ordering specified by z. As a concrete example, shown in <ref type="table" target="#tab_0">Table 1</ref> are attention masks for four different permutations of a three-element sequence. Notice that while the order of the input and output sequences remains constant, all four correspond to distinct AR models specified by the given permutation or factorization order. With this in mind, it can be seen that the standard AR training is just a special case of PLM where only one permutation, [1, 2, . . . , T ], is used.</p><p>In practice, we cannot train on all T ! factorizations due to the exponential increase in computational requirements. As a compromise, we only use K of the possible T ! permutations. Instead of sampling uniformly, we choose the K permutations in a specific way. We use K/2 permutation pairs. The first half consists of the left-to-right permutation, <ref type="bibr">[1, 2, .</ref> . . , T ], and K/2 ? 1 randomly sampled permutations. The other half consists of flipped versions of the first. We found that this sampling procedure results in a more stable training.</p><p>With K permutations and the ground truth label?, the full training loss is the mean of the individual cross-entropy losses for each permutation-derived attention mask m k :</p><formula xml:id="formula_6">L = 1 K K k=1 L ce (y k ,?)<label>(6)</label></formula><p>where y k = Dec(z, p, c, m k ). Padding tokens are ignored in the loss computation. More PLM details are in Appendix E. [B] y1 y2 y3</p><formula xml:id="formula_7">y1 1 0 0 0 y2 1 1 0 0 y3 1 1 1 0 [E] 1 1 1 1 (b) [3, 2, 1]</formula><p>[B] y1 y2 y3</p><formula xml:id="formula_8">y1 1 0 1 1 y2 1 0 0 1 y3 1 0 0 0 [E] 1 1 1 1 (c) [1, 3, 2]</formula><p>[B] y1 y2 y3</p><formula xml:id="formula_9">y1 1 0 0 0 y2 1 1 0 1 y3 1 1 0 0 [E] 1 1 1 1 (d) [2, 3, 1]</formula><p>[B] y1 y2 y3</p><formula xml:id="formula_10">y1 1 0 1 1 y2 1 0 0 0 y3 1 0 1 0 [E] 1 1 1 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding Schemes</head><p>PLM training coupled with the correct parameterization allows PARSeq to be used with various decoding schemes. In this work, we only use two contrasting schemes even though more are theoretically supported. Specifically, we elaborate the use of monotonic AR and NAR decoding, as well as iterative refinement. Autoregressive (AR) decoding generates one new token per iteration. The left-to-right attention mask <ref type="table">(Table 2a</ref>) is always used. For the first iteration, the context is set to <ref type="bibr">[B]</ref>, and only the first position query token p 1 is used. For any succeeding iteration i, position queries [p 1 , . . . , p i ] are used, while the context is set to the previous output, argmax(y) prepended with [B].</p><p>Non-autoregressive (NAR) decoding generates all output tokens at the same time. All position queries [p 1 , . . . , p T +1 ] are used but no attention mask is used <ref type="table">(Table 2b</ref>). The context is always <ref type="bibr">[B]</ref>.</p><p>Iterative refinement can be performed regardless of the initial decoding method (AR or NAR). The previous output (truncated at [E]) serves as the context for the current iteration similar to AR decoding, but all position queries [p 1 , . . . , p T +1 ] are always used. The cloze attention mask <ref type="table">(Table 2c)</ref> is used. It is created by starting with an all-one mask, then masking out the matching token positions. <ref type="table">Table 2</ref>. Illustration of information flow for the different decoding schemes. Conventions follow <ref type="table" target="#tab_0">Table 1</ref>. In NAR decoding, no mask is used; this is equivalent to using an all-one mask. ". . . " pertains to elements y3 to yT ?1 (a) left-to-right AR mask</p><formula xml:id="formula_11">[B] y1 y2 . . . yT y1 1 0 0 0 0 y2 1 1 0 0 0 . . . 1 1 1 . . . 0 yT 1 1 1 1 0 [E] 1 1 1 1 1 (b) NAR mask [B] y1 1 y2 1 . . . 1 yT 1 [E] 1 (c) cloze mask [B] y1 y2 . . . yT y1 1 0 1 1 1 y2 1 1 0 1 1 . . . 1 1 1 . . . 1 yT 1 1 1 1 0 [E] 1 1 1 1 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>In this section, we first discuss the experimental setup including the datasets, preprocessing methods, training and evaluation protocols, and metrics used. Next, we present our results and compare PARSeq to SOTA methods in terms of the said metrics and commonly used computational cost indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>STR models are traditionally trained on large-scale synthetic datasets because of the relative scarcity of labelled real data <ref type="bibr" target="#b2">[3]</ref>. However, in recent years, the amount of labelled real data has become sufficient for training STR models. In fact, training on real data was shown to be more sample-efficient than on synthetic data <ref type="bibr" target="#b3">[4]</ref>. Hence, in addition to the commonly used synthetic training datasets MJSynth (MJ) <ref type="bibr">[30]</ref> and SynthText (ST) [28], we also use real data for training. Specifically, we use COCO-Text (COCO) <ref type="bibr">[</ref>  <ref type="bibr" target="#b2">[3]</ref> provides an in-depth discussion of these datasets. We use the casesensitive annotations of Long and Yao [46] for IIIT5k, CUTE, SVT, and SVTP. Note that IC13 and IC15 have two versions of their respective test splits commonly used in the literature-857 and 1,015 for IC13; 1,811 and 2,077 for IC15. To avoid confusion, we refer to the benchmark as the union of IIIT5k, CUTE, SVT, SVTP, IC13 (1,015), and IC15 (2,077).</p><p>These six benchmark datasets only have a total of 7,672 test samples. This amount pales in comparison to benchmark datasets used in other vision tasks such as ImageNet [20] (classification, 50k samples) and COCO [42] (detection, 40k samples). Furthermore, the said datasets largely contain horizontal text only, as shown in <ref type="figure">Figure 4a</ref>, except for SVT, SVTP, and IC15 2,077 which contain a number of rotated text. In the real world, the conditions are less ideal, and captured text will most likely be blurry, vertically-oriented, rotated, or even occluded. In order to have a more comprehensive comparison, we also use the test sets of more recent datasets, shown in <ref type="figure">Figure 4b</ref>, such as COCO-Text (9.8k samples; low-resolution, occluded text), <ref type="bibr">ArT</ref>  Label preprocessing is done following prior work <ref type="bibr" target="#b18">[61]</ref>. For training, we set a maximum label length of T = 25, and use a charset of size S = 94 which contains mixed-case alphanumeric characters and punctuation marks.</p><p>Image preprocessing is done like so: images are first augmented, resized, then finally normalized to the interval [?1, 1]. The set of augmentation operations consists primarily of RandAugment [18] operations, excluding Sharpness.</p><p>Invert is added due to its effectiveness in house number data <ref type="bibr">[17]</ref>. GaussianBlur and PoissonNoise are also used due to their effectiveness in STR data augmentation <ref type="bibr" target="#b0">[1]</ref>. A RandAugment policy with 3 layers and a magnitude of 5 is used. Images are resized unconditionally to 128?32 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Protocol and Metrics</head><p>All experiments are performed on an NVIDIA Tesla A100 GPU system. Reported mean?SD values are obtained from four replicates per model. A t-test (? = 0.05) is used to determine if model differences are statistically-significant. There can be multiple best results in a column if the differences are not statistically-significant. PARSeq results are obtained from the same model using two different decoding schemes: PARSeq A denotes AR decoding with one refinement iteration, while PARSeq N denotes NAR decoding with two refinement iterations (ablation study in Appendix H).</p><p>Word accuracy is the primary metric for STR benchmarks. A prediction is considered correct if and only if characters at all positions match.</p><p>Charset may vary at inference time. Subsets of the training charset can be used for evaluation. Specifically, the following charsets are used: 36-character (lowercase alphanumeric), 62-character (mixed-case alphanumeric), and 94-character (mixed-case alphanumeric with punctuation). In Python, these correspond to array slices [:36], [:62], and [:94] of string.printable, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation on training permutations vs test accuracy</head><p>As discussed in Section 3.2, training on all possible permutations is not feasible in practice due to the exponential increase in computational requirements. We instead sample a number of permutations from the pool of all possible permutations. <ref type="table">Table 3</ref> shows the effect of the number of training permutations on the test accuracy for all decoding schemes. With K = 1, only the left-to-right ordering is used and the training simplifies to the standard AR modeling. In this setup, NAR decoding does not work at all, while AR decoding works well as expected. Meanwhile, the refinement or cloze accuracy is at a dismal 71.14% (this is very low considering that the ground truth itself is used as the initial prediction). All decoding schemes start to perform satisfactorily only at K &gt;= 6. This result shows that PLM is indeed required to achieve a unified STR model. Intuitively, NAR decoding will not work when training on just the forward and/or reverse orderings (K &lt;= 2) because the variety of training contexts is insufficient. NAR decoding relies on the priors for each character which could only be sufficiently trained if all characters in the charset naturally exist as the first character of a sequence. Ultimately, K = 6 provides the best balance between decoding accuracy and training time. The very high cloze accuracy (?94%) of our internal LM highlights the advantage of jointly using image features and language context for prediction refinement. After all, the primary input signal in STR is the image, not the language context. <ref type="table">Table 3</ref>. 94-char word accuracy on the benchmark vs number of permutations (K) used for training PARSeq. No refinement iterations were used for both AR and NAR decoding. cloze acc. pertains to the word accuracy of one refinement iteration. It was measured by using the ground truth label as the initial prediction </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison to state-of-the-art (SOTA)</head><p>We compare PARSeq to popular and recent SOTA methods. In addition to the published results, we reproduce a select number of methods for a fair comparison <ref type="bibr" target="#b2">[3]</ref>. In <ref type="table" target="#tab_6">Table 6</ref>, most reproduced methods attain higher accuracy compared to the original results. The exception is ABINet (around 1.4% decline in combined accuracy) which originally used a much longer training schedule (with pretraining of 80 and 8 epochs for LM and VM, respectively) and additional data (WikiText-103). For both synthetic and real data, PARSeq A achieves the highest word accuracies, while PARSeq N consistently places second or third. When real data is used, all reproduced models attain much higher accuracy compared to the original reported results, while PARSeq A establishes new SOTA results.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, we show the mean accuracy for each charset. When synthetic data is used for training, there is a steep decline in accuracy from the 36-to the 62-and 94-charsets. This suggests that diversity of cased characters is lacking in the synthetic datasets. Meanwhile, PARSeq A consistently achieves the highest accuracy on all charset sizes. Finally in <ref type="table" target="#tab_5">Table 5</ref>, PARSeq is the most robust against occlusion and text orientation variability. Appendix J contains more experiments on arbitrarily-oriented text. Notice that the accuracy gap between methods is better revealed by these larger and more challenging datasets. <ref type="figure" target="#fig_3">Figure 5</ref> shows the cost-quality trade-offs in terms of accuracy and commonly used cost indicators like parameter count, FLOPS, and latency. PARSeq-S is the base model used for all results, while -Ti is its scaled down variant (details in Appendix D). Note that for PARSeq, the parameter count is fixed regardless of the decoding scheme. PARSeq-S achieves the highest mean word accuracy and exhibits very competitive cost-quality characteristics across the three indicators. Compared to ABINet and TRBA, PARSeq-S uses significantly less parameters and FLOPS. In terms of latency (Appendix I), PARSeq-S with AR decoding is slightly slower than TRBA, but is still significantly faster than ABINet. Meanwhile, PARSeq-Ti achieves a much higher word accuracy vs CRNN in spite of similar parameter count and FLOPS. PARSeq-S is Pareto-optimal, while -Ti is a compelling alternative for low-resource applications.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We adapted PLM for STR in order to learn PARSeq, a unified STR model capable of context-free and -aware decoding, and iterative refinement. PARSeq achieves SOTA results in different charset sizes and real-world datasets by jointly conditioning on both image and text representations. By unifying different decoding schemes into a single model and taking advantage of the parallel computations in Transformers, PARSeq is optimal on accuracy vs parameter count, FLOPS, and latency. Due to its extensive use of attention, it also demonstrates robustness on vertical and rotated text common in many real-world images. Acknowledgments. This work was funded in part by CHED-PCARI IIID-2016-005 (Project AIRSCAN). We are also grateful to the PCARI Prime team, led by Roel Ocampo, who ensured the uptime of our GPU servers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Issues with unidirectionality of AR models in STR</head><p>As discussed in the main text, the unidirectionality of AR models could result in spurious addition of suffixes and direction-dependent decoding. Shown in <ref type="table" target="#tab_7">Table 7</ref> is a sample output of a left-to-right (LTR) AR model trained on a 36-character lowercase charset. Since the input is fairly clear and horizontal, the model was very confident in the predictions for the first 10 characters. However, since it was trained on alphanumeric characters only, it did not know how to recognize the exclamation mark. The language context swayed the output of the model to add the -ly suffix in order to make sense of the unrecognized character. A right-to-left (RTL) AR model would not add the suffix due to the lack of context (since the right-most characters would have to be predicted first). This directiondependent decoding is further illustrated in <ref type="table">Table 8</ref> where two AR models trained on opposing directions produce different outputs. In this case, the input contains ambiguity on the uppercase N character. If read from left to right, the context of the earlier characters can be used to infer that the ambiguous character is N. However, when read in the opposite direction, the context of OPE is not yet available, prompting the RTL model to recognize two l 's in place of a single N character.  <ref type="table" target="#tab_0">(Table 10</ref>), the same model gets a top-1 word accuracy of only 50.44% (36-char). This means that even if the Vision Model (VM) is perfect (always predicting the correct label), the LM will produce a wrong output 50% of the time. In summary, the external LM's dedicated compute cost, underutilization relative to its parameter and memory requirements, and dismal word accuracy show the inefficiency of this approach. For STR, an internal LM might be more appropriate since the primary input signal is the image, not the language context.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Multi-head Attention</head><p>The attention mechanism is central to the operation of Transformers <ref type="bibr" target="#b26">[69]</ref>. In scaled dot-product attention, the similarity scores between two d k -dimensional vectors q (query) and k (key), computed using their dot-product, are used to transform a d v -dimensional vector v (value). Formally, scaled dot-product attention is defined as:</p><formula xml:id="formula_12">Attn(q, k, v) = sof tmax qk T ? d k v<label>(7)</label></formula><p>It accepts an optional attention mask that limits which keys the queries could attend to. In a Transformer with token dimensionality of d model ,</p><formula xml:id="formula_13">d k = d v = d model .</formula><p>Multi-head Attention (MHA) is the extension of scaled dot-product attention to multiple representation subspaces or heads. To keep the computational cost of MHA practically constant regardless of the number of heads, the dimensionality of the vectors are reduced to d head = d model /h, where h is the number of heads. A head corresponds to an invocation of Equation <ref type="formula" target="#formula_12">(7)</ref> on projected versions of q, k, and v using parameter matrices W q ? R d model ?d head , W k ? R d model ?d head , and W v ? R d model ?d head , respectively, as shown in Equation <ref type="bibr" target="#b7">(8)</ref>. The final output is obtained in Equation <ref type="formula">(9)</ref> by concatenating the heads and multiplying by the output projection matrix W o ? R d model ?d model .</p><formula xml:id="formula_14">head i = Attn(qW q i , kW k i , vW v i ) (8) M HA(q, k, v) = Concat(head 1 , ..., head h )W o (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Model Architecture</head><p>PARSeq uses an encoder which largely follows the original ViT [23], and a pre-LayerN orm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">74]</ref> decoder with more heads. The architectures are practically unchanged but are reproduced here for the convenience of the reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 ViT Encoder</head><p>The encoder is composed of 12 layers. All layers share the same architecture shown in <ref type="figure" target="#fig_5">Figure 6</ref>. The output of the last encoder layer goes through a final LayerN orm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Visio-lingual Decoder</head><p>The decoder <ref type="figure">(Figure 7</ref>) consists of only a single layer. The immediate outputs of all M HA and M LP layers go through Dropout (p = 0.1, not shown). Image Features are already LayerN orm'd by the encoder (hence no LayerN orm prior to input). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Architecture Configuration</head><p>The main results are obtained from the base model, PARSeq-S, which has a similar configuration to DeiT-S <ref type="bibr" target="#b24">[67]</ref> but uses an image size of 128?32 and a patch size of 8?4 (a change also adapted in our reproduction of ViTSTR-S). Based on our experiments, scaling up the model only marginally improves word accuracy on the benchmark. We instead explore scaling down the model to make it more suitable for edge devices. PARSeq-Ti, which uses a configuration similar to DeiT-Ti <ref type="bibr" target="#b24">[67]</ref>, is more similar to CRNN <ref type="bibr" target="#b16">[59]</ref> in terms of parameter count and FLOPS. The detailed configuration parameters are shown in <ref type="table" target="#tab_0">Table 11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Permutation Language Modeling</head><p>In this section, we provide additional details about the adaptation of PLM for use in PARSeq. We give a concrete illustration of masked multi-head attention first. Next, the intuition behind the usage of permutation pairs is discussed. Lastly, implementation details and considerations about the training procedure are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Illustration of attention masking</head><p>As discussed in the main text, Transformers process all tokens in parallel. In order to enforce the AR constraint which limits the conditional dependencies for each token, attention masking is used. <ref type="figure">Figure 8</ref> shows a concrete example of masked multi-head attention for a sequence y. The position tokens always serve as the query vectors, while the context tokens (context embeddings with position information) serve as the key and value vectors. Note that the sequence order is fixed, and that only the AR factorization order (specified by the attention mask) is permuted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Permutation Sampling</head><p>As discussed in the main text, we sample permutations in a specific way. We use pairs of permutations, and the left-to-right permutation is always used. Thus, we only sample K/2 ? 1 permutations every training step. To illustrate the intuition behind the usage of flipped permutation pairs, we give the following example.  <ref type="figure">Fig. 8</ref>. Masked MHA for a three-element sequence y = [y1, y2, y3] given the factorization order <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>. c are context embeddings with position information p(y) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref> = p(y 1 )p(y 3 |y 1 )p(y 2 |y 1 , y 3 ) p(y) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref> = p(y 2 )p(y 3 |y 2 )p(y 1 |y 2 , y 3 )</p><p>For each permutation pair, if we group the probabilities per element, we get <ref type="table" target="#tab_0">Table 12</ref>. Notice that the probabilities of each element for every permutation pair consists of disjoint sets of conditioning variables. For example, the probabilities of element y 1 for [1, 2, 3] (left-to-right permutation) and [3, 2, 1] (right-to-left permutation) are p(y 1 ) and p(y 1 |y 2 , y 3 ), respectively. The first term is the prior probability of y 1 . It is not conditioned on any other element of the text label, unlike the second term which is conditioned on all other elements, y 2 and y 3 . Similarly for y 2 , the first term is conditioned only on y 1 while the second term is conditioned only on y 3 . In our experiments, we find that using flipped permutation pairs results in more stable training dynamics where the loss is smoother and less erratic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Special handling of end-of-sequence [E] token</head><p>Although the [E] token is part of the sequence, it is handled in a specific way in order to make training simpler. First, no character c ? C, where C is the training charset, is conditioned on [E]. Intuitively, it means that [E] marks the end of the sequence (hence its name) since no more characters are expected after it is produced by the model. More formally, it means that p(c|[E]) = 0. This is achieved by masking the positions of [E] in the input context. Second, we train [E] on only two permutations, left-to-right and right-to-left. The left-to-right lookahead mask provides the longest context to [E] (conditioned on all other characters in the sequence), while the right-to-left mask provides no context, which is necessary for NAR decoding. We could also train [E] on different subsets of the input context, but doing so needlessly complicates the training procedure without offering any advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Considerations for batched training</head><p>Text labels of varying lengths can be included in a mini-batch. However, the sampled permutations for the mini-batch are always based on the longest sequence.</p><p>Hence, it is possible that after accounting for padding, multiple permutations would become equivalent. To see why this is the case, consider a mini-batch containing two samples: the first label has a single character, while the second label has four characters. The first label has a sequence length of one and total number of permutations also equal to one. On the other hand, the second label has a sequence length of four which corresponds to 24 total permutations. If we use K = 6 permutations, then it means that the permutations for the first label would be oversampled since there is only one valid permutation for T = 1. We find that this oversampling actually helps training. We experimented with a modified training procedure wherein sequences with T &lt; 4 are grouped together (i.e. 1-, 2-, and 3-character sequences are grouped separately). This training procedure results in increased training time due to the mini-batch being split further into smaller batches, but it does not improve accuracy nor hasten convergence. Thus, we stick with the simpler batched training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Dataset Matters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Open Images Datasets</head><p>TextOCR and OpenVINO are datasets both derived from Open Images-a large dataset with very diverse images often containing complex scenes with several objects (8.4 per image on average). Open Images is not specifically collected for STR. Thus, it contains text of varying resolutions, orientations, and quality, as shown in cropped word boxes in <ref type="figure">Figure 9</ref>. TextOCR and OpenVINO significantly overlap in terms of source scene images, as shown in <ref type="table" target="#tab_0">Table 13</ref>. Samples of source scene images common to both are shown in <ref type="figure" target="#fig_0">Figure 10</ref>. Only the validation set of OpenVINO and the test set of TextOCR do not overlap any other image set. The labels of TextOCR's test set are kept private.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Data preparation for LMDB storage</head><p>We use the archives released by Baek et al . <ref type="bibr" target="#b3">[4]</ref> for RCTW17, Uber-Text, ArT, LSVT, MLT19, and ReCTS. Thus, we only preprocess data for the remaining datasets.</p><p>For COCO-Text, we use the v1.4 test annotations released as part of the ICDAR 2017 challenge. For train and val, we use the latest (v2.0) annotations. We preprocess TextOCR, OpenVINO, and COCO-Text with minimal filtering and modifications, in contrast to the usual practice of removing non-horizontal text and special characters. We only filter illegible and non-machine printed text. The only modification we perform is the removal of whitespace on either side of the label, or duplicate whitespace between non-whitespace characters.</p><p>For IC13 and IC15, we use the original data from the ICDAR competition website and perform no modifications to the data. We emulate the previous filtering methods <ref type="bibr">[73,</ref><ref type="bibr" target="#b13">14]</ref> to create the subsets used for evaluation.</p><p>Long and Yao <ref type="bibr">[46]</ref> have reannotated IIIT5k, CUTE, SVT, and SVTP because the original annotations are case-insensitive and lack punctuation marks. However, both the reannotations and the originals contain some errors. Hence, we review inconsistencies between the two versions and manually reconcile them to correct the errors. <ref type="table" target="#tab_0">Table 14</ref> provides a detailed summary of how each dataset was used.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Label preprocessing</head><p>Preprocessing and filtering are done as follows. Whitespace characters are removed from the labels. Unicode characters are normalized using the NFKD normalization form and then converted to ASCII. Next, labels longer than T characters are filtered. Case-sensitivity is inferred from the charset. If all letters in the charset are lowercase, the label is transformed to its lowercase version. If the charset consists of purely uppercase letters, the label is converted to its uppercase version. If the charset is mixed-case, no case conversion is done. Lastly, all characters not specified in the charset are removed from the labels.   <ref type="figure" target="#fig_0">Figure 11</ref> shows how the word accuracy and latency evolve as functions of the number of refinement iterations. For AR decoding, refinement iterations after the first provide negligible increase in accuracy. For NAR decoding, the accuracy increase becomes insignificant after the second iteration. Hence, we use one and two refinement iterations for the AR and NAR decoding schemes, respectively.  We measure model latency in an isolated manner. By doing so, we can reliably factor out the effects of data loading, storage, and CPU latency. We use the builtin benchmarking tool of PyTorch and measure latency for different label lengths, as shown in <ref type="figure" target="#fig_0">Figure 12</ref>. As expected, NAR methods including PARSeq N exhibit near-constant latency regardless of output label length. Meanwhile, the latency of AR methods increases linearly as a function of the output label length. The latency increase of PARSeq A is steeper than TRBA. However, since the average length of words in the test datasets is quite short at 5.4, the actual difference in mean latency between TRBA and PARSeq A is only about 2.3 msec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Learning Rate Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Accuracy of decoding schemes vs latency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Experiments on arbitrarily-oriented text</head><p>In STR, the focus has mainly been on horizontal text, with a few explicitly tackling arbitrarily-oriented text [54, <ref type="bibr" target="#b14">15,</ref><ref type="bibr">51,</ref><ref type="bibr">77,</ref><ref type="bibr">78]</ref>. In our experiments, we observe that existing attention-based models are capable of recognizing text in arbitrary orientation, as shown in <ref type="table" target="#tab_0">Table 17</ref>. Only CRNN, a CTC-based model, exhibits dismal orientation robustness. We conjecture that the direct correspondence of visual feature positions to textual feature positions in CTC-based models causes this poor performance. On the other hand, attention-based models compute feature similarity scores on-the-fly, resulting in a more dynamic alignment between visual and textual features.</p><p>We hypothesize that regardless of architecture and training procedure, the attention mechanism makes STR models generally robust against orientation variations. To test our hypothesis, we created a pose-corrected version of Tex-tOCR which contains text in canonical orientation (practically horizontal), as opposed to the original version which contains text in arbitrary orientation. One possible contributor to the orientation robustness of TRBA is its image rectification module <ref type="bibr" target="#b17">[60]</ref>. To test if this is the case, we also train TRBC, the CTC-based version of TRBA. We train all models on either TextOCR variants exclusively and show the results in <ref type="table" target="#tab_0">Table 18</ref>. We observe that both CTC-based and attention-based models can be trained on arbitrarily-oriented text. The mean 0?accuracy decreased for CRNN but the decrease was not statistically-significant. For other models, the mean accuracy even increased with TRBA and PARSeq showing statistically-significant improvements. This suggests that the common practice of filtering non-horizontal text might be unnecessary. As far as arbitrarily-oriented text recognition is concerned, training on arbitrarily-oriented text expectedly improves the accuracy across all models. However, the improvement is minimal in CTC-based models compared to attention-based models. Moreover, TRBC exhibits slightly better orientation robustness compared to CRNN, but it still performs badly compared to TRBA. This suggests that the contribution of the image rectification module to orientation robustness is minimal, and that the attention mechanism is the primary contributor to orientation robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Combined word accuracy</head><p>Six small datasets are typically used to benchmark STR methods, resulting in six different mean values for word accuracy. The combined word accuracy is typically reported too, but we did not include it in <ref type="table" target="#tab_4">Table 4</ref> of the main text because of space constraints and possible confusion due to inconsistencies in test sets used. <ref type="table" target="#tab_0">Table 19</ref> shows the combined word accuracy on the benchmark (7,672 samples) and on the smaller test subset (consisting of IC13 857 and IC15 1,811) with a total of 7,248 samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Qualitative Results</head><p>In the following tables, shown are qualitative results for all test datasets and for some images obtained from the internet. The input images are shown in their original orientations and in aspect ratios close to their original. For predictions which are roughly aligned to the ground truth, wrong characters are highlighted in red while missing characters are indicated by a red underscore . <ref type="table" target="#tab_20">Table 20</ref> shows the results for samples from regular datasets like IIIT5k, SVT, and IC13. Most of the models did not have a problem recognizing the fairly clear, horizontal, and high-resolution input images. The only exception is CRNN failing to recognize any character from the tilted CITY image sample of SVT. No model was able to correctly recognize Verbandstoffe due to the ambiguity caused by motion blur, making the character o look like an e.  <ref type="table" target="#tab_0">Table 21</ref> shows the qualitative results for samples from the IC15 dataset. Context-free methods, ViTSTR and CRNN, were not able to correctly predict Kappa possibly due to the ambiguity caused by distortion on the first p char-acter. ABINet and CRNN both have difficulty in recognizing vertically-oriented (CONCIERGE ) and rotated text (UNSEEN ). No model correctly predicted epi-Centre due to the case ambiguity of the character C. Only PARSeq and CRNN were able to correctly read the telephone number.   <ref type="table" target="#tab_22">Table 22</ref> shows the qualitative results for SVTP samples. All models except ABINet were able to recognize MINT. No model correctly recognized the vertically-oriented text, REDWOOD, with ViTSTR and PARSeq producing the two closest predictions. Surprisingly, both PARSeq and ViTSTR fail at the relatively easy HOUSE, where the character S is occluded. In PARSeq, the visual features have a stronger effect on the final output than the textual features due to the image-position MHA being closer to the final decoder hidden state. Thus, a low-confidence visual feature might sway the output to the wrong character given enough magnitude relative to the textual features. All models correctly recognized Restaurant even though the image is relatively blurry. All models except CRNN correctly recognized the vertically-oriented text, CARLTON.  <ref type="table" target="#tab_23">Table 23</ref> shows the results for CUTE80, a dataset which primarily contains curved text. The samples are high-resolution and of good quality resulting in generally accurate recognition across models. The only exceptions are BALLYS for ABINet and the relatively vertical texts CLUB, and SALMON for CRNN.  <ref type="table" target="#tab_4">Table 24</ref> shows the results for samples from ArT, a dataset of arbitrarilyoriented and curved text. CRNN fails to recognize text which are verticallyoriented. Only ViTSTR is able to recognize FONDENTE correctly, with PARSeq and ABINet both predicting I in place of T. For the almost upside down TO-MORROW'S, only TRBA and ViTSTR are able to recognize it. PARSeq possibly mistook W for a V due to aspect ratio distortion (vertical image being rescaled into a horizontal one). <ref type="table" target="#tab_5">Table 25</ref> shows the results for COCO-Text samples. No model was able to recognize ANTS, possibly due to the presence of small stray characters around the main text. All models were able to recognize XT-862K in spite of the blurry image, and People in spite of the occluded o and p characters. Chevron is a particularly hard sample due to the last two characters being occluded by two different objects. Only PARseq was able to detect the last two characters and correctly recognize the o character, while all other models only recognized the first five characters. GUNNESS is another hard sample due to its low resolution and occluded character. Only PARSeq was able to infer the occluded character correctly.  <ref type="table" target="#tab_6">Table 26</ref> shows the qualitative results for ReCTS, a dataset which contains fairly high-resolution text with unconventional font styles. Model performance across all samples is generally good since they are clear and horizontally-oriented. No model correctly predicted the string of digits, with PARSeq and TRBA producing the closest predictions with only one wrong character. Most models correctly predicted AWON except for PARSeq and CRNN which mistook the occluded W for an N. <ref type="table" target="#tab_7">Table 27</ref> shows the results for Uber-Text, a dataset which contains many vertical or rotated text from outdoor signages. PARSeq is the only model to correctly recognize all samples.   <ref type="table" target="#tab_28">Table 28</ref> shows results from additional samples obtained from the Internet. Overall, the samples are high-resolution, horizontally-oriented, and use unconventional fonts similar to ReCTS. PARSeq is the only model to correctly recognize all samples, particularly Creative which uses a cursive handwriting type of font.  <ref type="table">TOGARASHI TOGARASHI TOGARASHI TOGARASHI TOGARASHI TOGARASH!</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) State-of-the-art method ABINet [24]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>PARSeq architecture and training overview. LayerN orm and Dropout layers are omitted due to space constraints. [B], [E], and [P] stand for beginning-of-sequence (BOS), end-of-sequence (EOS), and padding tokens, respectively. T = 25 results in 26 distinct position tokens. The position tokens both serve as query vectors and position embeddings for the input context. For [B], no position embedding is added. Attention masks are generated from the given permutations and are used only for the contextposition attention. Lce pertains to the cross-entropy loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Mean word accuracy (94-char) vs computational cost. P-S and P-Ti are shorthands for PARSeq-S and PARSeq-Ti, respectively. For TRBA and PARSeqA, FLOPS and latency correspond to mean values measured on the benchmark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>16. Chng, C.K., Liu, Y., Sun, Y., Ng, C.C., Luo, C., Ni, Z., Fang, C., Zhang, S.,Han, J., Ding, E., et al.: Icdar2019 robust reading challenge on arbitrary-shaped textrrc-art. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1571-1576. IEEE (2019) 17. Cubuk, E.D., Zoph, B., Man?, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 113-123 (2019). https://doi.org/10.1109/CVPR.2019.00020 18. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with a reduced search space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 702-703 (2020) 19. Cui, M., Wang, W., Zhang, J., Wang, L.: Representation and correlation enhanced encoder-decoder framework for scene text recognition. In: Llad?s, J., Lopresti, D., Uchida, S. (eds.) Document Analysis and Recognition -ICDAR 2021. pp. 156-170. Springer International Publishing, Cham (2021) 20. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR09 (2009) 21. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4171-4186. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/ N19-1423 22. Doll?r, P., Singh, M., Girshick, R.: Fast and accurate model scaling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 924-932 (2021) 23. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2020) 24. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7098-7107 (6 2021) 25. Golovin, D., Solnik, B., Moitra, S., Kochanski, G., Karro, J.E., Sculley, D. (eds.): Google Vizier: A Service for Black-Box Optimization (2017), http://www.kdd.org/kdd2017/papers/view/ google-vizier-a-service-for-black-box-optimization 26. Goyal, P., Doll?r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., He, K.: Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677 (2017) 27. Graves, A., Fern?ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In: Proceedings of the 23rd international conference on Machine learning. pp. 369-376 (2006) 28. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images. In: IEEE Conference on Computer Vision and Pattern Recognition (2016) 29. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., Wilson, A.: Averaging weights leads to wider optima and better generalization. In: Silva, R., Globerson, A., Globerson, A. (eds.) 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018. pp. 876-885. 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, Association For Uncertainty in Artificial Intelligence (AUAI) (2018) 30. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and artificial neural networks for natural scene text recognition. In: Workshop on Deep Learning, NIPS (2014) 31. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., et al.: Icdar 2015 competition on robust reading. In: 2015 13th International Conference on Document Analysis and Recognition (ICDAR). pp. 1156-1160. IEEE (2015) 32. Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R., Mas, J., Mota, D.F., Almazan, J.A., De Las Heras, L.P.: Icdar 2013 robust reading competition. In: 2013 12th International Conference on Document Analysis and Recognition. pp. 1484-1493. IEEE (2013) 33. Kasai, J., Pappas, N., Peng, H., Cross, J., Smith, N.: Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation. In: International Conference on Learning Representations (2021), https://openreview.net/forum?id= KpfasTaLUpq 34. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International Conference on Learning Representations (ICLR) (2015) 35. Krasin, I., Duerig, T., Alldrin, N., Ferrari, V., Abu-El-Haija, S., Kuznetsova, A., Rom, H., Uijlings, J., Popov, S., Kamali, S., Malloci, M., Pont-Tuset, J., Veit, A., Belongie, S., Gomes, V., Gupta, A., Sun, C., Chechik, G., Cai, D., Feng, Z., Narayanan, D., Murphy, K.: Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages (2017), https://storage.googleapis.com/ openimages/web/index.html 36. Krylov, I., Nosov, S., Sovrasov, V.: Open images v5 text annotation and yet another mask text spotter. In: Balasubramanian, V.N., Tsang, I. (eds.) Proceedings of The 13th Asian Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 157, pp. 379-389. PMLR (17-19 Nov 2021), https://proceedings. mlr.press/v157/krylov21a.html 37. Lee, C.Y., Osindero, S.: Recursive recurrent nets with attention modeling for ocr in the wild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (6 2016) 38. Lee, J., Park, S., Baek, J., Oh, S.J., Kim, S., Lee, H.: On recognizing texts of arbitrary shapes with 2d self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 546-547 (2020) 39. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., Gonzalez, J.: Train big, then compress: Rethinking model size for efficient training and inference of transformers. In: International Conference on Machine Learning. pp. 5958-5968. PMLR (2020) 40. Liao, Y., Jiang, X., Liu, Q.: Probabilistically masked language model capable of autoregressive generation in arbitrary word order. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 263-274 (2020) 41. Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J.E., Stoica, I.: Tune: A research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118 (2018) 42. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision. pp. 740-755. Springer (2014) 43. Litman, R., Anschel, O., Tsiper, S., Litman, R., Mazor, S., Manmatha, R.: Scatter: Selective context attentional scene text recognizer. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 44. Liu, W., Chen, C., Wong, K.Y.K., Su, Z., Han, J.: Star-net: a spatial attention residue network for scene text recognition. In: BMVC. vol. 2, p. 7 (2016) 45. Long, S., He, X., Yao, C.: Scene text detection and recognition: The deep learning era. International Journal of Computer Vision 129(1), 161-184 (2021) 46. Long, S., Yao, C.: Unrealtext: Synthesizing realistic scene text images from the unreal world. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 47. Mansimov, E., Wang, A., Welleck, S., Cho, K.: A generalized framework of sequence generation with application to undirected sequence models. arXiv preprint arXiv:1905.12790 (2019) 48. Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer sentinel mixture models. In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017), https://openreview.net/forum?id=Byj72udxe 49. Mishra, A., Alahari, K., Jawahar, C.: Scene text recognition using higher order language priors. In: BMVC-British Machine Vision Conference. BMVA (2012) 50. Mou, Y., Tan, L., Yang, H., Chen, J., Liu, L., Yan, R., Huang, Y.: Plugnet: Degradation aware scene text recognition supervised by a pluggable super-resolution unit. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV 16. pp. 158-174. Springer (2020) 51. Munjal, R.S., Prabhu, A.D., Arora, N., Moharana, S., Ramena, G.: Stride: Scene text recognition in-device. In: 2021 International Joint Conference on Neural Networks (IJCNN). pp. 1-8. IEEE (2021) 52. Nayef, N., Patel, Y., Busta, M., Chowdhury, P.N., Karatzas, D., Khlif, W., Matas, J., Pal, U., Burie, J.C., Liu, C.l., et al.: Icdar2019 robust reading challenge on multi-lingual scene text detection and recognition-rrc-mlt-2019. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1582-1587. IEEE (2019) 53. Nguyen, N., Nguyen, T., Tran, V., Tran, M.T., Ngo, T.D., Nguyen, T.H., Hoai, M.: Dictionary-guided scene text recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7383-7392 (6 2021) 54. Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with perspective distortion in natural scenes. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 569-576 (2013) 55. Qi, W., Gong, Y., Jiao, J., Yan, Y., Chen, W., Liu, D., Tang, K., Li, H., Chen, J., Zhang, R., et al.: Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining. In: International Conference on Machine Learning. pp. 8630-8639. PMLR (2021) 56. Qiao, Z., Zhou, Y., Yang, D., Zhou, Y., Wang, W.: Seed: Semantics enhanced encoder-decoder framework for scene text recognition. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (6 2020) 57. Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A robust arbitrary text detection system for natural scene images. Expert Systems with Applications 41(18), 8027-8048 (2014) 73. Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition. In: 2011 International Conference on Computer Vision. pp. 1457-1464. IEEE (2011) 74. Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.F., Chao, L.S.: Learning deep transformer models for machine translation. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 1810-1822 (2019) 75. Wang, Y., Xie, H., Fang, S., Wang, J., Zhu, S., Zhang, Y.: From two to one: A new scene text recognizer with visual language modeling network. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 14194-14203 (10 2021) 76. Xiao, T., Dollar, P., Singh, M., Mintun, E., Darrell, T., Girshick, R.: Early convolutions help transformers see better. Advances in Neural Information Processing Systems 34 (2021) 77. Yan, R., Peng, L., Xiao, S., Yao, G.: Primitive representation learning for scene text recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 284-293 (6 2021) 78. Yan, R., Peng, L., Xiao, S., Yao, G., Min, J.: Mean: Multi-element attention network for scene text recognition. In: 2020 25th International Conference on Pattern Recognition (ICPR). pp. 1-8. IEEE (2021) 79. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems 32 (2019) 80. Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards accurate scene text recognition with semantic reasoning networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12113-12122 (2020) 81. Yue, X., Kuang, Z., Lin, C., Sun, H., Zhang, W.: Robustscanner: Dynamically enhancing positional clues for robust text recognition. In: European Conference on Computer Vision. pp. 135-151. Springer (2020) 82. Zhang, H., Yao, Q., Yang, M., Xu, Y., Bai, X.: Autostr: Efficient backbone search for scene text recognition. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIV 16. pp. 751-767. Springer (2020) 83. Zhang, R., Zhou, Y., Jiang, Q., Song, Q., Li, N., Zhou, K., Wang, L., Wang, D., Liao, M., Yang, M., et al.: Icdar 2019 robust reading challenge on reading chinese text on signboard. In: 2019 international conference on document analysis and recognition (ICDAR). pp. 1577-1581. IEEE (2019) 84. Zhang, Y., Gueguen, L., Zharkov, I., Zhang, P., Seifert, K., Kadlec, B.: Uber-text: A large-scale dataset for optical character recognition from street-level imagery. In: SUNw: Scene Understanding Workshop -CVPR 2017. Hawaii, U.S.A. (2017), http://sunw.csail.mit.edu/abstract/uberText.pdf</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of a ViT layer from Dosovitskiy et al . [23]. Norm pertains to LayerN orm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Given a three-element text label y = [y 1 , y 2 , y 3 ] and K = 4 permutations:<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, [3, 2, 1],<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>, and [2, 3, 1]. The first two permutations are the left-to-right and right-to-left orderings, respectively. Both are always used as long as K &gt; 1. The corresponding factorizations of the joint probability per pair are as follows: p(y)<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> = p(y 1 )p(y 2 |y 1 )p(y 3 |y 1 , y 2 ) p(y)<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref> = p(y 3 )p(y 2 |y 3 )p(y 1 |y 2 , y 3 ) (a) MHA for output token y1 (b) MHA for output token y2 (c) MHA for output token y3 (d) MHA for output token [E]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Examples of source scene images common to TextOCR and OpenVINO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>PARSeq word accuracy and single-image latency for each decoding scheme. The number of refinement iterations used is indicated for each point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Model latency vs output label length as measured by PyTorch's benchmark timer on an NVIDIA Tesla A100 GPU. Each point corresponds to a mean of five runs of Timer.blocked autorange(). Lower is better. Mean word length is the average length of the labels from all test datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Illustration of AR attention masks for each permutation.</figDesc><table><row><cell>The table header</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>70], RCTW17 [62], Uber-Text (Uber) [84], ArT [16], LSVT [65], MLT19 [52], and ReCTS [83]. A comprehensive discussion about these datasets is available in Baek et al . [4]. In addition, we also use two recent large-scale real datasets based on Open Images [35]: TextOCR [63] and annotations from the OpenVINO toolkit [36]. More details in Appendix F. Following prior works [3], we use IIIT 5k-word (IIIT5k) [49], CUTE80 (CUTE) [57], Street View Text (SVT) [73], SVT-Perspective (SVTP) [54], ICDAR 2013 (IC13) [32], and ICDAR 2015 (IC15) [31] as the datasets for evaluation. Baek et al .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>[16] (35.1k samples; curved and rotated text), and Uber-Text [84] (80.6k samples; vertical and rotated text).</figDesc><table /><note>(a) Samples from the benchmark datasets (b) Samples from Uber, COCO, ArT Fig. 4. Sample test images from the datasets used 4.2 Training Protocol and Model Selection All models are trained in a mixed-precision, dual-GPU setup using PyTorch DDP for 169,680 iterations with a batch size of 384. Learning rates vary per model (Appendix G.2). The Adam [34] optimizer is used together with the 1cycle [64] learning rate scheduler. At iteration 127,260 (75% of total), Stochastic Weight Averaging (SWA) [29] is used and the 1cycle scheduler is replaced by the SWA scheduler. Validation is performed every 1,000 training steps. Since SWA aver- ages weights at the end of each epoch, the last checkpoint at the end of training is selected. For PARSeq, K = 6 permutations are used (Section 4.4). A patch size of 8 ? 4 is used for PARSeq and ViTSTR. More details are in Appendix G.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>K</head><label></label><figDesc>AR acc. NAR acc. cloze acc. Training hours</figDesc><table><row><cell>1</cell><cell>93.04</cell><cell>0.01</cell><cell>71.14</cell><cell>5.86</cell></row><row><cell>2</cell><cell>93.48</cell><cell>22.69</cell><cell>94.55</cell><cell>7.30</cell></row><row><cell>6</cell><cell>93.34</cell><cell>92.22</cell><cell>94.81</cell><cell>8.48</cell></row><row><cell>12</cell><cell>92.91</cell><cell>91.71</cell><cell>94.59</cell><cell>10.10</cell></row><row><cell>24</cell><cell>92.67</cell><cell>91.72</cell><cell>94.36</cell><cell>13.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Mean word accuracy on the benchmark vs evaluation charset size</figDesc><table><row><cell>Method</cell><cell>Train data</cell><cell>36-char</cell><cell>62-char</cell><cell>94-char</cell></row><row><cell>CRNN</cell><cell>S</cell><cell>83.2?0.2</cell><cell>56.5?0.3</cell><cell>54.8?0.2</cell></row><row><cell>ViTSTR-S</cell><cell>S</cell><cell>88.6?0.0</cell><cell>69.5?1.0</cell><cell>67.7?1.0</cell></row><row><cell>TRBA</cell><cell>S</cell><cell>90.6?0.1</cell><cell>71.9?0.9</cell><cell>69.9?0.8</cell></row><row><cell>ABINet</cell><cell>S</cell><cell>89.8?0.2</cell><cell>68.5?1.1</cell><cell>66.4?1.0</cell></row><row><cell>PARSeq N</cell><cell>S</cell><cell>90.7?0.2</cell><cell>72.5?1.1</cell><cell>70.5?1.1</cell></row><row><cell>PARSeq A</cell><cell>S</cell><cell cols="3">91.9?0.2 75.5?0.6 73.0?0.7</cell></row><row><cell>CRNN</cell><cell>R</cell><cell>88.5?0.1</cell><cell>87.2?0.1</cell><cell>85.8?0.1</cell></row><row><cell>ViTSTR-S</cell><cell>R</cell><cell>94.3?0.1</cell><cell>92.8?0.1</cell><cell>91.8?0.1</cell></row><row><cell>TRBA</cell><cell>R</cell><cell>95.2?0.2</cell><cell>93.7?0.1</cell><cell>92.5?0.1</cell></row><row><cell>ABINet</cell><cell>R</cell><cell>95.2?0.1</cell><cell>93.7?0.1</cell><cell>92.4?0.1</cell></row><row><cell>PARSeq N</cell><cell>R</cell><cell>95.2?0.1</cell><cell>93.7?0.1</cell><cell>92.7?0.1</cell></row><row><cell>PARSeq A</cell><cell>R</cell><cell cols="3">96.0?0.0 94.6?0.0 93.3?0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>36-char word accuracy on larger and more challenging datasets</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test datasets and # of samples</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell>Train data</cell><cell>ArT 35,149</cell><cell>COCO 9,825</cell><cell>Uber 80,551</cell><cell>Total 125,525</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CRNN</cell><cell>S</cell><cell>57.3?0.1</cell><cell>49.3?0.6</cell><cell>33.1?0.3</cell><cell>41.1?0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ViTSTR-S</cell><cell>S</cell><cell>66.1?0.1</cell><cell>56.4?0.5</cell><cell>37.6?0.3</cell><cell>47.0?0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TRBA</cell><cell>S</cell><cell>68.2?0.1</cell><cell>61.4?0.4</cell><cell>38.0?0.3</cell><cell>48.3?0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ABINet</cell><cell>S</cell><cell>65.4?0.4</cell><cell>57.1?0.8</cell><cell>34.9?0.3</cell><cell>45.2?0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PARSeq N</cell><cell>S</cell><cell>69.1?0.2</cell><cell>60.2?0.8</cell><cell>39.9?0.5</cell><cell>49.7?0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PARSeq A</cell><cell>S</cell><cell>70.7?0.1 64.0?0.9 42.0?0.5 51.8?0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CRNN</cell><cell>R</cell><cell>66.8?0.2</cell><cell>62.2?0.3</cell><cell>51.0?0.2</cell><cell>56.3?0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ViTSTR-S</cell><cell>R</cell><cell>81.1?0.1</cell><cell>74.1?0.4</cell><cell>78.2?0.1</cell><cell>78.7?0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TRBA</cell><cell>R</cell><cell>82.5?0.2</cell><cell>77.5?0.2</cell><cell>81.2?0.3</cell><cell>81.3?0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ABINet</cell><cell>R</cell><cell>81.2?0.1</cell><cell>76.4?0.1</cell><cell>71.5?0.7</cell><cell>74.6?0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PARSeq N</cell><cell>R</cell><cell>83.0?0.2</cell><cell>77.0?0.2</cell><cell>82.4?0.3</cell><cell>82.1?0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PARSeq A</cell><cell>R</cell><cell>84.5?0.1 79.8?0.1 84.5?0.1 84.1?0.0</cell></row><row><cell></cell><cell>94</cell><cell></cell><cell>P-S A</cell><cell></cell><cell></cell></row><row><cell></cell><cell>92</cell><cell>P-Ti A</cell><cell>P-S N</cell><cell cols="2">TRBA ABINet</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">ViTSTR-S</cell></row><row><cell>Accuracy [%]</cell><cell>90</cell><cell>P-Ti N</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>86</cell><cell cols="2">CRNN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell>40</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Parameters [M]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Word accuracy on the six benchmark datasets (36-char). For Train data: Synthetic datasets (S) -MJ [30] and ST [28]; Benchmark datasets (B) -SVT, IIIT5k, IC13, and IC15; Real datasets (R) -COCO, RCTW17, Uber, ArT, LSVT, MLT19, ReCTS, TextOCR, and OpenVINO; "*" denotes usage of character-level labels. In our experiments, bold indicates the highest word accuracy per column. 1 Used with SCAT-TER [43]. 2 SynthText without special characters (5.5M samples). 3 LM pretrained on WikiText-103 [48]. Combined accuracy values are available in Appendix K 0?0.2 91.7?0.4 95.1?0.7 94.2?0.7 82.7?0.1 78.7?0.1 83.9?0.6 88.2?0.6 CRNN S 91.2?0.2 85.7?0.7 92.1?0.7 90.9?0.5 74.4?1.0 70.8?0.9 73.5?0.6 78.7?0.7 TRBA S 96.3?0.2 92.8?0.9 96.3?0.3 95.0?0.4 84.3?0.1 80.6?0.2 86.9?1.3 91.3?1.6 ABINet S 95.3?0.2 93.4?0.2 97.1?0.4 95.0?0.3 83.1?0.3 79.1?0.2 87.1?0.6 89.7?2.3 PARSeq N (Ours) S 95.7?0.2 92.6?0.3 96.3?0.4 95.5?0.6 85.1?0.1 81.4?0.1 87.9?0.9 91.4?1.5 PARSeq A (Ours) S 97.0?0.2 93.6?0.4 97.0?0.3 96.2?0.4 86.5?0.2 82.9?0.2 88.9?0.9 92.2?1.2 1?0.2 95.8?0.4 97.6?0.3 97.7?0.3 88.4?0.4 87.1?0.3 91.4?0.2 96.1?0.4 CRNN R 94.6?0.2 90.7?0.4 94.1?0.4 94.5?0.3 82.0?0.2 78.5?0.2 80.6?0.3 89.1?0.4 TRBA R 98.6?0.1 97.0?0.2 97.6?0.3 97.6?0.2 89.8?0.4 88.7?0.4 93.7?0.3 97.7?0.2 ABINet R 98.6?0.2 97.8?0.3 98.0?0.4 97.8?0.2 90.2?0.2 88.5?0.2 93.9?0.8 97.7?0.7 PARSeq N (Ours) R 98.3?0.1 97.5?0.4 98.0?0.1 98.1?0.1 89.6?0.2 88.4?0.4 94.6?1.0 97.7?0.9 PARSeq A (Ours) R 99.1?0.1 97.9?0.2 98.3?0.2 98.4?0.2 90.7?0.3 89.6?0.3 95.7?0.9 98.3?0.6</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Test datasets and # of samples</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell cols="2">Train IIIT5k data 3,000</cell><cell>SVT 647</cell><cell>857</cell><cell>IC13 1,015</cell><cell cols="2">IC15 1,811 2,077</cell><cell>SVTP 645</cell><cell>CUTE 288</cell></row><row><cell></cell><cell>PlugNet [50]</cell><cell>S</cell><cell>94.4</cell><cell>92.3</cell><cell>-</cell><cell>95.0</cell><cell>-</cell><cell>82.2</cell><cell>84.3</cell><cell>85.0</cell></row><row><cell></cell><cell>SRN [80]</cell><cell>S</cell><cell>94.8</cell><cell>91.5</cell><cell>95.5</cell><cell>-</cell><cell>82.7</cell><cell>-</cell><cell>85.1</cell><cell>87.8</cell></row><row><cell></cell><cell cols="2">RobustScanner [81] S,B</cell><cell>95.4</cell><cell>89.3</cell><cell>-</cell><cell>94.1</cell><cell>-</cell><cell>79.2</cell><cell>82.9</cell><cell>92.4</cell></row><row><cell>Published Results</cell><cell>TextScanner [71] AutoSTR [82] RCEED [19] PREN2D [77] VisionLAN [75] Bhunia et al. [9] CVAE-Feed. 1 [8] STN-CSTR [12] ViTSTR-B [2]</cell><cell>S* S S,B S S S S S S 2</cell><cell>95.7 94.7 94.9 95.6 95.8 95.2 95.2 94.2 88.4</cell><cell>92.7 90.9 91.8 94.0 91.7 92.2 -92.3 87.7</cell><cell>---96.4 95.7 --96.3 93.2</cell><cell>94.9 94.2 ---95.5 95.7 94.1 92.4</cell><cell>-81.8 -83.0 83.7 --86.1 78.5</cell><cell>83.5 -82.2 --84.0 84.6 82.0 72.6</cell><cell>84.8 81.7 83.6 87.6 86.0 85.7 88.9 86.2 81.8</cell><cell>91.6 -91.7 91.7 88.5 89.7 89.7 -81.3</cell></row><row><cell></cell><cell>CRNN [4]</cell><cell>S</cell><cell>84.3</cell><cell>78.9</cell><cell>-</cell><cell>88.8</cell><cell>-</cell><cell>61.5</cell><cell>64.8</cell><cell>61.3</cell></row><row><cell></cell><cell>TRBA [4]</cell><cell>S</cell><cell>92.1</cell><cell>88.9</cell><cell>-</cell><cell>93.1</cell><cell>-</cell><cell>74.7</cell><cell>79.5</cell><cell>78.2</cell></row><row><cell></cell><cell>ABINet [24]</cell><cell>S 3</cell><cell>96.2</cell><cell>93.5</cell><cell>97.4</cell><cell>-</cell><cell>86.0</cell><cell>-</cell><cell>89.3</cell><cell>89.2</cell></row><row><cell>Experiments</cell><cell cols="3">ViTSTR-S 94.ViTSTR-S S R 98.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Example of a spurious suffix from a left-to-right AR model. GT refers to the ground truth label, while Confidence pertains to per-character prediction confidence As mentioned in the main text, ensemble methods such as ABINet [24] and SRN [80] utilize a standalone or external Language Model (LM). InTable 9, we show the cost measurements of fvcore on the full ABINet model for a single input, as well as the measurement breakdown for its component models. We can see that while the LM accounts for around 34.48% of the parameter count, it only uses 13.65% of the overall FLOPS and 15.78% of the overall activations (a measure shown to be correlated with model runtime[22,76]). When evaluated in spelling correction on the 36-character set, the LM achieves a top-5 word accuracy of only 41.9%[24]. With the ground truth label itself as input</figDesc><table><row><cell>Input</cell><cell></cell><cell>GT</cell><cell>Prediction</cell><cell>Confidence</cell></row><row><cell></cell><cell></cell><cell cols="3">terrifying terrifyingly [1.00, . . . , 1.00, 0.97, 0.72]</cell></row><row><cell cols="5">Table 8. Example of direction-dependent decoding with two AR models</cell></row><row><cell>Input</cell><cell cols="3">GT Direction Prediction</cell><cell>Confidence</cell></row><row><cell></cell><cell>open</cell><cell>LTR RTL</cell><cell>open opell</cell><cell>[1.00, 1.00, 1.00, 0.66] [1.00, 1.00, 0.52, 0.57, 0.94]</cell></row><row><cell cols="5">B Inefficiency of External Language Models in STR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Commonly used cost indicators as measured by fvcore for ABINet. Full Model pertains to the overall measurements</figDesc><table><row><cell>Module</cell><cell># of Parameters (M)</cell><cell cols="2">FLOPS (G) # of Activations (M)</cell></row><row><cell>Full Model</cell><cell>36.858 (100.00%)</cell><cell>7.289 (100.00%)</cell><cell>10.785 (100.00%)</cell></row><row><cell>-Vision</cell><cell>23.577 (63.97%)</cell><cell>6.249 (85.73%)</cell><cell>9.036 (83.78%)</cell></row><row><cell>-Language</cell><cell>12.707 (34.48%)</cell><cell>0.995 (13.65%)</cell><cell>1.702 (15.78%)</cell></row><row><cell>-Alignment</cell><cell>0.574 (1.55%)</cell><cell>0.045 (0.62%)</cell><cell>0.047 (0.44%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Performance of ABINet's LM when the ground truth label itself is used as the input</figDesc><table><row><cell cols="4">Dataset # of samples Word acc. (%) 1 -NED</cell></row><row><cell>IIIT5k</cell><cell>3,000</cell><cell>47.33</cell><cell>69.50</cell></row><row><cell>SVT</cell><cell>647</cell><cell>65.38</cell><cell>83.48</cell></row><row><cell>IC13</cell><cell>1,015</cell><cell>62.07</cell><cell>78.77</cell></row><row><cell>IC15</cell><cell>2,077</cell><cell>40.49</cell><cell>67.72</cell></row><row><cell>SVTP</cell><cell>645</cell><cell>65.27</cell><cell>83.08</cell></row><row><cell>CUTE80</cell><cell>288</cell><cell>46.88</cell><cell>68.65</cell></row><row><cell>Combined</cell><cell>7,672</cell><cell>50.44</cell><cell>72.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc>Configurations for the base (PARSeq-S) and smaller (PARSeq-Ti) model variants. d model refers to the dimensionality of the model which dictates the dimensions of the vectors and feature maps. h refers to the number of attention heads used in MHA layers. dMLP refers to the dimension of the intermediate features within the MLP layer. depth refers to the number of encoder or decoder layers usedFig. 7. Visio-lingual decoder architecture with LayerN orm layers shown.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>encoder</cell><cell></cell><cell></cell><cell>decoder</cell><cell></cell></row><row><cell>Variants</cell><cell>d model</cell><cell>h</cell><cell>dMLP</cell><cell>depth</cell><cell>h</cell><cell>dMLP</cell><cell>depth</cell></row><row><cell>PARSeq-Ti</cell><cell>192</cell><cell>3</cell><cell>768</cell><cell>12</cell><cell>6</cell><cell>768</cell><cell>1</cell></row><row><cell>PARSeq-S</cell><cell>384</cell><cell>6</cell><cell>1536</cell><cell>12</cell><cell>12</cell><cell>1536</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Probability terms grouped by permutation pairs.</figDesc><table><row><cell>Perm.</cell><cell>y1</cell><cell>y2</cell><cell>y3</cell></row><row><cell cols="2">[1, 2, 3] p(y1)</cell><cell>p(y2|y1)</cell><cell>p(y3|y1, y2)</cell></row><row><cell cols="3">[3, 2, 1] p(y1|y2, y3) p(y2|y3)</cell><cell>p(y3)</cell></row><row><cell cols="2">[1, 3, 2] p(y1)</cell><cell cols="2">p(y2|y1, y3) p(y3|y1)</cell></row><row><cell cols="3">[2, 3, 1] p(y1|y2, y3) p(y2)</cell><cell>p(y3|y2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>Overlap between TextOCR and OpenVINO in terms of the number of common source scene images.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>TextOCR</cell><cell></cell></row><row><cell></cell><cell></cell><cell>train</cell><cell>val</cell><cell>test</cell></row><row><cell>OpenVINO</cell><cell>train 1 train 2 train 5 train f validation</cell><cell>1,612 1,444 1,302 1,068 0</cell><cell>225 230 184 157 0</cell><cell>0 0 0 0 0</cell></row></table><note>Fig. 9. Cropped word boxes from Open Images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Summary of dataset usage after on-the-fly filtering for the 94-character set. Numbers indicate how many samples were used from each dataset. t and v refer to splits that were repurposed as training and validation data, respectively. * indicates private ground truth labels. -indicates that the dataset does not have a particular split. IC13 and IC15 have two versions of their respective test splits commonly used in the literature.</figDesc><table><row><cell>Dataset</cell><cell>train</cell><cell>val</cell><cell>test</cell></row><row><cell>MJSynth</cell><cell>7,224,586</cell><cell>802,731 t</cell><cell>891,924 t</cell></row><row><cell>SynthText</cell><cell>6,975,301</cell><cell>-</cell><cell>-</cell></row><row><cell>LSVT</cell><cell>41,439</cell><cell>-</cell><cell>-</cell></row><row><cell>MLT19</cell><cell>56,727</cell><cell>-</cell><cell>-</cell></row><row><cell>RCTW17</cell><cell>10,284</cell><cell>-</cell><cell>-</cell></row><row><cell>ReCTS</cell><cell>21,589</cell><cell>-</cell><cell>2,467 t</cell></row><row><cell>TextOCR</cell><cell>710,994</cell><cell>107,093 t</cell><cell>0 *</cell></row><row><cell>OpenVINO</cell><cell>1,912,784</cell><cell>158,757 t</cell><cell>-</cell></row><row><cell>ArT</cell><cell>32,028</cell><cell>-</cell><cell>35,149</cell></row><row><cell>COCO</cell><cell>59,733</cell><cell>13,394 t</cell><cell>9,825</cell></row><row><cell>Uber</cell><cell>91,732</cell><cell>36,188 t</cell><cell>80,587</cell></row><row><cell>IIIT5k</cell><cell>2,000 v</cell><cell>-</cell><cell>3,000</cell></row><row><cell>SVT</cell><cell>257 v</cell><cell>-</cell><cell>647</cell></row><row><cell>IC13</cell><cell>848 v</cell><cell>-</cell><cell>857 / 1,015</cell></row><row><cell>IC15</cell><cell>4,468 v</cell><cell>-</cell><cell>1,811 / 2,077</cell></row><row><cell>SVTP</cell><cell>-</cell><cell>-</cell><cell>645</cell></row><row><cell>CUTE</cell><cell>-</cell><cell>-</cell><cell>288</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 .</head><label>15</label><figDesc>Training schedule comparison vs reproduced methods. Sorted from shortest to longest schedule based on the sample count (essentially batch size ? number of iterations)</figDesc><table><row><cell>Method</cell><cell cols="3">Batch size # of iterations Sample count (M)</cell></row><row><cell>CRNN and TRBA [4]</cell><cell>128</cell><cell>200,000</cell><cell>25.6</cell></row><row><cell>ViTSTR [2]</cell><cell>192</cell><cell>300,000</cell><cell>57.6</cell></row><row><cell>Ours</cell><cell>384</cell><cell>169,680</cell><cell>65.2</cell></row><row><cell>ABINet (VM + full) [24]</cell><cell>384</cell><cell>745,074</cell><cell>286.1</cell></row><row><cell>ABINet (LM) [24]</cell><cell>4,096</cell><cell>1,688,720</cell><cell>6,917.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 .</head><label>16</label><figDesc>Learning rates used for training. The Base LR is the raw value set in the configuration, while the Effective LR is the actual value used for training. During pretraining, ABINet (LM) is used for ABINet's language model , where nGP U refers to the number of GPUs used (i.e. nGP U = 2 for a dual-GPU setup) and bsize refers to the batch size (i.e. bsize = 384).</figDesc><table><row><cell>Model</cell><cell cols="2">Base LR Effective LR</cell></row><row><cell>CRNN</cell><cell>5.10 ? 10 ?4</cell><cell>1.08 ? 10 ?3</cell></row><row><cell>ViTSTR-S</cell><cell>8.90 ? 10 ?4</cell><cell>1.89 ? 10 ?3</cell></row><row><cell>TRBC</cell><cell>1.00 ? 10 ?4</cell><cell>2.12 ? 10 ?4</cell></row><row><cell>TRBA</cell><cell>6.90 ? 10 ?4</cell><cell>1.46 ? 10 ?3</cell></row><row><cell cols="2">ABINet (LM) 3.00 ? 10 ?4</cell><cell>6.36 ? 10 ?4</cell></row><row><cell>ABINet</cell><cell>3.40 ? 10 ?4</cell><cell>7.21 ? 10 ?4</cell></row><row><cell>PARSeq</cell><cell>7.00 ? 10 ?4</cell><cell>1.48 ? 10 ?3</cell></row><row><cell cols="3">the configuration space. Finally, a grid search over the narrowed down learning</cell></row><row><cell cols="3">rate range was performed with models trained to completion. The final learn-</cell></row><row><cell cols="3">ing rates used for training are shown in Table 16. The base learning rates are scaled using two multipliers: the DDP factor ( ? nGP U ) and the batch size linear</cell></row><row><cell>scaling rule (bsize/256) [26]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 .</head><label>17</label><figDesc>Orientation robustness benchmark. Word accuracy (94-char) on rotated versions of the six benchmark datasets. %dec refers to the percentage decrease of the Mean accuracy w.r.t. the 0?accuracy.</figDesc><table><row><cell>Method</cell><cell>0??</cell><cell>90??</cell><cell>180??</cell><cell cols="3">270?? Mean? %dec?</cell></row><row><cell>CRNN</cell><cell>85.8</cell><cell>11.8?0.1</cell><cell>6.4?0.5</cell><cell>10.7?0.2</cell><cell>9.6</cell><cell>88.8</cell></row><row><cell>ViTSTR-S</cell><cell>91.8</cell><cell>87.9?0.2</cell><cell>78.9?0.3</cell><cell>80.6?0.9</cell><cell>82.4</cell><cell>10.2</cell></row><row><cell>TRBA</cell><cell>92.5</cell><cell>84.6?0.1</cell><cell>83.5?0.2</cell><cell>78.6?0.3</cell><cell>82.3</cell><cell>11.0</cell></row><row><cell>ABINet</cell><cell>92.4</cell><cell>66.0?1.5</cell><cell>77.1?0.9</cell><cell>65.5?1.8</cell><cell>69.6</cell><cell>24.7</cell></row><row><cell>PARSeq N</cell><cell>92.7</cell><cell>86.7?0.3</cell><cell>83.2?0.9</cell><cell>81.1?0.3</cell><cell>83.7</cell><cell>9.7</cell></row><row><cell>PARSeq A</cell><cell cols="4">93.3 88.0?0.1 86.6?0.3 84.1?0.1</cell><cell>86.2</cell><cell>7.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 18 .</head><label>18</label><figDesc>Effect of training on horizontally-oriented (H) vs arbitrarily-oriented (A) variants of TextOCR. 0?pertains to model accuracy (94-char) on non-rotated benchmark datasets. Rotated refers to the mean accuracy on 90?, 180?, and 270?rotations of the benchmark datasets. In H vs A per row, bold indicates significantly higher accuracy.</figDesc><table><row><cell>Method</cell><cell>H</cell><cell cols="3">0?Rotated A H</cell><cell>A</cell></row><row><cell>CRNN</cell><cell>84.7?0.4</cell><cell>84.0?0.4</cell><cell>0.8?0.0</cell><cell cols="2">8.7?0.4</cell></row><row><cell>ViTSTR-S</cell><cell>87.8?0.7</cell><cell>88.1?0.3</cell><cell>2.5?0.2</cell><cell cols="2">72.8?1.3</cell></row><row><cell>TRBC</cell><cell>87.0?0.2</cell><cell>87.3?0.2</cell><cell>1.3?0.1</cell><cell cols="2">17.3?1.9</cell></row><row><cell>TRBA</cell><cell>89.7?0.0</cell><cell>90.1?0.2</cell><cell>2.7?0.1</cell><cell cols="2">76.3?0.4</cell></row><row><cell>ABINet</cell><cell>90.3?0.2</cell><cell>90.7?0.2</cell><cell>3.0?0.3</cell><cell cols="2">63.9?1.5</cell></row><row><cell>PARSeq N</cell><cell>89.8?0.3</cell><cell>90.4?0.0</cell><cell>6.4?0.4</cell><cell cols="2">77.9?0.6</cell></row><row><cell>PARSeq A</cell><cell>90.6?0.0</cell><cell>91.3?0.2</cell><cell>8.4?0.4</cell><cell cols="2">80.7?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 19 .</head><label>19</label><figDesc>Word accuracy on the six benchmark datasets (36-character set). For Train data: Synthetic datasets (S) -MJ [30] and ST [28]; Benchmark datasets (B) -SVT, IIIT5k, IC13, and IC15; Real datasets (R) -COCO, RCTW17, Uber, ArT, LSVT, MLT19, ReCTS, TextOCR, and OpenVINO; "*" denotes usage of character-level labels. In our experiments, bold indicates the highest word accuracy per column. 1 Used with SCATTER [43]. 2 SynthText without special characters (5.5M samples). 3 LM Pretrained on WikiText-103 [48]</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Test datasets and # of samples</cell></row><row><cell></cell><cell>Method</cell><cell cols="2">Train Total data 7,248</cell><cell>Total (benchmark ) 7,672</cell></row><row><cell></cell><cell>PlugNet [50]</cell><cell>S</cell><cell>-</cell><cell>89.8</cell></row><row><cell></cell><cell>SRN [80]</cell><cell>S</cell><cell>90.4</cell><cell>-</cell></row><row><cell></cell><cell cols="2">RobustScanner [81] S,B</cell><cell>-</cell><cell>89.2</cell></row><row><cell>Published Results</cell><cell>TextScanner [71] AutoSTR [82] RCEED [19] PREN2D [77] VisionLAN [75] Bhunia et al. [9] CVAE-Feed. 1 [8] STN-CSTR [12] ViTSTR-B [2]</cell><cell>S* S S,B S S S S S S 2</cell><cell>---91.5 91.2 ---85.6</cell><cell>91.0 ----90.9 --83.8</cell></row><row><cell></cell><cell>CRNN [4]</cell><cell>S</cell><cell>-</cell><cell>75.8</cell></row><row><cell></cell><cell>TRBA [4]</cell><cell>S</cell><cell>-</cell><cell>85.7</cell></row><row><cell></cell><cell>ABINet [24]</cell><cell>S 3</cell><cell>92.7</cell><cell>-</cell></row><row><cell></cell><cell>ViTSTR-S</cell><cell>S</cell><cell>90.0?0.1</cell><cell>88.6?0.0</cell></row><row><cell></cell><cell>CRNN</cell><cell>S</cell><cell>84.5?0.2</cell><cell>83.2?0.2</cell></row><row><cell>Experiments</cell><cell>TRBA ABINet PARSeq N (Ours) PARSeq A (Ours) ViTSTR-S CRNN TRBA</cell><cell cols="2">S S S S R 94.7?0.1 92.0?0.2 91.3?0.2 92.0?0.2 93.2?0.2 R 89.6?0.1 R 95.7?0.1</cell><cell>90.6?0.1 89.8?0.1 90.7?0.2 91.9?0.2 94.3?0.1 88.5?0.0 95.2?0.1</cell></row><row><cell></cell><cell>ABINet</cell><cell cols="2">R 95.9?0.2</cell><cell>95.2?0.1</cell></row><row><cell></cell><cell>PARSeq N (Ours)</cell><cell cols="2">R 95.7?0.1</cell><cell>95.2?0.1</cell></row><row><cell></cell><cell>PARSeq A (Ours)</cell><cell cols="2">R 96.4?0.0</cell><cell>96.0?0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 20 .</head><label>20</label><figDesc>Qualitative results on samples from regular datasets IIIT5k, SVT, and IC13. GT refers to the ground truth label.</figDesc><table><row><cell>Predictions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 21 .</head><label>21</label><figDesc>Qualitative results from IC15 samples.</figDesc><table><row><cell>Predictions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 22 .</head><label>22</label><figDesc>Qualitative results from SVTP samples.</figDesc><table><row><cell>Predictions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 23 .</head><label>23</label><figDesc>Qualitative results from CUTE80 samples.</figDesc><table><row><cell>Predictions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 24 .</head><label>24</label><figDesc>Qualitative results from ArT samples.</figDesc><table><row><cell>Predictions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 25 .</head><label>25</label><figDesc>Qualitative results from COCO-Text samples.</figDesc><table><row><cell>Predictions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 26 .</head><label>26</label><figDesc>Qualitative results from ReCTS samples.</figDesc><table><row><cell>Predictions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 27 .</head><label>27</label><figDesc>Qualitative results from Uber-Text samples.</figDesc><table><row><cell>Predictions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 28 .</head><label>28</label><figDesc>Qualitative results from samples obtained from the internet.</figDesc><table><row><cell>Predictions</cell></row></table><note>Creative Creative Creat ne Crestire Creat ee Cedrre</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data augmentation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Atienza</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW54120.2021.00181</idno>
		<ptr target="https://doi.org/10.1109/ICCVW54120.2021.00181" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision transformer for fast and efficient scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Atienza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What if we only use real datasets for scene text recognition? toward scene text recognition with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3113" to="3122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByxZX20qFQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balandat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bakshy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.06403" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards the unseen: Iterative text recognition by distilling from errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14950" to="14959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint visual semantic reasoning: Multi-stage decoder for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14940" to="14949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bidirectional scene text recognition with a single decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2020</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2664" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rosetta: Large scale system for text detection and recognition in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Revisiting classification perspective on scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.10884" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text recognition in the wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aon: Towards arbitrarilyoriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nrtr: A no-recurrence sequence-to-sequence model for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="781" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icdar2017 competition on reading chinese text in the wild (rctw-17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1429" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8802" to="8812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<title level="m">Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1557" to="1562" />
		</imprint>
	</monogr>
	<note>2019 International Conference on Document Analysis and Recognition (ICDAR)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Train once, and decode as you like</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="280" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<ptr target="http://vision.cornell.edu/se3/wp-content/uploads/2016/01/1601.07140v1.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Textscanner: Reading characters in order for robust scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12120" to="12127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gated recurrent convolution neural network for ocr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
