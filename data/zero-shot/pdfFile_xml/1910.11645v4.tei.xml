<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing Domain Gap by Reducing Style Bias</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
							<email>hsnam@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Lee</surname></persName>
							<email>hjlee@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
							<email>jcpark@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Yoon</surname></persName>
							<email>wonjun@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
							<email>dgyoo@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing Domain Gap by Reducing Style Bias</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) often fail to maintain their performance when they confront new test domains, which is known as the problem of domain shift. Recent studies suggest that one of the main causes of this problem is CNNs' strong inductive bias towards image styles (i.e. textures) which are sensitive to domain changes, rather than contents (i.e. shapes). Inspired by this, we propose to reduce the intrinsic style bias of CNNs to close the gap between domains. Our Style-Agnostic Networks (SagNets) disentangle style encodings from class categories to prevent style biased predictions and focus more on the contents. Extensive experiments show that our method effectively reduces the style bias and makes the model more robust under domain shift. It achieves remarkable performance improvements in a wide range of cross-domain tasks including domain generalization, unsupervised domain adaptation, and semi-supervised domain adaptation on multiple datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite the huge success of Convolutional Neural Networks (CNNs) fueled by large-scale training data, their performance often degrades significantly when they encounter test data from unseen environments. This phenomenon, known as the problem of domain shift <ref type="bibr" target="#b40">[41]</ref>, comes from the representation gap between training and testing domains. For a more reliable deployment of CNNs to ever-changing real-world scenarios, the community has long sought to make CNNs robust to domain shift under various problem settings such as Domain Generalization (DG) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>, Unsupervised Domain Adaptation (UDA) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref>, and Semi-Supervised Domain Adaptation (SSDA) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>In stark contrast to the vulnerability of CNNs against domain shift, the human visual recognition system generalizes incredibly well across domains. For example, young children learn many object concepts from pictures, but they naturally transfer their knowledge to the real world <ref type="bibr" target="#b9">[10]</ref>.</p><p>Similarly, people can easily recognize objects in cartoons or paintings even if they have not seen the same style of an image before. Where does such a difference come from?</p><p>A recent line of studies has revealed that standard CNNs have an inductive bias far different from human vision: while humans tend to recognize objects based on their contents (i.e. shapes) <ref type="bibr" target="#b26">[27]</ref>, CNNs exhibit a strong bias towards styles (i.e. textures) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>. This may explain why CNNs are intrinsically more sensitive to domain shift because image styles are more likely to change across domains than the contents. Geirhos et al. <ref type="bibr" target="#b13">[14]</ref> supported this hypothesis by showing that CNNs trained with heavy augmentation on styles become more robust against various image distortions. Research on CNN architectures <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28]</ref> has also demonstrated that adjusting the style information in CNNs helps to address multi-domain tasks.</p><p>In this paper, we experimentally analyze the relation between CNNs' inductive bias and representation gap across domains, and exploit this relation to address domain shift problems. We propose Style-Agnostic Networks (SagNets) which effectively improve CNNs' domain transferability by controlling their inductive bias, without directly reducing domain discrepancy. Our framework consists of separate content-biased and style-biased networks on top of a feature extractor. The content-biased network is encouraged to focus on contents by randomizing styles in a latent space. The style-biased network is led to focus on styles in the opposite way, against which the feature extractor adversarially makes the styles incapable of discriminating class categories. At test time, the prediction is made by the combination of the feature extractor and the content-biased network, where the style bias is substantially reduced.</p><p>We show that there exists an apparent correlation between CNNs' inductive bias and their ability to handle domain shift: reducing style bias reduces domain discrepancy. Based on this property, SagNets make significant improvements in a wide range of domain shift scenarios including DG, UDA, and SSDA, across several cross-domain benchmarks such as PACS <ref type="bibr" target="#b28">[29]</ref>, Office-Home <ref type="bibr" target="#b46">[47]</ref>, and DomainNet <ref type="bibr" target="#b41">[42]</ref>.</p><p>Our method is orthogonal to the majority of existing domain adaptation and generalization techniques that utilize  <ref type="figure">Figure 1</ref>: Our Style-Agnostic Network (SagNet) reduces style bias to reduce domain gap. It consists of three sub-networks-a feature extractor, a content-biased network, and a style-biased network-that are jointly trained end-to-end. The content-biased network is led to focus on the content of the input via style randomization (SR), where the style of the input is replaced by an arbitrary style through AdaIN <ref type="bibr" target="#b23">[24]</ref>. Conversely, the style-biased network is led to focus on the style by content randomization (CR), while an adversarial learning makes the feature extractor generate less style-biased representation.</p><p>domain information for training (e.g. aligning the source and target domains <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>). In other words, SagNets only control the intrinsic bias of CNNs without even requiring domain labels nor multiple domains. This approach is not only scalable to more practical scenarios where domain boundaries are unknown or ambiguous, but also able to complement existing methods and bring additional performance boosts as demonstrated in our extensive experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Inductive biases of CNNs. Our work is motivated by the recent findings that CNNs tend to learn styles rather than contents. Geirhos et al. <ref type="bibr" target="#b13">[14]</ref> observed that standard ImageNettrained CNNs are likely to make a style-biased decision on ambiguous stimuli (e.g. images stylized to different categories). Some studies have also shown that CNNs perform well when only local textures are given while global shape structures are missing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref>, but work poorly in the reverse scenario <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. Others have attempted to mitigate the style bias of CNNs under the assumption that it deteriorates CNNs' generalization capability <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>. However, the practical impact of CNNs' bias on domain shift problems remains unclear, which we aim to explore in this paper.</p><p>Style Manipulation. We utilize convolutional feature statistics to control the style bias of CNNs. This owes to previous works dealing with the feature statistics in CNNs, mostly in generative frameworks to manipulate image styles. Gatys et al. <ref type="bibr" target="#b11">[12]</ref> showed that feature statistics of a CNN effectively capture the style information of an image, which paved the way for neural style transfer <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>. In particular, adaptive instance normalization (AdaIN) <ref type="bibr" target="#b23">[24]</ref> demonstrated that the style of an image can easily be changed by adjusting the mean and variance of convolutional feature maps. Style-GAN <ref type="bibr" target="#b25">[26]</ref> also produced impressive image generation results by repeatedly applying AdaIN operations in a generative network. Manipulating styles has also benefited discriminative problems. BIN <ref type="bibr" target="#b39">[40]</ref> improved classification performance by reducing unnecessary style information using trainable normalization, and SRM <ref type="bibr" target="#b27">[28]</ref> extended this idea to style-based feature recalibration.</p><p>Domain Generalization and Adaptation. Domain Generalization (DG) aims to make CNNs robust against novel domains outside the training distribution. A popular approach is learning a shared feature space across multiple source domains, for example, by minimizing Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16]</ref> or adversarial feature alignment <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Some studies divide the model into domain-specific and domain-invariant components using low-rank parameterization <ref type="bibr" target="#b28">[29]</ref> or layer aggregation modules <ref type="bibr" target="#b6">[7]</ref>, assuming that the domain-invariant part well generalizes to other domains. Meta-learning frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> split the source domains into meta-train and meta-test domains to simulate domain shift, and JiGen <ref type="bibr" target="#b4">[5]</ref> employed self-supervised signals such as solving jigsaw puzzles to improve generalization by learning image regularities.</p><p>Unsupervised Domain Adaptation (UDA) tackles the problem of domain shift where unlabeled data from the target domain are available for training. The mainstream approach is aligning the source and the target distributions by minimizing MMD <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>, adversarial learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34]</ref>, or image-level translation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>. Another problem setting with domain shift is Semi-Supervised Domain Adaptation (SSDA) where a few target labels are additionally provided. It is commonly addressed by simultaneously minimizing the distance between the domains as well as imposing a regularization constraint on the target data to prevent overfitting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">50]</ref>. Recently, MME <ref type="bibr" target="#b43">[44]</ref> optimized a minimax loss on the conditional entropy of unlabeled data and achieved large performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Style-Agnostic Networks</head><p>We present a general framework to make the network agnostic to styles but focused more on contents in an end-toend manner ( <ref type="figure">Fig. 1</ref>). It contains three networks: a feature extractor, a content-biased network, and a style-biased network. The content-biased network is encouraged to exploit image contents when making decisions by randomizing intermediate styles, which we call content-biased learning. Conversely, the style-biased network is trained to be biased towards styles by randomizing contents, but it adversarially makes the feature extractor less style-biased, which is referred to as adversarial style-biased learning. The final prediction at test time is made by the feature extractor followed by the content-biased network. Following the common practice <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28]</ref>, we utilize the summary statistics of CNN features (i.e. channel-wise mean and standard deviation) as style representation and their spatial configuration as content representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Content-Biased Learning</head><p>In content-biased learning, we enforce the model to learn content-biased features by introducing a style randomization (SR) module. It randomizes the styles during training by interpolating the feature statistics between different examples regardless of their class categories. Consequently, the network is encouraged to be invariant to styles and biased toward contents in predicting the class labels.</p><p>Given an input training image x and a randomly selected image x , we first extract their intermediate feature maps z, z ? R D?H?W from the feature extractor G f , where H and W indicate spatial dimensions, and D is the number of channels. Then we compute the channel-wise mean and standard deviation ?(z), ?(z) ? R D as style representation:</p><formula xml:id="formula_0">?(z) = 1 HW H h=1 W w=1 z hw ,<label>(1)</label></formula><formula xml:id="formula_1">?(z) = 1 HW H h=1 W w=1 (z hw ? ?(z)) 2 + .<label>(2)</label></formula><p>The SR module constructs a randomized style?,? ? R D by interpolating between the styles of z and z . Then it replaces the style of the input with the randomized style through adaptive instance normalization (AdaIN) <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_2">? = ? ? ?(z) + (1 ? ?) ? ?(z ), (3) ? = ? ? ?(z) + (1 ? ?) ? ?(z ),<label>(4)</label></formula><formula xml:id="formula_3">SR(z, z ) =? ? z ? ?(z) ?(z) +?,<label>(5)</label></formula><p>where ? ? Uniform(0, 1) is a random interpolation weight. The style-randomized representation SR(z, z ) is fed into the content-biased network G c to obtain a content-biased loss L c , which jointly optimizes the feature extractor G f and the content-biased network G c :</p><formula xml:id="formula_4">min Gf,Gc L c = ?E (x,y)?S K k=1 y k log G c (SR(G f (x), z )) k ,<label>(6)</label></formula><p>where K is the number of class categories, y ? {0, 1} K is the one-hot label for input x and S is the training set. By randomizing styles during training, the content-biased network can no longer rely on the style but focus more on the content in making a decision. At test time, we remove the SR module and directly connect G f to G c , which ensures independent predictions as well as saves computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adversarial Style-Biased Learning</head><p>In addition to the content-biased learning, we constrain the feature extractor from learning style-biased representation by adopting an adversarial learning framework. In other words, we make the styles encoded by the feature extractor incapable of discriminating the class categories. To achieve this, we build an auxiliary style-biased network G s as a discriminator to make style-biased predictions utilizing a content randomization (CR) module. Then we let the feature extractor G f fool G s via an adversarial framework, which we name adversarial style-biased learning.</p><p>Contrary to SR which leaves the content of the input and randomizes its style, the CR module does the opposite: it maintains the style and switches the content to that of an arbitrary image. Given intermediate feature maps z, z ? R D?H?W corresponding to the input x and the randomly chosen image x , we apply AdaIN to the content of z with the style of z:</p><formula xml:id="formula_5">CR(z, z ) = ?(z) ? z ? ?(z ) ?(z ) + ?(z),<label>(7)</label></formula><p>which can be interpreted as a content-randomized representation of z. It is taken as an input to the style-biased network G s which is trained to make a style-biased prediction by minimizing a style-biased loss L s :</p><formula xml:id="formula_6">min Gs L s = ?E (x,y)?S K k=1 y k log G s (CR(G f (x), z )) k .<label>(8)</label></formula><p>Algorithm 1: Optimization Process of SagNets</p><formula xml:id="formula_7">Input: training data S = (x i , y i ) M i=1 ; batch size N ; hyperparameter ? adv &gt; 0 Initialize: feature extractor G f ; content-biased network G c ; style-biased network G s while not converged do X, Y = SAMPLEBATCH(S, N ) Z = G f (X) Z = SHUFFLE(Z) // randomly shuffle Z along the batch dimension. Content-Biased Learning: Z (c) = SR(Z, Z ) L c = ? 1 N N j=1 K k=1 Y jk log G c (Z (c) j ) k Minimize L c w.r.t. G f and G c Adversarial Style-Biased Learning: Z (s) = CR(Z, Z ) L s = ? 1 N N j=1 K k=1 Y jk log G s (Z (s) j ) k Minimize L c w.r.t. G s L adv = ?? adv 1 N N j=1 K k=1 1 K log G s (Z (s) j ) k Minimize L adv w.r.t. G f end Output: G c ? G f</formula><p>The feature extractor G f is then trained to fool G s by minimizing an adversarial loss L adv computed by the crossentropy between the style-biased prediction and uniform distribution 2 :</p><formula xml:id="formula_8">min Gf L adv = ?? adv E (x,?)?S K k=1 1 K log G s (CR(G f (x), z ))) k ,<label>(9)</label></formula><p>where ? adv is a weight coefficient. Following previous work that utilizes affine transformation parameters (i.e. normalization parameters) to effectively manipulate the style representation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>, we perform the adversarial learning with respect to the affine transformation parameters of G f . We can also efficiently control the trade-off between the content and style biases by adjusting the coefficient ? adv , which is explored in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>Our framework can be readily integrated into modern CNN architectures and trained end-to-end. Given a CNN such as a ResNet <ref type="bibr" target="#b18">[19]</ref>, the feature extractor G f comprises first few stages 3 of the CNN, while the rest of the network becomes the content-biased network G c ; the style-biased network G s forms the same structure as G c . Consequently, the output network G c ?G f has exactly the same architecture as the original CNN-in other words, it does not impose any overhead in terms of both parameters and computations at test time. To minimize the computational overhead during training, we choose the arbitrary example within a minibatch: given intermediate feature maps Z ? R N ?D?H?W from a minibatch of size N , we construct new feature maps Z corresponding to the arbitrary examples by randomly shuffling Z along the batch dimension. The overall training procedure of SagNets is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Extension to Un/Semi-Supervised Learning</head><p>Although our framework does not require training data from the target domain, some problem settings such as un/semi-supervised domain adaptation allow access to unlabeled target data for training. To fully leverage those unlabeled data, we utilize a simple extension of SagNets based on consistency learning <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b48">49]</ref>. For each training example x from a set of unlabeled data S unl , we obtain two prediction vectors from the network: one applied SR and the other not applied SR. The inconsistency between the two predictions are estimated by the mean square error to form an unsupervised loss L unl and minimized for all unlabeled data:</p><formula xml:id="formula_9">min Gf,Gc L unl = ? unl E x?Sunl K k=1 {G c (SR(G f (x), z )) k ? G c (G f (x)) k } 2 ,<label>(10)</label></formula><p>where ? unl is the corresponding coefficient which is set to 0.01. Furthermore, our adversarial learning is naturally extended to unlabeled data because the adversarial loss (Eq. 9) does not require ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first conduct an experimental analysis to gain an insight into the effect of SagNets (Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Biases and Domain Gap</head><p>We examine the effect of SagNets on CNNs' inductive biases and domain discrepancy using 16-class-ImageNet <ref type="bibr" target="#b14">[15]</ref> and the texture-shape cue conflict stimuli <ref type="bibr" target="#b13">[14]</ref>. 16-class-ImageNet is a subset of ImageNet containing 213,555 images from 16 entry-level categories. The texture-shape cue conflict stimuli were introduced to quantify the intrinsic biases  <ref type="figure">Figure 2</ref>: Examples of the texture-shape cue conflict stimuli <ref type="bibr" target="#b13">[14]</ref> created by randomly combining the shape (top labels) and texture (bottom labels) of different images through style transfer <ref type="bibr" target="#b12">[13]</ref>. of CNNs, which consist of 1,280 images and share the same 16 categories as 16-class-ImageNet. They are generated by blending the texture (style) and shape (content) from different images via style transfer <ref type="bibr" target="#b12">[13]</ref> (see examples in <ref type="figure">Fig. 2</ref>), so that we can observe whether a CNN makes a decision based on the texture or shape. We train SagNets with a ResNet-18 baseline on 16-class-ImageNet only, using SGD with batch size 256, momentum 0.9, weight decay 0.0001, initial learning rate 0.001, and cosine learning rate scheduling for 30 epochs. The randomization stage (the stage after which the randomizations are performed. i.e. the number of stages in the feature extractor) is set to 2, and we vary the adversarial coefficient ? adv .</p><p>Texture/Shape Bias. As proposed in <ref type="bibr" target="#b13">[14]</ref>, we quantify the texture and shape biases of networks by evaluating them on the cue conflict stimuli and counting the number of predictions that correctly classify the texture or shape of images. Specifically, shape bias is defined as the fraction of predictions matching the shape within the predictions matching either the shape or texture, and texture bias is defined in a similar way. <ref type="figure" target="#fig_2">Fig. 3(a)</ref> shows the texture/shape accuracy on the stimuli of the ResNet-18 baseline and SagNets with varying adversarial weight ? adv , which clearly demonstrates that SagNets increase the shape accuracy and decrease the texture accuracy. This consequently increases the shape bias as shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>, which is also equivalent to decreasing the texture bias. Furthermore, the shape and texture biases are effectively controlled by the adversarial weight ? adv , i.e. the shape bias increases as ? adv increases. In practice, increasing ? adv does not always improve the final accuracy because it makes the optimization more difficult, thus we need to find a fair trade-off, which we investigate in Sec 4.2.</p><p>Domain Gap. We further investigate the capability of Sag-Nets in reducing domain discrepancy. We treat 16-class-ImageNet and the cue conflict stimuli dataset as two different domains because they share the same object categories but exhibit different appearances. We then measure the distance between the two domains using the features from the penultimate layer of the network. Following <ref type="bibr" target="#b32">[33]</ref>, we calculate a proxy A-distance d A = 2(1 ? ) where is a generalization error of an SVM classifier trained to distinguish the examples from the two domains. As illustrated in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>, SagNets effectively reduce the domain discrepancy as the adversarial weight increases. By plotting the A-distance against the shape bias as presented in <ref type="figure" target="#fig_2">Fig. 3(d)</ref>, we observe an explicit correlation between the bias and domain gap: shape-biased representation generalizes better across domains, which confirms the common intuition <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Domain Generalization</head><p>DG is a problem to train a model on a single or multiple source domain(s), and test on an unseen target domain. We evaluate the efficacy of SagNets against recent DG methods including D-SAM <ref type="bibr" target="#b6">[7]</ref>, JiGen <ref type="bibr" target="#b4">[5]</ref>, Epi-FCR <ref type="bibr" target="#b29">[30]</ref>, MASF <ref type="bibr" target="#b8">[9]</ref>,   We split the training data of PACS into 70% training and 30% validation following the official split <ref type="bibr" target="#b28">[29]</ref>, and Office-Home into 90% training and 10% validation following <ref type="bibr" target="#b6">[7]</ref>. Our networks are trained by SGD with batch size 96, momentum 0.9, weight decay 0.0001, initial learning rate 0.004 (0.002 with AlexNet) and cosine scheduling for 2K iterations (4K with AlexNet or Office-Home). The randomization stage and the adversarial weight of SagNets are fixed to 3 and 0.1, respectively, throughout all remaining experiments unless otherwise specified.</p><p>Multi-Source Domain Generalization. We first examine multi-source DG where the model needs to generalize from multiple source domains to a novel target domain. We train our SagNets and the DeepAll baselines (i.e. na?ve supervised learning) on the combination of all training data from the source domains regardless of their domain labels. Table 1 and 2 demonstrate that SagNets not only significantly improve the accuracy over the DeepAll baselines but also outperform the competing methods. Furthermore, experiments without content-biased learning (SagNet ?CBL ) and adversarial style-biases learning (SagNet ?ASBL ) verify the effectiveness of each component of SagNets. It is also worth noting that while all the compared methods except JiGen exploit additional layers on top of the baseline CNN at test time, SagNets do not require any extra parameters nor computations over the baseline.</p><p>Ablation. We perform an extensive ablation study for Sag-Nets on multi-source DG using PACS. We first examine the effect of the randomization stage where the randomizations are applied, while keeping the adversarial weight to 0.1. Karras et al. <ref type="bibr" target="#b25">[26]</ref> demonstrated that styles at different layers  encode distinct visual attributes: the styles from fine spatial resolutions (lower layers in our network) encode low-level attributes such as color and microtextures, while the styles from coarse spatial resolutions (higher layers in our network) encode high-level attributes such as global structures and macrotextures. In this regard, the randomization modules of SagNets need to be applied at a proper level, where the style incurs undesired bias. As shown in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>, SagNets offer the best improvement when the randomizations are applied after stage 3, while randomizing too low-level styles is less helpful in reducing style bias; randomizing too high-level styles may lose important semantic information. We also conduct ablation on the adversarial coefficient ? adv , while the randomization stage is fixed to 3. <ref type="figure" target="#fig_3">Fig. 4(b)</ref> illustrates the accuracy of SagNets with varying values of ? adv . Although increasing ? adv tends to steadily improve the shape bias as shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>, the actual performance peaks around ? adv = 0.1. This indicates that while increasing the shape bias leads to reducing domain discrepancy, it may complicate the optimization and damage the performance when exceeding a proper range.</p><p>Domain Gap. Reducing domain gap in DG setting is particularly challenging compared with other tasks such as UDA or SSDA since one can not make any use of the target distribution. To demonstrate the effect of SagNets in such scenario, we measure the A-distances between the source domains and the target domain following the same procedure as in Sec. 4.1, and average them over the 4 generalization tasks of PACS. As shown in <ref type="figure" target="#fig_3">Fig. 4(c)</ref>, SagNets considerably reduce the domain discrepancy compared to the ResNet-18 baseline, JiGen 4 , and ablated SagNets.</p><p>Single-Source Domain Generalization. Our framework seamlessly extends to single-source DG where only a single training domain is provided, because it does not require domain labels nor multiple source domains (which are necessary in the majority of DG methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9]</ref>). We train SagNets on each domain of PACS and evaluate them on the remaining domains. As reported in <ref type="table" target="#tab_3">Table 3</ref>, SagNets remarkably boost the generalization performance, while JiGen and ADA 5 <ref type="bibr" target="#b47">[48]</ref> are technically applicable to single-source DG but outperformed by our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unsupervised Domain Adaptation</head><p>UDA is a task of transferring knowledge from a source to the target domain, where unlabeled target data are available for training. Besides Office-Home that we used for DG, we also employ DomainNet <ref type="bibr" target="#b41">[42]</ref> which is a large-scale dataset containing about 0.6 million images from 6 domains and 345 categories. Since some domains and classes are very noisy, we follow <ref type="bibr" target="#b43">[44]</ref> to utilize its subset of 145,145 images from  4 domains (Clipart, Painting, Sketch, and Real) with 126 categories, and consider 7 adaptation scenarios. We demonstrate that SagNets are not only effective when used alone, but also able to complement popular UDA methods such as DANN <ref type="bibr" target="#b10">[11]</ref>, JAN <ref type="bibr" target="#b34">[35]</ref>, CDAN <ref type="bibr" target="#b33">[34]</ref>, and SymNet <ref type="bibr" target="#b50">[51]</ref> to make further improvements. We reproduce all compared methods and their SagNet variants on top of the same baseline and training policy for fair comparison. As shown in <ref type="table" target="#tab_4">Table 4</ref>, SagNets lead to impressive performance improvements when combined with any adaptation method. Furthermore, while CDAN does not make meaningful improvement on DomainNet due to the complexity of the dataset in terms of scale and diversity, SagNets consistently improve the accuracy in most adaptation scenarios as presented in <ref type="table" target="#tab_5">Table 5</ref>. These results indicate that the effectiveness of SagNets is orthogonal to existing approaches and generates a synergy by complementing them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semi-Supervised Domain Adaptation</head><p>We also conduct SSDA on DomainNet where a few target labels are provided. Saito et al. <ref type="bibr" target="#b43">[44]</ref> showed that common UDA methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref> often fail to improve their performance in such scenario, and proposed minimax entropy (MME) setting the state-of-the-art for SSDA. Here we show that SagNets are successfully applied to SSDA, bringing further improvements over MME. For a fair comparison, SagNets are built upon the official implementation of MME 6 , 6 https://github.com/VisionLearningGroup/SSDA_MME trained using the same training policy and the few-shot labels specified by <ref type="bibr" target="#b43">[44]</ref>. <ref type="table" target="#tab_6">Table 6</ref> illustrates the results of SSDA with various architectures, where S+T indicates the baseline few-shot learning algorithm based on cosine similarity learning <ref type="bibr" target="#b5">[6]</ref>. Our method consistently improves the performance with considerable margins in most adaptation tasks with respect to all tested architectures and baseline methods, which verifies its scalability to various conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present Style-Agnostic Networks (SagNets) that are robust against domain shift caused by style variability across domains. By randomizing styles in a latent feature space and adversarially disentangling styles from class categories, Sag-Nets are trained to rely more on contents rather than styles in their decision making process. Our extensive experiments confirm the effectiveness of SagNets in controlling the inductive biases of CNNs and reducing domain discrepancy in a broad range of problem settings.</p><p>Our work is orthogonal to many existing domain adaptation approaches that explicitly align the distributions of different domains, and achieves further improvements by complementing those methods. The principle of how we deal with the intrinsic biases of CNNs can also be applied to other areas such as improving robustness under image corruptions <ref type="bibr" target="#b19">[20]</ref> and defending against adversarial attacks <ref type="bibr" target="#b17">[18]</ref>, which are left for future investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. 4.1), then perform comprehensive evaluation on a wide range of cross-domain tasks including domain generalization (DG) (Sec. 4.2), unsupervised domain adaptation (UDA) (Sec. 4.3), and semi-supervised domain adaptation (SSDA) (Sec. 4.4) compared with existing methods. All networks are pretrained on ImageNet classification [43].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Domain discrepancy vs shape bias The effect of SagNets on (a) texture/shape accuracy and (b) shape bias on the cue conflict stimuli. The domain discrepancy (A-distance) between 16-class-ImageNet and the cue conflict stimuli against (c) the adversarial weight and (d) the shape bias. SagNets significantly increase the shape accuracy and bias, while reducing the domain discrepancy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy of SagNets on PACS with varying (a) randomization stage and (b) adversarial weight; (c) the domain discrepancy (A-distance) between the source domains and the target domain. Results are averaged over the 4 target domains and 3 repetitions, where error bars denote the standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Multi-source domain generalization accuracy (%) on PACS. Each column title indicates the target domain. The results of our DeepAll baselines (standard supervised learning on the mixture of source domains) and SagNets are averaged over three repetitions. SagNet ?CBL and SagNet ?ASBL refer to SagNets without content-biased learning and adversarial style-biased learning, respectively. Our approach outperforms all competing methods, where each component of our method contributes to the performance improvements.</figDesc><table><row><cell></cell><cell cols="5">Art paint. Cartoon Sketch Photo Avg.</cell></row><row><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>D-SAM</cell><cell>63.87</cell><cell>70.70</cell><cell cols="3">64.66 85.55 71.20</cell></row><row><cell>JiGen</cell><cell>67.63</cell><cell>71.71</cell><cell cols="3">65.18 89.00 73.38</cell></row><row><cell>Epi-FCR</cell><cell>64.7</cell><cell>72.3</cell><cell>65.0</cell><cell>86.1</cell><cell>72.0</cell></row><row><cell>MASF</cell><cell>70.35</cell><cell>72.46</cell><cell cols="3">67.33 90.68 75.21</cell></row><row><cell>MMLD</cell><cell>69.27</cell><cell>72.83</cell><cell cols="3">66.44 88.98 74.38</cell></row><row><cell>DeepAll</cell><cell>65.19</cell><cell>67.83</cell><cell cols="3">63.75 90.08 71.71</cell></row><row><cell>SagNet</cell><cell>71.01</cell><cell>70.78</cell><cell cols="3">70.26 90.04 75.52</cell></row><row><cell></cell><cell></cell><cell>ResNet-18</cell><cell></cell><cell></cell><cell></cell></row><row><cell>D-SAM</cell><cell>77.33</cell><cell>72.43</cell><cell cols="3">77.83 95.30 80.72</cell></row><row><cell>JiGen</cell><cell>79.42</cell><cell>75.25</cell><cell cols="3">71.35 96.03 80.51</cell></row><row><cell>Epi-FCR</cell><cell>82.1</cell><cell>77.0</cell><cell>73.0</cell><cell>93.9</cell><cell>81.5</cell></row><row><cell>MASF</cell><cell>80.29</cell><cell>77.17</cell><cell cols="3">71.69 94.99 81.04</cell></row><row><cell>MMLD</cell><cell>81.28</cell><cell>77.16</cell><cell cols="3">72.29 96.09 81.83</cell></row><row><cell>DeepAll</cell><cell>78.12</cell><cell>75.10</cell><cell cols="3">68.43 95.37 79.26</cell></row><row><cell>SagNet ?CBL</cell><cell>78.86</cell><cell>77.05</cell><cell cols="3">73.28 95.43 81.15</cell></row><row><cell>SagNet ?ASBL</cell><cell>82.94</cell><cell>76.73</cell><cell cols="3">74.74 95.07 82.37</cell></row><row><cell>SagNet</cell><cell>83.58</cell><cell>77.66</cell><cell cols="3">76.30 95.47 83.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="6">Multi-source domain generalization accuracy (%)</cell></row><row><cell cols="5">on Office-Home with a ResNet-18 backbone.</cell></row><row><cell></cell><cell>Art</cell><cell cols="4">Clipart Product Real-World Avg.</cell></row><row><cell>D-SAM</cell><cell cols="2">58.03 44.37</cell><cell>69.22</cell><cell>71.45</cell><cell>60.77</cell></row><row><cell>JiGen</cell><cell cols="2">53.04 47.51</cell><cell>71.47</cell><cell>72.79</cell><cell>61.20</cell></row><row><cell>DeepAll</cell><cell cols="2">58.51 41.44</cell><cell>70.06</cell><cell>73.28</cell><cell>60.82</cell></row><row><cell>SagNet ?CBL</cell><cell cols="2">60.00 42.85</cell><cell>70.11</cell><cell>73.12</cell><cell>61.52</cell></row><row><cell cols="3">SagNet ?ASBL 59.31 41.89</cell><cell>70.44</cell><cell>73.52</cell><cell>61.29</cell></row><row><cell>SagNet</cell><cell cols="2">60.20 45.38</cell><cell>70.42</cell><cell>73.38</cell><cell>62.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Single-source domain generalization accuracy (%) on PACS averaged over three repetitions (A: Art Painting, C: Cartoon, S: Sketch, P: Photo). A?C A?S A?P C?A C?S C?P S?A S?C S?P P?A P?C P?S Avg.</figDesc><table><row><cell>ResNet-18</cell><cell>62.3</cell><cell>49.0</cell><cell>95.2</cell><cell>65.7</cell><cell>60.7</cell><cell>83.6</cell><cell>28.0</cell><cell>54.5</cell><cell>35.6</cell><cell>64.1</cell><cell>23.6</cell><cell>29.1 54.3</cell></row><row><cell>JiGen</cell><cell>57.0</cell><cell>50.0</cell><cell>96.1</cell><cell>65.3</cell><cell>65.9</cell><cell>85.5</cell><cell>26.6</cell><cell>41.1</cell><cell>42.8</cell><cell>62.4</cell><cell>27.2</cell><cell>35.5 54.6</cell></row><row><cell>ADA</cell><cell>64.3</cell><cell>58.5</cell><cell>94.5</cell><cell>66.7</cell><cell>65.6</cell><cell>83.6</cell><cell>37.0</cell><cell>58.6</cell><cell>41.6</cell><cell>65.3</cell><cell>32.7</cell><cell>35.9 58.7</cell></row><row><cell>SagNet</cell><cell>67.1</cell><cell>56.8</cell><cell>95.7</cell><cell>72.1</cell><cell>69.2</cell><cell>85.7</cell><cell>41.1</cell><cell>62.9</cell><cell>46.2</cell><cell>69.8</cell><cell>35.1</cell><cell>40.7 61.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="14">: Unsupervised domain adaptation accuracy (%) on Office-Home with varying adaptation methods and their SagNet</cell></row><row><cell cols="14">combinations (A: Art, C: Clipart, P: Product, R: Real-World). SagNets consistently boost the performance when combined</cell></row><row><cell cols="2">with various adaptation methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="13">SagNet A?C A?P A?R C?A C?P C?R P?A P?C P?R R?A R?C R?P Avg.</cell></row><row><cell>ResNet-50</cell><cell>41.3 45.7</cell><cell>63.8 64.1</cell><cell>71.4 72.6</cell><cell>49.1 49.6</cell><cell>59.6 60.0</cell><cell>61.4 63.5</cell><cell>46.8 49.9</cell><cell>36.1 40.7</cell><cell>68.8 71.1</cell><cell>63.0 64.8</cell><cell>45.9 50.9</cell><cell>76.5 78.1</cell><cell>57.0 59.2</cell></row><row><cell>DANN</cell><cell>44.7 48.8</cell><cell>62.7 65.2</cell><cell>70.3 71.4</cell><cell>47.1 50.3</cell><cell>60.1 61.4</cell><cell>61.4 62.5</cell><cell>46.1 50.7</cell><cell>41.7 45.7</cell><cell>68.5 71.8</cell><cell>62.3 65.4</cell><cell>50.9 55.2</cell><cell>76.7 78.6</cell><cell>57.7 60.6</cell></row><row><cell>JAN</cell><cell>45.0 50.1</cell><cell>63.3 66.8</cell><cell>72.6 73.9</cell><cell>53.3 56.9</cell><cell>66.0 64.7</cell><cell>64.4 66.1</cell><cell>50.9 54.9</cell><cell>40.8 45.6</cell><cell>72.1 75.2</cell><cell>64.9 70.0</cell><cell>49.4 55.3</cell><cell>78.8 80.1</cell><cell>60.1 63.3</cell></row><row><cell>CDAN</cell><cell>50.6 53.2</cell><cell>69.0 69.2</cell><cell>74.9 74.9</cell><cell>54.6 55.9</cell><cell>66.1 67.8</cell><cell>67.9 68.6</cell><cell>57.2 58.1</cell><cell>46.9 51.8</cell><cell>75.6 76.4</cell><cell>69.1 69.8</cell><cell>55.8 58.1</cell><cell>80.6 80.4</cell><cell>64.0 65.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Unsupervised domain adaptation accuracy (%) on DomainNet (C: Clipart, P: Painting, S: Sketch, R: Real). SagNets improve the performance over both ResNet-18 and CDAN<ref type="bibr" target="#b33">[34]</ref> baselines.MethodSagNet R?C R?P P?C C?S S?P R?S P?R Avg.</figDesc><table><row><cell>ResNet-18</cell><cell>53.1 54.4</cell><cell>57.7 58.0</cell><cell>52.4 53.1</cell><cell>47.5 49.2</cell><cell>52.0 52.2</cell><cell>43.4 46.4</cell><cell>68.5 67.4</cell><cell>53.5 54.4</cell></row><row><cell>CDAN</cell><cell>53.0 54.4</cell><cell>57.4 59.4</cell><cell>52.3 52.8</cell><cell>48.0 49.5</cell><cell>52.3 52.4</cell><cell>43.4 45.9</cell><cell>67.2 67.1</cell><cell>53.4 54.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Semi-supervised domain adaptation accuracy (%) on DomainNet (C: Clipart, P: Painting, S: Sketch, R: Real). SagNets consistently improve the performance over various baselines in terms of backbone architectures and adaptation methods. MME 70.0 72.2 67.7 69.7 69.0 71.7 56.3 61.8 64.8 66.8 61.0 61.9 76.1 78.5 66.4 68.9 72.3 74.2 69.0 70.5 70.8 73.2 61.7 64.6 66.9 68.3 64.3 66.1 75.3 78.4 68.6 70.8</figDesc><table><row><cell>Backbone</cell><cell>Method SagNet</cell><cell>R ? C 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot R ? P P ? C C ? S S ? P R ? S P ? R Avg.</cell></row><row><cell>AlexNet</cell><cell>S+T MME</cell><cell>43.3 47.1 42.4 45.0 40.1 44.9 33.6 36.4 35.7 38.4 29.1 33.3 55.8 58.7 40.0 43.4 45.8 49.1 45.6 46.7 42.7 46.3 36.1 39.4 37.1 39.8 34.2 37.5 54.0 57.0 42.2 45.1 48.9 55.6 48.0 49.0 46.7 51.7 36.3 39.4 39.4 43.0 33.3 37.9 56.8 60.7 44.2 48.2 54.1 58.6 49.8 52.2 48.7 54.4 39.6 43.4 40.4 43.4 39.7 42.8 57.0 60.3 47.0 50.7</cell></row><row><cell>VGG-16</cell><cell>S+T MME</cell><cell>49.0 52.3 55.4 56.7 47.7 51.0 43.9 48.5 50.8 55.1 37.9 45.0 69.0 71.7 50.5 54.3 51.8 54.9 57.8 59.4 50.4 54.2 48.9 52.9 53.1 56.3 45.6 49.4 68.3 70.9 53.7 56.9 60.6 64.1 63.3 63.5 57.0 60.7 50.9 55.4 60.5 60.9 50.2 54.8 72.2 75.3 59.2 62.1 64.9 67.8 64.5 66.0 60.4 65.8 54.7 59.0 59.8 62.0 56.6 59.6 71.1 74.2 61.7 64.9</cell></row><row><cell>ResNet-34</cell><cell>S+T</cell><cell>55.6 60.0 60.6 62.2 56.8 59.4 50.8 55.0 56.0 59.5 46.3 50.1 71.8 73.9 56.9 60.0 59.4 62.0 61.9 62.9 59.1 61.5 54.0 57.1 56.6 59.0 49.7 54.4 72.2 73.4 59.0 61.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This can also be interpreted as maximizing the entropy of style-biased predictions. We empirically found that matching with a uniform distribution provides more stable convergence than directly maximizing the entropy or using a gradient reversal layer<ref type="bibr" target="#b10">[11]</ref>.<ref type="bibr" target="#b2">3</ref> A stage refers to a group of layers that share the same feature map size.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We provide additional comparison with JiGen because it does not use domain labels as same as ours. The results are reproduced with their official code (https://github.com/fmcarlucci/JigenDG) and the optimal hyperparameters provided in their paper.<ref type="bibr" target="#b4">5</ref> We re-implement their method on top of our baseline, which gives better performance than reproduction based on their published code.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep convolutional networks do not classify based on global object shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gennady</forename><surname>Erlikhman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Kellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using metaregularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the performance of googlenet and alexnet applied to sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">Matsumura</forename><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximating cnns with bag-of-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Antonio D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation with instance constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transfer between picture books and the real world by very young children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><forename type="middle">Bloom</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><forename type="middle">S</forename><surname>Pickard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De-Loache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognition and Development</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalisation in humans and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Temme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain adaptive neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRICAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The origins and prevalence of texture bias in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cycada: Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Assessing shape bias property of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baicen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayoore</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The importance of shape in early lexical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognition and Development</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Srm: A style-based recalibration module for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyo-Eun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Batch-instance normalization for adaptively style-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyo-Eun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A survey on transfer learning. TKDE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV, 2015. 4</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation with subspace learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Domainsymmetric networks for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
