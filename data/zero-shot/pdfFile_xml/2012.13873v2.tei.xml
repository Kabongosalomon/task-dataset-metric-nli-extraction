<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AN EMBARRASSINGLY SIMPLE MODEL FOR DIALOGUE RELATION EXTRACTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Centre for Frontier AI Research</orgName>
								<orgName type="institution" key="instit2">Agency for Science, Technology and Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjie</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng-Siong</forename><surname>Chng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AN EMBARRASSINGLY SIMPLE MODEL FOR DIALOGUE RELATION EXTRACTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Dialogue Relation Extraction</term>
					<term>Multi- Relations</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dialogue relation extraction (RE) is to predict the relation type of two entities mentioned in a dialogue. In this paper, we propose a simple yet effective model named SimpleRE for the RE task. SimpleRE captures the interrelations among multiple relations in a dialogue through a novel input format named BERT Relation Token Sequence (BRS). In BRS, multiple [CLS] tokens are used to capture possible relations between different pairs of entities mentioned in the dialogue. A Relation Refinement Gate (RRG) is then designed to extract relation-specific semantic representation in an adaptive manner. Experiments on the DialogRE dataset show that SimpleRE achieves the best performance, with much shorter training time. Further, SimpleRE outperforms all direct baselines on sentence-level RE without using external resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Relation extraction (RE) is to identify the semantic relation type between two entities mentioned in a piece of text, e.g., a sentence or a dialogue. <ref type="table">Table 1</ref> shows an example dialogue. The RE task is to predict the relation type of a pair of entities like "Monica" and "S2" (i.e., an argument pair) mentioned in the dialogue, from a set of predefined relations. Researchers have tried to improve Dialogue RE by considering speaker information <ref type="bibr" target="#b0">[1]</ref> or trigger tokens <ref type="bibr" target="#b1">[2]</ref>. There are also solutions based on graph attention network, where a graph models speaker, entity, entity-type, and utterance nodes <ref type="bibr" target="#b2">[3]</ref>. However, Transformer-based models remain strong competitors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>A dialogue may mention multiple pairs of entities, reflected by annotations in the DialogRE dataset <ref type="bibr" target="#b0">[1]</ref> (see <ref type="table">Table 1</ref>). Among multiple pairs of entities, the relations mentioned in the same dialog often interrelate with each other to some extent. An example is shown in <ref type="table">Table 1</ref>, "Richard" and "Monica" in the first few utterances show two possible relations, i.e., "positive impression" or "girl/boyfriend". The last utterance <ref type="table">Table 1</ref>. An example from DialogRE dataset <ref type="bibr" target="#b0">[1]</ref>. Relations of two pairs of entities are annotated. S1: Where the hell have you been?! S2: I was making a coconut phone with the professor. S1: Richard told Monica he wants to marry her! S2: What?! S1: Yeah! Yeah, I've been trying to find ya to tell to stop messing with her and maybe I would have if these damn boat shoes wouldn't keep flying off! S2: My-Oh my God! S1: I know! They suck!! S2: He's not supposed to ask my girlfriend to marry him! I'm supposed to do that! Argument pair Relation type R1 (Monica, S2) girl/boyfriend R2 (Richard, Monica) positive impression indicates that "Monica" is girlfriend of "S2"; hence "Richard" and "Monica" can only be related by "positive impression". We argue that such interrelationships could be helpful for relation extraction.</p><p>In this paper, we propose SimpleRE, an extremely simple model, to reason and learn interrelations among tokens and relations. SimpleRE is built on top of BERT. Due to its strong modeling capability, BERT is the natural choice to model such interrelationships. We first design a BERT Relation Token Sequence (BRS). BRS contains multiple "[CLS]" tokens in input sequence, with the aim to capture relations between multiple pairs of entities. We then propose a Relation Refinement Gate (RRG) to refine the semantic representation of each relation for target relation prediction in an adaptive manner.</p><p>On the DialogRE dataset, SimpleRE achieves best F 1 over two BERT-based methods, BERTs <ref type="bibr" target="#b0">[1]</ref> and GDPNet <ref type="bibr" target="#b1">[2]</ref>, by a large margin. As a simple model, the training of SimpleRE is at least 5 times faster than these two models. We also show that BRS is effective on sentence-level RE, and the adapted SimpleRE beats all direct baselines on the TACRED dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SimpleRE</head><p>The architecture of SimpleRE is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Its novelty are two-fold: (i) BERT Relation Token Sequence (BRS), i.e., the input format to BERT, and (ii) Relation Refinement Gate (RRG), i.e., the way to utilize BERT encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem formulation</head><p>Let R be a set of predefined relation types. Let X = {x 1 , x 2 , . . . , x T } be a text sequence with T tokens, where x t is the token at t-th position. X denotes an entire dialogue for Dialogue RE, or a single sentence for sentence-level RE. Between n pairs of entities mentioned in X, there could be multiple relations R = {r 1 , r 2 , . . . , r n }. The i-th relation r i ? R is predicted for an argument pair: subject entity E i s and object entity E i o . Note that, an entity may contain one or more tokens. In this problem setting, the pairs of entities whose relations are to be predicted are known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">BERT Relation Token Sequence</head><p>BERT <ref type="bibr" target="#b3">[4]</ref> based models are powerful in modeling semantics in text sequences <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. In SimpleRE, we adopt BERT to model the interrelations among all possible relations in a text sequence, through BRS.</p><p>Given a sequence X, which contains a set of subject entities E s = {E 1 s , E 2 s , . . . , E n s }, and a set of object entities</p><formula xml:id="formula_0">E o = {E 1 o , E 2 o , . . . , E n o }, we form a BRS as input to BERT: BRS = [CLS], X, [SEP], E 1 s , [CLS], E 1 o , [SEP], . . . , [SEP], E n s , [CLS], E n o , [SEP] .</formula><p>[CLS] and [SEP] are the classification and separator tokens, respectively. The [CLS] tokens at different positions in the BRS input may carry different meanings, due to the different contexts.</p><p>Multiple [CLS] tokens have been used to learn hierarchical representations of a document, where one [CLS] is put in front of a sentence <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. In BRS, multiple [CLS] tokens are for capturing different relations between entity pairs and their interrelations, because these multiple [CLS] tokens are in the same input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Relation Refinement Gate</head><p>In BRS, representation of the first [CLS] token (denoted by h 0 ) encodes the semantic information of entire sequence. Representations of the subsequent [CLS] tokens capture the relations between each pair of entities. We denote the i-th relation representation as h ri . To predict the relation type of r i , in Relation Refinement Gate, we concatenate semantic representations of h 0 and h ri as c i = [h 0 ; h ri ]. We then use Shallow-Deep Networks <ref type="bibr" target="#b9">[10]</ref> to compute a confidence score:</p><formula xml:id="formula_1">s c = max Sigmoid f (c i )<label>(1)</label></formula><p>Here f denotes a single layer feed-forward neural network (FFN). If s c is larger than a predefined threshold ? , c i is used to predict the target relation between E i s and E i o , by a classifier. 1 Otherwise, we refine h 0 to be more relation-specific to h ri , since h 0 is weakly related to the target relation <ref type="bibr" target="#b1">[2]</ref>. To this end, we define a refinement mechanism to extract task-specific semantic information by updating h 0 for the prediction of r i :</p><formula xml:id="formula_2">h 0 = ReLU g(h ri ) + h 0<label>(2)</label></formula><p>Here g is a single layer FFN and h 0 denotes the updated semantic representation. Then h 0 is used to predict the relation or updated further, depending on the recomputed s c . To avoid possible endless refinement, we set an upper bound B to limit the maximum number of iterations for refining h 0 . B = 3 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We conduct experiments on Dialogue RE and sentence-level RE tasks to evaluate SimpleRE against baseline models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>DialogRE is the first human-annotated Dialogue RE dataset <ref type="bibr" target="#b0">[1]</ref> originated from the transcripts of American comedy "Friends". It contains 1, 788 dialogues and 36 predefined relation types (see example in <ref type="table">Table 1</ref>). Recently, <ref type="bibr" target="#b0">[1]</ref> released a modified English version and a Chinese version of DialogRE. We evaluate SimpleRE on all three versions. TACRED is a widely-used sentence-level RE dataset. It contains more than 106K sentences drawn from the yearly TACKBP4 challenge, and has 42 different relations (including a special "no relation" type). We evaluate SimpleRE on both TACRED and TACRED-Revisit (TACREV) datasets; TACREV is a modified version of TACRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Settings</head><p>We compare SimpleRE with two recent BERT-based methods, BERTs <ref type="bibr" target="#b0">[1]</ref> and GDPNet <ref type="bibr" target="#b1">[2]</ref>. We also include popular baselines AGGCN <ref type="bibr" target="#b10">[11]</ref>, LSR <ref type="bibr" target="#b11">[12]</ref>, and DHGAT <ref type="bibr" target="#b2">[3]</ref> in our experiments. For a fair comparison with BERTs and GDP-Net, we utilize the same hyperparameter settings, except for batch size. Specifically, we set batch size to 6 rather than 24 for SimpleRE because it predicts multiple relations (i.e., all relations annotated in one dialogue) per forward process. To set threshold ? , we conduct a preliminary study on the development set with different ? values. Reported in <ref type="table" target="#tab_2">Table 4</ref>, SimpleRE achieves best performance when ? ? 0.6. Thus,  we set ? = 0.6 throughout the experiments, unless specified otherwise. We set the maximal refinement iterations B as 3 for Dialogue RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results on DialogRE</head><p>3.3.1. Performance by F 1.  <ref type="table">Table 3</ref> shows that SimpleRE achieves better performance than baselines on both versions of DialogRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Efficiency by training time.</head><p>The average training time per epoch is reported in <ref type="table" target="#tab_3">Table 5</ref>, after training for 20 epochs. SimpleRE is about 5? faster than baselines, despite its smaller batch size. Note a dialogue in Dialogue RE may contain multiple relations. Existing models only infer one relation per forward process. On the contrary, SimpleRE predicts multiple relations per forward process. Its simple structure leads to better efficiency than baselines, e.g., GDPNet with SoftDTW <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Ablation study.</head><p>We conduct ablation studies on DialogRE, for the effectiveness of components in SimpleRE: BERT Relation Token Sequence (BRS) and Relation Refinement Gate (RRG). To evaluate their impacts, we remove BRS and RRG from our model separately.</p><p>To remove BRS, we change the input format to predict one   <ref type="table" target="#tab_5">Table 6</ref>, the results show that removing BRS leads to large performance degradation, indicating interrelations among relations have a significant impact on RE performance. Meanwhile, RRG module also contributes to the performance gains. Another interesting finding is that BRS-v2 and -v3 cannot model the relations well although they both use multiple [CLS] tokens in the input sequence. The F 1 score decreases from 66.3 to 62.8 and 63.5, respectively. This result further shows that [CLS] token in BRS is sensitive to its position in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results on TACRED</head><p>We now adapt SimpleRE to sentence-level RE. Note that Sim-pleRE was not evaluated on document-level RE due to the difference in problem settings. We leave the adaptation of SimpleRE to document-level RE as our future work.</p><p>Because  two entities with a [CLS] token. RRG is not applicable here because there is only one relation in each sentence. Hence, it is unnecessary to refine h 0 to be target relation specific.</p><p>For fair comparison, we refer <ref type="bibr" target="#b1">[2]</ref> to use SpanBERT as the backbone (i.e., BERT in <ref type="figure" target="#fig_0">Figure 1</ref>). Results on TACRED and TACREV are summarized in <ref type="table">Table 7</ref>. SimpleRE outperforms all baselines including KnowBERT <ref type="bibr" target="#b16">[17]</ref> on both datasets. Note that KnowBERT incorporates external knowledge base during training. <ref type="table" target="#tab_7">Table 8</ref> summarizes the results of ablation studies on TA-CRED. We first replace the second [CLS] token with a [SEP] token, which leads to 1.1% performance degradation. This result suggests that [CLS] is necessary to capture the relation between entities near it. Moreover, the performance of our model further drops without relation representation h r , i.e., predicting relation purely based on h 0 instead of [h 0 : h r ]. Poorer performance is also observed when only h r is used for prediction. Hence, both h 0 and h r contribute to the correct prediction of relations.</p><p>In short, through experiments on both dialogue and sentence-level RE tasks, we show SimpleRE is a strong competitor. Although simple, both components, BRS and RRG, are essential in SimpleRE's model design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we propose a simple yet effective model for dialogue relation extraction. Building on top of the powerful modeling capability of BERT, SimpleRE is designed to learn and reason the interrelations among multiple relations in a dialogue. The most important component in SimpleRE is the BERT Relation Token Sequence, where multiple [CLS] tokens are used to capture relations between entity pairs. The Relation Refinement Gate is designed to further improve the semantic representation in an adaptive manner. Through experiments and ablation studies, we show that both components contribute to the success of SimpleRE. Due to its simple structure and fast training speed, we believe SimpleRE serves a good baseline in Dialogue RE task. The SimpleRE can also be easily adapted to sentence-level relation extraction. On datasets for both tasks, DialogRE and TACRED, we show that our simple model is a strong competitor for relation extraction tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of SimpleRE. An entity may contain one or more tokens as illustrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>relation each time with a modified input format: [CLS], X, [SEP], E s , [CLS], E o , [SEP] . To remove RRG, all relations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>h0 hr1 h0 hr2 h0 hr3 Relation Refinement Gate Relation Refinement Gate h0 hr3 Sigmoid Confident Linear ReLU Unconfident h0 hr3 Classifier hr3 h0 h0updated W h9 E1 E2 E3 E2</head><label></label><figDesc></figDesc><table><row><cell>h0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hr1</cell><cell></cell><cell></cell><cell></cell><cell>hr2</cell><cell></cell><cell>hr3</cell></row><row><cell>h0</cell><cell>h1</cell><cell>h2</cell><cell>h3</cell><cell>h4</cell><cell>h5</cell><cell>h6</cell><cell>h7</cell><cell>h8</cell><cell>h10</cell><cell>h11</cell><cell>h12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[CLS]</cell><cell>W</cell><cell>W</cell><cell>[SEP]</cell><cell>W</cell><cell>[CLS]</cell><cell>W</cell><cell>[SEP]</cell><cell>W</cell><cell>[CLS]</cell><cell>W</cell><cell>[SEP]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison with baselines on DialogRE. Results are 5-run averaged F 1 with standard deviation (?). Comparison with baselines on new versions of Dialo-gRE. Results are 5-run averaged F 1 with standard deviation.</figDesc><table><row><cell>Model</cell><cell></cell><cell>F 1 ? ?</cell></row><row><cell>CNN [1]</cell><cell></cell><cell>48.0?1.5</cell></row><row><cell cols="2">LSTM [1]</cell><cell>47.4?0.6</cell></row><row><cell cols="2">BiLSTM [1]</cell><cell>48.6?1.0</cell></row><row><cell cols="2">AGGCN [11]</cell><cell>46.2</cell></row><row><cell>LSR [12]</cell><cell></cell><cell>44.4</cell></row><row><cell cols="2">DHGAT [3]</cell><cell>56.1</cell></row><row><cell>BERT [4]</cell><cell></cell><cell>58.5?2.0</cell></row><row><cell cols="2">BERTs [1]</cell><cell>61.2?0.9</cell></row><row><cell cols="2">GDPNet [2]</cell><cell>64.9?1.1</cell></row><row><cell cols="3">SimpleRE (Ours) 66.3?0.7</cell></row><row><cell>BERT [4]</cell><cell>60.6?0.5</cell><cell>61.6?0.4</cell></row><row><cell>BERTs [1]</cell><cell>61.8?0.6</cell><cell>63.8?0.6</cell></row><row><cell>GDPNet [2]</cell><cell>64.3?1.1</cell><cell>62.2?0.9</cell></row><row><cell>SimpleRE (Ours)</cell><cell>66.7?0.7</cell><cell>65.2?1.1</cell></row></table><note>Model English V2 (F 1 ? ?) Chinese (F 1 ? ?)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Performance with different threshold ? values.F 1 67.9 68.3 68.0 69.1 68.6 68.1 68.4</figDesc><table><row><cell>?</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Average training time (in minutes) per epoch on Dialogue RE</figDesc><table><row><cell>Model</cell><cell>Average Time (mins)</cell></row><row><cell>BERT [4]</cell><cell>4.7</cell></row><row><cell>BERTs [1]</cell><cell>4.7</cell></row><row><cell>GDPNet [2]</cell><cell>12.6</cell></row><row><cell>SimpleRE (Ours)</cell><cell>0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 summarizes</head><label>2</label><figDesc>Note that, the two modified DialogRE datasets, English V2 and Chinese version, are released recently. Since most existing models have not reported their performance, we obtain results by running author released codes of existing BERTbased models.</figDesc><table /><note>the results on DialogRE. Observe that BERT-based models significantly outperform non-BERT mod- els. Among the three BERT-based models, SimpleRE sur- passes GDPNet and BERTs by 1.4% and 5.1% respectively, on DialogRE, by F 1 measure.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation study of SimpleRE on DialogRE. ; h ri ], without updating h 0 . Besides, we also design two alternative BRS, i.e., BRS-v2 and BRS-v3, for comparison.</figDesc><table><row><cell>Model</cell><cell></cell><cell>F 1 ? ?</cell></row><row><cell>SimpleRE</cell><cell></cell><cell>66.3?0.7</cell></row><row><cell cols="2">SimpleRE w/o BRS</cell><cell>60.4?0.9</cell></row><row><cell cols="2">SimpleRE w/ BRS-v2</cell><cell>62.8?1.1</cell></row><row><cell cols="2">SimpleRE w/ BRS-v3</cell><cell>63.5?0.8</cell></row><row><cell cols="2">SimpleRE w/o RRG</cell><cell>65.5?0.7</cell></row><row><cell cols="3">Table 7. F 1 of all models on TACRED and TACRED-Revisit</cell></row><row><cell cols="3">(TARREV), the sentence-level RE datasets.</cell></row><row><cell>Model</cell><cell cols="2">TACRED TACREV</cell></row><row><cell>LSTM [14]</cell><cell>62.7</cell><cell>70.6</cell></row><row><cell>PA-LSTM [14]</cell><cell>65.1</cell><cell>74.3</cell></row><row><cell cols="2">C-AGGCN [11] 68.2</cell><cell>75.5</cell></row><row><cell cols="2">LST-AGCN [15] 68.8</cell><cell>-</cell></row><row><cell cols="2">SpanBERT [16] 70.8</cell><cell>78.0</cell></row><row><cell>GDPNet [2]</cell><cell>70.5</cell><cell>80.2</cell></row><row><cell cols="2">SimpleRE (Ours) 71.7</cell><cell>80.7</cell></row><row><cell cols="2">KnowBERT [17] 71.5</cell><cell>79.3</cell></row><row><cell cols="3">are predicted based on the corresponding token representations</cell></row><row><cell cols="3">[h 0 For BRS-v2, we exchange the [CLS] tokens between entity</cell></row><row><cell cols="3">pairs with [SEP] tokens before subject entities. Similarly, we</cell></row><row><cell cols="3">exchange the [CLS] tokens with [SEP] tokens after object</cell></row><row><cell>entities in BRS-v3.</cell><cell></cell><cell></cell></row><row><cell>Reported in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>each sentence only contains a single relation in sentence-level RE dataset, BRS becomes [CLS], X, [SEP], E s , [CLS], E o , [SEP] . The representations of the two [CLS] tokens are concatenated for relation prediction. Compared to typical RE input sequence [CLS], X, [SEP], E s , [SEP], E o , [SEP] , SimpleRE replaces the [SEP] token between</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Ablation study of SimpleRE on TACRED. For the model without 2 nd [CLS] token, we use [SEP] token. Instead of using [h 0 : h r ], we have evaluated SimpleRE with either h 0 or h r alone for relation prediction.</figDesc><table><row><cell>Model</cell><cell>F 1</cell></row><row><cell>SimpleRE</cell><cell>71.7</cell></row><row><cell>SimpleRE w/o 2 nd [CLS] token</cell><cell>70.6</cell></row><row><cell>SimpleRE w/o relation representation hr</cell><cell>70.0</cell></row><row><cell cols="2">SimpleRE w/o semantic representation h0 70.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use a linear layer as a classifier for Dialogue RE and a linear layer with softmax for sentence-level RE.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project #A19E2b0098) and the National Research Foundation Singapore under its AI Singapore Programme (Award Number: AISG-100E-2018-006).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dialogue-based relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="4927" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gdpnet: Refining latent multi-view graph for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng Siong</forename><surname>Chng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06780</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dialogue relation extraction with document-level heterogeneous graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05092</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How does bert answer questions? a layer-wise analysis of transformer representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Betty Van Aken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>L?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1823" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Symmetric regularization based bert for pairwise semantic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2020</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2020</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1901" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fine-tune bert for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10318</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Group, extract and aggregate: Summarizing a large amount of finance news for forex movement prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiko</forename><surname>Harimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Economics and Natural Language Processing</title>
		<meeting>the Second Workshop on Economics and Natural Language Processing<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shallow-deep networks: Understanding and mitigating network overthinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigitcan</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Dumitras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3301" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACl</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Soft-DTW: a differentiable loss function for time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="894" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation extraction with convolutional network over learnable syntax-transport graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Mensah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8928" to="8935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
