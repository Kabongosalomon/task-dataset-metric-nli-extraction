<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAAS: Multi-modal Assignation for Active Speaker Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Le?n Alc?zar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
							<email>ali.thabet@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MAAS: Multi-modal Assignation for Active Speaker Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Active speaker detection requires a mindful integration of multi-modal cues. Current methods focus on modeling and fusing short-term audiovisual features for individual speakers, often at frame level. We present a novel approach to active speaker detection that directly addresses the multimodal nature of the problem and provides a straightforward strategy, where independent visual features (speakers) in the scene are assigned to a previously detected speech event. Our experiments show that a small graph data structure built from local information can approximate an instantaneous audio-visual assignment problem. Moreover, the temporal extension of this initial graph achieves a new state-of-the-art performance on the AVA-ActiveSpeaker dataset with a mAP of 88.8%. We have made our code available at https://github.com/fuankarion/MAAS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Active speaker detection aims at identifying the current speaker (if any) from a set of candidate face detections in an arbitrary video. This research problem is an inherently multi-modal task that requires the integration of subtle facial motion patterns and the characteristic waveform of speech. Despite its multiple applications such as speaker diarization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>, human-computer interaction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b60">61]</ref> and bio-metrics <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref>, the detection of active speakers in-the-wild remains an open problem.</p><p>Current approaches for active speaker detection are based on recurrent neural networks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> or 3D convolutional models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63]</ref>. Their main focus is to jointly model the audio and visual streams to maximize the performance of single speaker prediction over short sequences. Such an approach is suitable for single speaker scenarios, but is overly simplified for the general (multi-speaker) case.</p><p>The general (multi-speaker) scenario has two major challenges. First, the presence of multiple speakers allows for incorrect face-voice assignations. For instance, false positives emerge when facial gestures closely resemble the motion patterns observed while speaking (e.g. laughing, grinning). Second, it must enforce temporal consistency over multi-modal data, which quickly evolves over time, e.g., Active speaker detection is highly ambiguous. Even if we analyze joint audiovisual information, unrelated facial gestures can easily resemble the natural motion of lips while speaking. In a) we show two face crops from a sequence, where a speech event was detected. The gestures, illumination, and capture angle make it hard to asses which face (if any) is the active speaker. Our strategy b) focuses on the attribution of speech segments in video. If a speech event is detected, we holistically analyse every speaker along with the audio track to discover the most likely active speaker. when active speakers switch during a fluid conversation.</p><p>In this paper, we address the general multi-speaker problem in a principled manner. Our key insight is that, instead of optimizing active speaker predictions over individual audiovisual embeddings, we can jointly model a set of visual representations from every speaker in the scene along with a single audio representation extracted from the shared audio track. While simple, this modification allows us to map the active speaker detection task into an assignation problem, whose goal is to match multiple visual representations with a singleton audio embedding. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates some of the challenges in active speaker detection and provides a general insight for our approach.</p><p>Our approach, dubbed "Multi-modal Assignation for Active Speaker detection" (MAAS) relies on multi-modal graph neural networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b51">52]</ref> to approach the local (framewise) assignation problem, but it is flexible enough to also propagate information from a long-term analysis window by simply updating the underlying graph connectivity. In this framework, we define the active speaker as the local visual representation with the highest affinity to the audio embedding. Our empirical findings highlight that reformulating the problem into a multi-modal assignation problem brings sizable improvements over current state-of-the-art methods. On the AVA Active speaker benchmark, MAAS outperforms all other methods by at least 1.7%. Additionally, when compared with methods that analyze a short temporal span, MAAS brings a performance boost of at least 1.1%. Contributions. This paper proposes a novel strategy for active speaker detection, which explicitly learns multi-modal relationships between audio and facial gestures by sharing information across modalities. Our work brings the following contributions: (1) We devise a novel formulation for the active speaker detection problem. It explicitly matches the visual features from multiple speakers to a shared audio embedding of the scene (Section 3.2). (2) We empirically show that this assignation problem can be solved by means of a Graph Convolutional Network (GCN), which endows flexibility on the graph structure and is able to achieve state of the art results (Section 4.1). (3) We present a novel dataset for active speaker detection, called "Talkies", as a new benchmark composed of 10,000 short clips gathered from challenging and diverse scenes (Section 5).</p><p>To ensure reproducible results and promote future research, all the resources of this project, including source code, model weights, official benchmark results, and labeled data will be publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the realm of multi-modal learning, different information sources are fused with the goal of establishing more effective representations <ref type="bibr" target="#b36">[37]</ref>. In the video domain, a common multi-modal paradigm involves combining representations from visual and audio features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51]</ref>. Such representation allows the exploration of new approaches to well established problems, such as person reidentification <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58]</ref>, audio-visual synchronization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, speaker diarization <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62]</ref>, bio-metrics <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref>, and audio-visual source separation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref>. Active speaker detection is a special instance of audiovisual source separation, where the sources are people in a video, and the goal is to detect and assign a segment of speech to one of those sources <ref type="bibr" target="#b41">[42]</ref>. Active Speaker Detection. The work of Cutler et al. <ref type="bibr" target="#b11">[12]</ref> pioneered research in active speaker detection in the early 2000s. It detected correlated audiovisual signals by means of a time-delayed neural network <ref type="bibr" target="#b47">[48]</ref>. Follow up works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43]</ref> approached the task relying only on visual information, focusing strictly on the evolution of facial gestures. Such visual-only modeling was possible as they addressed a simplified version of the problem with a single candidate speaker. Recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> have approached the more general multi-speaker scenario and relied on fusing multi-modal information from individual speakers. A parallel corpus of work has focused on audiovisual feature alignment, which resulted in methods that rely on audio as the primary source of supervision <ref type="bibr" target="#b3">[4]</ref>, or as an alternative to jointly train a deep audiovisual embedding <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>The work of Roth et al. <ref type="bibr" target="#b41">[42]</ref> introduced the AVA-ActiveSpeaker dataset and benchmark, the first large-scale video dataset for the active speaker detection task. In the AVA-ActiveSpeaker challenge of 2019, Chung et al. <ref type="bibr" target="#b5">[6]</ref> presented an improved architecture of their previous work <ref type="bibr" target="#b9">[10]</ref>, which trains a large 3D model with the need for largescale audiovisual pre-training <ref type="bibr" target="#b35">[36]</ref>. Zhang et al. <ref type="bibr" target="#b62">[63]</ref> also leveraged a hybrid 3D-2D architecture with large-scale pretraining <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. This method achieved its best performance when the feature embedding was optimized using a contrastive loss <ref type="bibr" target="#b18">[19]</ref>. Follow up works focused on modeling an attention process over face tracks, where attention was estimated either from the audio alignment <ref type="bibr" target="#b0">[1]</ref> or from an ensemble of speaker features <ref type="bibr" target="#b1">[2]</ref>. We approach the active speaker problem in a more principled manner, as we go beyond the aggregation of contextual information from multiple-speakers and propose an approach that explicitly seeks to model the correspondence of a shared audio embedding with all potential speakers in the video.</p><p>Datasets for Active Speaker Detection. Apart from the development of the AVA-ActiveSpeaker benchmark, there are few public datasets specific to this problem. The most well known alternative is the Columbia dataset <ref type="bibr" target="#b4">[5]</ref>, which contains 87 minutes of labeled speech from a panel discussion. It is much smaller and less diverse than AVA. Modern audiovisual datasets <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref> have been adapted for the large scale pre-training of some active speaker methods <ref type="bibr" target="#b5">[6]</ref>. Nevertheless, these datasets were designed for related tasks such as speaker identification and speaker diarization. In this paper, we present the Talkies dataset as a new benchmark for active speaker detection gathered from social media clips. It contains 800,000 manually labeled face detections and includes challenging scenarios that contain multiple speakers, occlusion, and out of screen speech.</p><p>Graph Convolutional Networks (GCNs). GCNs <ref type="bibr" target="#b27">[28]</ref> have recently gained popularity, due to the greater interest in non-Euclidean data. In computer vision, GCNs have been successfully applied to scene graph generation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60]</ref>, 3D understanding <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>, and action recognition in video <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59]</ref>. In MAAS, we design a DeepGCN-like architecture <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>, which addresses a special scenario, namely the multi-modal nature of audiovisual data. We rely on the well-known EdgeConv operator <ref type="bibr" target="#b51">[52]</ref> to model interactions between different modalities for graph nodes identified across multiple frames. This enables us to model both the multi-modal relations and the temporal dependencies in a single graph structure. We create two feature graphs: one with static connections that model local temporal relations between the audio track and the visible persons; in parallel, we allow a secondary stream in the network to discover relationships given the estimated feature embeddings. c): We estimate a frame-level affinity between the visual nodes and the local audio node such that the active speaker will have the highest affinity with the audio node. d): Finally, we extend the network by modelling a longer temporal window. We jointly optimize the local affinities, while enforcing temporal consistency. We select the active speaker (green bounding box) as the most likely speaker to have generated the sequence of speech events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-modal Active Speaker Assignation</head><p>Our approach is based on a straightforward idea. Instead of assessing the likelihood of individual audiovisual patterns to belong to an active speaker, we directly model the correspondence between the local audio and the facial gestures of all the individuals present in the scene. This approach is motivated by the nature of the active speaker problem, which first identifies if any speech patterns are present, and then attributes those patterns to a single speaker.</p><p>Overall, our approach simultaneously solves three subtasks. First, we detect speech events in a short-term temporal window. Second, we iterate over all the visible speakers in a single frame, and decide which one is most likely to be an active speaker given the local information. Third, we extend this frame-level analysis along the temporal dimension, leveraging the inherent temporal consistency of video data to improve frame-level predictions. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates an overview of our MAAS approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Frame-Level Video Features</head><p>Following recent works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b62">63]</ref>, we extract the initial frame-level features from a two-stream convolutional encoder. The visual stream takes as input a tensor of dimensions H ? W ? (3c), where H and W are the image width and height, and c is the number of time consecutive face crops sampled from a single tracklet. Similar to <ref type="bibr" target="#b41">[42]</ref>, we transform the original audio waveform into a Melspectrogram and use it as input for the audio stream.</p><p>Our approach relies on independent audio and video features. To obtain these independent features (and to make fair comparison to state-of-the-art techniques), we train a joint model as described by <ref type="bibr" target="#b41">[42]</ref>, but drop the final two layers at inference time. These layers are responsible for the feature fusion and final prediction.</p><p>At time t, a forward pass of our feature encoder yields N + 1 feature vectors for a frame with N possible speakers (detected persons). One shared audio embedding (a t ) and N independent visual descriptors v t = {v t,0 , v t,1 , ..., v t,n?1 } one for each of the N visible persons (see <ref type="figure" target="#fig_1">Figure 2</ref>-a). We define (s t ) as the local set of features at time t, such that s t = {a t ?v t }. The feature set s t is used for the optimization of the basic graph structure in MAAS, the Local Assignation Network, described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Assignation Network (LAN)</head><p>We model the local assignation problem by generating a directed graph over the feature set s t . Our local graph consists of an audio node and one video node for each potential speaker. We create a bidirectional connectivity between the audio node and each visual node thus leveraging a GCN that operates on a directed graph generated from s t . <ref type="figure" target="#fig_2">Figure 3</ref> (left) illustrates this graph structure. We call this graph structure the Local Assignation Graph and the GCN that operates over it the Local Assignation Network (LAN).</p><p>The goals of LAN are two-fold: (i) to detect local speech events, (ii) if there is a speech event, to assign the most likely speaker from the set of candidates. We achieve these two goals by fully supervising every node in LAN. Visual nodes are supervised by the groundtruth, l tv , of the corresponding speaker. On the other hand, each audio node receives a binary ground-truth label indicating whether there is at least one active speaker, i.e. max({l t,0 , l t,1 , ..., l t,n?1 }); otherwise, there is silence. LAN is tasked to discover active speakers at the frame-level (i.e. t is fixed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Assignation Network (TAN)</head><p>While the LAN is effective at finding local correspondences between audio patterns and visible faces, it models information sampled from short video clips (s t ). This sampling strategy can lead to inaccurate predictions from noisy or ambiguous local estimations (e.g. audio noise, blurred faces, ambiguous facial gestures, etc.). Therefore, we extend our proposed approach to include temporal information from adjacent frames.</p><p>We extend the local graph in LAN by sampling s t over a temporal window (w) centered around time t. w = [i, i + 1, ..., t, ..., j] and define a temporal feature set b w = [s i , s i+1 , ..., s t , ..., s j ]. Following the LAN structure outlined in 3.2, we can build (j ? i) independent local graph structures out of b w (one for every time step). We augment this set of independent graphs by adding temporal links between time adjacent representations of frame-level features. We follow two rules to build these connections: we create temporal connections between time adjacent audio nodes, and we create temporal connections between time adjacent video nodes, only if they belong to the same person. No additional cross-modal connections are built. We call the resulting graph, the Temporal Assignation Graph, which allows for information flow between time adjacent audio and video features, thereby allowing for temporal consistency in the audio and video modalities. <ref type="figure" target="#fig_2">Figure 3</ref> (right) illustrates this graph structure.</p><p>We build a GCN over the extended graph topology and call it the Temporal Assignment Network (TAN). TAN allows us to directly identify speech segments as continuous positive predictions over audio nodes. Likewise, it detects active speech segments over continuous predictions for same-speaker video features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic Stream &amp; Global Prediction</head><p>Finally, we account for potential connection patterns that go beyond our initial insights. We augment our architecture and define a second stream that will operate on the very same data as the static stream (including multiple temporal timestamps). However, we do not define a fixed connectivity pattern for this stream. Instead, we aim at creating a dynamic graph structure based on the node distribution in feature space. In this stream, we allow the GCN to estimate an arbitrary graph structure by calculating the K nearest neighbors in feature space for each node, and by establishing edges based on these neighbouring nodes. In practice, we replicate the static stream, drop the definition of the static graph, and use the dynamic version of the edgeconvolution <ref type="bibr" target="#b51">[52]</ref>, allowing for independent dynamic graph estimation at every layer.</p><p>The final prediction is achieved through slow fusion <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b56">57]</ref>. At every GCN layer, we merge the feature set from the dynamic layer with the feature set from the static layers. The final prediction is achieved using a shared fully connected layer and softmax activation over every node. This architecture is depicted in <ref type="figure" target="#fig_4">Figure 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and Implementation Details</head><p>Following <ref type="bibr" target="#b41">[42]</ref>, we implement a two-stream feature encoder based on the ResNet-18 architecture <ref type="bibr" target="#b19">[20]</ref> pre-trained on ImageNet <ref type="bibr" target="#b12">[13]</ref>. We perform the same modifications at the first layer to adapt for the extended input tensor (stack of face crops and spectrogram). We train the network end- to-end using the Pytorch library <ref type="bibr" target="#b38">[39]</ref> for 100 epochs with the ADAM optimizer <ref type="bibr" target="#b26">[27]</ref> using Cross-Entropy Loss. We use 3 ? 10 ?4 as initial learning rate that decreases with annealing of ? = 0.1 at epochs 40 and 80. We empirically set c = 11 and augment the input videos via random flips and corner crops. Unlike other methods, MAAS does not require any large-scale audiovisual pre-training. We also incorporate the sampling strategy proposed by <ref type="bibr" target="#b1">[2]</ref> in training to alleviate overfitting. During training, we follow the supervision strategy outlined by <ref type="bibr" target="#b41">[42]</ref>, where two extra auxiliary loss functions (L a , L v ) are adopted to supervise the final layer of the audio and video streams. This favors the estimation of useful features from both streams.</p><p>Training MAAS After optimizing the feature encoder, we implement MAAS (LAN and TAN networks) using the PyTorch Geometric library <ref type="bibr" target="#b15">[16]</ref>. We choose edgeconvolution <ref type="bibr" target="#b52">[53]</ref> to propagate the neighbor information between nodes. Our network model contains 4 GCN layers on both streams, each with filters of 64 dimensions. We apply dimensionality reduction to map features from their original 512 dimension to 64 using a fully connected layer. We find that this dimensionality reduction favors the final performance and largely reduces the computational cost.</p><p>Since we process data from different modalities, we use two different dimensionality reduction layers, one for video features and another for audio features. We train the MAAS-LAN and MAAS-TAN networks using the same procedure and set of hyper-parameters, the only difference being their underlying graph structure. We use the ADAM optimizer with an initial learning rate of 3 ? 10 ?4 and train for 4 epochs. Both GCNs are trained from random weights and use a pre-activation <ref type="bibr" target="#b20">[21]</ref> linear layer (Batch Normalization ? ReLU ? Linear Layer) to map the concatenated node features inside the edge convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Validation</head><p>In this section, we provide an empirical analysis of our proposed MAAS method. We focus on the large-scale AVA-ActiveSpeaker dataset <ref type="bibr" target="#b41">[42]</ref> to assess the performance of MAAS and present additional evaluation results on Talkies. This section is divided in three parts. First, we compare MAAS with state-of-the-art techniques. Then, we ablate our proposal to analyse all of its individual design choices. Finally, we test MAAS on known challenging scenarios to explore common failure modes.</p><p>AVA-ActiveSpeaker Dataset. The AVA-ActiveSpeaker dataset <ref type="bibr" target="#b41">[42]</ref> is the first large-scale testbed for active speaker detection. It is composed of 262 Hollywood movies: 120 of those on the training set, 33 on validation, and the remaining 109 on testing. The AVA-ActiveSpeaker dataset contains normalized bounding boxes for 5.3 million faces, all manually curated from automatic detections. Facial detections are manually linked across time to produce face tracks (tracklets) depicting a single identity. Each face detection is labeled as speaking, speaking but not audible, or non-speaking. All AVA-ActiveSpeaker results reported in this paper were measured using the official evaluation tool provided by the dataset creators, which uses mean average precision (mAP) as the main metric for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">State-of-the-art Comparison</head><p>We begin our analysis by comparing MAAS to state-ofthe-art methods. The results reported for MAAS-TAN are obtained from a two-stream model composed of 13 temporally linked local graphs, which span about 1.59 seconds. We set K = 3 for the number of nearest neighbors in the dynamic stream and limit the number of video nodes to 4 per frame. The results reported for MAAS-LAN are obtained from a two-stream model, which includes a single timestamp and 4 video nodes. For sequences with 5 or more visible speakers, we make sure that one video node contains the features from the active speaker, and randomly sample the remaining three. If no active speaker is present, we just randomly sample 4 speakers without replacement. At inference time, we split the speakers in non overlapping groups of 4, and perform multiple forward passes. Results in the validation set are summarized in <ref type="table">Table 1</ref>.</p><p>Our best model, MAAS-TAN, ranks first on the AVA-ActiveSpeaker validation set. We highlight two aspects of these results. First, at 88.8% mAP, MAAS-TAN outperforms the best results reported on this dataset by at least  <ref type="bibr" target="#b5">[6]</ref> 85.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAAS-LAN (Ours)</head><p>85.1 Zhang et al. <ref type="bibr" target="#b62">[63]</ref> 84.0 Sharma et al. <ref type="bibr" target="#b43">[44]</ref> 82.0 Roth et al. <ref type="bibr" target="#b41">[42]</ref> 79.2</p><p>Test Set MAAS-TAN (Ours) 88.3 Naver Corporation <ref type="bibr" target="#b5">[6]</ref> 87.8 Active Speaker Context <ref type="bibr" target="#b1">[2]</ref> 86.7 University of Chinese Academy of Sciences <ref type="bibr" target="#b62">[63]</ref> 83.5 Google Baseline <ref type="bibr" target="#b41">[42]</ref> 82.1 <ref type="table">Table 1</ref>. State-of-the-art Comparison on AVA-ActiveSpeaker.</p><p>We compare MAAS against state-of-the-art methods on the AVA-ActiveSpeaker validation set. Results are measured with the official evaluation tool as published by <ref type="bibr" target="#b41">[42]</ref>. We report an improvement of 1.7% mAP over the current state-of-the-art.</p><p>1.7%. It must be noted that some state-of-the-art methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b62">63]</ref> rely on large 3D models and large-scale audiovisual pre-training, while MAAS uses only the standard Im-ageNet initialization for both streams. Second, while the MAAS-LAN network does not achieve state-of-the-art performance, it outperforms every other method that does not rely on long-term temporal processing <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b62">63]</ref>. It also remains competitive with those methods that rely only on long-term context <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2]</ref>, being outperformed only by the temporal version of <ref type="bibr" target="#b1">[2]</ref> by a margin of 0.6% and falling 2.1% behind the full method of <ref type="bibr" target="#b1">[2]</ref> (temporal context and multi-speaker). After evaluating the performance of our MAAS method against state-of-the-art techniques, we ablate our best model (MAAS-TAN) to validate the individual contributions of each design choice, namely: network depth, network width, independent stream contributions, and the number of neighbors for the dynamic stream.</p><p>Network Architecture We begin by ablating the proposed architecture. We explore the effects of changing the network depth, layer size, and the number of neighbors (K) for the dynamic stream. We also control for the individual contribution of each stream.</p><p>We summarize our ablation results for the MAAS-TAN network in <ref type="table">Table 3</ref>. In 3-a), we identify the depth of the network as a relevant hyper-parameter for its performance. Shallow networks underperform, but increasingly get better as depth increases, reaching an optimal value at 4 layers. Deeper networks have a better capacity for estimating use-  <ref type="table">Table 2</ref>. Graph Structure in MAAS. We ablate the size of the MAAS-TAN network which is the core data structure of our approach. We empirically find it beneficial to model multiple speakers at the same time, and find the optimal number of speakers to be 4. Likewise, longer temporal sampling favors the performance but diminishes with sequences longer than 13 frames.</p><p>ful features and have the chance to propagate relevant features over a large number of connected nodes, not only the immediate neighbors. In 3-b), we show that wider networks have a beneficial effect but saturate quickly with 64 or more filters. Beyond that size, the networks do not yield improvements at the expense of additional network complexity. In 3-c), we demonstrate the complementary nature of the two stream approach in MAAS. While the static stream has the best individual performance, the dynamic stream is capable of finding relationships that are beyond the insights we use to create the static graph structure, thus increasing the final performance by 0.9%. Finally, 3-d) shows how the selected number of clusters on the dynamic stream affects the final performance of MAAS. Interestingly, the optimal number of neighbors (K = 3) matches the number of valid assignations in the active speaker problem (audio with speech, active speaker), (audio with speech, silent speaker) and (audio with silence, silent speaker).</p><p>Graph Structure After assessing the design choices in the architecture, we proceed to evaluate the proposed graph structure. Here, we test for the incremental addition of LAN graphs into a TAN graph that analyses N timestamps. Additionally, we test for the maximum number of video nodes that get linked to an audio node at training time. <ref type="table">Table 2</ref> summarizes these results. Overall, we notice that MAAS benefits from modelling longer temporal sequences or modelling more visible speakers. We interpret this as a consequence of our modelling strategy that focuses on the assignation of locally consistent visual and audio patterns, while remaining compatible with the mainstream approach of modelling long-term temporal sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset Properties</head><p>We continue our analysis following the evaluation protocol of <ref type="bibr" target="#b41">[42]</ref> and report MAAS-TAN results in known hard scenarios, namely multiple possible speakers and small faces.  <ref type="table">Table 3</ref>. Architecture choices in MAAS. We ablate the design choices in our proposed GCN-based MAAS-TAN network. We analyse the network depth in a), and empirically find that a deeper network favors the final result, but saturates at 4 layers. We also analyse the number of filters per layer in b) and find the optimal to be at 64. From c), we observe that the static stream is far more effective by itself than the dynamic stream; however, the latter stream still incorporates information that is complementary leading to overall improvement. In d), we empirically find the most suitable number of neighbors in the dynamic stream and set it to <ref type="bibr">3.</ref> In <ref type="table">Table 4</ref>, we provide a breakdown of MAAS results according to the number of possible speakers. Overall, we see a significant performance increase when comparing MAAS to the AVA baseline <ref type="bibr" target="#b41">[42]</ref> and improvements in all scenarios when compared to the multi-speaker stack of <ref type="bibr" target="#b1">[2]</ref>. Clearly, the multi-speaker scenario is still quite challenging, but the improvements highlight that our speech assignation-based method is especially effective when two or more possible speakers are present.  <ref type="table">Table 4</ref>. Performance evaluation by number of faces. We evaluate MAAS according to the number of faces visible in the video frame. While performance decreases with more visible people, our method outperforms the AVA baseline and current state-of-the-art.</p><p>In <ref type="table">Table 5</ref>, we provide a breakdown of MAAS results according to the size of the face crop. We follow the evaluation procedure of <ref type="bibr" target="#b41">[42]</ref> and create 3 sets of faces: (S) denotes faces smaller than 64?64 pixels, (M) denotes faces between 64?64 and 128?128 pixels, and (L) denotes any face larger than 128?128 pixels. Although MAAS does not explicitly addresses specific face sizes, we observe a large performance gap when compared to the AVA baseline, and we improve in most scenarios when compared to the method of Alcazar et al. <ref type="bibr" target="#b1">[2]</ref>. We think this increase in performance is a consequence of better predictions in related faces, i.e. smaller faces are typically seen in cluttered scenes with multiple other visible individuals, so our method improves the prediction on these smaller faces by integrating more reliable information from other speakers.</p><p>To conclude this section we report a final result for MAAS on the AVA-ActiveSpeaker dataset. We assess the relevance of supervision for audio nodes, we empirically find that, by supervising only the video nodes in MASS the performance is slightly reduced to 88.5%. We think this additional supervision over audio nodes allows for better estimation of the speech events, mitigating some false positives on the speaker detection.  <ref type="table">Table 5</ref>. Performance evaluation by face size. We evaluate MAAS in another challenging scenario: small and medium sized faces, which cover less than 128?128 pixels and 64?64 pixels, respectively. We observe that MAAS outperforms the current stateof-the-art, in most scenarios.</p><p>Dubbed audio. The AVA-ActiveSpeaker dataset contains videos with its native audio as well as dubbed audio. We assess the effect of this imperfect match between the actor's facial gestures and the audio track resulting from dubbing. Since the dataset metadata does not indicate which videos are dubbed, we manually label the validation set videos as either having native audio (20 videos) or dubbed audio <ref type="bibr">(13 videos)</ref>. MAAS achieves an mAP of 86.6% for videos with native audio and 91.6% for dubbed videos. This suggests that MAAS is a viable option for dubbed videos, as it is not so sensitive to the mismatch between facial gestures and the audio waveform resulting from dubbing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The Talkies Dataset</head><p>Given the scarcity of in-the-wild active speaker datasets, we introduce "Talkies", a manually labeled dataset for the active speaker detection task. Talkies contains 23,507 face tracks extracted from 421,997 labeled frames that yield a total of 799,446 individual face detections.</p><p>In comparison, the Columbia dataset <ref type="bibr" target="#b4">[5]</ref> has about 150,000 face crops, while AVA-ActiveSpeaker <ref type="bibr" target="#b41">[42]</ref> contains about 5.3 millions (760,000 in validation). Although AVA-ActiveSpeaker has a larger number of individual samples, we argue that Talkies is an interesting, complementary benchmark for three reasons. First, Talkies is more focused on the challenging multi-speaker scenario with 2.3 speakers per frame on average, while AVA-ActiveSpeaker averages only 1.6 speakers per frame. Second, Talkies does not focus on a single source of videos, as in AVA-ActiveSpeaker (Hollywood movies). As a consequence, Talkies contains a more diverse set of actors and scenes, with actors rarely overlapping between clips. This strikes a hard contrast with Hollywood movies, where a small cast takes most of the screen time. Finally, out of screen speech (another challenge for active speaker detection) is not common in Hollywood movies, but it appears more often in Talkies.   <ref type="figure">Figure 5</ref>. Talkies Dataset Properties. We analyze 3 relevant properties in the talkies dataset. In a) we show that most of the dataset contains medium to large faces (64x64 and above), only 16.3% of Talkies is composed of small faces. In b) we show that most scenes (59.9%) contain multiple speakers although the most common scenario is a single speaker. In c) we show the proportion of videos labeled with off-screen speech. Talkies is the first active speaker detection dataset to address such scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Creating the Talkies Dataset</head><p>We complement the description of Section 5 on the talkies dataset. Our main goal collecting Talkies is to create a diverse dataset that contains complex scenarios for the active speaker task including: multi-speaker, out of screen speech and noise. Therefore, we use Youtube as our main source of videos, and gather an initial set of clips using search keywords that are likely to contain speech and multiple visible humans, among others: "interview", "daily vlog", "family vlog", "reacting to X", "commentary/review of X". We do not include any further constrain on this initial search.</p><p>After gathering an initial pool of videos, we run the voice activity detector of <ref type="bibr" target="#b48">[49]</ref> and the face detector of <ref type="bibr" target="#b13">[14]</ref>. This helps us eliminate a large part of the temporal segments, as we keep only a subset where we simultaneously detect human speech and at least 1 face. We do not include any further filtering or manual selection at this step, as it allows for a key element in talkies: the out of screen speech when a set of silent faces is visible and speech from a person out of screen is heard. On the final step, we want to favor diversity, so we randomly select a single clip per video. This process results in 10.000 short clips (1.5 seconds each) which the video set of Talkies.</p><p>We take the full set of videos in talkies and run the tracker of <ref type="bibr" target="#b53">[54]</ref>. We manually label each of the estimated tracklets as 'silent' or 'active speaker'. This process results in a total of 23.508 manually labeled tracks composed of 799.446 individual face detections. Additionally, we provide a video-level annotation for the special case of out of screen speech. That is, every tracklet is labeled as 'silent' and the video itself is associated with label 'OFFSCREEN'.</p><p>In total, Talkies contains 4 hours and 10 minutes of manually labeled video data. <ref type="table">Table 6</ref> summarizes some of the most important statistics for Talkies and compares it with the other active speakers datasets. AVA-ActiveSpeakers is clearly a larger dataset regarding number of labeled videos and face crops. Nevertheless talkies, has some relevant properties: it has more speakers per frame (2.3 vs 1.6) and is sampled from a much larger video corpus than AVA-ActiveSpeakers (10.000 vs 218). We think Talkies can be an interesting transfer-dataset for future research or used for approaching the active-speaker problem in a semisupervised strategy.</p><p>We calculate some relevant statistics for the complex scenarios in talkies, namely number of simultaneous faces in a video and size of the face crops, <ref type="figure">figure 5</ref> summarizes these results. Overall, we find that most images in talkies fall in the Large (larger than 128 ? 128) and Medium category (between 64 ? 64 and 128 ? 128), with only a 16 % of the face crops into the (harder) category of small images. Additionally, we show the distibution of the number of speakers in a frame, like AVA-ActiveSpeakers the most common scenario is to have a single speaker. However, this  <ref type="table">Table 6</ref>. Dataset Comparison. We compare some relevant statistics of the talkies dataset and the other 2 well know datasets for active speaker detection. While talkies is smaller than the Ava-Active speaker dataset it remains an interesting benchmark given its large video pool, diversity and large number of possible speakers. represent only the 39.1% percent of the dataset with the remaining 59.9% having two or more speakers. In consequence, the average number of speakers stands at 2.3, much higher than the 1.6 of AVA-ActiveSpeakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Faces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Analysis on Talkies</head><p>We complement the dataset statistics, and assess the effectiveness of MAAS and two baseline methods in some well known challenging scenarios. We follow a similar procedure to <ref type="bibr" target="#b41">[42]</ref>, and ablate these methods in Talkies according to the number of visible faces and the size of the face. <ref type="table" target="#tab_6">Table 7</ref> shows the ablation results according to the face size. Talkies shows a similar behaviour to AVA-ActiveSpeakers where smaller faces (less than 64 ? 64) are harder to classify and large faces are easier. We will also highlight the improvement of MAAS in every scenario in comparison to <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b1">[2]</ref>, which is consistent with the results in AVA-ActiveSpeakers.</p><p>We continue our analysis by testing the performance of different baseline methods on Talkies, according to the number of visible faces. <ref type="table" target="#tab_7">Table 8</ref> summarizes these results. Again, these numbers are consistent with those obtained in the AVA-ActiveSpeaker dataset. We highlight that a larger portion of the errors are contained in those clips with 3 or more visible speakers. Once again MAAS outperforms the state-of-the-art in every category except the single speaker baseline, where <ref type="bibr" target="#b1">[2]</ref> obtains 0.1% more. Now, we evaluate the transferability of our MAAS method, trained on AVA-ActiveSpeaker, to the Talkies dataset. No fine-tuning is performed in this case.</p><p>In <ref type="table">Table 9</ref>, we compare the results of our best model  <ref type="table">Table 9</ref>. Performance on Talkies. We evaluate MAAS performance on the Talkies dataset. Without any fine-tuning on Talkies, MAAS (pre-trained on AVA-ActiveSpeaker) outperforms the baseline by 7.6% and the state-of-the-art by 1.7%. A simple augmentation targeting out of screen speech during AVA-ActiveSpeaker training leads to a direct improvement in the challenging scenes of Talkies.</p><p>(MAAS-TAN) against the AVA baseline of <ref type="bibr" target="#b41">[42]</ref> and the ensemble model of <ref type="bibr" target="#b1">[2]</ref> on our new dataset. MAAS outperforms these models by 7.6% and 1.7%, respectively. These results suggest that the core strategy proposed in MAAS is not domain-specific and can be applied to diverse scenarios without fine-tuning. Moreover, we explore an interesting attribute of Talkies, namely out of screen speech. To do so, we augment the training of MAAS on AVA-ActiveSpeaker, such that we randomly replace a silent audio track (its corresponding frames do not show any active speaker) with a track that contains speech. This simulates out of screen speech scenarios. This artificial substitution is done only during training and with a probability of 20%. This augmentation does not increase the amount of supervision at training time and has no empirical impact on the performance of MAAS on AVA-ActiveSpeaker, since out of screen speech is not common in Hollywood movies. However, this augmentation strategy does bring an improvement of 0.6% on Talkies, indicating that MAAS may be flexible enough to handle scenarios more general than those in AVA-ActiveSpeaker. <ref type="figure">Figure 6</ref>. Qualitative Results. MAAS-TAN includes a dynamic stream that estimates graph structures from nearest neighbours in feature space. The connection patterns in this stream are very diverse and often create edges not present in our static graph. We find that such connectivity allows for information flow between distant audio clips (magenta), inter-speaker relations over multiple time-stamps, and cross-modal arcs that involve nodes in different frames (orange). For easier visualization, we only show a subset of all the dynamic connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Analysis</head><p>We conclude our assessment of MAAS by briefly looking at the connectivity patterns estimated by the dynamic stream. In <ref type="figure">Figure 6</ref>, we show a complex clip from the Talkies dataset, in this clip speaker 4 is the only active speaker. In fact, he narrates over the first frames of this clip. This scenario (out of screen speech) makes it very difficult for the baseline to generate accurate predictions on the first frames resulting in some false positive predictions (see speaker 2). MAAS on the other hand performs significantly better, reducing the false positives and effectively detecting the active speaker.</p><p>Empirically, we find that this clip mAP increases from 64% (baseline) to 97.9% (MAAS). We think this improvement is explained by two factors. First, MAAS builds more consistent temporal relationships for individual speakers, as its graph structure enforces consistent assignments across the temporal dimension. Second, the dynamic stream allows for unconventional, yet useful connectivity patterns. We show some of these patterns in the bottom row of <ref type="figure">Figure 6</ref>. In blue, we highlight inter-speaker connections across timestamps. These connections are not part of the static graph structure, and can potentially encode semantic relationships between face crops. In magenta, we highlight audio-to-audio connections that go beyond our initial insight of linking adjacent audio clips. We think these con-nections allow for long-term temporal consistency between audio clips. Such consistency is key to resolve complex scenarios, as is the case with the narration in the selected clip. Finally, we highlight in orange cross-modal connections of nodes at different time-stamps. These connections also differ from those modeled in our static graph, and they reflect semantic similarities in the audiovisual embedding of MAAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced MAAS, a novel multi-modal assignation technique based on graph convolutional networks, for the active speaker detection task. Our method focuses on directly optimizing a graph that simultaneously detects speech events and estimates the best source (active speaker). Additionally, we present Talkies, a novel benchmark with challenging scenarios for active speaker detection, which serves as a challenging transfer dataset for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Audiovisual assignment for active speaker detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of MAAS Pipeline. a): Our approach begins by sampling independent audio and video features. Video features (cyan) are extracted from a stack of face crops that belong to a single person. Audio features (yellow) are extracted from the audio-spectrogram and are shared at the frame-level. b):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Assignation Graphs. The base static graphs for MAAS are composed of multi-modal nodes, visual nodes (Cyan), and audio nodes (Yellow). The local Assignation Graph (left) defines frame level connectivity of the individual features. The Temporal Assignation Graph (right) is composed of multiple local graphs (5 in the figure) and defines a temporal extension of the frame-level relations (we depict local relations in light gray to avoid visual clutter). While a local graph solves an instantaneous assignation problem. The temporal graph optimizes a subset of nodes thereby incorporating temporal information in the individual local graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>GCN Architecture in MAAS. Our graph neural network implements a two stream architecture. The first stream (top) uses the edge convolution operator and operates over static local and temporal graphs. The second stream (bottom) relies on dynamic edge convolutions and complements the feature embedding discovered by the static stream by means of slow fusion. After every GCN layer, we fuse the features from the dynamic and static streams and use them as input to the next layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>a) Face Size Distribution in Talkies. b) Number of Simultaneous Visible Speakers in Talkies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>c)</head><label></label><figDesc>Videos With Off-screen Speech labels in Talkies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Talkies Face Size We evaluate MAAS and some baseline methods in Talkies according to the number size of the face in the video. As observed in Ava-ActiveSpeakers, smaller faces are harder to classify.</figDesc><table><row><cell cols="4">Size MAAS ASC [2] AVA Baseline [42]</cell></row><row><cell>Small</cell><cell>57.0</cell><cell>55.4</cell><cell>41.7</cell></row><row><cell>Mid</cell><cell>73.9</cell><cell>71.6</cell><cell>64.8</cell></row><row><cell>Large</cell><cell>84.8</cell><cell>83.9</cell><cell>80.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Performance evaluation by number of faces. We evaluate MAAS and some baseline methods in Talkies according to the number of faces visible in the video frame. Performance decreases with more visible people.</figDesc><table><row><cell cols="4">Number of Faces MAAS AVA Baseline [42] ASC [2]</cell></row><row><cell>1</cell><cell>84.5</cell><cell>83.0</cell><cell>84.6</cell></row><row><cell>2</cell><cell>81.1</cell><cell>72.8</cell><cell>77.6</cell></row><row><cell>3 or more</cell><cell>71.3</cell><cell>63.3</cell><cell>70.3</cell></row><row><cell>Training</cell><cell cols="3">MAAS AVA Baseline [42] ASC [2]</cell></row><row><cell>AVA</cell><cell>79.1</cell><cell>71.5</cell><cell>77.4</cell></row><row><cell>AVA augmented</cell><cell>79.7</cell><cell>N/A</cell><cell>N/A</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised learning of audio-visual objects from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04237</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active speakers in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Leon Alcazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speaker diarization: A review of recent research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bozonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinne</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Who&apos;s speaking? audio-supervised classification of active speakers in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayeh</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Van Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction (ICMI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active speaker detection with audio-visual co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Zegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction (ICMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Naver at activitynet challenge 2019-task b active speaker detection (ava)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Joon Son</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10555</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Triantafyllos Afouras, and Andrew Zisserman. Spot the conversation: speaker diarisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01216</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05622</idno>
		<title level="m">Voxceleb2: Deep speaker recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perfect match: Improved cross-modal embeddings for audiovisual synchronisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Whan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Goo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Look who&apos;s talking: Speaker detection using video and audio correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Retinaface</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00641</idno>
		<title level="m">Singlestage dense face localisation in the wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Taking the bite out of automated naming of characters in tv video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="545" to="559" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speaker diarization using deep neural network embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mccree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4930" to="4934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02739</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Mesh r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural predictive coding using convolutional neural networks toward unsupervised learning of speaker characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Jati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panayiotis</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1577" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On learning associations of faces and voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Hijung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Kaspar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learnable pins: Cross-modal embeddings for person identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seeing voices and hearing faces: Cross-modal biometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ava-activespeaker: An audiovisual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01342</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kevin Wilson, James Glass, and Trevor Darrell. Visual speech recognition with loosely synchronized feature streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Siracusa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Crossmodal learning for audio-visual speech event localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Somandepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04358</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised methods for speaker diarization: An integrated and iterative approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?da</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2015" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bimodal recurrent neural network for audiovisual voice activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1938" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An overview of automatic speaker diarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas A</forename><surname>Tranter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1557" to="1565" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalized end-to-end loss for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Papir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4879" to="4883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Philip Andrew Mansfield, and Ignacio Lopz Moreno. Speaker diarization with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlton</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5239" to="5243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Point clouds learning with attention-based graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13445</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning discriminative features for speaker identification and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Active learning based constrained clustering for speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2188" to="2198" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fully supervised speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aonan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Multi-task learning for audio-visual active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
