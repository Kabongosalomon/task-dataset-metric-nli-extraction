<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Learning of Action Detection from Frame Glimpses in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
							<email>serena@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
							<email>mori@cs.sfu.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Learning of Action Detection from Frame Glimpses in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and refinement: observing moments in video, and refining hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and when to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent's decision policy. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2% or less) of the video frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action detection in long, real-world videos is a challenging problem in computer vision. Algorithms must reason not only about whether an action occurs somewhere in a video, but also on the temporal extent of when it occurs. Most existing work <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b46">46]</ref> take the approach of building frame-level classifiers, running them exhaustively over a video at multiple temporal scales, and applying postprocessing such as duration priors and non-maximum suppression. However, this indirect modeling of action localization is unsatisfying in terms of both accuracy as well as computational efficiency.</p><p>In this work, we introduce an end-to-end approach to action detection that reasons directly on the temporal bounds of actions. Our key intuition ( <ref type="figure" target="#fig_0">Fig. 1)</ref> is that the process of detecting an action is one of continuous, iterative observation and refinement. Given a single or a few frame observations, a human can already formulate hypotheses about when an action may occur. We can then skip ahead or back some frames to verify, and quickly narrow down the action location (e.g. swinging a baseball bat in <ref type="figure" target="#fig_0">Fig. 1</ref>). We are able to sequentially decide where to look and how to refine our hypotheses to obtain precise localization of the action with far less exhaustive search compared to existing algorithms. Based on this intuition, we present a single coherent model that takes a long video as input, and outputs the temporal bounds of detected action instances. Our model is formulated as an agent that learns a policy for sequentially forming and refining hypotheses about action instances. Casting this into a recurrent neural network-based architecture, we train the model in a fully end-to-end fashion using a combination of backpropagation and REINFORCE <ref type="bibr" target="#b42">[42]</ref>.</p><p>Our model draws inspiration from works that have used REINFORCE to learn spatial glimpse policies for image classification and captioning <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b43">43]</ref>. However, action detection presents the additional challenge of how to handle a variable-sized set of structured detection outputs.</p><p>To address this, we present a model that decides both which frame to observe next as well as when to emit a prediction, and we introduce a reward mechanism that enables learning this policy. To the best of our knowledge, this is the first end-to-end approach for learning to detect actions in video.</p><p>We show that our model is able to reason effectively on the temporal bounds of actions, and achieve state-of-the-art performance on the THUMOS'14 <ref type="bibr" target="#b11">[11]</ref> and ActivityNet <ref type="bibr" target="#b2">[3]</ref> datasets. Moreover, because it learns policies for which frames to observe, or temporally glimpse, it is able to do so while observing only a fraction (2% or less) of the frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is a long history of work in video analysis and activity recognition <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b50">50]</ref>. For a survey we refer to Poppe <ref type="bibr" target="#b24">[24]</ref> and Weinland et al. <ref type="bibr" target="#b40">[40]</ref>. Here we review recent work relevant to temporal action detection. Temporal action detection. Canonical work in this vein is Ke et al. <ref type="bibr" target="#b14">[14]</ref>. Rohrbach et al. <ref type="bibr" target="#b27">[27]</ref> and Ni et al. <ref type="bibr" target="#b21">[21]</ref> use hand-centric and object-centric features, respectively, to detect fine-grained cooking actions in a fixed-camera kitchen setting. More related to our work is the unconstrained and untrimmed setting of the THUMOS'14 action detection dataset. Oneata et al. <ref type="bibr" target="#b22">[22]</ref>, Wang et al. <ref type="bibr" target="#b39">[39]</ref>, Karaman et al. <ref type="bibr" target="#b13">[13]</ref>, and Yuan et al. <ref type="bibr" target="#b46">[46]</ref> use fusions of dense trajectories, frame-level CNN features, and/or sound features in a sliding window framework to perform temporal action detection. Sun et al. <ref type="bibr" target="#b34">[34]</ref> uses web images as a prior to improve detection performance. Pirsiavash and Ramanan <ref type="bibr" target="#b23">[23]</ref> build grammars over complex actions and additionally detect sub-components in time.</p><p>Methods for spatio-temporal action detection have also been developed. Within the context of "unconstrained" internet videos, this includes a body of work on spatiotemporal action proposals <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b41">41]</ref>. Analysis of broader surveillance scenes for action detection is also an active area of research. Shu et al. <ref type="bibr" target="#b32">[32]</ref> reason about groups of people, Loy et al. <ref type="bibr" target="#b18">[18]</ref> across multi-camera setups, and Kwak et al. <ref type="bibr" target="#b15">[15]</ref> based on quadratic programmingbased instantiations of rules. Common among these works is reasoning on spatio-temporal action proposals or human tracks, typically using sliding window-based approaches in the temporal dimension. Furthermore, these works are in the context of trimmed or constrained-setting video clips. In contrast, we address the task of temporal action detection in untrimmed, unconstrained videos, with an efficient method for determining which frames to examine. End-to-end detection. Our goal of directly reasoning on the temporal bounds of actions shares philosophy with work in object detection that has regressed from full images to object bounds <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b25">25]</ref>. In contrast, existing action detection methods typically use exhaustive slidingwindow approaches and post-processing to produce action instances <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b46">46]</ref>. To the best of our knowledge, our work is the first to address learning of temporal action detection in an end-to-end framework. Learning task-specific policies. We draw inspiration from recent approaches that have used REINFORCE <ref type="bibr" target="#b42">[42]</ref> to learn task-specific policies. Mnih et al. <ref type="bibr" target="#b19">[19]</ref>, Ba et al. <ref type="bibr" target="#b0">[1]</ref>, and Sermanet et al. <ref type="bibr" target="#b30">[30]</ref> learn spatial attention policies for image classification, and Xu et al. <ref type="bibr" target="#b43">[43]</ref> for image caption generation. In a non-visual task, Zaremba et al. <ref type="bibr" target="#b47">[47]</ref> learn policies for a Reinforcement Learning Neural Turing Machine. Our method builds on these directions and uses REINFORCE to learn policies addressing the task of action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to take a long sequence of video and output any instances of a given action. <ref type="figure">Fig. 2</ref> shows our model structure. The model is formulated as a reinforcement learning agent that interacts with a video over time. The agent receives a sequence of video frames V = {v 1 , ..., v T } as input, and can observe a fixed proportion of the frames. It must learn to effectively utilize these observations, or frame glimpses, to reason on the temporal bounds of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>The model consists of two main components: an observation network (Sec. 3.1.1), and a recurrent network (Sec. 3.1.2). The observation network encodes visual representations of video frames. The recurrent network sequentially processes these observations and decides both which frame to observe next and when to emit a prediction. We now describe each of these in more detail. Later in Sec. 3.2, we explain how we use a combination of backpropagation and REINFORCE to train the model in end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Observation Network</head><p>As shown in <ref type="figure">Fig. 2</ref>, the observation network f o , parameterized by ? o , observes a single video frame at each timestep. It encodes the frame into a feature vector o n and provides this as input to the recurrent network.</p><p>Importantly, o n encodes information about both where in the video an observation was taken as well as what was seen. The inputs to the observation network therefore consist of the normalized temporal location of the observation, l n ? [0, 1], and the corresponding video frame v ln .</p><p>The architecture of the observation network is inspired by the spatial glimpse network of <ref type="bibr" target="#b19">[19]</ref>. Both l n and v ln are mapped to a hidden space and then combined with a fully connected layer. v ln is typically mapped with a sequence of convolutional, pooling, and fully connected layers; in our experiments we extract fc7 features from a fine-tuned VGG-16 network <ref type="bibr" target="#b33">[33]</ref> and use o n ? R 1024 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Recurrent Network</head><p>The recurrent network f h , parameterized by ? h , forms the core of the learning agent. As can be seen in <ref type="figure">Fig. 2</ref>, the input to the network at each timestep n is observation feature vector o n . The network's hidden state h n , a function of both o n and the previous hidden state h n?1 , models temporal hypotheses about action instances.</p><p>As the agent reasons on a video, three outputs are produced at each timestep: candidate detection d n , binary indicator p n signaling whether to emit d n as a prediction, and temporal location l n+1 indicating the frame to observe next. We now describe each of these in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Frames</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Predictions</head><p>Observation Network ? ? [ ] <ref type="figure">Figure 2</ref>: The input to the model is a sequence of video frames, and the output is a set of action predictions. We illustrate an example of a forward pass. At timestep n, the agent observes the orange video frame and produces candidate detection dn, however prediction indicator output pn suppresses it from being emitted into the prediction set. Observation location output ln+1 signals to observe the the green video frame at the next timestep. The process repeats, and here again pn+1 suppresses dn+1 from being emitted. ln+2 signals to now go backwards in the video to observe the blue frame. At timestep n + 2, the action hypothesis is sufficiently refined, and the agent uses prediction indicator pn+2 to emit dn+2 into the prediction set (red arrow). The agent then continues proceeding through the video.</p><p>Candidate detection. A candidate detection d n is produced using the function d n = f d (h n ; ? d ), where f d is a fully connected layer. d n is a tuple (s n , e n , c n ) ? [0, 1] 3 , where s n and e n are the normalized start and end locations of the detection, and c n is the confidence level of the detection. This candidate detection represents the agent's hypothesis surrounding a current action instance. However, it is not emitted as a prediction at each timestep, which would lead to a large amount of noise and many false positives. Instead, the agent uses a separate prediction indicator output to signal when a candidate detection should be emitted as a prediction.</p><p>Prediction indicator. The binary prediction indicator p n signals whether corresponding candidate detection d n should be emitted as a prediction. p n = f p (h n ; ? p ), where f p is a fully connected layer followed by a sigmoid nonlinearity. At training time, f p is used to parameterize a Bernoulli distribution from which p n is sampled; at test time, the maximum a posteriori estimate is used. The combination of the candidate detection and prediction indicator is crucial for the detection problem, where positive instances may occur anywhere or not at all. It enables the network to indicate when it has identified a unique action instance to add to the prediction set, and essentially folds non-maximum suppression in as a learnable component of our end-to-end framework.</p><p>Location of next observation. The temporal location l n+1 ? [0, 1] indicates the video frame that the agent chooses to observe next. This location is not constrained, and the agent may skip both forwards and backwards around a video.</p><p>The location is computed as l n+1 = f l (h n ; ? l ), where f l is a fully connected layer, such that the agent's decision is a function of its past observations and their temporal locations. At training time, l n+1 is sampled from a Gaussian distribution with a mean of f l (h n ; ? l ) and a fixed variance; at test time, the maximum a posteriori estimate is used. <ref type="figure">Fig. 2</ref> further illustrates the roles of these outputs and their interaction with an example of a forward pass through the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>Our end goal is to learn to output a set of detected actions. To achieve this, we need to train the three outputs at each step of the agent's recurrent network: candidate detection d n , prediction indicator p n , and next observation location l n+1 . Given supervision from temporal action annotations in long videos, training these involves challenges of designing suitable loss and reward functions, and handling non-differentiable model components. We now explain how we address these challenges. We use standard backpropagation to train d n , and REINFORCE to train p n and l n+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Candidate detections</head><p>Candidate detections are trained using backpropagation to maximize the correctness of each candidate. We wish to maximize correctness regardless of whether a candidate is ultimately emitted, since the candidates encode the agent's hypotheses about actions. This requires matching each candidate with a ground truth instance during training. We use the insight that at each timestep, the agent should form a hypothesis around the action instance (if any) nearest its current location in the video. This enables us to design a simple yet effective matching function.</p><p>Matching to ground truth. Given a set of candidate detections D = {d n |n = 1, ..., N } produced by a recurrent network of N timesteps, and given ground truth action instances g 1,...,M , each candidate is matched to one ground truth instance, or none if M = 0.</p><p>We define matching function</p><formula xml:id="formula_0">y nm = 1 if m = arg min j=1,...,M dist(l n , g j ) 0 otherwise (1)</formula><p>In other words, candidate d n is matched to ground truth g m if the agent's temporal location l n at timestep n is closer to g m than any other ground truth instance. Defining g m = (s m , e m ) as the start and end location of a ground truth instance, dist(l n , g m ) is simply min(|s m ? l n |, |e m ? l n |).</p><p>Loss function. Once candidate detections have been matched to ground truth instances, we optimize a multi-task classification and localization loss function over the set D:</p><formula xml:id="formula_1">L(D) = n L cls (dn) + ? n m 1[ynm = 1]L loc (dn, gm)<label>(2)</label></formula><p>Here the classification term L cls (d n ) is a standard crossentropy loss on the detection confidence c n , encouraging the confidence to be closer to 1 if detection d n is matched to a ground truth instance, and 0 otherwise.</p><p>If the detection is matched to a ground truth g m (i.e. y nm = 1), the localization term L loc (d n , g m ) is an L 2regression loss that further encourages minimizing the distance (s n , e n ) ? (s m , e m ) between the two segments.</p><p>We optimize this loss function using backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Observation and emission sequences</head><p>The observation location and prediction indicator outputs are non-differentiable components of our model that cannnot be trained with standard backpropagation. However, REINFORCE <ref type="bibr" target="#b42">[42]</ref> is a powerful approach that enables learning in non-differentiable settings. We first briefly describe REINFORCE below. We then introduce a reward function that we use with REINFORCE to learn effective policies for observation and prediction emission sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REINFORCE.</head><p>Given A, a space of action sequences, and p ? (a), a distribution over a ? A and parameterized by ?, the REINFORCE objective can be expressed as</p><formula xml:id="formula_2">J(?) = a?A p ? (a)r(a)<label>(3)</label></formula><p>Here r(a) is a reward assigned to each possible action sequence, and J(?) is the expected reward under the distribution of possible action sequences. In our case we wish to learn network parameters ? that maximize the expected reward of a sequence of location and prediction indicator outputs.</p><p>The gradient of the objective is</p><formula xml:id="formula_3">?J(?) = a?A p ? (a)? log p ? (a)r(a)<label>(4)</label></formula><p>This leads to a non-trivial optimization problem due to the high-dimensional space of possible action sequences. REINFORCE addresses this by learning network parameters using Monte Carlo sampling and an approximation to the gradient equation:</p><formula xml:id="formula_4">?J(?) ? 1 K K i=1 N n=1 ? log ? ? (a i n |h i 1:n , a i 1:n?1 )R i n<label>(5)</label></formula><p>Given an agent interacting with an environment, in our case a video, ? ? is the agent's policy. This is a learned distribution over actions conditioned on the interaction sequence thus far. At timestep n, a n is the policy's current action (e.g. location l n+1 or prediction indicator p n ), h 1:n is the history of past states including the current, and a 1:n?1 is the history of past actions. R n = N t=n r t is the cumulative future reward obtained from the current timestep onward, for a sequence of N timesteps. The approximate gradient is computed by running an agent's current policy in its environment to obtain K interaction sequences.</p><p>To reduce the variance of the gradient estimate, a baseline reward b i n is often estimated, e.g. via a separate network, and subtracted so that the gradient equation becomes:</p><formula xml:id="formula_5">?J(?) ? 1 K K i=1 N n=1 ? log ? ? (a i n |h i 1:n , a i 1:n?1 )(R i n ? b i n )<label>(6)</label></formula><p>REINFORCE learns model parameters according to this approximate gradient. The log-probability log ? ? of actions leading to high future reward are increased, and those leading to low reward are decreased. Model parameters can then be updated using backpropagation.</p><p>Reward function. Training with REINFORCE requires designing an appropriate reward function. Our goal is to learn policies for the location and prediction indicator outputs that lead to action detection with both high recall and high precision. We therefore introduce a reward function that seeks to maximize true positive detections while minimizing false positives:</p><formula xml:id="formula_6">rN = Rp if M &gt; 0 and Np = 0 N+R+ + N?R? otherwise<label>(7)</label></formula><p>All reward is provided at the N th (final) timestep, and is 0 for n &lt; N , since we want to learn policies that jointly lead to high overall detection performance. M is the number of ground truth action instances, and N p is the number of predictions emitted by the agent. N + is the number of true positive predictions, N ? is the number of false positive predictions, and R + and R ? are positive and negative rewards contributed by each of these predictions, respectively. A prediction is considered correct if its overlap with a ground truth is both greater than a threshold and higher than that of any other prediction. In order to encourage the agent not to be overly conservative, a negative reward R p is provided if the the video contains ground truth instances (M &gt; 0) but the model did not emit any predictions (N p = 0).</p><p>We use this function with REINFORCE to train the location and prediction indicator outputs, and learn observation and emission policies optimized for action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our model on two datasets -THU-MOS'14 <ref type="bibr" target="#b11">[11]</ref> and ActivityNet <ref type="bibr" target="#b2">[3]</ref>. We show that our endto-end approach enables the model to outperform state-ofthe-art results by a large margin on both datasets. Furthermore, the learned policy of frame observations is both effective and efficient; the model achieves these results while observing in total only 2% or less of the video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We learn a 1-vs-all model for each action class. In the observation network, we use a VGG-16 network <ref type="bibr" target="#b33">[33]</ref> finetuned on the dataset to extract visual features from observed video frames. Fc7-layer features are extracted and embedded with the temporal location of the frame into a 1024dimensional observation vector.</p><p>For the recurrent network, we use a 3-layer LSTM network with 1024 hidden units in each layer. Videos are downsampled to 5fps in THUMOS'14 and 1fps in Activ-ityNet, and processed in sequences of 50 frames. The agent is given a fixed number of observations for each sequence, typically 6 in our experiments. All temporal locations are normalized to [0, 1] in a video sequence. Any predictions overlapping or crossing sequence bounds are merged with a simple union rule. We learn with mini-batches of 256 sequences, and use RMSProp <ref type="bibr" target="#b37">[37]</ref> to modulate the perparameter learning rate during optimization. Other hyperparameters are learned through cross-validation. The ratio of sequences containing positive examples in each minibatch is an important hyperparameter to prevent the model from being overly conservative. Approximately one-third to one-half positive examples is typically used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">THUMOS'14 Dataset</head><p>The action detection task of THUMOS'14 <ref type="bibr" target="#b11">[11]</ref> consists of 20 classes of sports, and <ref type="table" target="#tab_0">Table 1</ref> shows results on this dataset. Since the task comprises only 20 of the 101 action classes in the dataset, we first coarsely filter the full set of test videos for these classes, using video-level average pooling over class probabilities that are computed every 300 frames (0.1 fps). We report mAP for different IOU thresholds ?, and compare with the top 3 performers on the THUMOS'14 challenge leaderboard <ref type="bibr" target="#b11">[11]</ref>. All these methods compute dense trajectories <ref type="bibr" target="#b38">[38]</ref> and/or CNN features over temporal windows, and use a sliding window approach with non-maximum suppression to obtain predictions. <ref type="bibr" target="#b13">[13]</ref> uses dense trajectories only, <ref type="bibr" target="#b39">[39]</ref> uses temporal windows of combined dense trajectories and CNN features, and <ref type="bibr" target="#b22">[22]</ref> uses temporal windows of dense trajectories with videolevel CNN classification predictions. Our model outperforms existing methods at all values of ?. The relative margin increases as we decrease ?, indicating that our model more frequently predicts actions near ground truth instances even when not precisely localized. Our model achieves these results while processing only 2% of videos frames using its learned observation policy. Ablation experiments. <ref type="table" target="#tab_0">Table 1</ref> also shows results for ablation experiments analyzing the contributions of different model components. The ablation models are as follows:</p><p>? Ours w/o d pred removes the prediction indicator output. The candidate detection at every timestep is emitted, and merged with non-maximum suppression. ? Ours w/o d obs removes the location output indicating where to observe next. Observations are instead determined by uniform sampling with the same total number of observations. ? Ours w/o d obs w/o d pred removes both the prediction indicator and location observation outputs. ? Ours w/o loc removes localization regression. All emitted detections are of median length from the training set, and centered on the currently observed frame. ? CNN with NMS removes direct prediction of temporal action bounds. Per-frame class probabilities from the VGG-16 Network <ref type="bibr" target="#b33">[33]</ref> used in our observation network are densely obtained at multiple temporal scales and aggregated with non-maximum suppression, similar to existing work <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>Ours w/o d pred obtains lower performance compared to the full model, due to many false positives. Ours w/o d obs also lowers performance since uniform sampling does not provide sufficient resolution to localize action boundaries ( <ref type="figure" target="#fig_1">Fig. 3)</ref>. Interestingly, removing d obs cripples the model more than removing d pred , highlighting the importance of the observation policy. As expected, removing both outputs in Ours w/o d obs w/o d pred decreases performance further. Ours w/o loc is the poorest performing model at ? = 0.5, even below the CNN, showing the importance of temporal regression. The relative difference with the CNN decreases and then flips when we decrease ?, indicating that <ref type="bibr" target="#b22">[22]</ref> Ours <ref type="bibr">[</ref>   the model still detects the rough location of actions but suffers from imprecise localization. Finally, the CNN with NMS achieves significantly lower performance than all ablation models except the Ours w/o loc model, quantifying the contribution of our end-to-end framework. Its performance is also in the range of but lower than <ref type="bibr" target="#b39">[39]</ref>, which uses dense trajectories <ref type="bibr" target="#b38">[38]</ref> and Imagenet-pretrained <ref type="bibr" target="#b28">[28]</ref> CNN features. This suggests that additionally incorporating motion-based features would further improve the performance of our model. As an additional baseline, we perform NMS on top of an LSTM, a standard temporal network which produces framelevel smoothing and consistency <ref type="bibr" target="#b3">[4]</ref>. The LSTM with NMS achieves lower performance than the CNN with NMS, despite adding greater temporal consistency. The main reason appears to be that increasing the temporal smoothness of frame-level class probabilities is actually harmful, not beneficial, to the task of action instance detection, where precise localization of temporal boundaries is required. Finally, we experimented with different numbers of observations per video sequence, e.g. 4, 8, and 10. Detection performance was not substantially different across this range. This is consistent with other work on CNNs for action recognition using max-pooling <ref type="bibr" target="#b48">[48]</ref>, highlighting the importance of learning effective frame observation policies. Per-class breakdown. <ref type="table" target="#tab_2">Table 2</ref> shows the per-class AP breakdown of our model, and comparison with the top performer <ref type="bibr" target="#b22">[22]</ref> on the THUMOS'14 leaderboard. Our model outperforms <ref type="bibr" target="#b22">[22]</ref> on 12 out of 20 classes. Notably, it shows significant improvement on some of the most challenging classes in the dataset such as basketball dunk, diving, and frisbee catch. <ref type="figure" target="#fig_2">Fig. 4</ref> shows examples of our model's predictions, including several from these challenging classes. The model's ability to reason holistically on action extents enables it to infer temporal boundaries even when frame appearance is challenging: e.g. similar pose and environment, or abrupt scene change in the second diving example. Observation policy analysis. <ref type="figure" target="#fig_3">Fig. 5</ref> shows examples of ob-servation policies that our model learns, as well as accompanying predictions. For reference, we also show framelevel CNN probabilities from the VGGNet used in our observation network, to provide insight into frame-level signal for the action. In the top example of a javelin throw, the model begins to take more frequent observations once the person begins running. Near the end boundary of the action, it takes a step backwards to refine its hypothesis and then emits a prediction before moving on.</p><p>The lower example of diving is a challenging case where two action instances occur in quick succession. While the strength of the frame-level CNN probabilities over the sequence would be difficult for standard sliding-window approaches to handle, our model is able to discern the two separate instances. The model once again takes steps backwards to refine its prediction, including once (frame 93) when motion blur makes it difficult to discern much from the frame. However, the predictions are also somewhat longer than the ground truth, and upon observing its first frame of the second instance (frame 101), the model immediately emits a prediction of comparable but slightly longer duration than the first. This suggests that the model may have learned duration priors that, while generally beneficial, were overly strong in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ActivityNet Dataset</head><p>The ActivityNet action detection dataset <ref type="bibr" target="#b2">[3]</ref> consists of 68.8 hours of temporal annotations in 849 hours of untrimmed, unconstrained video. There are 1.41 action instances per video and 193 instances per class. <ref type="table" target="#tab_4">Tables 3  and 4</ref> show per-class and mAP performance on the "Playing sports" and "Work, main job" subsets of ActivityNet, respectively. Evaluation uses the ActivityNet validation set, and hyperparameters are cross-validated on the training set.</p><p>Our model outperforms existing work <ref type="bibr" target="#b2">[3]</ref>, which is based on a combination of dense trajectories, SIFT, and ImageNet-pretrained CNN features, by significant margins. It outperforms <ref type="bibr" target="#b2">[3]</ref> in 13 out of 21 classes on the Sports subset and in 10 out of 15 classes on the Work subset. The improvement is particularly large on the Work subset. This is partially attributable to the fact that work activities are generally less well-defined and have less discriminative movements. In the example sequence of the Organizing Boxes action in <ref type="figure" target="#fig_4">Fig. 6</ref>, this is evident in the weaker, more diffuse frame-level CNN probabilities for the action. While this creates a challenge for approaches that rely on postprocessing, our model's direct reasoning on action extents enables it to still produce strong predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In conclusion, we have introduced an end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet <ref type="bibr" target="#b2">[3]</ref> Ours <ref type="bibr">[</ref>   action detection datasets while observing only a fraction of frames. A direction for future work is to extend our framework to learn joint spatio-temporal observation policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>We would like to thank Juan Carlos Niebles and Fabian Caba Heilbron for help with ActivityNet baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Action detection is a process of observation and refinement. Effectively choosing a sequence of frame observations allows us to quickly narrow down when the baseball swing occurs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of Our full model with the Ours w/o d obs model. Refer to Fig. 5 caption for explanation of figure structure and color scheme. Each model's observed frames are shown in green, and the prediction extent in red. Allowing the model to choose which frames to observe enables the necessary resolution to reason precisely on action bounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Examples of predicted action instances on THU-MOS'14. Each row shows sampled frames during or just outside the temporal extent of a detected action. Faded frames indicate location outside the detection and illustrate localization ability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Examples of learned observation policies on THUMOS'14. The top example shows a javelin throw and the bottom example shows diving. Observed frames are colored in green and labeled with the frame index. Prediction extents are shown in red, and ground truth in grey. For reference, we also show frame-level CNN probabilities from the VGGNet used in our observation network; higher intensity indicates higher probability and provides insight into frame-level signal for the class. Dashed arrows indicate the observation sequence, and red arrows indicate frames where a prediction was emitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Example of a learned observation policy on the Work subset of ActivityNet. The action is Organizing Boxes. Refer to Fig. 5 for explanation of figure structure and color scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Action detection results on THUMOS'14. Comparison with the top 3 performers on the THUMOS'14 challenge leaderboard is shown, as well as with ablation models. mAP is reported for different intersection-over-union (IOU) thresholds ?.</figDesc><table><row><cell></cell><cell>?=0.5</cell><cell>?=0.4</cell><cell>?=0.3</cell><cell>?=0.2</cell><cell>?=0.1</cell></row><row><cell>Karaman et al. [13]</cell><cell>0.9</cell><cell>1.4</cell><cell>2.1</cell><cell>3.4</cell><cell>4.6</cell></row><row><cell>Wang et al. [39]</cell><cell>8.3</cell><cell>11.7</cell><cell>14.0</cell><cell>17.0</cell><cell>18.2</cell></row><row><cell>Oneata et al. [22]</cell><cell>14.4</cell><cell>20.8</cell><cell>27.0</cell><cell>33.6</cell><cell>36.6</cell></row><row><cell>Ours (full)</cell><cell>17.1</cell><cell>26.4</cell><cell>36.0</cell><cell>44.0</cell><cell>48.9</cell></row><row><cell></cell><cell cols="2">Ablation Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours w/o d pred</cell><cell>12.4</cell><cell>19.3</cell><cell>26.0</cell><cell>32.5</cell><cell>37.0</cell></row><row><cell>Ours w/o d obs</cell><cell>9.3</cell><cell>15.2</cell><cell>20.6</cell><cell>26.5</cell><cell>31.2</cell></row><row><cell>Ours w/o d obs w/o d pred</cell><cell>8.6</cell><cell>14.6</cell><cell>20.0</cell><cell>27.1</cell><cell>33.3</cell></row><row><cell>Ours w/o loc</cell><cell>5.5</cell><cell>9.9</cell><cell>16.2</cell><cell>22.7</cell><cell>27.5</cell></row><row><cell>CNN with NMS</cell><cell>6.4</cell><cell>9.6</cell><cell>12.8</cell><cell>16.7</cell><cell>18.5</cell></row><row><cell>LSTM with NMS</cell><cell>5.6</cell><cell>7.8</cell><cell>10.3</cell><cell>13.9</cell><cell>15.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Per-class breakdown (AP) on THUMOS'14, at IOU of ? = 0.5.</figDesc><table><row><cell>Baseball</cell><cell>Pitch</cell></row><row><cell>Baseball</cell><cell>Pitch</cell></row><row><cell>Basketball</cell><cell>Dunk</cell></row><row><cell>Basketball</cell><cell>Dunk</cell></row><row><cell cols="2">Diving</cell></row><row><cell cols="2">Diving</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Per-class breakdown and mAP on the ActivityNet Sports subset, at IOU of ? = 0.5.</figDesc><table><row><cell></cell><cell>[3]</cell><cell>Ours</cell><cell></cell><cell>[3]</cell><cell>Ours</cell></row><row><cell>Attend Conf.</cell><cell>28.3</cell><cell>56.5</cell><cell>Phoning</cell><cell>34.7</cell><cell>52.1</cell></row><row><cell>Search Security</cell><cell>24.5</cell><cell>33.9</cell><cell>Pumping Gas</cell><cell>54.7</cell><cell>34.0</cell></row><row><cell>Buy Fast Food</cell><cell>34.4</cell><cell>45.8</cell><cell>Setup Comp.</cell><cell>37.4</cell><cell>30.3</cell></row><row><cell>Clean Laptop Fan</cell><cell>26.0</cell><cell>35.8</cell><cell>Sharp. Knife</cell><cell>36.3</cell><cell>35.2</cell></row><row><cell>Making Copies</cell><cell>18.2</cell><cell>41.7</cell><cell>Sort Books</cell><cell>29.3</cell><cell>16.7</cell></row><row><cell>Organizing Boxes</cell><cell>29.6</cell><cell>19.1</cell><cell>Using Comp.</cell><cell>37.4</cell><cell>50.2</cell></row><row><cell>Organiz. Cabin.</cell><cell>19.0</cell><cell>43.7</cell><cell>Using ATM</cell><cell>29.5</cell><cell>64.9</cell></row><row><cell>Packing</cell><cell>28.0</cell><cell>39.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP</cell><cell></cell><cell></cell><cell></cell><cell>31.1</cell><cell>39.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Per-class breakdown and mAP on the ActivityNet Work subset, at IOU of ? = 0.5.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1395" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2155" to="2162" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Fast r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.6031</idno>
		<title level="m">Finding action tubes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2012" to="2019" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient feature extraction, encoding, and classification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2593" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-agent event detection: Localization and role assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative figure-centric models for joint action localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2003" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsza?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incremental activity modeling in multiple disjoint cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1799" to="1813" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recognizing multitasked activities from video using stochastic context-free grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple granularity analysis for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Paramathayalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The lear submission at thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A survey on vision-based human action recognition. IVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="976" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7054</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning temporal sequence model from partially labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1631" to="1638" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint inference of groups, events and human roles in aerial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00983</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">high-quality object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2642" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>2012. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of visionbased methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01929</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A hough transform-based voting framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2061" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1302" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adsc submission at thumos challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00521</idno>
		<title level="m">Reinforcement learning neural turing machines</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Exploiting image-trained cnn architectures for unconstrained video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04144</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Detecting unusual activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<meeting>the 2004 IEEE Computer Society Conference on</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">819</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06724</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
