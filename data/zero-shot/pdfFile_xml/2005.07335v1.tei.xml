<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Image HDR Reconstruction Using a CNN with Masked Features and Perceptual Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07">2020. July 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">Santana</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centro de Inform?tica</orgName>
								<orgName type="department" key="dep2">Centro de Inform?tica</orgName>
								<orgName type="institution" key="instit1">Universidade Federal de Pernambuco TSANG ING REN</orgName>
								<orgName type="institution" key="instit2">Universidade Federal de Pernambuco NIMA KHADEMI KALANTARI</orgName>
								<orgName type="institution" key="instit3">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">Santana</forename><surname>Santos</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centro de Inform?tica</orgName>
								<orgName type="institution">Universidade Federal</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><forename type="middle">Khademi</forename><surname>Kalantari</surname></persName>
							<email>nimak@tamu.edu.</email>
							<affiliation key="aff2">
								<orgName type="department">Tsang Ing Ren, Centro de Inform?tica, Universi-dade Federal de Pernambuco</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">Santana</forename><surname>Santos</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Texas A&amp;M</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ing</forename><surname>Tsang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Texas A&amp;M</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ren</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Texas A&amp;M</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalantari</forename><surname>Khademi</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Texas A&amp;M</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single Image HDR Reconstruction Using a CNN with Masked Features and Perceptual Loss</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Trans. Graph</title>
						<imprint>
							<biblScope unit="volume">39</biblScope>
							<biblScope unit="issue">4</biblScope>
							<date type="published" when="2020-07">2020. July 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3386569.3392403</idno>
					<note>Authors&apos; addresses: This is the author&apos;s version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in ACM Transactions on Graphics, https:// ACM Reference Format:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Computational photography Additional Key Words and Phrases: high dynamic range imaging</term>
					<term>con- volutional neural network</term>
					<term>feature masking</term>
					<term>perceptual loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>regions that are simple to reconstruct, we propose a sampling strategy to select challenging training patches during the HDR fine-tuning stage. We demonstrate through experimental results that our approach can reconstruct visually pleasing HDR results, better than the current state of the art on a wide range of scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. We propose a novel deep learning system for single image HDR reconstruction by synthesizing visually pleasing details in the saturated areas. We introduce a new feature masking approach that reduces the contribution of the features computed on the saturated areas, to mitigate halo and checkerboard artifacts. To synthesize visually pleasing textures in the saturated regions, we adapt the VGG-based perceptual loss function to the HDR reconstruction application. Furthermore, to effectively train our network on limited HDR training data, we propose to pre-train the network on inpainting task. Our method can reconstruct regions with high luminance, such as the bright highlights of the windows (red inset), and generate visually pleasing textures (green insert). See <ref type="figure">Figure 7</ref> for comparison against several other approaches. All images have been gamma corrected for display purposes.</p><p>Digital cameras can only capture a limited range of real-world scenes' luminance, producing images with saturated pixels. Existing single image high dynamic range (HDR) reconstruction methods attempt to expand the range of luminance, but are not able to hallucinate plausible textures, producing results with artifacts in the saturated areas. In this paper, we present a novel learning-based approach to reconstruct an HDR image by recovering the saturated pixels of an input LDR image in a visually pleasing way. Previous deep learning-based methods apply the same convolutional filters on wellexposed and saturated pixels, creating ambiguity during training and leading to checkerboard and halo artifacts. To overcome this problem, we propose a feature masking mechanism that reduces the contribution of the features from the saturated areas. Moreover, we adapt the VGG-based perceptual loss function to our application to be able to synthesize visually pleasing textures. Since the number of HDR images for training is limited, we propose to train our system in two stages. Specifically, we first train our system on a large number of images for image inpainting task and then fine-tune it on HDR reconstruction. Since most of the HDR examples contain smooth</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The illumination of real-world scenes is high dynamic range, but standard digital cameras sensors can only capture a limited range of luminance. Therefore, these cameras typically produce images with <ref type="bibr">ACM Trans. Graph.,</ref><ref type="bibr">Vol. 39,</ref><ref type="bibr">No. 4</ref>, Article 80. Publication date: July 2020. arXiv:2005.07335v1 [eess.IV] 15 May 2020 <ref type="bibr">80:2 ? Marcel Santana Santos,</ref><ref type="bibr">Tsang Ing Ren,</ref><ref type="bibr">and Nima Khademi Kalantari</ref> under/over-exposed areas. A large number of approaches propose to generate a high dynamic range (HDR) image by combining a set of low dynamic range images (LDR) of the scene at different exposures <ref type="bibr" target="#b4">[Debevec and Malik 1997]</ref>. However, these methods either have to handle the scene motion <ref type="bibr" target="#b18">[Hu et al. 2013;</ref><ref type="bibr" target="#b19">Kalantari and Ramamoorthi 2017;</ref><ref type="bibr" target="#b20">Kang et al. 2003;</ref><ref type="bibr" target="#b35">Oh et al. 2014;</ref><ref type="bibr" target="#b39">Sen et al. 2012;</ref><ref type="bibr" target="#b43">Wu et al. 2018]</ref> or require specialized bulky and expensive optical systems <ref type="bibr" target="#b32">[McGuire et al. 2007;</ref><ref type="bibr" target="#b41">Tocci et al. 2011]</ref>. Single image dynamic range expansion approaches avoid these limitations by reconstructing an HDR image using one image. These approaches can work with images captured with any standard camera or even recover the full dynamic range of legacy LDR content. As a result, they have attracted considerable attention in recent years.</p><p>Several existing methods extrapolate the light intensity using heuristic rules <ref type="bibr" target="#b0">[Banterle et al. 2006;</ref><ref type="bibr" target="#b2">Bist et al. 2017;</ref><ref type="bibr" target="#b37">Rempel et al. 2007</ref>], but are not able to properly recover the brightness of saturated areas as they do not utilize context. On the other hand, recent deep learning approaches <ref type="bibr" target="#b8">[Eilertsen et al. 2017;</ref><ref type="bibr" target="#b10">Endo et al. 2017;</ref><ref type="bibr" target="#b25">Lee et al. 2018a</ref>] systematically utilize contextual information using convolutional neural networks (CNNs) with large receptive fields. However, these methods usually produce results with blurriness, checkerboard, and halo artifacts in saturated areas.</p><p>In this paper, we propose a novel learning-based technique to reconstruct an HDR image by recovering the missing information in the saturated areas of an LDR image. We design our approach based on two main observations. First, applying the same convolutional filters on well-exposed and saturated pixels, as done in previous approaches, results in ambiguity during training and leads to checkerboard and halo artifacts. Second, using simple pixel-wise loss functions, utilized by most existing approaches, the network is unable to hallucinate details in the saturated areas, producing blurry results. To address these limitations, we propose a feature masking mechanism that reduces the contribution of features generated from the saturated content by multiplying them to a soft mask. With this simple strategy, we are able to avoid checkerboard and halo artifacts as the network only relies on the valid information of the input image to produce the HDR image. Moreover, inspired by image inpainting approaches, we leverage the VGG-based perceptual loss function, introduced by Gatys et al. <ref type="bibr">[2016]</ref>, and adapt it to the HDR reconstruction task. By minimizing our proposed perceptual loss function during training, the network can synthesize visually realistic textures in the saturated areas.</p><p>Since a large number of HDR images, required for training a deep neural network, are currently not available, we perform the training in two stages. In the first stage, we train our system on a large set of images for the inpainting task. During this process, the network leverages a large number of training samples to learn an internal representation that is suitable for synthesizing visually realistic texture in the incomplete regions. In the next step, we fine-tune this network on the HDR reconstruction task using a set of simulated LDR and their corresponding ground truth HDR images. Since most of the HDR examples contain smooth regions that are simple to reconstruct, we propose a simple method to identify the textured patches and only use them for fine-tuning.</p><p>Our approach can reconstruct regions with high luminance and hallucinate textures in the saturated areas, as shown in <ref type="figure">Figure 1</ref>. We demonstrate that our approach can produce better results than the state-of-the-art methods both on simulated images ( <ref type="figure">Figure 7</ref>) and on images taken with real-world cameras ( <ref type="figure">Figure 9</ref>). In summary, we make the following contributions:</p><p>(1) We propose a feature masking mechanism to avoid relying on the invalid information in the saturated regions (Section 3.1). This masking approach significantly reduces the artifacts and improves the quality of the final results ( <ref type="figure">Figure 10</ref>). (2) We adapt the VGG-based perceptual loss function to the HDR reconstruction task (Section 3.2). Compared to pixel-wise loss functions, our loss can better reconstruct sharp textures in the saturated regions ( <ref type="figure">Figure 12</ref>). (3) We propose to pre-train the network on inpainting before fine-tuning it on HDR generation (Section 3.3). We demonstrate that the pre-training stage is essential for synthesizing visually pleasing textures in the saturated areas ( <ref type="figure">Figure 11</ref>). (4) We propose a simple strategy for identifying the textured HDR areas to improve the performance of training (Section 3.4). This strategy improves the network ability to reconstruct sharp details ( <ref type="figure">Figure 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The problem of single image HDR reconstruction, also known as inverse tone-mapping <ref type="bibr" target="#b0">[Banterle et al. 2006</ref>], has been extensively studied in the last couple of decades. However, this problem remains a major challenge as it requires recovering the details from regions with missing content. In this section, we discuss the existing techniques by classifying them into two categories of non-learning and learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Non-learning Methods</head><p>Several approaches propose to perform inverse tone-mapping using global operators. <ref type="bibr" target="#b24">Landis [2002]</ref> applies a linear or exponential function to the pixels of the LDR image above a certain threshold. <ref type="bibr" target="#b2">Bist et al. [2017]</ref> approximates tone expansion by a gamma function. They use the characteristics of the human visual system to design the gamma curve. <ref type="bibr" target="#b28">Luzardo et al. [2018]</ref> improve the brightness of the result by utilizing an operator based on the mid-level mapping. A number of techniques propose to handle this application through local heuristics. <ref type="bibr" target="#b0">Banterle et al. [2006]</ref> use median-cut <ref type="bibr" target="#b3">[Debevec 2005</ref>] to find areas with high luminance. They then generate an expandmap to extend the range of luminance in these areas, using an inverse operator. <ref type="bibr" target="#b37">Rempel et al. [2007]</ref> also utilize an expand-map but use a Gaussian filter followed by an edge-stopping function to enhance the brightness of saturated areas. <ref type="bibr" target="#b23">Kovaleski and Oliveira [2014]</ref> extend the approach by Rempel et al. <ref type="bibr">[2007]</ref> using a cross bilateral filter. These approaches simply extrapolate the light intensity by using heuristics and, thus, often fail to recover saturated highlights, introducing unnatural artifacts.</p><p>A few approaches propose to handle this application by incorporating user interactions in their system. <ref type="bibr" target="#b5">Didyk et al. [2008]</ref> enhance bright luminous objects in video sequences by using a semiautomatic classifier to classify saturated regions as lights, reflections, or diffuse surfaces. <ref type="bibr" target="#b42">Wang et al. [2007]</ref> recover the textures in the saturated areas by transferring details from the user-selected regions. Their approach demands user interactions that take several minutes, even for an expert user. In contrast to these methods, we propose a learning-based approach to systematically reconstruct HDR images from a wide range of different scenes, instead of relying on heuristics strategies and user inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning-based Methods</head><p>In recent years, several approaches have proposed to tackle this application using deep convolutional neural networks (CNN). Given a single input LDR image, <ref type="bibr" target="#b10">Endo et al. [2017]</ref> use an auto-encoder [Hinton and Salakhutdinov 2006] to generate a set of LDR images with different exposures. These images are then combined to reconstruct the final HDR image. <ref type="bibr" target="#b25">Lee et al. [2018a]</ref> chain a set of CNNs to sequentially generate the bracketed LDR images. Later, they propose <ref type="bibr" target="#b26">[Lee et al. 2018b</ref>] to handle this application through a recursive conditional generative adversarial network (GAN) <ref type="bibr" target="#b14">[Goodfellow et al. 2014</ref>] combined with a pixel-wise l 1 loss.</p><p>In contrast to these approaches, a few methods <ref type="bibr" target="#b8">[Eilertsen et al. 2017;</ref><ref type="bibr" target="#b31">Marnerides et al. 2018;</ref><ref type="bibr" target="#b46">Yang et al. 2018</ref>] directly reconstruct the HDR image without generating bracketed images. Eilertsen et al.</p><p>[2017] use a network with U-Net architecture to predict the values of the saturated areas, whereas linear non-saturated areas are obtained from the input. <ref type="bibr" target="#b31">Marnerides et al. [2018]</ref> present a novel dedicated architecture for end-to-end image expansion. <ref type="bibr" target="#b46">Yang et al. [2018]</ref> reconstruct HDR image for image correction application. They train a network for HDR reconstruction to recover the missing details from the input LDR image, and then a second network transfers these details back to the LDR domain.</p><p>While these approaches produce state-of-the-art results, their synthesized images often contains halo and checkerboard artifacts and lacks textures in the saturated areas. This is mainly because of using standard convolutional layers and pixel-wise loss functions. Note that, several recent methods <ref type="bibr" target="#b21">[Kim et al. 2019;</ref><ref type="bibr" target="#b26">Lee et al. 2018b;</ref><ref type="bibr" target="#b34">Ning et al. 2018;</ref><ref type="bibr" target="#b44">Xu et al. 2019</ref>] use adversarial loss instead of pixelwise loss functions, but they still do not demonstrate results with high-quality textures. This is potentially because the problem of HDR reconstruction is constrained in the sense that the synthesized content should properly fit the input image using a soft mask. Unfortunately, GANs are known to have difficulty handling these scenarios <ref type="bibr" target="#b1">[Bau et al. 2019]</ref>. In contrast, we propose a feature masking strategy and a more constrained VGG-based perceptual loss to effectively train our network and produce results with visually pleasing textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Our goal is to reconstruct an HDR image from a single LDR image by recovering the missing information in the saturated highlights. We achieve this using a convolutional neural network (CNN) that takes an LDR image as the input and estimates the missing HDR information in the bright regions. We compute the final HDR image by combining the well-exposed content of the input image and the output of the network in the saturated areas. Formally, we reconstruct the final HDR image? , as follows:</p><formula xml:id="formula_0">H = M ? T ? + (1 ? M) ? [exp(? ) ? 1],<label>(1)</label></formula><formula xml:id="formula_1">1 1 x ? ?(x) 0 Fig. 2.</formula><p>We use this function to measure how well-exposed a pixel is. The value 1 indicates that the pixel is well-exposed, while 0 is assigned to the pixels that are fully saturated. In our implementation, we set the threshold ? = 0.96.</p><p>where the ? = 2.0 is used to transform the input image to the linear domain, and ? denotes element-wise multiplication. Here, T is the input LDR image in the range [0, 1],? is the network output in the logarithmic domain (Section 3.2), and M is a soft mask with values in the range [0, 1] that defines how well-exposed each pixel is. We obtain this mask by applying the function ?(?) (see <ref type="figure">Figure 2</ref>) to the input image, i.e., M = ?(T ). In the following sections, we discuss our proposed feature masking approach, loss function, as well as the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Masking</head><p>Standard convolutional layers apply the same filter to the entire image to extract a set of features. This is reasonable for a wide range of applications, such as image super-resolution <ref type="bibr" target="#b6">[Dong et al. 2015]</ref>, style transfer <ref type="bibr" target="#b12">[Gatys et al. 2016]</ref>, and image colorization <ref type="bibr" target="#b48">[Zhang et al. 2016]</ref>, where the entire image contains valid information. However, in our problem, the input LDR image contains invalid information in the saturated areas. Since meaningful features cannot be extracted from the saturated contents, na?ve application of standard convolution introduces ambiguity during training and leads to visible artifacts ( <ref type="figure">Figure 10</ref>).</p><p>We address this problem by proposing a feature masking mechanism ( <ref type="figure">Figure 3</ref>) that reduces the magnitude of the features generated from the invalid content (saturated areas). We do this by multiplying the feature maps in each layer by a soft mask, as follows:</p><formula xml:id="formula_2">Z l = X l ? M l ,<label>(2)</label></formula><p>where X l ? R H ?W ?C is the feature map of layer l with height H , width W , and C channels. M l ? [0, 1] H ?W ?C is the mask for layer l and has values in the range [0, 1]. The value of one indicates that the features are computed from valid input pixels, while zero is assigned to the features that are computed from invalid pixels. Here, l = 1 refers to the input layer and, thus, X l =1 is the input LDR image.</p><p>Similarly, M l =1 is the input mask M = ?(T ). Note that, since our masks are soft, weak signals in the saturated areas are not discarded using this strategy. In fact, by suppressing the invalid pixels, these weak signals can propagate through the network more effectively.</p><p>Once the features of the current layer l are masked, the features in the next layer X l +1 are computed as usual:</p><formula xml:id="formula_3">X l +1 = ? l (W l * Z l + b l ),<label>(3)</label></formula><p>where W l and b l refer to the weight and bias of the current layer, respectively. Moreover, ? l is the activation function and * is the standard convolution operation. We compute the masks at each layer by applying the convolutional filter to the masks at the previous layer (See <ref type="figure">Figure 4</ref> for visualization of some of the masks). The basic idea is that since the features are computed by applying a series of convolutions, the same filters can be used to compute the contribution of the valid pixels <ref type="figure">Fig. 3</ref>. Illustration of the proposed feature masking mechanism. The features at each layer are multiplied with the corresponding mask before going through the convolution process. The masks at each layer are obtained by updating the masks at the previous layer using Eq. 4.  <ref type="figure">Fig. 4</ref>. On the left, we show the input image and the corresponding mask. On the right, we visualize a few masks at different layers of the network. Note that, as we move deeper through the network, the masks become blurrier and more uniform. This is expected since the receptive field of the features become larger in the deeper layers.</p><p>in the features. However, since the masks are in the range [0, 1] and measure the percentage of the contributions, the magnitude of the filters is irrelevant. Therefore, we normalize the filter weights before convolving them with the masks as follows:</p><formula xml:id="formula_4">M l +1 = |W l | ?W l ? 1 + ? * M l ,<label>(4)</label></formula><p>where ? ? ? 1 is the l 1 function and | ? | is the absolute operator. Here, |W l | is a R H ?W ?C tensor and ?W l ? 1 is a R 1?1?C tensor. To perform the division, we replicate the values of ?W l ? 1 to obtain a tensor with the same size as |W l |. The constant ? is a small value to avoid division by 0 (10 ?6 in our implementation). Note that a couple of recent approaches have proposed strategies to overcome similar issues in image inpainting <ref type="bibr" target="#b27">[Liu et al. 2018;</ref><ref type="bibr" target="#b47">Yu et al. 2019</ref>]. Specifically, <ref type="bibr" target="#b27">Liu et al. [2018]</ref> propose to modify the convolution process to only apply the filter to the pixels with valid information. Unfortunately, this approach is specially designed for cases with binary masks. However, the masks in our application are soft and, thus, this method is not applicable. <ref type="bibr" target="#b47">Yu et al. [2019]</ref> propose to multiply the features at each layer with a soft mask, similar to our feature masking strategy. The key difference is that their mask at each layer is learnable, and it is estimated using a small network from the features in the previous layer. Because of the additional parameters and complexity, training this approach on limited HDR images is difficult. Therefore, this approach is not able to produce high-quality HDR images (see Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Function</head><p>The choice of the loss function is critical in each learning system. Our goal is to reconstruct an HDR image by synthesizing plausible textures in the saturated areas. Unfortunately, using only pixel-wise loss functions, as utilized by most previous approaches, the network tends to produce blurry images <ref type="figure">(Figure 12)</ref>. Inspired by the recent image inpainting approaches <ref type="bibr" target="#b15">[Han et al. 2019;</ref><ref type="bibr" target="#b27">Liu et al. 2018;</ref><ref type="bibr" target="#b45">Yang et al. 2017]</ref>, we train our network using a VGG-based perceptual loss function. Specifically, our loss function is a combination of an HDR reconstruction loss L r and a perceptual loss L p , as follows:</p><formula xml:id="formula_5">L = ? 1 L r + ? 2 L p<label>(5)</label></formula><p>where ? 1 = 6.0 and ? 2 = 1.0 in our implementation.</p><p>Reconstruction Loss: The HDR reconstruction loss is a simple pixel-wise l 1 distance between the output and ground truth images in the saturated areas. Since the HDR images could potentially have large values, we define the loss in the logarithmic domain. Given the estimated HDR image? (in the log domain) and the linear ground truth image H , the reconstruction loss is defined as:</p><formula xml:id="formula_6">L r = ?(1 ? M) ? (? ? log(H + 1))? 1 .<label>(6)</label></formula><p>The multiplication by (1 ? M) ensures that the loss is computed in the saturated areas.</p><p>Perceptual Loss: Our perceptual term is a combination of the VGG and style loss functions as follows:</p><formula xml:id="formula_7">L p = ? 3 L v + ? 4 L s .<label>(7)</label></formula><p>In our implementation, we set ? 3 = 1.0 and ? 4 = 120.0. The VGG loss function L v evaluates how well the features of the reconstructed image match with the features extracted from the ground truth. This allows the model to produce textures that are perceptually similar to the ground truth. This loss term is defined as follows:</p><formula xml:id="formula_8">L v = l ?? l (T (H )) ? ? l (T (H ))? 1<label>(8)</label></formula><p>where ? l is the feature map extracted from the l th layer of the VGG network. Moreover, the imageH is obtained by combining the information of the ground truth H in the well-exposed regions and the content of the network's output? in the saturated areas using the mask M, as follows:</p><formula xml:id="formula_9">H = M ? H + (1 ? M) ?? .<label>(9)</label></formula><p>We useH in our loss functions to ensure that the supervision is only provided in the saturated areas. Finally, T (?) in Eq. 8 is a function that compresses the range to [0, 1]. Specifically, we use the differentiable ?-law range compressor:</p><formula xml:id="formula_10">T (H ) = log(1 + ?H ) log(1 + ?) ,<label>(10)</label></formula><p>where ? is a parameter defining the amount of compression (? = 500 in our implementation). This is done to ensure that the input to the VGG network is similar to the ones that it has been trained on. </p><formula xml:id="formula_11">L s = l ?G l (T (H )) ? G l (T (H ))? 1 ,<label>(11)</label></formula><p>where G l (X ) is the Gram matrix of the features in layer l and is defined as follows:</p><formula xml:id="formula_12">G l (X ) = 1 K l ? l (X ) T ? l (X ).<label>(12)</label></formula><p>Here, K l is a normalization factor computed as C l H l W l . Note that, the feature ? l is a matrix of shape (H l W l ) ? C l and, thus, the Gram matrix has a size of C l ?C l . In our implementation, we use the VGG-19 <ref type="bibr" target="#b40">[Simonyan and Zisserman 2015]</ref> network and extract features from layers pool1, pool2 and pool3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inpainting Pre-training</head><p>Training our system is difficult as large-scale HDR image datasets are currently not available. Existing techniques <ref type="bibr" target="#b8">[Eilertsen et al. 2017]</ref> overcome this limitation by pre-training their network on simulated HDR images that are created from standard image datasets like the MIT Places <ref type="bibr" target="#b49">[Zhou et al. 2014</ref>]. They then fine-tune their network on real HDR images. Unfortunately, our network is not able to learn to synthesize plausible textures with this strategy (see <ref type="figure">Figure 11</ref>), as the saturated areas are typically in the bright and smooth regions.</p><p>To address this problem, we propose to pre-train our network on image inpainting tasks. Intuitively, during inpainting, our network leverages a large number of training data to learn an appropriate internal representation that is capable of synthesizing visually pleasing textures. In the HDR fine-tuning stage, the network adapts the learned representation to the HDR domain to be able to synthesize HDR textures. We follow <ref type="bibr">Liu et al. 's approach [2018]</ref> and use their loss function and mask generation strategy during pre-training. Note that we still use our feature masking mechanism for pre-training, but the input masks are binary. We fine-tune the network on real HDR images using the loss function, discussed in Section 3.2.</p><p>One major problem is that the majority of the bright areas in the HDR examples are smooth and textureless. Therefore, during fine-tuning, the network adapts to these types of patches and, as a result, has difficulty producing textured results (see <ref type="figure">Figure 11</ref>). In the next section, we discuss our strategy to select textured and challenging patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Patch Sampling</head><p>Our goal is to select the patches that contain texture in the saturated areas. We perform this by first computing a score for each patch and then choosing the patches with a high score. The main challenge here is finding a good metric that properly detects the textured patches. One way to do this is to compute the average of the gradient magnitude in the saturated regions. However, since our images are in HDR and can have large values, this approach can detect a smooth region with bright highlights as textured.</p><p>To avoid this issue, we propose to first decompose the HDR image into base and detail layers using a bilateral filter <ref type="bibr" target="#b7">[Durand and Dorsey 2002]</ref>. We use the average of the gradients (Sobel operator) of the detail layer in the saturated areas as our metric to detect the textured patches. We consider all the patches with a mean gradient above a certain threshold (0.85 in our implementation) as textured, and the rest are classified as smooth. Since the detail layer only contains variations around the base layer, this metric can effectively measure the amount of textures in an HDR patch. <ref type="figure" target="#fig_0">Figure 5</ref> shows example of patches selected using this metric. As shown in <ref type="figure">Figure 11</ref>, this simple patch sampling approach is essential for synthesizing HDR images with sharp and artifact-free details in the saturated areas. The summary of our patch selection strategy is listed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>Architecture. We use a network with U-Net architecture <ref type="bibr" target="#b38">[Ronneberger et al. 2015]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 6</ref>. We use the feature masking strategy in all the convolutional layers and up-sample the  <ref type="figure">Fig. 7</ref>. We compare our method against state-of-the-art approaches of <ref type="bibr" target="#b10">Endo et al. [2017]</ref>, <ref type="bibr" target="#b8">Eilertsen et al. [2017]</ref>, and <ref type="bibr" target="#b31">Marnerides et al. [2018]</ref> on a diverse set of synthetic scenes. Our method is able to synthesize textures in the saturated areas better than the other approaches (rows one to four), while producing results with similar or better quality in the bright highlights (fifth row).</p><p>features in the decoder using nearest neighbor. All the encoder layers use Leaky ReLU activation function <ref type="bibr" target="#b29">[Maas et al. 2013</ref>]. On the other hand, we use ReLU <ref type="bibr" target="#b33">[Nair and Hinton 2010]</ref> in all the decoder layers, with the exception of the last one, which has a linear activation function. We use skip connections between all the encoder layers and their corresponding decoder layers.</p><p>Dataset. We use different datasets for each training step. For the image inpainting step, we use the MIT Places <ref type="bibr" target="#b49">[Zhou et al. 2014]</ref> dataset with the original train, test, and validation splits. We choose Places for this step because it contains a large number of scenes (? 2.5M images) with diverse textures. We use the method of <ref type="bibr" target="#b27">Liu et al. [2018]</ref> to generate masks of random streaks and holes of arbitrary shapes and sizes. On the other hand, for the HDR fine-tuning step, we collect approximately 2,000 HDR images from 735 HDR images and 34 HDR videos. From each HDR image, we extract 250 random patches of size 512?512 and generate the input LDR patches following the approach by <ref type="bibr" target="#b8">Eilertsen et al. [2017]</ref>. We then select a subset of these patches using our patch selection strategy. We also discard patches with no saturated content, since they do not provide any source of learning to the network. Our final training dataset is a set of 100K input and corresponding ground truth patches.</p><p>Training. We initialize our network using the Xavier approach <ref type="bibr" target="#b13">[Glorot and Bengio 2010]</ref> and train it on image inpainting task until convergence. We then fine-tune the network on HDR reconstruction. We train the network with a learning rate of 2 ? 10 ?4 in both stages. However, during the second stage, we reduce the learning rate by a factor of 2.0 when the optimization plateaus. The training process is performed until convergence. Both inpainting and HDR fine-tuning stages are optimized using Adam [Kingma and Ba 2015] with the default parameters ? 1 = 0.9 and ? 2 = 0.999 and mini-batch size of 4. The entire training takes approximately 11 days on a machine with an Intel Core i7, 16GB of memory, and an Nvidia GTX 1080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We implement our network in PyTorch <ref type="bibr" target="#b36">[Paszke et al. 2019</ref>], but write the data pre-processing, data augmentation, and patch sampling code in C++. We implement the feature masking mechanism using the existing standard convolutional layer in PyTorch. We compare our approach against three existing learning-based single image HDR reconstruction approaches of <ref type="bibr" target="#b10">Endo et al. [2017]</ref>, <ref type="bibr" target="#b8">Eilertsen et al. [2017]</ref>, and <ref type="bibr" target="#b31">Marnerides et al. [2018]</ref>. We use the source code provided by the authors to generate the results for all the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Images</head><p>We begin by quantitatively comparing our approach against the other methods in terms of mean squared error (MSE) and HDR-VDP-2 <ref type="bibr" target="#b30">[Mantiuk et al. 2011]</ref> in <ref type="table" target="#tab_2">Table 1</ref>. The errors are computed on a test set of 75 randomly selected HDR images, with resolutions ranging from 1024 ? 768 to 2084 ? 2844. We generate the input LDR images using various camera curves and exposures, similar to the approach by <ref type="bibr" target="#b8">Eilertsen et al. [2017]</ref>. We compute the MSE values on the gamma corrected images and HDR-VDP-2 scores are obtained on the linear HDR images. As seen, our method produces significantly better results, which demonstrate the ability of our network to accurately recover the full range of luminance. Next, we compare our approach against the other methods on five challenging scenes in <ref type="figure">Figure 7</ref>. Overall other approaches are not able to synthesize texture and produce results with blurriness, discoloration, and checkerboard artifacts. However, our approach can effectively utilize the information in the non-saturated color channels and the contextual information to synthesize visually pleasing textures. It is worth noting that although our approach has been trained using a perceptual loss, it can still properly recover the bright highlights. For example, our results in <ref type="figure">Figure 7</ref> (fifth row) are similar to <ref type="bibr" target="#b8">Eilertsen et al. [2017]</ref> and better than <ref type="bibr" target="#b10">Endo et al. [2017]</ref> and <ref type="bibr" target="#b31">Marnerides et al. [2018]</ref>.</p><p>We also demonstrate that our approach can consistently generate high-quality results on images with different amount of saturated areas in <ref type="figure">Figure 8</ref>. As can be seen, the results of all the other approaches degrade quickly by increasing the percentage of the saturated pixels in the input LDR image. On the other hand, our approach is able to produce high-quality results with sharp details and bright highlights in all the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real Images</head><p>We show the generality of our approach by producing results on a set of real images, captured with standard cameras, in <ref type="figure">Figure 9</ref>. Specifically, the top three images are from Google HDR+ dataset <ref type="bibr" target="#b16">[Hasinoff et al. 2016]</ref>, captured with a variety of smartphones, such as Nexus 5/6/5X/6P, Pixel, and Pixel XL. The image in the last row is captured by a Canon 5D Mark IV camera. All the other approaches are not able to properly reconstruct the saturated regions, producing results with discoloration and blurriness, as indicated by the arrows. On <ref type="bibr">Eilertsen Endo</ref> Ours Input Marnerides 4% 8% 10% <ref type="figure">Fig. 8</ref>. We compare the performance of the proposed method against previous methods for various amounts of saturated areas. The numbers indicate the percentage of the total number of pixels that are saturated in the input. Although our method slightly degrades as the saturation increases, we consistently present better results than the previous methods. the other hand, our method is able to properly increase the dynamic range by synthesizing realistic textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>Inpainting Pre-training. We begin studying the effect of the proposed inpainting pre-training step by comparing it against the commonly-used synthetic HDR pre-training in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="figure">Figure 11</ref>. As seen, our pre-training ("FMask + Inp. pre-training (Ours)") performs better than HDR pre-training ("FMask + HDR pre-training") both numerically and visually. Specifically, as shown in <ref type="figure">Figure 11</ref>, our network using inpainting pre-training is able to learn better features and synthesizes sharp textures in the saturated areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eilertsen Endo</head><p>Ours Input Marnerides <ref type="figure">Fig. 9</ref>. Comparison against state-of-the-art approaches on images captured by standard cameras. Zoom in to the electronic version to see the differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Masking (Ours)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard Convolution</head><p>Ground truth Feature Masking (Ours) <ref type="figure">Fig. 10</ref>. In regions with both saturated and well-exposed content (boundaries of sky and mountain and bright building lights), the response of the invalid saturated areas in standard convolution dominates the feature maps. Therefore, the network cannot properly utilize the content of the valid regions, introducing high frequency checkerboard artifacts (top row) and blurriness and halo (bottom row). Our approach suppresses the features from the saturated content and allows the network to synthesize the image using the well-exposed information.</p><p>Feature Masking. Here, we compare our feature masking strategy against several other approaches in <ref type="table" target="#tab_3">Table 2</ref>. Specifically, we compare our method against standard convolution (SConv), gated convolution <ref type="bibr" target="#b47">[Yu et al. 2019</ref>] (GConv), and the simpler version of our masking strategy where the mask is only applied to the input (IMask). For completeness, we include the result of each method with both inpainting and HDR pre-training. As seen, our masking strategy is considerably better than the other methods. It is worth noting that unlike other methods, the performance of gated convolution with inpainting pre-training is worse than HDR pre-training. This is mainly because gated convolution estimates the masks at each layer using a separate set of networks which become unstable after transitioning from inpainting pre-training to HDR fine-tuning.</p><p>We also visually compare our feature masking method against standard convolution in <ref type="figure">Figure 10</ref>. Standard convolution produces results with checkerboard artifacts (top) and halo and blurriness (bottom), while our network with feature masking produces considerably better results. Moreover, we visually compare our approach against other masking strategies in <ref type="figure">Figure 11</ref>. Note that, for each masking strategy, we only show the combination of masking and pre-training that produces the best numerical results in <ref type="table" target="#tab_3">Table 2</ref>, i.e., gated convolution (GConv) with HDR pre-training and input masking (IMask) with inpainting pre-training. Gated convolution is not able to produce high frequency textures in the saturated areas. Input masking performs reasonably well, but still introduces noticeable artifacts. Our feature masking method, however, is able to synthesize visually pleasing textures.</p><p>Patch Sampling. We show our result without patch sampling (Section 3.4) to demonstrate its effectiveness in <ref type="figure">Figure 11</ref>. As seen, by training on the textured patches (ours), the network is able to synthesize textures with more details and fewer objectionable artifacts.</p><p>Loss Function. Finally, we compare the proposed perceptual loss function against a simple pixel-wise (l 1 ) loss. As seen in <ref type="figure">Figure 12</ref>, using only the pixel-wise loss function our network tends to produce blurry images, while the network trained using the proposed perceptual loss function can produce visually realistic textures in the saturated regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND FUTURE WORK</head><p>Single image HDR reconstruction is a notoriously challenging problem. Although our method can recover the luminance and hallucinate textures, it is not always able to reconstruct all the details. One of such cases is shown in <ref type="figure">Figure 13 (top)</ref>, where our approach fails to reconstruct the wrinkles on the curtain. Nevertheless, our result is still better than the other approaches as they overestimate the  <ref type="figure">Fig. 11</ref>. From left to right, we compare our method against two other masking strategies as well as a pre-training method, and evaluate the effect of patch sampling. Here, GConv, IMask, and FMask refer to gated convolution <ref type="bibr" target="#b47">[Yu et al. 2019]</ref>, only masking the input image, and our full feature masking method, respectively. Moreover, Inp. pre-training refers to our proposed pre-training on inpainting task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Only pixel-wise loss Perceptual loss (ours) Ground truth <ref type="figure">Fig. 12</ref>. We compare the results of our network trained with only a pixelwise loss (l 1 ) and the proposed perceptual loss. Using the perceptual loss function, our network can synthesize visually realistic textures, while the network trained with only a pixel-wise loss produces blurry results.</p><p>brightness of the window and produce blurry results. Moreover, as shown in <ref type="figure">Figure 13</ref> (middle), when the input lacks sufficient information about the underlying texture, our method could potentially introduce patterns that do not exist in the ground truth image. Despite that, our result is still comparable to or better than the other approaches. Additionally, in some cases, our method reconstructs the saturated areas with an incorrect color, as shown in <ref type="figure">Figure 13</ref> (bottom). It is worth noting that the network reconstruct the building in blue since trees and skies are usually next to each other in the training data. As seen, other approaches also reconstruct parts of the building in blue color. Although our network can be used to reconstruct an HDR video from an LDR video, our result is not temporally stable. This is mainly because we synthesize the content of every frame independently. In the future, it would be interesting to address this problem through temporal regularization <ref type="bibr" target="#b9">[Eilertsen et al. 2019</ref>]. Moreover, we would like to experiment with the architecture of the networks to increase the efficiency of our approach and reduce the memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We present a novel learning-based system for single image HDR reconstruction using a convolutional neural network.  <ref type="figure">Fig. 13</ref>. Failure cases of our approach. From top to bottom, our method fails to reconstruct the wrinkles on the curtain, introduces textures that are not in the ground truth, and incorrectly reconstructs the building with sky color. Note that, the top two examples are synthetic, but the bottom one is real for which we do not have access to the ground truth image.</p><p>the artifacts caused by conditioning the convolutional layer on the saturated pixels, we propose a feature masking mechanism with an automatic mask updating process. We show that this strategy reduces halo and checkerboard artifacts caused by standard convolutions. Moreover, we propose a perceptual loss function that is designed specifically for the HDR reconstruction application. By minimizing this loss function during training, the network is able to synthesize visually realistic textures in the saturated areas. We further propose to train the system in two stages where we pre-train the network on inpainting before fine-tuning it on HDR generation. To encourage the network to synthesize textures, we propose a sampling strategy to select challenging patches in the HDR examples. Our model can robustly handle saturated areas and can reconstruct high-frequency details in a realistic manner. We show quantitatively and qualitatively that our method outperforms previous methods on both synthetic and real-world images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>A few example patches selected by our patch sampling approach. These are challenging examples as the HDR images corresponding to these patches contain complex textures in the saturated areas.The style loss in Eq. 7 (L s ) captures style and texture by comparing global statistics with a Gram matrix<ref type="bibr" target="#b11">[Gatys et al. 2015]</ref> collected over the entire image. Specifically, the style loss is defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 .</head><label>6</label><figDesc>The proposed network architecture. The model takes as input the RGB LDR image and outputs an HDR image. We use a feature masking mechanism in all the convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Numerical comparison in terms of mean square error (MSE) and HDR-VDP-2<ref type="bibr" target="#b30">[Mantiuk et al. 2011</ref>] against existing learning-based single image HDR reconstruction approaches.</figDesc><table><row><cell>Method</cell><cell>MSE</cell><cell>HDR-VDP-2</cell></row><row><cell>Endo et al. [2017]</cell><cell>0.0390</cell><cell>55.67</cell></row><row><cell>Eilertsen et al. [2017]</cell><cell>0.0387</cell><cell>59.11</cell></row><row><cell cols="2">Marnerides et al. [2018] 0.0474</cell><cell>54.31</cell></row><row><cell>Ours</cell><cell>0.0356</cell><cell>63.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">. We evaluate the effectiveness of our masking and pre-training</cell></row><row><cell cols="3">strategies by comparing against other alternatives in terms of MSE and HDR-</cell></row><row><cell cols="3">VDP-2 [Mantiuk et al. 2011]. Here, SConv, GConv, IMask, and FMask refer to</cell></row><row><cell cols="3">standard convolution, gated convolution [Yu et al. 2019], only masking the</cell></row><row><cell cols="3">input image, and our full feature masking approach, respectively. Moreover,</cell></row><row><cell cols="3">Inp. pre-training and HDR pre-training correspond to our proposed pre-</cell></row><row><cell cols="3">training on inpainting and HDR reconstruction tasks, respectively.</cell></row><row><cell>Method (Masking + Pre-training)</cell><cell>MSE</cell><cell>HDR-VDP-2</cell></row><row><cell>SConv + HDR pre-training</cell><cell>0.0402</cell><cell>58.43</cell></row><row><cell>SConv + Inp. pre-training</cell><cell>0.0374</cell><cell>60.03</cell></row><row><cell>GConv + HDR pre-training</cell><cell>0.0398</cell><cell>53.32</cell></row><row><cell>GConv + Inp. pre-training</cell><cell>0.1017</cell><cell>43.13</cell></row><row><cell>IMask + HDR pre-training</cell><cell>0.0398</cell><cell>58.39</cell></row><row><cell>IMask + Inp. pre-training</cell><cell>0.0369</cell><cell>61.27</cell></row><row><cell>FMask + HDR pre-training</cell><cell>0.0393</cell><cell>58.81</cell></row><row><cell>FMask + Inp. pre-training (Ours)</cell><cell>0.0356</cell><cell>63.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>To alleviate</figDesc><table><row><cell>Input</cell><cell>Ours</cell><cell>Ground truth</cell></row><row><cell>Endo</cell><cell>Eilertsen</cell><cell>Marnerides</cell></row><row><cell>Input</cell><cell>Ours</cell><cell>Ground truth</cell></row><row><cell>Endo</cell><cell>Eilertsen</cell><cell>Marnerides</cell></row><row><cell>Input</cell><cell></cell><cell>Ours</cell></row><row><cell>Endo</cell><cell>Eilertsen</cell><cell>Marnerides</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 80. Publication date: July 2020.Single Image HDR Reconstruction Using a CNN with Masked Features and Perceptual Loss ? 80:3</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 80. Publication date: July 2020.Single Image HDR Reconstruction Using a CNN with Masked Features and Perceptual Loss ? 80:5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 80. Publication date: July 2020.Single Image HDR Reconstruction Using a CNN with Masked Features and Perceptual Loss ? 80:7</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 80. Publication date: July 2020.Single Image HDR Reconstruction Using a CNN with Masked Features and Perceptual Loss ? 80:9</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the reviewers for their constructive comments. M. Santos is funded by the Brazilian agency CNPQ grant 161268/2018-8. T. Ren is partially supported by FACEPE grant APQ-0192-1.03/14. N. Kalantari is in part funded by a TAMU T3 grant 246451. ACM Trans. Graph., Vol. 39, No. 4, Article 80. Publication date: July 2020.   80:10 ? Marcel Santana Santos, Tsang Ing Ren, and Nima Khademi Kalantari   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia</title>
		<meeting>the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic Photo Manipulation with a Generative Image Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tone expansion using lighting style aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cambodge</forename><surname>Bist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cozot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Madec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Ducloux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="77" to="86" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A median cut algorithm for light probe sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2005 Posters</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">66</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recovering high dynamic range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pe Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the SPIE: Image Sensors</title>
		<meeting>eeding of the SPIE: Image Sensors</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">3965</biblScope>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancement of bright video features for HDR displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Didyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast bilateral filtering for the display of highdynamic-range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Dorsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 29th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HDR image reconstruction from a single exposure using deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafa?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">178</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single-frame Regularization for Temporally Stable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?mantiuk</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep reverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Kanamori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="177" to="178" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FiNet: Compatible and Diverse Fashion Image Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4481" to="4491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Burst photography for high dynamic range and low-light imaging on mobile cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">192</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HDR deghosting: How to deal with saturation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kari</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1163" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep high dynamic range imaging of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khademi</forename><surname>Nima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="144" to="145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High dynamic range video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="319" to="325" />
			<date type="published" when="2003" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Jsi-gan: Gan-based joint superresolution and inverse tone-mapping with pixel-wise task-specific filters for UHD HDR video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Soo Ye Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04391</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-quality reverse tone mapping for a wide range of exposures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Kovaleski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Production-ready global illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Landis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Course Notes</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep chain hdri: Reconstructing a high dynamic range image from a single low dynamic range image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk-Ju</forename><surname>Gwon Hwan An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="49913" to="49924" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Recursive HDRI: Inverse Tone Mapping using Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk-Ju</forename><surname>Gwon Hwan An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="596" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully-Automatic Inverse Tone Mapping Preserving the Content Creator&apos;s Artistic Intentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Luzardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Aelterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiep</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ochoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Rousseaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Picture Coding Symposium (PCS)</title>
		<imprint>
			<biblScope unit="page" from="199" to="203" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafat</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kil Joong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">G</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ExpandNet: A deep convolutional neural network for high dynamic range expansion from low dynamic range content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetris</forename><surname>Marnerides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bashford-Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hatchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optical splitting trees for high-precision monocular imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree K</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="32" to="42" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning an inverse tone mapping network with a generative adversarial regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1383" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust high dynamic range imaging by rank minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1219" to="1232" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ldr2hdr: on-the-fly reverse tone mapping of legacy video and photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seetzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorne</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-NET: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust patch-based HDR reconstruction of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Nima Khademi Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Yaesoubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="203" to="204" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A versatile HDR video production system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Michael D Tocci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Kiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Tocci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High dynamic range image hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lvdi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Eurographics Conference on Rendering Techniques. Eurographics Association</title>
		<meeting>the 18th Eurographics Conference on Rendering Techniques. Eurographics Association</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep high dynamic range imaging with large foreground motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="117" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gan Based Multi-Exposure Inverse Tone Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Highresolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6721" to="6729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image correction via deep reciprocating HDR transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
