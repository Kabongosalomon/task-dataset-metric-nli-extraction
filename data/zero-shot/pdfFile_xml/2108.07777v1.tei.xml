<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised 3D Human Pose Estimation with Multiple-View Geometry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arij</forename><surname>Bouazizi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit?t Ulm</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Wiederer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit?t Ulm</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Kressel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universit?t Ulm</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised 3D Human Pose Estimation with Multiple-View Geometry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a self-supervised learning algorithm for 3D human pose estimation of a single person based on a multiple-view camera system and 2D body pose estimates for each view. To train our model, represented by a deep neural network, we propose a four-loss function learning algorithm, which does not require any 2D or 3D body pose ground-truth. The proposed loss functions make use of the multiple-view geometry to reconstruct 3D body pose estimates and impose body pose constraints across the camera views. Our approach utilizes all available camera views during training, while the inference is single-view. In our evaluations, we show promising performance on Human3.6M and HumanEva benchmarks, while we also present a generalization study on MPI-INF-3DHP dataset, as well as several ablation results. Overall, we outperform all self-supervised learning methods and reach comparable results to supervised and weakly-supervised learning approaches. Our code and models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image-based human pose estimation experienced an enormous improvement the past few years thanks to deep neural networks and large-scale annotated databases. In particular, 2D human pose estimation approaches showed excellent results in almost any kind of image context <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Unlike, 3D human pose estimation is not at the same level due to the difficulty of obtaining ground-truth information and the higher complexity of the mapping to the three-dimensional space. Multi-camera systems and motion capture sensors are not massively available while setting up such a system is much more complicated than annotating 2D body postures in images.</p><p>In this work, we address the problem of 3D human pose estimation for a single person without demanding 2D or 3D body pose ground-truth information. Our supervision comes from 2D body pose estimates and the geometry properties of a multiple-view calibrated camera system. We propose a self-supervised learning-based algorithm to map a 2D body pose estimate, coming from an image, to the 3D body pose in the camera coordinate system. Our learning algorithm makes use of all available camera views in the training, while the inference is single-view.</p><p>Learning-based approaches for 3D human pose estimation still reach the best results with supervised learning <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b41">[42]</ref>. However, other types of supervision have been recently explored to minimize the process of 3D body pose annotation. Weakly-and semi-supervised learning methods E-mail: firstname.lastname@{daimler.com, uni-ulm.de}. <ref type="bibr" target="#b0">1</ref> Source Code: https://github.com/vru2020/Pose_3D/ Year <ref type="bibr">Wu et al.</ref> [47] Tung et al. <ref type="bibr" target="#b42">[43]</ref> Martinez et al. <ref type="bibr" target="#b27">[28]</ref> Lu et al. <ref type="bibr" target="#b26">[27]</ref> Drover et al. <ref type="bibr" target="#b10">[11]</ref> Chen et al. <ref type="bibr" target="#b7">[8]</ref> Wandt et al. <ref type="bibr" target="#b43">[44]</ref> Kocabas et al. <ref type="bibr" target="#b22">[23]</ref> Rhodin et al. <ref type="bibr" target="#b37">[38]</ref> Kundu et al. <ref type="bibr" target="#b25">[26]</ref> Jenni et al. <ref type="bibr" target="#b19">[20]</ref> He et al. <ref type="bibr" target="#b14">[15]</ref> Qiu et al. <ref type="bibr" target="#b35">[36]</ref> Ours <ref type="figure">Fig. 1</ref>: Human3.6M <ref type="bibr" target="#b16">[17]</ref> Results. We present a selfsupervised learning approach for single human 3D body pose estimation. Compared to supervised and weakly-supervised approaches, we deliver promising results by relying on pseudo-labels and four-loss functions for self-supervision. make use of unpaired 2D -3D body pose data <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> or rely on a small set of annotated 3D body pose data to finetune a generic model <ref type="bibr" target="#b37">[38]</ref>. Lately, self-supervised learning has attracted much attention because of the independence from ground-truth information <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b40">[41]</ref>. In these approaches, adversarial training often takes care of the body pose geometry learning task. In our approach, we follow the idea of self-supervision, but instead, rely on a multiple-view geometry to learn the body pose constraints. We propose an algorithm with four-loss functions for learning to predict the 3D body pose from 2D body pose estimate input. We first generate 3D body pose estimates by triangulating the 2D pose estimates. The 3D estimates (pseudo-labels) serve as supervision for training our deep neural network. Second, we introduce the re-projection loss for self-supervision that minimizes the difference between the 2D body pose projections of the 3D predictions and 2D body pose input estimates. To obtain extra supervision and impose geometric constraints, we project the 3D body pose prediction to all camera views and minimize the difference to the 2D body pose estimates. Third, we impose body pose geometric constraints across all camera views by forcing the 3D body pose predictions from all views to be the same. Finally, we propose to project the 3D body predictions and triangulate again to obtain additional 3D body pose estimates (considered as ground-truth too). We show in practice that triangulating the projected 3D body pose predictions results in improving the final performance. The motivation for this loss function is to be independent of the input 2D estimates, which contribute to the rest of the loss functions. In our experiments, we rely on two standard benchmarks to evaluate the four proposed loss functions for self-supervision. First, we show state-of-the-art performance on Human3.6M <ref type="bibr" target="#b16">[17]</ref> dataset and then on HumanEva <ref type="bibr" target="#b38">[39]</ref> dataset. In addition, we present an ablation study and a transfer learning evaluation on MPI-INF-3DHP <ref type="bibr" target="#b28">[29]</ref> to further examine the generalization and analyze our approach.</p><p>To sum up, our work makes the following contributions: 1) we rely only on 2D body pose estimates and multipleview calibrated system to obtain supervision for monocular 3D human pose estimation -we make thus use of pseudolabels instead of 3D ground-truth information, 2) we propose a model agnostic self-supervised learning algorithm with four-loss functions, 3) we present an extensive evaluation using standard benchmarks where we reach the best results in the field for learning without 3D ground-truth approaches and comparable results to supervised and weakly-supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Human body pose estimation is a long-standing problem in computer vision. Below, we present the relevant prior work on learning-based 3D human pose estimation. a) Supervised Learning: In the past, learning-based approaches normally relied on body part detectors and multipleview geometry <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>. With the deep learning revolution, ConvNet-based detectors have significantly improved 3D human pose estimation <ref type="bibr" target="#b15">[16]</ref>, while at the same time the multi-view information utilization became less necessary. Single-view 3D human pose estimation showed comparable and often better results than multi-view approaches, using deep neural networks <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Nevertheless, multipleview geometric priors still lead to state-of-the-art performance <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Despite the remarkable results, these approaches require ground-truth data to be trained. In this work, we show how to reach similar results without relying on 3D ground-truth information.</p><p>b) Weakly Supervised Learning: Weakly supervised learning relaxed the assumption of annotating all data. The annotation stems from fitting a given 3D human model to the image data <ref type="bibr" target="#b47">[48]</ref> or unpaired 2D and 3D body pose annotations <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Using unpaired 2D and 3D pose data is usually combined with adversarial learning. For instance, Wandt et. al. <ref type="bibr" target="#b43">[44]</ref> propose the re-projection network to learn the mapping from 2D to the 3D body pose distribution using adversarial learning. In particular, the critic network improves the generated 3D body pose estimate based on the Wasserstein loss <ref type="bibr" target="#b2">[3]</ref> and unpaired 2D and 3D body poses. Similarly, Drover et. al. <ref type="bibr" target="#b10">[11]</ref> rely on a discriminator network for supervision of 2D body pose projections. However, the method additionally utilizes 3D ground-truth data to synthetic 2D body joints for training. Instead of adversarial learning, we rely on a multiple-view calibrated system to reach the same goal. In addition, our approach does not demand any kind of 3D ground-truth body poses. c) Self-Supervised Learning without 3D ground-truth: Self-supervised Learning has recently shown promising 3D body pose results thanks to the robust 2D pose estimation algorithms <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref> and the self-supervision loss functions <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b18">[19]</ref>. The self-supervision usually comes from the multiple-view geometry <ref type="bibr" target="#b22">[23]</ref> or the video constraints <ref type="bibr" target="#b25">[26]</ref>. Adversarial learning offers self-supervision too <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Kocabas et. al. <ref type="bibr" target="#b22">[23]</ref>, triangulate 2D pose estimate in multiview environment to generate pseudo-labels for 3D body pose training. Chen et. al. <ref type="bibr" target="#b7">[8]</ref> propose the self-consistency loss and adversarial training for self-supervised training. The self-consistency loss is motivated by the fact that predicted 3D skeletons can be randomly rotated and projected without any change in the distribution of the resulting 2D skeletons. In our work, we explore both ideas of pose estimate triangulation and 3D body pose consistency for a multipleview system. Since the pose estimates can be noisy, we additionally propose another self-supervised loss that does not depend on the pose estimate. Tripathi et. al. <ref type="bibr" target="#b40">[41]</ref> propose a knowledge distillation approach to compensate for the lack of ground-truth. The approach also relies on dilated convolutions to model the temporal dynamics which act as extra self-supervision. Lately, Kundu et. al. <ref type="bibr" target="#b25">[26]</ref> present a self-supervised 3D pose estimation method with an interpretable latent space that allow view synthesis in addition to 3D human pose estimation and body part segmentation. The approach relies though on unpaired 3D annotation to improve the final result. Rhodin et. al. <ref type="bibr" target="#b37">[38]</ref> rely on a multiview configuration to learn a latent representation without supervision. However, some annotated samples are required to fine-tune the model on a semi-supervised manner. Unlike, we require neither unpaired 3D poses nor 3D ground-truth information to train our approach. Instead, we generate 3D pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we present our learning algorithm for 3D human body pose estimation from 2D body pose estimates. The supervision stems from the multi-view geometry properties of a calibrated system and the 2D pose detection from a body pose estimation algorithm. We present below how we train a deep neural network based on our proposed loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem definition</head><p>Consider a multi-view system with C cameras, which is synchronized in time and calibrated. For each camera view c, a set of 2D body pose estimates S c = {? s c } |Sc| s=1 , is available using a pose estimation algorithm. A 2D body pose estimate? s c corresponds to a tuple of N landmarks in the image plane, such that? s c = (? s c,1 , . . . ,? s c,N ). For the human pose estimation problem, an estimated landmark y s c,n = (x s c,n , y s c,n ) usually corresponds to a body joint. To obtain the estimates, we can rely on some 2D human pose estimation algorithm <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Given the calibrated system and the 2D body pose estimates only, our goal is to learn the mapping from the 2D body pose y to the 3D body pose Y with a deep neural network f ? (?), parametrized by ?. The network parameters ? are learned in a self-supervised way with four-loss functions that rely on the 2D pose estimates across all views and 3D pose estimates, coming from network input triangulation and the triangulation of the projected network output. Note that our approach targets single person 3D body pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Algorithm</head><p>Our learning algorithm is illustrated in <ref type="figure" target="#fig_1">Fig 2.</ref> Given the training sample s, i.e. a set of C images, we first apply the human pose detector on each view to extract 2D body pose estimates, which serve as input to f ? (?), and for obtaining the 3D human pose estimate? s in by triangulating based on all camera views, where? s in = (? s in,1 , . . . ,? s in,N ) and Y s in,n = (X s in,n , Y s in,n , Z s in,n ) are 3D coordinates in the camera coordinate system. The input triangulation serves as supervision for the input triangulation loss L in , which minimizes the difference of the 3D body pose prediction Y s and triangulated 3D body pose? s in (set as ground-truth). In addition, we introduce our second loss function which is applied on each camera view. In detail, the re-projection loss L proj adds self-supervision by minimizing the difference between the input of each camera view? s c (taken as groundtruth) and the projection of the 3D body pose prediction y s c to the camera view c. Third, we force consistency between the 3D body pose predictions across all views. For this reason, we present the consistency loss L con that considers all pairs of views and minimizes the difference between the predicted 3D body poses. Fourth, we propose to perform triangulation of the projected 3D body pose prediction across all views. We refer to this triangulation as output triangulation and consider it as self-supervision to train the model. For this reason, we design the output triangulation loss L out that minimizes the difference between the output triangulation? s out (taken as ground-truth) and the predicted 3D body pose Y s given the sample s. This loss function applies to all camera views as well.</p><p>To learn the parameters ?, we train our model based on the proposed loss functions and using the training samples from all camera views {S c } C c=1 . We obtain the model parameters by minimizing the following objective:</p><formula xml:id="formula_0">? = arg min ? ? 1 L in + ? 2 L proj + ? 3 L con + ? 4 L out ,<label>(1)</label></formula><p>where ? 1 , ? 2 , ? 3 , and ? 4 are weighing factors for each loss. Next, we describe in detail each loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Functions</head><p>We present the motivation and elements of each loss function below. a) Input Triangulation Loss: To compute the 3D position for each landmark based on the camera view detections, we make use of the Direct Linear Triangulation (DLT) method <ref type="bibr" target="#b11">[12]</ref>, which is differentiable and thus suits-well to our optimization. The same transformation applies to all body pose landmarks. Similar to <ref type="bibr" target="#b22">[23]</ref>, we consider the obtained 3D human pose estimate? s in as ground-truth. We define the input triangulation loss as:</p><formula xml:id="formula_1">L in = S s=1 C c=1 ? w?c (? s in ) ? f ? (? s c ) 2 ,<label>(2)</label></formula><p>where ? w?c (?) corresponds to the transformation from the world coordinate system w to the camera c coordinate system. The loss depends on the quality of the detected landmarks for each camera view. We further discuss this point in the experiments. b) Re-Projection Loss: The re-projection loss <ref type="bibr" target="#b42">[43]</ref> provides multiple-view supervision, which mitigates the ambiguities and occlusions that arise from the single view 2D detections. For each camera view c, the predicted 3D body pose is projected to all other camera views and to the current one. Then, the difference between the input 2D pose (considered as ground-truth) and all projections is minimized. We define the re-projection loss as:</p><formula xml:id="formula_2">L proj = S s=1 C c=1 C c =1 ? s c ? ? c (f ? (? s c )) ,<label>(3)</label></formula><p>where ? c corresponds to the projection of the predicted 3D pose to the camera view c. We choose the L1 loss for the minimization since we observed faster convergence in our experiments. The double summation in Eq. 3 shows that we project the predicted 3D body pose across all camera views. We empirically found that the additional operations, compared to single camera projection, have a positive impact on the model performance. Similar to the input triangulation loss (Eq. 2), the re-projection loss is depended on the input 2D pose detection quality. However, we make the loss more robust to false detections by considering all camera views. c) Consistency Loss: We impose geometric body pose constraints in the 3D predictions with the consistency loss. In principle, the 3D body pose prediction from each camera view should be the same given the training sample s, regardless the 2D body pose input. The only difference is the coordinate system in which the 3D body pose is expressed. Based on this observation, we propose the consistency loss that is given by:</p><formula xml:id="formula_3">L con = S s=1 C c=1 C c =1 c =c f ? (? s c ) ? ? c ?c (f ? (? s c ))<label>(4)</label></formula><p>where ? c ?c (?) corresponds to the transformation from the camera c to the camera c coordinate system. The idea is to transform the 3D prediction from the camera view c to all As input to our model we assume 2D pose detections from multiple views shown on the left. As first geometric self-supervision, we compute the triangulated 3D pose from the 2D detections as estimated ground-truth for the input triangulation loss L in . For 3D consistency between the 3D body pose predictions from multiple views, we define the consistency loss L con . We re-project the 3D poses estimated by the network to the image plane to define two extra loss functions. The output triangulation loss L out introduces an additional self-supervision path by triangulating the re-projected 3D predictions. For consistency in 2D the re-projected predictions are evaluated in a re-projection loss L proj .</p><p>other cameras and assure that the predictions looks the same. Note that we consider the camera view c as the groundtruth to minimize the difference from the transformed 3D prediction. Similar to the re-projection loss, we empirically select the L1 loss for the minimization due to the fast convergence. d) Output Triangulation Loss: The three presented loss functions are directly influenced by the quality of the 2D input estimates. To address this point, we propose the output triangulation loss that is independent of the 2D pose detection input. For the output triangulation loss, we first create a new 3D body pose estimate (as ground-truth) by projecting the predicted 3D body pose from each camera view to the image plane and use the C body pose projections to triangulate again. This triangulated 3D body pose Y out , which is based on the projected predictions, serves as ground-truth for the output triangulation loss defined as:</p><formula xml:id="formula_4">L out = S s=1 C c=1 ? w?c ( Y s out ) ? f ? (? s c ) 2 .<label>(5)</label></formula><p>where ? w?c (?) corresponds to the transformation from the world coordinate system w to the camera c coordinate system. In practice, we observed that this loss helps to the model convergence at the last stage of training where the predictions are meaningful. For this reason, we combine it with the rest loss functions once the model makes reasonable 3D body pose predictions. Since we rely on a single network architecture for all experiments, this rule works well for all of our evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Architecture</head><p>In our problem, it is common to design fully connected network architectures to estimate the 3D keypoints. For that reason, we implement the model of Martinez et al. <ref type="bibr" target="#b27">[28]</ref>. The encoder takes N 2D landmarks as input, e.g. 16, represented by a vector of 2N elements, to a fully connected layer with 1024 output channels with four subsequent residual blocks. A residual block is composed of a pair of fully connected layers, each with 1024 neurons followed by batch normalization and ReLu activation function. The decoder has the same architecture as the encoder computing an output vector with 3N dimensions representing N 3D joint locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Single-View Inference</head><p>In training, our learning algorithm is based on a calibrated camera-system. Nevertheless, we could skip the calibration and extract the camera parameters as in <ref type="bibr" target="#b22">[23]</ref>.</p><p>During inference, our approach works with the 2D pose estimate from a single view to make 3D body pose predictions given by:</p><formula xml:id="formula_5">Y = f ? (?).<label>(6)</label></formula><p>Next, we present the evaluation of our learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate our approach on two standard benchmarks for 3D human pose estimation, namely Human3.6M <ref type="bibr" target="#b16">[17]</ref> and HumanEva <ref type="bibr" target="#b38">[39]</ref> datasets. Furthermore, we examine the contribution of each loss function in the ablation study. At  <ref type="bibr" target="#b46">[47]</ref> 98.4 AIGN. <ref type="bibr" target="#b42">[43]</ref> 97.2 Wandt et al. <ref type="bibr" target="#b43">[44]</ref> 65.1 Drover et al. <ref type="bibr" target="#b10">[11]</ref> 64.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self supervised</head><p>Rhodin et al. <ref type="bibr" target="#b37">[38]</ref> 98.4 Kundu et al. <ref type="bibr" target="#b25">[26]</ref> 85.8 Jenni et al. <ref type="bibr" target="#b19">[20]</ref> 78.4 Chen et al. <ref type="bibr" target="#b7">[8]</ref> 68.0 Kocabas et al. <ref type="bibr" target="#b22">[23]</ref> 67.5</p><p>Ours 62.0 last, we determine the generalization of our approach by applying a pre-trained model from Human3.6M <ref type="bibr" target="#b16">[17]</ref> to the MPII-INF-3DHP <ref type="bibr" target="#b28">[29]</ref> dataset.</p><p>A. Implementation details a) Body Pose Representation: Similar to the prior work <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we make 3D predictions on the camera coordinate system where we set the origin of the coordinate system at the pelvis joint. We set the same origin for the image plane too. b) 2D Body Pose Detection: The 2D body pose estimates come from pose detectors, which we select according to the dataset. In the Human3.6M <ref type="bibr" target="#b16">[17]</ref> evaluation, we use the same 2D detections as <ref type="bibr" target="#b34">[35]</ref>, which are obtained after running Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> to all camera views and obtaining the 2D body pose with the fine-tuned Cascade Pyramid Network <ref type="bibr" target="#b9">[10]</ref>. In HumanEva and MPII-INF-3DHP, we first rely on Mask R-CNN to detect the subject in the image plane. Then, we apply the Stacked Hourglass model <ref type="bibr" target="#b30">[31]</ref> pre-trained on the MPII Human Pose dataset <ref type="bibr" target="#b1">[2]</ref> to extract the 2D body pose. Finally, we follow the prior work to down-sample all the videos from 50 to 10 frames per second <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b22">[23]</ref>. c) Training: For the Human3.6M training, we have a batch size of 8192, learning rate 1e ? 3, 500 training epochs and rely on the Adam optimizer. For the HumanEva training, we use the same optimizer for 5000 epochs with a learning rate of 1e ? 3 and a batch size of 1600. Since some frames are corrupted by sensor dropout, we skip them and only use the valid frames for training. Finally, we empirically found that the output triangulation loss works at best when used at the last 10% of training epochs. The weighting factors for each loss are ? 1 = 1, ? 2 = 1, ? 3 = 0.001, and ? 4 = 0.01, found empirically. We set the weighting factors fixed for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Human3.6M Evaluation</head><p>Human3.6M <ref type="bibr" target="#b16">[17]</ref> is the de facto benchmark for 3D human pose estimation. In the dataset 7 professional actors perform 15 different daily actions, including discussion, eating, sitting or walking motions. The dataset contains 3.6 million 3D human body poses, generated based on a motion capture system and four cameras. Similar to the related work <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b42">[43]</ref>, our metric is the mean per joint position error after Procrustes alignment (P-MPJPE). We follow the standard protocol using the five actors 1, 5, 6, 7 and 8 for training and the remaining two actors 9 and 11 for testing. We summarize our results in <ref type="table" target="#tab_0">Table I. In Table II</ref>, we additionally provide the results per action and compare with approaches that include the same evaluation. We outperform all related work for self-supervised learning without 3D ground-truth information. Besides, we outperform all weakly supervised learning approaches, as well as some of the supervised learning too. Similar to us, the approaches of <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b22">[23]</ref> leverage multiple-view information for 3D human pose estimation. In particular, we have a common loss function with <ref type="bibr" target="#b22">[23]</ref>, namely the input triangulation loss. Nevertheless, we clearly show better performance than both approaches thanks to our four-loss function formulation. We demonstrate some visual results in <ref type="figure">Fig. 3a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. HumanEva Evaluation</head><p>HumanEva <ref type="bibr" target="#b38">[39]</ref> has three subjects recorded from three camera views at 60 Hz. For our evaluation, we follow the protocol from <ref type="bibr" target="#b27">[28]</ref>. We adopt the 15-joint skeleton model and make use of the provided train and test splits. Inline with prior works <ref type="bibr" target="#b27">[28]</ref>, we train on all actions and subject and evaluate on Walking and Jogging. The evaluation metric is the mean per joint position error after the Procrustes alignment <ref type="bibr" target="#b27">[28]</ref>, similar to Human3.6M evaluation. <ref type="table" target="#tab_0">Table III</ref> summarizes our results. Due to the lack of unsupervised learning approaches (as well as weakly supervised ones), we compare our results with supervised learning methods. We rank somewhere in the middle of supervised learning approaches without using a single annotation during training. Finally, we present qualitative evaluation in <ref type="figure">Fig. 3b</ref>. It can be seen that we make correct predictions even for occluded body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We evaluate each loss function to show how the P-MPJPE metric is reduced by incrementally adding all loss functions of our algorithm. The results are reported in <ref type="table" target="#tab_0">Table IV</ref> where the order of including additional loss functions is the same as the presentation of our loss functions in Sec. III-C.</p><p>We obtain already a competitive result using only the input triangulation loss. By incrementally adding the rest loss functions, we can improve our approach by 8%. We provide qualitative results in <ref type="figure">Fig. 3a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Generalization Evaluation</head><p>In this experiment, we examine the generalization of our approach by testing a pre-trained model on Human3.6M on  <ref type="figure">Fig. 3</ref>: Qualitative results on the three benchmarks, Human3.6M <ref type="bibr" target="#b16">[17]</ref>, HumanEVA <ref type="bibr" target="#b38">[39]</ref> and MPII-INF-3DHP <ref type="bibr" target="#b28">[29]</ref>. From left to right we show the detected 2D pose on the image, the 3D body pose ground-truth (3D GT) and the the 3D prediction of our model. The model can generalize to unseen scenes and subjects.</p><p>a different configuration without training on it. We assume that our method can generalize to another database since it does not directly depend on the image data or the calibration system. Nevertheless, we are obliged to the object  detection performance, which affects the performance of our approach. As test set, we consider the MPII-INF-3DHP <ref type="bibr" target="#b28">[29]</ref> dataset, which has 4 male and 4 female professional actors performing different actions, including diverse clothing and viewpoints. In our experiments, we rely only on the test set that has 2935 frames from 6 subjects performing 7 actions. We quantitatively evaluate our model pre-trained on Human3.6M to unseen scenes and subjects, and report the results in <ref type="table">Table V</ref>. Our results are reported in 3D Percentage of Correct Keypoints (3DPCK) and the corresponding Area Under the Curve (AUC), which compose the standard metrics for the benchmark. We compare our method with related approaches from supervised and weakly supervised learning, as shown in <ref type="table">Table V</ref>. All approaches have been trained with the Hu-man3.6M database. We achieve the best performance for supervised and self-supervised learning, while we have competitive results in weakly supervised learning. Our promising performance without extra training makes our approach suitable for skeleton-based input to tasks such as gesture recognition <ref type="bibr" target="#b45">[46]</ref> or trajectory estimation <ref type="bibr" target="#b12">[13]</ref>. We provide some visual results in <ref type="figure">Fig. 3c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We presented a self-supervised learning algorithm for 3D human pose estimation, which is based on a multiple-view camera system and 2D body pose estimates for each view. To train our model, represented by a deep neural network, we propose a four-loss function training algorithm that does not require any kind of 2D or 3D body pose annotation. V: Our results on MPII-INF-3DHPE. All approaches are trained on Human3.6M. Our method outperforms all self-supervised methods trained without 3D ground-truth information and achieves similar performance to supervised and weakly-supervised approaches in the transfer task. We evaluated our approach on Human3.6M and HumanEva databases, the standard benchmarks for 3D human pose estimation. Finally, we further examined the generalization of our approach on a different environment and also analysed the contribution of our loss functions. Overall, our method achieves state-of-the-art performance in all evaluations when compared to self-supervised learning approaches without 3D ground-truth. Besides, we reach comparable results to supervised and weakly-supervised learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head><p>Part of this work was supported by the research project "KI Delta Learning" (project number: 19A19013A) funded by the Federal Ministry for Economic Affairs and Energy (BMWi) on the basis of a decision by the German Bundestag.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of our learning algorithm. During training we optimize the deep neural network f ? (?) to map a 2D pose estimate to 3D body pose with four loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Results on the Human3.6M dataset. We show the P-MPJPE on the test set of Human3.6M for models with different levels of supervision. The best performing method is marked in bold. In comparison with all current state-ofthe art self-supervised and weakly supervised methods, our approach performs best.</figDesc><table><row><cell cols="3">Supervision Method</cell><cell>P-MPJPE</cell></row><row><cell>Supervised</cell><cell></cell><cell cols="2">Akther and Black [1] 181.1 Zhou et al. [49] 106.7 Bogo et al. [7] 82.3 Martinez et al. [28] 47.7 Lu et al. [27] 46.6</cell></row><row><cell>Weakly</cell><cell>supervised</cell><cell>Wu et al.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Results on the Human3.6M dataset. Comparison of our self-supervised approach with state-of-the-art supervised and weakly supervised methods following evaluation Protocol-II (with rigid alignment) individually for all 15 actions. Qualitative results on the Human3.6M dataset. The model is able to reconstruct the 3D body pose from a single 2D pose detection in various actions like phoning, eating or discussing. Qualitative results on the HumanEva dataset. The test set of HumanEva includes three actions, walking, jogging and boxing. In all actions, the model shows results close to the ground-truth. Qualitative results on the MPII-INF-3DHP dataset. Without fine-tuning the model can generalize to the challenging MPII-INF-3DHP dataset, which contains scenes recorded in-the-wild, and sufficiently predicted the 3D body pose.</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell>Dir.</cell><cell>Dis.</cell><cell>Eat</cell><cell>Greet</cell><cell cols="2">Phone Photo</cell><cell>Pose</cell><cell>Purch.</cell><cell>Sit</cell><cell>SitD</cell><cell>Smoke</cell><cell>Wait</cell><cell>WalkD</cell><cell>Walk</cell><cell>WalkT</cell><cell>Avg</cell></row><row><cell>Sueprvised</cell><cell></cell><cell cols="2">Akther &amp; Black [1] Zhou et al. [49] Bogo et al. [7] Martinez et al. [28] Lu et al. [27]</cell><cell cols="4">199.2 177.6 161.8 197.8 99.7 95.8 87.9 116.8 62.0 60.2 67.8 76.5 39.5 43.2 46.4 47.0 40.8 44.6 42.1 45.1</cell><cell>176.2 108.3 92.1 51.0 48.3</cell><cell>186.5 107.3 77.0 56.0 54.6</cell><cell>195.4 93.5 73.0 41.4 41.2</cell><cell>167.3 95.3 75.3 40.6 42.9</cell><cell cols="2">160.7 173.7 109.1 137.5 100.3 137.3 56.5 69.4 55.5 69.9</cell><cell>177.8 106.0 83.4 49.2 46.7</cell><cell>181.9 102.2 77.3 45.0 42.5</cell><cell>198.6 110.4 79.7 38.0 36.0</cell><cell>176.2 106.5 48.0 49.0 48.0</cell><cell>192.7 115.2 87.7 43.1 41.4</cell><cell>181.1 106.7 82.3 47.7 46.6</cell></row><row><cell>Weakly</cell><cell>supervised</cell><cell>Wu et al. [47] AIGN. [43] Wandt et al. [44] Drover et al. [11]</cell><cell></cell><cell>78.6 77.6 53.0 60.2</cell><cell>90.8 91.4 58.3 60.7</cell><cell>92.5 89.9 59.6 59.2</cell><cell>89.4 88.0 66.5 65.1</cell><cell>108.9 107.3 72.8 65.5</cell><cell>112.4 110.1 71.0 63.8</cell><cell>77.1 75.9 56.7 59.4</cell><cell>106.7 107.5 69.6 59.4</cell><cell cols="2">127.4 139.0 124.2 137.8 78.3 95.2 69.1 88.0</cell><cell>103.4 102.2 66.6 64.8</cell><cell>91.4 90.3 58.5 60.8</cell><cell>79.1 78.6 63.2 64.9</cell><cell>--57.5 63.9</cell><cell>--49.9 65.2</cell><cell>98.4 97.2 65.1 64.6</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours (self-supervised)</cell><cell>49.4</cell><cell>51.7</cell><cell>61.7</cell><cell>56.5</cell><cell>64.9</cell><cell>67.1</cell><cell>51.6</cell><cell>52.1</cell><cell>83.9</cell><cell>111.3</cell><cell>60.5</cell><cell>54.7</cell><cell>56.9</cell><cell>45.9</cell><cell>53.6</cell><cell>62.0</cell></row><row><cell></cell><cell cols="2">2D Pose Detection</cell><cell>3D GT</cell><cell></cell><cell>Ours</cell><cell></cell><cell cols="2">2D Pose Detection</cell><cell></cell><cell>3D GT</cell><cell></cell><cell>Ours</cell><cell cols="3">2D Pose Detection</cell><cell>3D GT</cell><cell></cell><cell>Ours</cell></row><row><cell cols="3">(a) 2D Pose Detection</cell><cell>3D GT</cell><cell></cell><cell>Ours</cell><cell></cell><cell cols="2">2D Pose Detection</cell><cell></cell><cell>3D GT</cell><cell></cell><cell>Ours</cell><cell cols="3">2D Pose Detection</cell><cell>3D GT</cell><cell></cell><cell>Ours</cell></row><row><cell cols="3">(b) 2D Pose Detection</cell><cell>3D GT</cell><cell></cell><cell>Ours</cell><cell></cell><cell cols="2">2D Pose Detection</cell><cell></cell><cell>3D GT</cell><cell></cell><cell>Ours</cell><cell></cell><cell>2D Pose Detection</cell><cell></cell><cell>3D GT</cell><cell></cell><cell>Ours</cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Results on HumanEva<ref type="bibr" target="#b38">[39]</ref> and comparison with previous supervised approaches. Our model outperforms some of the supervised methods.Radwan  et al. [37] 75.1 99.8 93.8 79.2 89.8 99.4 89.5 Wang et al. [45] 71.9 75.7 85.3 62.6 77.7 54.4 71.3 Simo-Serra et al. [40] 65.1 48.6 73.5 74.2 46.6 32.2 56.7 Kostrikov et al. [24] 44.0 30.9 41.7 57.2 35.0 33.3 40.3 Yasin et al. [18] 35.8 32.4 41.6 46.6 41.4 35.4 38.9 Pavlakos et al. [34] 22.1 21.9 29.0 29.8 23.6 26.0 25.5 Martinez et al. [28] 19.7 17.4 46.8 26.9 18.2 18.6 24.6 Ours (self-supervised) 59.2 60.3 52.2 38.2 61.7 81.3 58.9</figDesc><table><row><cell>Method</cell><cell>Walking</cell><cell>Jogging</cell></row><row><cell></cell><cell cols="2">S1 S3 S3 S1 S2 S3 Avg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation Study Results. The loss ablations show the effect of each loss function on the final model performance. We incrementally add each of the proposed loss functions.</figDesc><table><row><cell>Loss Functions</cell><cell>P-MPJPE</cell></row><row><cell>Lin</cell><cell>67.5</cell></row><row><cell>Lproj</cell><cell>68.0</cell></row><row><cell>Lin + Lproj</cell><cell>65.3</cell></row><row><cell>Lin + Lproj + Lcon</cell><cell>63.9</cell></row><row><cell>Lin + Lproj + Lcon + Lout</cell><cell>62.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Holistic human pose estimation with regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno>abs/1904.04812</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Forecasting people trajectories and head poses by jointly reasoning on tracklets and vislets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsesmelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1267" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7779" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d human pose estimation from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised learning of interpretable keypoints from unlabelled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8787" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised multi-view synchronization learning for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-toend recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3d human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised adversarial learning of 3d human pose from 2d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ogaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Odagiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6152" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1811.04989</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation using transfer learning and improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>abs/1611.09813</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">C3dpo: Canonical 3d pose networks for non-rigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7688" to="7697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coarseto-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1888" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="750" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A joint model for 2d and 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3634" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Posenet3d: Unsupervised 3d human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-toimage translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4364" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Traffic control gesture recognition for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiederer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouazizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10676" to="10683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="365" to="382" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10350</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1648" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
