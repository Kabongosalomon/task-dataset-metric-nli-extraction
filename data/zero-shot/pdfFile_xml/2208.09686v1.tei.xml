<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YOLOV: Making Still Image Object Detectors Great at Video Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Shi</surname></persName>
							<email>yuheng@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University ? TuSimple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University ? TuSimple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University ? TuSimple Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">YOLOV: Making Still Image Object Detectors Great at Video Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video object detection (VID) is challenging because of the high variation of object appearance as well as the diverse deterioration in some frames. On the positive side, the detection in a certain frame of a video, compared with in a still image, can draw support from other frames. Hence, how to aggregate features across different frames is pivotal to the VID problem. Most of existing aggregation algorithms are customized for two-stage detectors. But, the detectors in this category are usually computationally expensive due to the two-stage nature. This work proposes a simple yet effective strategy to address the above concerns, which spends marginal overheads with significant gains in accuracy. Concretely, different from the traditional two-stage pipeline, we advocate putting the region-level selection after the one-stage detection to avoid processing massive low-quality candidates. Besides, a novel module is constructed to evaluate the relationship between a target frame and its reference ones, and guide the aggregation. Extensive experiments and ablation studies are conducted to verify the efficacy of our design, and reveal its superiority over other state-of-the-art VID approaches in both effectiveness and efficiency. Our YOLOX-based model can achieve promising performance (e.g., 87.5% AP50 at over 30 FPS on the ImageNet VID dataset on a single 2080Ti GPU), making it attractive for large-scale or real-time applications. The implementation is simple, the demo code and models have been made available at https://github.com/YuHengsss/YOLOV.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Object detection, as a key component in a wide spectrum of vision-based intelligent applications <ref type="bibr" target="#b5">(Dalal and Triggs 2005;</ref><ref type="bibr" target="#b8">Felzenszwalb, McAllester, and Ramanan 2008)</ref>, aims to simultaneously locate and classify objects in images. Thanks to the strong ability of Convolutional Neural Networks (CNN) <ref type="bibr" target="#b22">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, numerous CNN-based object detection models have been recently proposed, which can be roughly divided into two categories, say one-stage and two-stage object detectors, according to their detection processes. Specifically, two-stage detectors first select possible object regions (proposals), and then classify these regions. The series of Region-based CNN (R-CNN) <ref type="bibr" target="#b12">(Girshick et al. 2014;</ref><ref type="bibr" target="#b11">Girshick 2015;</ref><ref type="bibr" target="#b31">Ren et al. 2015)</ref> is the pioneer of two-stage object detectors with a variety of follow-ups <ref type="bibr" target="#b23">Lin et al. 2017a;</ref><ref type="bibr" target="#b4">Dai et al. 2016;</ref><ref type="bibr" target="#b2">Cai and Vasconcelos 2018;</ref><ref type="bibr" target="#b18">He et al. 2017;</ref> Base Detector Ours <ref type="figure">Figure 1</ref>: The frames suffer various degradation, like motion blur and occlusion, making the base YOLOX fail to accomplish the task. Our method can precisely predict the target. Given region-level features, these detectors for still images can be easily transferred to more complicated tasks such as segmentation and video object detection. However, due to the two-stage nature, the efficiency is a bottleneck for practical use, while for one-stage object detectors, the location and classification are jointly and directly produced by the dense prediction from feature maps. The YOLO family <ref type="bibr" target="#b29">(Redmon et al. 2016;</ref><ref type="bibr" target="#b30">Redmon and Farhadi 2017;</ref><ref type="bibr" target="#b1">Bochkovskiy, Wang, and Liao 2020)</ref> and SSD <ref type="bibr" target="#b27">(Liu et al. 2016)</ref> are representatives in this group. Without directly involving region proposals, compared to the aforementioned two-stage approaches, the speed of one-stage detectors is superior and suitable for scenarios with real-time requirements. Though the accuracy of one-stage detectors are typically inferior at the beginning, following designs <ref type="bibr" target="#b24">(Lin et al. 2017b;</ref><ref type="bibr">Ge et al. 2021b,a;</ref><ref type="bibr" target="#b36">Tian et al. 2019</ref>) largely alleviate the accuracy gap.</p><p>Video object detection can be viewed as an advanced version of still image object detection. Intuitively, one can process video sequences via feeding frames one-by-one into still image object detectors. But, by this means, the temporal information across frames will be wasted, which could be the key to eliminating/reducing the ambiguity happened in a single image. As shown in <ref type="figure">Fig. 1</ref>, degradation such as motion blur, camera defocus, and occlusion often appears in video frames, significantly increasing the difficulty of detection. For instance, via solely looking at the last frame in <ref type="figure">Fig. 1</ref>, it is hard or even impossible for human beings to tell where and what the object is. On the other hand, video sequences can provide richer information than single still images. In other words, other frames in the same sequence can possibly support the prediction for a certain frame. Hence, how to effectively aggregate temporal messages from different frames is crucial to the accuracy. As can be seen from <ref type="figure">Fig. 1</ref>, our proposed method gives correct answers.</p><p>In the literature, there are two main types of frame aggregation, i.e., box-level and feature-level. These two technical routes can boost detection accuracy from different perspectives. Regarding box-level methods, they connect predictions by still object detectors through linking bounding boxes to form tubelet, and then refine the results in the same tubelet. The box-level methods can be viewed as post-processing, which are flexible to be applied to both one-stage and two-stage detectors. While for feature-level schemes, the features of a keyframe are enhanced by finding and aggregating similar features from other frames (a.k.a., reference frames). The two-stage manner endows proposals with explicit representation from the backbone feature map extracted by Region Proposal Network (RPN) <ref type="bibr" target="#b31">(Ren et al. 2015)</ref>. Benefiting from this nature, two-stage detectors can be easily migrated to the video object detection problem. Hence, most of video object detectors are built on two-stage detectors. However, because of the introduction of seeking the relationship between proposals, these two-stage video object detectors are further decelerated, and thus hardly meet the need of real-time scenarios. Different from the two-stage bases, proposals are implicitly represented by elements of feature maps from a one-stage detector. Though, without explicit representations for objects, these elements of feature maps can still benefit from aggregating temporal information for the VID task. Again, as previously mentioned, the one-stage strategies usually execute faster than those twostage ones. Driven by these considerations, a natural question arises: Can we make such region-level designs available to one-stage detectors that merely contain pixel-level features for building a practical (accurate and fast) video object detector?</p><p>Contribution. This paper answers the above question via designing a simple yet effective strategy to aggregate fea-tures generated by one-stage detectors 1 (e.g., YOLOX <ref type="bibr" target="#b10">(Ge et al. 2021b)</ref> in this work to verify the primary claims). To connect the features of reference frames with those of the keyframe, we propose a feature similarity measurement module to construct an affinity matrix, which is then employed to guide the aggregation. To further mitigate the limitation of cosine similarity, an average pooling operator on reference features is customized. These two operations cost limited computational resources with significant gains in accuracy. Equipped with the proposed strategies, our model, termed as YOLOV, can achieve a promising accuracy 85.5% AP50 on the ImageNet VID dataset with 40+ FPS on a single 2080Ti GPU (please see <ref type="figure" target="#fig_0">Fig. 2</ref> for details) without bells and whistles, which is attractive for practical scenarios. By further introducing post-processing, its accuracy reaches higher up to 87.5% AP50 at over 30 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>This section will briefly review representative approaches in object detection closely related to this work.</p><p>Object Detection in Still Images. Thanks to the development of hardware, large-scale datasets <ref type="bibr" target="#b25">(Lin et al. 2014;</ref><ref type="bibr" target="#b22">Krizhevsky, Sutskever, and Hinton 2012)</ref> and sophisticated network structures <ref type="bibr" target="#b35">(Simonyan and Zisserman 2015;</ref><ref type="bibr" target="#b19">He et al. 2016;</ref><ref type="bibr" target="#b40">Xie et al. 2017;</ref><ref type="bibr" target="#b38">Wang et al. 2020)</ref>, the performance of object detection has continuously improved. Existing object detectors can be mainly divided into two-stage and onestage schemes. Representative two-stage detectors such as RCNN <ref type="bibr" target="#b12">(Girshick et al. 2014)</ref>, Faster RCNN <ref type="bibr" target="#b31">(Ren et al. 2015)</ref>, R-FCN <ref type="bibr" target="#b4">(Dai et al. 2016)</ref>, and Mask RCNN . The methods in this group first select candidate regions through RPN and then extract features for the candidates through some feature extraction modules like RoIPooling <ref type="bibr" target="#b31">(Ren et al. 2015)</ref> and RoIAlign ). Finally, they complete the bounding box regression and classification through the detection head. There are also many successful one-stage detectors such as the YOLO series <ref type="bibr" target="#b29">(Redmon et al. 2016;</ref><ref type="bibr" target="#b30">Redmon and Farhadi 2017;</ref><ref type="bibr" target="#b1">Bochkovskiy, Wang, and Liao 2020;</ref><ref type="bibr" target="#b10">Ge et al. 2021b</ref>), SSD <ref type="bibr" target="#b27">(Liu et al. 2016)</ref>, RetinaNet <ref type="bibr" target="#b24">(Lin et al. 2017b</ref>), FCOS <ref type="bibr" target="#b36">(Tian et al. 2019)</ref>. Different from the two-stage ones, the one-stage detectors perform dense prediction on feature maps and directly give the position and class probability without the step of region proposal. One-stage detectors are usually faster than two-stage ones, owing to the end-to-end manner. However, they lack explicit region-level semantic features that are widely used for feature aggregation in video object detection. Our work attempts to explore the feasibility of aggregation over pixel-level features for one-stage detectors.</p><p>Object Detection in Videos. Compared to still image object detection, degradation may frequently appear in some video frames. When the keyframe is polluted, temporal information could be used for better detection. One branch of existing video object detectors concentrate on video-level post-processing <ref type="bibr">(Han et al. 2016;</ref><ref type="bibr" target="#b0">Belhassen et al. 2019;</ref><ref type="bibr" target="#b33">Sabater, Montesano, and Murillo 2020</ref>  <ref type="figure">Figure 3</ref>: Framework of our design. Taking the YOLOX as an example base detector, the corresponding model is termed as YOLOV. We randomly sample a number of frames from a video and feed them into the base detector to extract features. According to the prediction of YOLOX, the Feature Selection Module (FSM) picks out top k confident proposals and applies the NMS on the selected proposals for further refinement. All the features from the FSM are input into our Feature Aggregation Module (FAM) to produce information for final classification. The proposed strategy can be easily applied to other bases.</p><p>group try to refine the prediction results from the still image detector in consecutive frames by forming the object tubelets. The final classification will be uniformly adjusted to each tubelet in hand-crafted fashions. Another branch aims to enhance the features of keyframes, expecting to alleviate degradation via utilizing the features from (selected) reference frames. These approaches can be roughly classified as optical flow-based <ref type="bibr" target="#b44">(Zhu et al. 2017a</ref><ref type="bibr" target="#b43">(Zhu et al. , 2018</ref>, attentionbased <ref type="formula">(</ref> Although these mentioned approaches boost the precision of detection, they are mostly based on two-stage detectors and hence suffer from the relatively slow inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Considering the characteristics of videos (various degradation vs. rich temporal information), instead of individually processing frames, how to seek supportive information from other frames for a target one (keyframe) plays a key role in boosting the accuracy of video detection. Recent attempts <ref type="bibr" target="#b6">(Deng et al. 2019;</ref><ref type="bibr" target="#b3">Chen et al. 2020;</ref><ref type="bibr" target="#b39">Wu et al. 2019;</ref><ref type="bibr" target="#b17">He et al. 2022)</ref> with noticeable improvement in accuracy corroborate the importance of temporal aggregation to the problem. However, most of the existing methods are two-stage based techniques. As previously discussed, their main drawback is relatively slow inference speed, compared to one-stage bases. To mitigate this limitation, we put the region/feature selection after the prediction head of onestage detectors. In this section, we choose the YOLOX as base to present our main claims. Our proposed framework is schematically depicted in <ref type="figure">Fig. 3</ref>. Let us recall the traditional two-stage pipeline: 1) massive candidate regions are first "selected" as proposals; and 2) determine if each proposal is an object or not and which class it belongs to. The computational bottleneck mainly comes from dealing with substantial low-confidence region candidates. As can be seen from <ref type="figure">Fig. 3</ref>, our pipeline also contains two stages. Differently, its first stage is prediction (having a large number of regions with low confidences discarded), while the second stage can be viewed as region-level refinement (taking advantage of other frames by aggregation). By this principle, our design can simultaneously benefit from the efficiency of one-stage detectors and the accuracy gained from temporal aggregation. It is worth to emphasize that such a small difference in design leads to a huge difference in performance.</p><p>The proposed strategy can be generalized to many base detectors such as YOLOX <ref type="bibr" target="#b10">(Ge et al. 2021b</ref>), FCOS <ref type="bibr" target="#b36">(Tian et al. 2019</ref>) and PPYOLOE <ref type="bibr" target="#b41">(Xu et al. 2022</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Design</head><p>From a possible perspective of human beings, the recognition procedure would first link related instances in temporal and identify which class they belong to until sufficiently confident messages are collected. Then, the determination can be broadcast to less confident cases. The mechanism of multi-head attention, as a key part of Transformers <ref type="bibr" target="#b37">(Vaswani et al. 2017)</ref>, seems to fit the situation well, which lifts the capability of long-range modeling. Given a sequence Z, the query, key, and value matrices are packed as Q, K and V , respectively. The self-attention can be calculated via:</p><formula xml:id="formula_0">SA(Z) = softmax (A) V with A = QK T / ? d, (1)</formula><p>where d is the dimension of each feature in Q (also K). Executing m self-attentions in parallel yields the multi-head attention by simply concatenating them together as follows:</p><formula xml:id="formula_1">MSA(Z) = concat SA 1 (Z), SA 2 (Z), ..., SA m (Z) .</formula><p>(2) Modern two-stage based video object detectors typically obtain candidate regions for feature aggregation by, for example, RPN <ref type="bibr" target="#b31">(Ren et al. 2015)</ref>. As a representative, Relation-Net <ref type="bibr" target="#b21">(Hu et al. 2018</ref>) first introduces the above multi-head attention to the still object detection task by viewing a sequence of proposals as input. The ROI pooling and/or alignment are applied to these proposals to extract region-level features. However, one-stage detectors make dense prediction directly from feature maps. Simply transferring the region-level feature aggregation to the whole feature maps of one-stage detectors will result in intensive computational cost. To address this issue, we propose an effective strategy for selecting features that suit the multi-head attention.</p><p>FSM: Feature Selection Module As most of the predictions are of low confidence, the detection head of one-stage detectors is a natural and rational option to select (highquality) candidates from the feature maps. Following the process of RPN, we first pick out top k (e.g., 750) predictions according to the confidence scores. Then, a fixed quantity a of predictions (e.g., a = 30) are chosen by the Non-Maximum Suppression (NMS) for reducing the redundancy. To obtain features for video object classification, the accuracy of the base detector should be largely guaranteed. In practice, we found that directly aggregating the selected features in the classification branch and backpropagating the classification loss of the aggregated features will result in unstable training. To address the above concerns, we insert two 3 ? 3 convolutional (Conv) layers into the model neck as a new branch, called video object classification branch, which generates features for aggregation. Then, we feed the features from both the video classification and regression branches with respect to the locations into our feature aggregation module.</p><p>FAM: Feature Aggregation Module Now we come to the step of connecting related RoIs. For a certain (key) frame, let F = {C 1 , C 2 , ..., C f ; R 1 , R 2 , ..., R f } denote the feature set selected by FSM. In addition, C i ? R dq?a = c i 1 , c i 2 , ..., c i a and R i ? R dq?a = r i 1 , r i 2 , ..., r i a designate the features of the i-th frame in F from the video classification and regression branches, respectively. The generalized cosine similarity is arguably the most widely used metric to compute the similarity between features or the attention weight <ref type="bibr" target="#b39">(Wu et al. 2019;</ref><ref type="bibr" target="#b34">Shvets, Liu, and Berg 2019;</ref><ref type="bibr" target="#b6">Deng et al. 2019)</ref>. Merely referring to the cosine similarity will find features most similar to the target. However, when the keyframe suffers from some degradation, the selected proposals corresponding to those similar features very likely have the same problem. We name this phenomenon the homogeneity issue.</p><p>To overcome this issue, we further take prediction confidences from the base into consideration, denoted as P = {P 1 , P 2 , ..., P f } with P i = p i 1 , p i 2 , ..., p i a . In this work, each column of P i only contains 2 scores, i.e., the classification score and IoU score respectively from the classification and regression heads. Then, similarly to <ref type="bibr" target="#b37">(Vaswani et al. 2017)</ref>, the query, key, and value matrices are constructed and fed into the multi-head attention. For instance, Q c and Q r are respectively formed by combining the features from the classification branch and the regression branch along the temporal dimension (i.e.,</p><formula xml:id="formula_2">Q c ? R f a?d = LP([C 1 , C 2 , ..., C f ] T ) and Q r ? R f a?d = LP([R 1 , R 2 , ..., R f ] T ), where LP(?)</formula><p>is the linear projection operator), while the others are done analogously. By the scaled dot-product in attention, we obtain the corresponding</p><formula xml:id="formula_3">A c = Q c K T c / ? d and A r = Q r K T r / ? d.</formula><p>Gathering all the scores in P gives a matrix [P 1 , P 2 , ..., P f ] of size 2 ? f a. To make these scores fit into the attention weights, we build two matrices, say S r ? R f a?f a and S c ? R f a?f a , through respectively repeating each of two rows f a times. As a consequence, the self-attentions for the classification and regression branches turn out to be:</p><formula xml:id="formula_4">SA c (C) = softmax (S c ? A c ) V c , SA r (R) = softmax (S r ? A r ) V c ,<label>(3)</label></formula><p>where ? represents the Hadamard product. To be clearer, let s ij stand for the (i, j)-th element in S, while Q i and K j represent the i-th row of Q and the j-th row of K, respectively. Each coefficient in the pre-softmax weight is s ij ? Q i (K j ) T = Q i (s ij ? K j ) T . This is to say, the selfattention considers not only the similarity between the query and key items, but also the quality of the key. Note that, since the main purpose is to refine the classification, SA c (C) and SA r (R) share the same value matrix V c . Our experiments demonstrate that replacing the original QK manner with Eq.</p><p>(3) (called affinity manner) in multi-head attention can significantly boost the performance of video object detector. Besides, we concatenate V c with the outputs of Eq. (3) for better preserving initial representations via:</p><formula xml:id="formula_5">SA(F) = concat((SA c (C) + SA r (R), V c ).<label>(4)</label></formula><p>Different from the traditional ViT, the positional information is not embedded, because the locations in a long temporal range would not be helpful as claimed in . Moreover, considering the characteristic of softmax, may a small part of reference features hold a large portion of weights. In other words, it often overlooks the features with low weights, which limits the diversity of reference features for possible follow-up use. To avoid such risks, we introduce an average pooling over reference features (A.P.). Concretely, we select all of the references with similarity scores above a threshold ? and apply the average pooling to these survivals. Note that the similarity in this work is computed via N (V c )N (V c ) T . The operator N (?) means the layer normalization, which guarantees the values to be in a certain range and thus eliminates the influence of scale difference. By doing so, we can maintain more information from related features. The average-pooled features and the key features are then transported into one linear projection layer for final classification. The procedure is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. One may ask that if N (Q c )N (K c ) T or N (Q r )N (K r ) T can perform as the similarity. Indeed, it is another option. But, in practice, it is not as stable as our choice during training due to the discrepancy between Q and K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Validation Implementation Details</head><p>Similar to previous works, we also initialize our base detector from COCO pre-trained weights provided by YOLOX. Following <ref type="bibr" target="#b44">(Zhu et al. 2017a;</ref><ref type="bibr" target="#b39">Wu et al. 2019;</ref><ref type="bibr" target="#b13">Gong et al. 2021)</ref>, we combine the videos in the ImageNet VID and those in the ImageNet DET with the same classes as our training data. Specifically, the ImageNet VID <ref type="bibr" target="#b32">(Russakovsky et al. 2015)</ref> contains 3,862 videos for training and 555 videos for validation. There are 30 categories in the VID dataset, i.e., a subset of the 200 basic-level categories of Im-ageNet DET <ref type="bibr" target="#b32">(Russakovsky et al. 2015)</ref>. Considering the redundancy of video frames, we randomly sample 1/10 frames in the VID training set instead of using all of them. The base detectors are trained for 7 epochs by SGD with batch size of 16 on 2 GPUs. As for the learning rate, we adopt the cosine learning rate schedule used in YOLOX with one epoch for warming up and disable the strong data augmentation for the last 2 epochs. When integrating the feature aggregation module into the base detectors, we fine-tune them for 150K iterations with batch size of 16 on a single 2080Ti GPU. In addition, we use warm up with the first 15K iterations and cosine learning rate schedule for the rest iterations. Only the linear projection layers in YOLOX prediction head, the newly added video object classification branch and the multi-head attention are fine-tuned for the sake of largely excluding the influence from other factors. It is positive that further improvements could be obtained when more layers in the model participate in the BP procedure. For training the feature aggregation module, the number of frames f is set to 16, and the threshold of NMS is set to 0.75 for rough feature selection. While for producing final detection boxes, we alternatively set the threshold of NMS to 0.5 for catching more confident candidates. In the training phase, the images are randomly resized from 352 ? 352 to 672 ? 672 with 32 strides. In the testing phase, the images are uniformly resized to 576 ? 576. The AP50 and inference speed are two metrics to reflect the performance in terms of accuracy and efficiency, respectively. Regarding the inference speed, we test all of the models with FP16-precision on a 2080Ti GPU unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>On the reference frame sampling strategy. Investigating frame sampling strategies to balance accuracy and efficiency is crucial for video object detectors. Several global and local sampling schemes have been discussed in previous twostage based methods <ref type="bibr" target="#b39">(Wu et al. 2019;</ref><ref type="bibr" target="#b13">Gong et al. 2021;</ref><ref type="bibr" target="#b3">Chen et al. 2020)</ref>. As for the global sampling schemes, f g frames are randomly selected from the whole video. With respect to the local sampling ones, f l consecutive frames are employed. To see the effect of different sampling strategies, we vary the numbers of reference frames in both global and local modes. The numerical results are reported in <ref type="table" target="#tab_1">Table 1</ref>. The performance of using only 3 global reference frames has already outperformed that of using 39 local reference frames, which corroborates the evidence given in <ref type="bibr" target="#b39">(Wu et al. 2019;</ref><ref type="bibr" target="#b13">Gong et al. 2021)</ref>. As a trade-off, we adopt the global sampling strategy with f g = 31 by default for the rest experiments according to <ref type="table" target="#tab_1">Table 1</ref>.</p><p>On the number of proposals in a single frame. In this experiment, we adjust the quantity of most confident proposals remained for each frame a from 10 to 100 in the FSM, to see its influence on the performance. As shown in Table 2, as a increases, the accuracy continuously goes higher and levels off until reaching 75. As the complexity of selfattention is O n 2 with respect to the amount of input proposals, dragging too many proposals per frame will dramat-  <ref type="figure">Figure 5</ref>: Visual comparison between reference proposals selected by three different methods for given key proposals. We display 4 reference proposals that contribute most in aggregation.  ically raise the time cost. By considering both the speed and accuracy, we adopt a = 30 that is much smaller than the optimal setting with 75 proposals used in the two-stage based method RDN <ref type="bibr" target="#b6">(Deng et al. 2019</ref>).</p><p>On the threshold in average pooing of reference features.</p><p>Here, we test the effect of different thresholds for average pooling over reference features. <ref type="table" target="#tab_3">Table 3</ref> lists the numerical results. As can be viewed, when all the features participate in the average pooling, i.e. ? = 0, the AP50 is mere 73.1%.</p><p>Lifting the selection standard results in better performance. When ? falls in [0.75, 0.85], the accuracy keeps steady high to 77.3%. But, when ? = 1, the average pooling is equivalent to only duplicating SA(F), the accuracy of which drops to 76.9%. Dynamically determining the threshold for different cases is desired and left as our future work. For the rest experiments, we adopt ? = 0.75 as the default threshold.</p><p>On the effectiveness of FAM. To validate the effectiveness of the affinity manner (A.M.) and the average pooling over reference features (A.P.), we evaluate the performance with and without these modules. The results in <ref type="table" target="#tab_4">Table 4</ref> reveal that these designs both can help the feature aggregation to catch better semantic representations from the one-stage detector. Compared to the YOLOX-S (69.5% AP50), the YOLOV-S only armed with A.M. gains 7.4% in accuracy. While equipping YOLOV-S with both A.M. and A.P. (complete YOLOV-S), the performance reaches 77.3% in AP50 with only about 2ms time cost in comparison with YOLOV-S. To be more convincing, we also plug our design into the YOLOX-L and YOLOX-X. Tabel 5 shows the detailed comparison between YOLOXs and YOLOVs. The ? indicates using strong augmentation (like MixUp <ref type="bibr" target="#b42">(Zhang et al. 2018a)</ref> and Mosaic (Bochkovskiy, Wang, and Liao 2020)) when fine-tuning our version. Our YOLOVs consistently outperform their respective bases by over 7% in AP50.</p><p>Moreover, we provide two cases to intuitively exhibit the advance of our FAM. They are the lion case with rare pose and the fox case with motion blur as shown in <ref type="figure">Fig 5.</ref> Without loss of generality, the top 4 reference proposals are listed for different feature selection modes, including the cosine similarity, QK manner in multi-head attention, and our affinity manner. As previously analyzed, the cosine manner selects proposals most similar to the key proposal but suffering the same degradation problem as the key proposal. Though the QK manner alleviates this problem, it is obviously inferior to the affinity manner. By introducing the confidence scores as guidance, our method selects better proposals and further boosts the detection accuracy.   <ref type="bibr" target="#b17">(He et al. 2022)</ref>. As can be observed, our method can achieve 85.5% AP50 with 21.1 ms per frame. With the help of REPP <ref type="bibr" target="#b33">(Sabater, Montesano, and Murillo 2020)</ref>, it reaches 87.5% AP50 spending extra 6 ms. In terms of inference efficiency, our method is much faster than the other methods. To be specific, in the upper part of <ref type="table" target="#tab_6">Table 6</ref>, we report the performance of the involved competing models without adopting any post-processing. Thanks to the property of one-stage detectors and the effectiveness of our strategy, the YOLOVs can remarkably leverage the detection accuracy and inference efficiency. For fair comparison, all the models listed in <ref type="table" target="#tab_6">Table 6</ref> are tested with the same hardware environment except for MAMBA and QueryProp 2 . The   <ref type="table" target="#tab_6">Table 6</ref> reports the results of our YOLOV and the other SOTA models with post-processing. The time cost of post-processing is tested on an i7-8700K CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Concerns</head><p>Performance on different motion speeds. We separately evaluate the detection performance on the ImageNet VID categories of slow, medium and fast moving objects. The categories are defined by their average IoU scores between objects across nearby frames (Slow: IoU&gt; 0.9, Medium: 0.9 ?IoU? 0.7, Fast: 0.7 &gt;IoU). Two other methods FGFA <ref type="bibr" target="#b44">(Zhu et al. 2017a</ref>), SELSA <ref type="bibr" target="#b39">(Wu et al. 2019</ref>) and our base detectors <ref type="bibr" target="#b10">(Ge et al. 2021b)</ref> with the same post processing strategy <ref type="bibr" target="#b33">(Sabater, Montesano, and Murillo 2020)</ref> are involved in the comparison. As shown in <ref type="table" target="#tab_8">Table 7</ref>, the effectiveness of our models is clearly verified on each category. The improvement enlarges as the moving speed increases.</p><p>Inference with different batches. In the main paper, we test our model with batch size = 1 and the reference frame features extracted for backbone are saved temporarily. Considering the offline situation, we can feed multiple frames to the model and give the result of these frames simultaneously, i.e., the batch inference manner. In this way, the inference speed can be further improved. In this setting, we select 32 frames randomly from a video to form a batch and test the speed of our models. Benefiting from the batch inference, the inference times of our small, large, and xlarge models are respectively deduced to 3.70 ms, 7.13 ms, and 12.10 ms, as shown in <ref type="table" target="#tab_9">Table 8</ref>.</p><p>Application to other base detectors. In order to validate the generalization ability of the proposed strategy, we also try it on widely-used one-stage detectors including PPYOLOE <ref type="bibr" target="#b41">(Xu et al. 2022</ref>) and FCOS <ref type="bibr" target="#b36">(Tian et al. 2019)</ref>. Specifically for PPYOLOE, it has different channel numbers at different FPN levels. To achieve the multi-scale feature  aggregation, we straightforwardly modify the channel numbers of the detection head at different scales to be the same. While for FCOS, the backbone is ResNet-50 . There exist 5 FPN levels in the original architecture for dealing with images with large image sizes (e.g., 1333 ? 800). To match the situation of the ImageNet VID, we maintain 3 FPN levels with the largest downsampling rate of 32. For the training procedure and other hyper-parameters settings, we simply keep them the same as those in the YOLOX. <ref type="table" target="#tab_10">Table 9</ref> shows that our strategy can consistently improve different base detectors by over 5% in terms of AP50. It is worth noting that searching more suitable hyper-parameters for different base detectors could obtain better performance.</p><p>Evaluation on the OVIS dataset. Besides the experiments on the ImageNet VID, we also verify our video object detector on the Occluded Video Instance Segmentation (OVIS) <ref type="bibr" target="#b28">(Qi et al. 2022)</ref> dataset. There are 607 videos for training and 140 for validation involving 25 classes in this dataset. With 4.72 objects per frame on average and a large portion of objects suffering severe occlusions, the OVIS dataset brings more challenging scenarios. For the training, the COCO pre-trained weights are finetuned for 10 epochs with batch size of 8 on 4 GPUs. The video object detectors are trained for 7 epochs. In addition, the proposal number per frame is set to 75 to match the object density. The images are randomly resized from 480 ? 720 to 800 ? 1200 with 32 strides (shorter side) for the multi-scale training. In the testing phase, the images are uniformly resized to 640 ? 960. We keep the other settings the same as the VID implementation. <ref type="table" target="#tab_1">Table 10</ref> shows the performance of ours and the base detectors evaluated at the server OVIS.</p><p>Visual comparisons. We offer visual results of detection on several samples from the VID dataset by our model, the base of our model-YOLOX, and a SOTA two-stage video object detector-MEGA in <ref type="figure">Fig. 6</ref>, for more intuitive comparisons. To reveal the robustness of our model against different types of degradation, we select three challenging cases with (a) motion blur, (b) rare poses, and (c) occlusion. As can be seen from the pictures, without temporal information, the base detector, even human beings, can hardly tell what the objects exactly are, e.g., the last frame in (a), the first frame in (b), and the third in (c). By equipping with our feature aggregation module, the prediction is significantly boosted precise. We would like to emphasize again that our model is about 10 times faster than MEGA. Moreover, we also visually compare our model with the base detector on samples from the OVIS dataset <ref type="figure" target="#fig_3">(Fig. 7)</ref>. As we can see, the severe occlusion in OVIS limits the performance of the base detector, while our method remarkably alleviates the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we developed a practical video object detector that jointly considers the detection accuracy and inference efficiency. To improve the detection accuracy, a feature aggregation module was designed to effectively aggregate temporal information. For saving computational resources, different from existing two-stage detectors, we proposed to put the region selection after the (rough) prediction. This subtle change makes our detectors significantly more efficient. Experiments and ablation studies have been conducted to verify the effectiveness of our strategy, and its advance over previous arts. The core idea is simple and general, which can potentially inspire further research works and broaden the applicable scenarios related to video object detection. <ref type="figure">Figure 6</ref>: Visual comparisons between YOLOX-X (base detector), our YOLOV-X, and MEGA-ResNet101. Three cases suffer from different types of degradation: (a) motion blur, (b) rare pose, and (c) occlusion. Our method exhibits its robustness against these challenging cases. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Performance comparison in accuracy (AP50) and speed. QueryProp and MBMBA are tested on a TITAN RTX GPU as reported in their papers, while the others are tested on a 2080Ti GPU. * denotes involving post-processing. et al. 2018), remarkably boosting the accuracy of detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b39">Wu et al. 2019;</ref><ref type="bibr" target="#b6">Deng et al. 2019;</ref><ref type="bibr" target="#b3">Chen et al. 2020;</ref><ref type="bibr" target="#b13">Gong et al. 2021;</ref><ref type="bibr" target="#b35">Sun et al. 2021</ref>) and tracking-based<ref type="bibr" target="#b7">(Feichtenhofer, Pinz, and Zisserman 2017;</ref><ref type="bibr" target="#b43">Zhang et al. 2018b)</ref> methods. Deep feature flow<ref type="bibr" target="#b45">(Zhu et al. 2017b</ref>) first introduces optimal flow for image-level feature alignment, and FGFA<ref type="bibr" target="#b44">(Zhu et al. 2017a</ref>) adopts the optical flow to aggregate features along motion paths. Considering the computation cost of image-level feature aggregation, several attentionbased methods have been developed. As a representative, SESLA<ref type="bibr" target="#b39">(Wu et al. 2019</ref>) proposes a long-range feature aggregation scheme according to the semantic similarity between region-level features. Inspired by the relation module from Relation Networks<ref type="bibr" target="#b21">(Hu et al. 2018</ref>) for still image detection, RDN<ref type="bibr" target="#b6">(Deng et al. 2019)</ref> captures the relationship between objects in both spatial and temporal contexts. Furthermore, MEGA) designs a memory enhanced global-local aggregation module for better modeling objects relationship. Alternatively, TROIA<ref type="bibr" target="#b13">(Gong et al. 2021</ref>) executes the ROI alignment operation for fine-grained feature aggregation, while HVR-Net<ref type="bibr" target="#b14">(Han et al. 2020)</ref> integrates intra-video and inter-video proposal relations for further improvement. Moreover, MBMBA (Sun et al. 2021) enlarges the reference feature set by introducing memory bank. QueryProp<ref type="bibr" target="#b17">(He et al. 2022</ref>) notices the high computational cost of video detectors and tries to speed up the process through a lightweight module. Besides the attention based methods, D&amp;T<ref type="bibr" target="#b7">(Feichtenhofer, Pinz, and Zisserman 2017)</ref> tries solving video object detection in a tracking manner by constructing correlation maps of different frame features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Our average pooling over reference features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>More visual comparisons between the base detector (upper row) and ours (lower row) on the OVIS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Effect of the numbers of global f g and local f l reference frames.</figDesc><table><row><cell>f g</cell><cell>3</cell><cell>7</cell><cell>15</cell><cell>23</cell><cell>31</cell><cell>39</cell></row><row><cell cols="7">AP50 (%) 74.6 76.1 77.0 77.3 77.3 77.3</cell></row><row><cell>f l</cell><cell>3</cell><cell>7</cell><cell>15</cell><cell>23</cell><cell>31</cell><cell>39</cell></row><row><cell cols="7">AP50 (%) 70.4 71.2 72.2 72.8 73.2 73.6</cell></row><row><cell cols="7">Table 2: Effect of the number of proposals a in FSM.</cell></row><row><cell>a</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>50</cell><cell cols="2">75 100</cell></row><row><cell cols="7">AP50 (%) 76.1 77.0 77.3 77.3 77.4 77.4</cell></row><row><cell cols="7">Time (ms) 10.4 10.7 11.3 13.8 19.5 29.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Influence of the threshold ? in average pooling over reference features.</figDesc><table><row><cell>?</cell><cell>0</cell><cell>0.2 0.5 0.65 0.75 0.85 1</cell></row><row><cell cols="3">AP50 (%) 73.1 73.9 76.1 77.1 77.3 77.3 76.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness of our affinity manner (A.M.) and average pooling over reference features (A.P.).</figDesc><table><row><cell cols="5">Methods A.M. A.P. Time (ms) AP50 (%)</cell></row><row><cell>YOLOX-S</cell><cell>-</cell><cell>-</cell><cell>9.38</cell><cell>69.5</cell></row><row><cell>YOLOV-S</cell><cell></cell><cell>-</cell><cell>10.95</cell><cell>76.9 ?7.4</cell></row><row><cell>YOLOV-S</cell><cell></cell><cell></cell><cell>11.30</cell><cell>77.3 ?7.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effectiveness of our strategy compared to bases. ? indicates using strong augmentation.</figDesc><table><row><cell>Model</cell><cell cols="4">Params GFLOPs Time (ms) AP50 (%)</cell></row><row><cell>YOLOX-S</cell><cell>8.95M</cell><cell>21.63</cell><cell>9.4</cell><cell>69.5</cell></row><row><cell cols="2">YOLOV-S 10.28M</cell><cell>26.18</cell><cell>11.3</cell><cell>77.3 ?7.8</cell></row><row><cell cols="3">YOLOX-L 54.17M 125.90</cell><cell>14.8</cell><cell>76.1</cell></row><row><cell cols="3">YOLOV-L 59.45M 143.10</cell><cell>16.4</cell><cell>83.6 ?7.5</cell></row><row><cell cols="3">YOLOX-X 99.02M 228.15</cell><cell>20.4</cell><cell>77.8</cell></row><row><cell cols="3">YOLOV-X 107.26M 254.72</cell><cell>22.7</cell><cell>85.0 ?7.2</cell></row><row><cell cols="3">YOLOV-X  ? 107.26M 254.72</cell><cell>22.7</cell><cell>85.5 ?7.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison in accuracy and efficiency. ? indicates with strong augmentation, T means the inference time is tested on a TITAN RTX GPU as reported in corresponding papers. MCSP stands for the Modified CSP v5 backbone adopted in YOLOX. The lower part involves postprocessing while the upper does not.</figDesc><table><row><cell>Method</cell><cell cols="3">Backbone AP50 (%) Time (ms)</cell></row><row><cell>FGFA</cell><cell>R101</cell><cell>76.3</cell><cell>104.2</cell></row><row><cell>SELSA</cell><cell>X101</cell><cell>83.1</cell><cell>153.8</cell></row><row><cell>RDN</cell><cell>R101</cell><cell>81.8</cell><cell>162.6</cell></row><row><cell>RDN</cell><cell>X101</cell><cell>83.2</cell><cell>-</cell></row><row><cell>MEGA</cell><cell>R101</cell><cell>82.9</cell><cell>230.4</cell></row><row><cell>MEGA</cell><cell>X101</cell><cell>84.1</cell><cell>-</cell></row><row><cell>TROIA</cell><cell>X101</cell><cell>84.3</cell><cell>285.7</cell></row><row><cell>MAMBA</cell><cell>R101</cell><cell>84.6</cell><cell>110.3(T )</cell></row><row><cell>HVR</cell><cell>X101</cell><cell>84.8</cell><cell>-</cell></row><row><cell>TransVOD</cell><cell>R101</cell><cell>81.9</cell><cell>-</cell></row><row><cell>QueryProp</cell><cell>R50</cell><cell>80.3</cell><cell>21.9(T )</cell></row><row><cell>QueryProp</cell><cell>R101</cell><cell>82.3</cell><cell>30.8(T )</cell></row><row><cell>YOLOV-S</cell><cell>MCSP</cell><cell>77.3</cell><cell>11.3</cell></row><row><cell>YOLOV-L</cell><cell>MCSP</cell><cell>83.6</cell><cell>16.3</cell></row><row><cell>YOLOV-X</cell><cell>MCSP</cell><cell>85.0</cell><cell>22.7</cell></row><row><cell>YOLOV-X  ?</cell><cell>MCSP</cell><cell>85.5</cell><cell>22.7</cell></row><row><cell>FGFA</cell><cell>R101</cell><cell>78.4</cell><cell>-</cell></row><row><cell>SELSA</cell><cell>X101</cell><cell>83.7</cell><cell>-</cell></row><row><cell>RDN</cell><cell>X101</cell><cell>84.7</cell><cell>-</cell></row><row><cell>MEGA</cell><cell>X101</cell><cell>85.4</cell><cell>-</cell></row><row><cell>HVR</cell><cell>X101</cell><cell>85.5</cell><cell>-</cell></row><row><cell>YOLOV-S</cell><cell>MCSP</cell><cell>80.1</cell><cell>11.3 + 6.9</cell></row><row><cell>YOLOV-L</cell><cell>MCSP</cell><cell>86.2</cell><cell>16.3 + 6.9</cell></row><row><cell>YOLOV-X</cell><cell>MCSP</cell><cell>87.2</cell><cell>22.7 + 6.1</cell></row><row><cell>YOLOV-X  ?</cell><cell>MCSP</cell><cell>87.5</cell><cell>22.7 + 6.1</cell></row><row><cell cols="4">Comparison with State-of-the-art Methods</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>summarizes the detailed information of the competi-</cell></row><row><cell>tors including FGFA (Zhu et al. 2017a), SELSA (Wu et al.</cell></row><row><cell>2019), RDN (Deng et al. 2019), MEGA (Chen et al. 2020),</cell></row><row><cell>TROIA (Gong et al. 2021), MAMBA (Sun et al. 2021),</cell></row><row><cell>HVR (Han et al. 2020), TransVOD (He et al. 2021) and</cell></row><row><cell>QueryProp</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Accuracy of detecting objects with different speeds.</figDesc><table><row><cell>Models</cell><cell cols="4">Backbone Slow Medium Fast</cell></row><row><cell>FGFA</cell><cell>R101</cell><cell>87.4</cell><cell>79.1</cell><cell>61.4</cell></row><row><cell>SELSA</cell><cell>R101</cell><cell>88.7</cell><cell>83.3</cell><cell>71.1</cell></row><row><cell>YOLOX-S</cell><cell>MCSP</cell><cell>80.1</cell><cell>71.4</cell><cell>55.3</cell></row><row><cell>YOLOX-L</cell><cell>MCSP</cell><cell>85.3</cell><cell>80.0</cell><cell>65.6</cell></row><row><cell>YOLOX-X</cell><cell>MCSP</cell><cell>87.9</cell><cell>80.8</cell><cell>68.6</cell></row><row><cell>YOLOV-S</cell><cell>MCSP</cell><cell>84.6</cell><cell>78.6</cell><cell>63.7</cell></row><row><cell>YOLOV-L</cell><cell>MCSP</cell><cell>89.3</cell><cell>85.8</cell><cell>72.6</cell></row><row><cell>YOLOV-X</cell><cell>MCSP</cell><cell>90.6</cell><cell>86.8</cell><cell>74.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Time cost in the offline mode with the batch inference. The post-processing is tested on an i7-8700K CPU.</figDesc><table><row><cell>Models</cell><cell>B1 (ms)</cell><cell cols="2">B32 (ms) AP50 (%)</cell></row><row><cell>YOLOV-S</cell><cell>11.3</cell><cell>3.70</cell><cell>77.3</cell></row><row><cell>YOLOV-L</cell><cell>16.3</cell><cell>7.13</cell><cell>83.6</cell></row><row><cell>YOLOV-X</cell><cell>22.7</cell><cell>12.10</cell><cell>85.5</cell></row><row><cell cols="3">YOLOV-X + Post 22.7 + 6.10 12.10 + 6.10</cell><cell>87.5</cell></row><row><cell>lower part of</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Effectiveness of our strategy on other bases.</figDesc><table><row><cell>Model</cell><cell cols="3">Params GFLOPs AP50 (%)</cell></row><row><cell>PPYOLOE-S</cell><cell>6.71M</cell><cell>12.40</cell><cell>69.5</cell></row><row><cell cols="2">PPYOLOE-S+Ours 8.04M</cell><cell>16.73</cell><cell>74.9 ?5.6</cell></row><row><cell>PPYOLOE-L</cell><cell cols="2">50.35M 89.19</cell><cell>76.9</cell></row><row><cell cols="3">PPYOLOE-L+Ours 55.63M 105.49</cell><cell>82.0 ?5.1</cell></row><row><cell>FCOS</cell><cell cols="2">31.00M 102.84</cell><cell>67.0</cell></row><row><cell>FCOS+Ours</cell><cell cols="2">36.28M 120.04</cell><cell>73.1 ?6.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Evaluation on the OVIS validation set. ?3.4 75.0 ?6.2 57.2 ?3.5</figDesc><table><row><cell>Model</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell>YOLOX-S</cell><cell>37.6</cell><cell>58.6</cell><cell>38.7</cell></row><row><cell cols="4">YOLOV-S 40.9 ?3.3 66.5 ?7.9 42.0 ?3.3</cell></row><row><cell>YOLOX-X</cell><cell>51.3</cell><cell>68.8</cell><cell>53.7</cell></row><row><cell cols="2">YOLOV-X 54.7</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please note that our design is general and suitable to many detectors with different backbones.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The codes and models of the methods with marks 'T' (the reported results are from their papers) and '-' are not available when this work is prepared.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving Video Object Detection by Seq-Bbox Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Belhassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fresse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-B</forename><surname>Bourennane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISIGRAPP (5: VISAPP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Memory enhanced global-local aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10337" to="10346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relation distillation networks for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7023" to="7032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ota: Optimal transport assignment for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Yolox: Exceeding yolo series in 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal ROI align for video object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1442" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining inter-video proposal relations for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="431" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<idno type="arXiv">arXiv:1602.08465</idno>
		<title level="m">Seq-NMS for video object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">QueryProp: Object Query Propagation for High-Performance Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="834" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN. In ICCV</title>
		<imprint>
			<biblScope unit="page" from="2961" to="2969" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-End Video Object Detection with Spatial-Temporal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1507" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SSD: Single Shot Multibox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Occluded Video Instance Segmentation: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2022" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust and efficient post-processing for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10536" to="10542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leveraging longrange temporal relationships between proposals for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9756" to="9764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MAMBA: Multi-level Aggregation via Memory Bank for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iclr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2620" to="2627" />
		</imprint>
	</monogr>
	<note>Very deep convolutional networks for large-scale image recognition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CSPNet: A new backbone that can enhance learning capability of CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-H</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="390" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16250</idno>
		<title level="m">PP-YOLOE: An evolved version of YOLO</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Integrated object detection and tracking with trackletconditioned detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11167</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7210" to="7218" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Towards high performance video object detection</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
