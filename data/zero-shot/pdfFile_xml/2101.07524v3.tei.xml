<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DuelGAN: A Duel Between Two Discriminators Stabilizes the GAN Training A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Wei</surname></persName>
							<email>jiahengwei@ucsc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhu</surname></persName>
							<email>angzhu@ucsc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
							<email>davis@cs.ucsc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>yangliu@ucsc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC Santa Cruz</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DuelGAN: A Duel Between Two Discriminators Stabilizes the GAN Training A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce DuelGAN, a generative adversarial network (GAN) solution to improve the stability of the generated samples and to mitigate mode collapse. Built upon the Vanilla GAN's two-player game between the discriminator D 1 and the generator G, we introduce a peer discriminator D 2 to the min-max game. Similar to previous work using two discriminators, the first role of both D 1 , D 2 is to distinguish between generated samples and real ones, while the generator tries to generate high-quality samples which are able to fool both discriminators. Different from existing methods, we introduce a duel between D 1 and D 2 to discourage their agreement and therefore increase the level of diversity of the generated samples. This property alleviates the issue of early mode collapse by preventing D 1 and D 2 from converging too fast. We provide theoretical analysis for the equilibrium of the min-max game formed among G, D 1 , D 2 . We offer convergence behavior of DuelGAN as well as stability of the min-max game. It's worth mentioning that DuelGAN operates in the unsupervised setting, and the duel between D 1 and D 2 does not need any label supervision. Experiments results on a synthetic dataset and on real-world image datasets (MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG, and FFHQ) demonstrate that DuelGAN outperforms competitive baseline work in generating diverse and high-quality samples, while only introduces negligible computation cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>two approaches replace loss functions of GAN by estimated variational f -divergence or least-square loss respectively), introducing auxiliary terms in the loss function <ref type="bibr" target="#b33">[34]</ref> and integral probability metric based GAN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref>. A detailed survey of methods for stabilizing GANs exists <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Multi-Player GANs</head><p>Multi-player GANs explore the situation where there are multiple generators or multiple discriminators. The first published work to introduce multiple discriminators to GANs is multi-adversarial networks, in which discriminators can range from an unfavorable adversary to a forgiving teacher <ref type="bibr" target="#b11">[12]</ref>. Nguyen et al. <ref type="bibr" target="#b36">[37]</ref> formulate D2GAN, a three-player min-max game which utilizes a combination of Kullback-Leibler (KL) and reverse KL divergences in the objective function and is the most closely related to our work. Albuquerque et al. <ref type="bibr" target="#b0">[1]</ref> show that training GAN variants with multiple discriminators is a practical approach even though extra capacity and computational cost are needed. Employing multiple generators and one discriminator to overcome the mode collapse issue and encourages diverse images has also been proposed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>In contrast to the above existing work, we demonstrate the possibility of improving GAN training with a computationally light modification by adding only one competing discriminator. We introduce a duel game among two discriminators and demonstrate the benefits of doing so in stabilizing and diversifying the training.</p><p>Our main contributions summarize as follows:</p><p>? We introduce a duel between two discriminators to encourage diverse predictions and avoid early failure. The intuition is that predictions with high consensus will be discouraged, and effectively both discriminators are rewarded for having diverse predictions. The introduced game between the two discriminators results in a different convergence pattern for the generator.</p><p>? Theoretically, we derive the equilibrium for discriminators and the generator. We show how DuelGAN alleviates the vanishing gradient issue and mode collapse intuitively and empirically. We derive evidence for how the peer discriminator helps the dynamics of the learning. In addition, we demonstrate that if the peer discriminator is better than a random guess classifier, the intermediate game and the objective function in DuelGAN are stable/robust to a bad peer discriminator.</p><p>? Experimental results on a synthetic dataset validate that DuelGAN addresses mode collapse. Results on real datasets demonstrate that DuelGAN generates high-quality image samples compared with baseline works. Besides, the introduced duel-game could also be viewed as a regularizer which complements well with existing methods and further improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We first review Vanilla GAN and D2GAN, which are the most relevant to understanding our proposed DuelGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vanilla GAN [15]</head><p>Let {x i } n i=1 ? X denote the given training dataset drawn from the unknown distribution p data . Traditional GAN formulates a two-player game: a discriminator D and a generator G. To learn the generator G's distribution over X , G maps a prior noise distribution p z (z) to the data space. ?x ? X , D(x) returns the probability that x belongs to p data rather than p g , where p g denotes the distribution of G(z) implicitly defined by G. GAN trains D to maximize the probability of assigning the correct label to both training samples and those from the generator G. Meanwhile, GAN trains G to minimize log(1 ? D(G(z))).</p><formula xml:id="formula_0">min G max D V (D, G) =E x?pdata [log D(x)] + E z?pz log 1 ? D G(z)</formula><p>.</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">D2GAN [37]</head><p>D2GAN is the most closely related method to DuelGAN. This three-player game aims to solve the mode collapse issue and the optimization task is equivalent to minimizing both KL divergence and Reverse-KL divergence between p data and p g . The formulation of D2GAN comes as follows:</p><formula xml:id="formula_1">min G max D1,D2 V (D 1 , D 2 , G) =? ? E x?pdata [log D 1 (x)] + E z?pz ? D 1 G(z) + E x?pdata [?D 2 (x)] + ? ? E z?pz log D 2 G(z) .<label>(2)</label></formula><p>Real image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise</head><p>Real GAN Loss Fake Duel Game</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real GAN Loss Fake</head><p>Discriminator Sample <ref type="figure" target="#fig_0">Figure 1</ref>: Illustration of the proposed DuelGAN. Compared with Vanilla GAN, DuelGAN has one more identical discriminator and a Duel Game between two discriminators. The introduced Duel Game induces diversified generated samples by discouraging the agreement between D 1 and D 2 . In D2GAN, although both discriminators are trained with different loss functions, they do not interfere with each other in the training.</p><p>Given a sample x in data space, D 1 (x) rewards a high score if x is drawn from p data , and gives a low score if generated from the generator distribution p g . In contrast, D 2 (x) returns a high score for x generated from p g and gives a low score for a sample drawn from p data . Our work is similar to D2GAN in containing a pair of discriminators, however instead of discriminators with different goals, we use identical discriminators and introduce a duel/competition between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DuelGAN: A Duel Between Two Discriminators</head><p>In this section, we first give the formulation and intuition of DuelGAN. Then we will present the equilibrium strategy of the generator and the discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Similar to related works, we assume that the data follows the distribution p data , our ultimate goal is to achieve p g = p data where p g is the generator's distribution. DuelGAN formulates a three-player game which consists of two discriminators D 1 , D 2 and one generator G. Denote by p duel an equal mixture of p data and p g , ?x: p duel (x) = pdata(x)+pg(x) 2</p><p>. Recall that p z denotes the prior noise distribution, now we are ready to formulate the min-max game of DuelGAN as follows:</p><formula xml:id="formula_2">min G max D1,D2 L(D 1 , D 2 , G) = min G max D1,D2 E x?pdata [log D 1 (x)] + E x?pdata [log D 2 (x)] + ? ? Duel-D + E z?pz [log (1 ? D 1 (G(z)))] + E z?pz [log (1 ? D 2 (G(z)))],<label>(3)</label></formula><p>where Duel-D introduces the duel (a peer competition game) among D 1 , D 2 , defined as:</p><formula xml:id="formula_3">Duel-D = E x?pduel ? D 1 (x), 1 D 2 (x) &gt; 1 2 Term 1a ?? ? ? D 1 (x p1 ), 1 D 2 (x p2 ) &gt; 1 2 Term 1b +E x?pduel ? D 2 (x), 1 D 1 (x) &gt; 1 2 Term 2a ?? ? ? D 2 (x p1 ), 1 D 1 (x p2 ) &gt; 1 2 Term 2b .<label>(4)</label></formula><p>In Duel-D, x p1 and x p2 are drawn randomly from p duel and that x, x p1 and x p2 are independent with each other. 1(?)</p><p>is the indicator function, ?, ? ? [0, 1] are hyper-parameters controlling the disagreement level and the weight of the competition game between two discriminators, respectively. ? is an evaluation function, for simplicity, we adopt ? = log(?), as commonly used in other terms in the min-max game. Thus, we have:</p><formula xml:id="formula_4">?(D i (x), y) = log D i (x) if y = 1; log 1 ? D i (x) if y = 0.<label>(5)</label></formula><p>To clarify the differences among Vanilla GAN <ref type="bibr" target="#b14">[15]</ref>, D2GAN <ref type="bibr" target="#b36">[37]</ref> and DuelGAN, we use an workflow to illustrate in <ref type="figure" target="#fig_0">Figure 1</ref>. The key differences in DuelGAN's formulation can be summarized as follows:</p><p>? Compared with Vanilla GAN (see Eqn.(1)), DuelGAN (see Eqn. <ref type="formula" target="#formula_2">(3)</ref>) introduces a peer discriminator D 2 which has the same objective function as D appeared in Eqn. <ref type="bibr" target="#b0">(1)</ref>. An intermediate duel game Duel-D is added which will be explained below.</p><p>? The difference between D2GAN (see Eqn.</p><p>(2)) and DuelGAN is highlighted with the underscores in red. Primarily, there is no interaction between discriminators in D2GAN, while our Duel-D term introduces another duel game between the discriminators, which we explain below. In addition to Duel-D, the objective function in DuelGAN encourages both discriminators to fit perfectly on both training samples and generated samples. While in D2GAN, one discriminator fits overly on training samples, the other fits overly on generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Competition Introduced by Duel-D</head><p>Duel-D bridges D 1 and D 2 by introducing 4 terms specified in Eqn.(4). Since we do not expect arbitrarily different discriminators, and both D i s should play against the generator G, Term 1a and Term 2a encourage agreements between D 1 and D 2 . With only these two terms, D 1 and D 2 will eventually be encouraged to converge to agree with each other. Mode collapse issue remains a possibility. DuelGAN introduces Term 1b and Term 2b to the objective function which punish D 1 and D 2 from over-agreeing with each other (where the duel happens), especially at the early phase of training. Particularly, the Term 1b and 2b are evaluating the agreements of D 1 and D 2 on two entirely independent samples x p1 , x p2 . Because of the independence, the two discriminators' predictions should not match with high probability.</p><p>Note that the calculation of Duel-D does not need label supervisions, which distinguishes our work from other works that introduces multiple discriminators but would require additional label supervisions <ref type="bibr" target="#b9">[10]</ref>.</p><p>We provide more details of our intuition as well as theoretical evidences of this property in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Max Game of Discriminators</head><p>Denote the true label of x as y = 1 if x comes from p data , otherwise, y = 0. For any given generator G, let us first analyze the best responding/optimal discriminator D * i,G (x), i ? 1, 2. We define the following quantities:</p><formula xml:id="formula_5">r i,G (x) := P x?pduel 1 D i (x) &gt; 1 2 = 1 , p i,G := E x?pduel [r i,G (x)],<label>(6)</label></formula><p>where r i,G (x) represents the probability/confidence of x being categorized as the real data by D i and p i,G is the</p><formula xml:id="formula_6">expectation of r i,G (x) for x ? p duel . Letr * i,G (x) := r i,G (x) ? ? ? p i,G .</formula><p>Given discriminator D i , when there is no confusion, we use D j to denote the peer discriminator without telling j ? = i in later sections. Proposition 1. For G fixed, denote by w := ? ? (1 ? ?), the optimal discriminators D 1 , D 2 are given by:</p><formula xml:id="formula_7">D * i,G (x) = p data (x) + ? ?r * j,G (x) ? p duel (x) p data (x) + p g (x) + w ? p duel (x) , i = 1, 2.<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Min Game of the Generator</head><p>Remember that the training objective for D i can be interpreted as maximizing the log-likelihood for estimating the conditional probability P(Y = y|x) where Y indicates whether x comes from p data (with y = 1) or from p g (with y = 0). With the introduce of Duel Game, the distributions p data and p g in the Vanilla GAN got changed due to the appearance of p duel . Thus, we define the corresponding updated distributions in DuelGAN w.r.t. discriminator D i as p datai and p gi , respectively. For a clean presentation, we defer the exact form of p datai , p gi in Appendix (Eqn. <ref type="formula" target="#formula_1">(22)</ref>).</p><p>Denote C(G) := max D L(G, D 1 , D 2 ), the inner-max game (C(G)) can be rewritten as (straightforward in the proof of Proposition 1 which is available in the Appendix A.1): 3.5 When r j,G (x) = 1 2 ? Note that r j,G (x) is merely representing the probability that D j classifies x to be real samples, p j,G is the probability that D j classifies a random sample as the real one. Without loss of generality, we assume real and generated samples are of uniform/equal prior. At the very beginning of the training process, the discriminator can do well in distinguishing real or generated samples, since the generator at this time generates low-quality samples. In this case, r j,G (x) is supposed to approach its max/min value, for example, r j,G (x) ? 0 if x is from generated samples, and otherwise, r j,G (x) ? 1.</p><formula xml:id="formula_8">C(G) =E x?pdata 1 [log D * 1,G (x)] + E x?pg 1 log 1 ? D * 1,G (x) +E x?pdata 2 [log D * 2,G (x)] + E x?pg 2 log 1 ? D * 2,G (x) .<label>(8)</label></formula><p>During the training process, the generator progressively tries to mislead the predictions made by discriminators, which means the discriminator can not decide whether the sample is being fake or real. Thus, r j,G (x) ? 1 2 . At this time, for ? = 0, i = 1, 2, we have:</p><formula xml:id="formula_9">D * i,G (x) = p data (x) + ? ?r * j,G (x) ? p duel (x) p data (x) + p g (x) + ? ? p duel (x) ? p data (x) + ? 2 ? p duel (x) p data (x) + p g (x) + ? ? p duel (x) .<label>(9)</label></formula><p>This allows us to rewrite C(G)</p><formula xml:id="formula_10">2 as: E x?pdata i log pdata(x)+ ? 2 ?pduel(x) pdata(x)+pg(x)+??pduel(x) + E x?pg i log pg(x)+ ? 2 ?pduel(x) pdata(x)+pg(x)+??pduel(x)</formula><p>. Our subsequent proof is then based on the above reformulation.</p><p>We summarize the overall DuelGAN algorithm in Algorithm 1. In experiments, we train G to minimize log(1 ? D i (G(z))) which is equivalent to maximizing log D i (G(z)). ? Combine two subsets T := X ? Z, and denote by T = {t 1 , ..., t 2m }.</p><p>? Update discriminator D i (i ? {1, 2}) by ascending the stochastic gradient:</p><formula xml:id="formula_11">? ? d i 1 m m i=1 log D i (x i ) + log 1 ? D i G(z i ) + ? 2m 2m j=1 ? CE D i (t j ), 1 D j (t j ) &gt; 1 2 ? ? ? ? CE D i (t p1 ), 1 D j (t p2 ) &gt; 1 2 ,<label>(10)</label></formula><p>where t p1 , t p2 are randomly selected (with replacement) samples from T . ? Update G by descending its stochastic gradient:</p><formula xml:id="formula_12">? ?g 1 m m i=1 log 1 ? D 1 G(z i ) + log 1 ? D 2 G(z i ) .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Properties of DuelGAN</head><p>In this section, we first illustrate how DuelGAN alleviates common issues in GAN training, for example, the vanishing gradients issue and the mode collapse issue. Then we present properties of DuelGAN including its stability guarantee and converging behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DuelGAN and Common Issues in GAN Training</head><p>Vanishing Gradients Issue In training GAN, discriminators might be too good for the generator to fool with and to improve progressively. When training with neural networks with back-propagation or gradient-based learning approaches, a vanishing small gradient only results in minor changes even with a large weight. As a result, the generator training may fail due to the vanishing gradients issue.</p><p>Mode Collapse Issue Mode collapse refers to the phenomenon that the generator will rotate through a small set of output types. For the given fixed discriminator, the generator over-optimizes in each iteration. Thus, the corresponding discriminator fails to learn its way out of the trap.</p><p>How DuelGAN Alleviates the Vanish Gradient and Mode Collapse DuelGAN alleviates the above two issues by preventing discriminators from "colluding" on its discrimination ability. In DuelGAN, for either discriminator D i , recall that x p1 and x p2 are randomly drawn from p duel which are independent from each other. Then the max game of D i , given its peer discriminator D j , is to perform the following task:</p><formula xml:id="formula_13">max Di L(D i , G)| Dj = max Di Term a ? E x?pdata [log D i (x)] + E z?pz log 1 ? D i G(z) +? ? E x?pduel ? D i (x), 1 D j (x) &gt; 1 2 Term b ? ?? ? ? D i (x p1 ), 1 D j (x p2 ) &gt; 1 2 Term c ? .<label>(12)</label></formula><p>Term a ? maximizes the probability of assigning the correct label to both real samples and generated samples. Term b ? maximizes the probability of matching predicted label with peer discriminator predicted ones. In other words, Term b ? controls the agreement level of D i with respect to its peer discriminator D j . However, note that Term c ? checks on the predictions of D j on two different tasks x p1 , x p2 . When D i agrees/fits overly on D j , Term c ? returns a lower value if D j 's predictions on these two different tasks are matching, mathematically,</p><formula xml:id="formula_14">1 D j (x p1 ) &gt; 1 2 = 1(D j x p2 ) &gt; 1 2 . And Term c ? will return a high value if D j 's predictions on these two different tasks are indeed different 1 D j (x p1 ) &gt; 1 2 ? = 1 D j (x p2 ) &gt; 1 2 . The weight ? controls this disagreement level compared with Term b</formula><p>? by referring to the fact that a larger ? encourages more disagreement/diverse predictions from discriminators.</p><p>Based on the above intuitions, when two discriminators are of a high disagreement level, there exists a set S dis such</p><formula xml:id="formula_15">that 1(D i (x) &gt; 1 2 ) ? = 1(D j (x) &gt; 1</formula><p>2 ) for x ? S dis and S dis is non-negligible. Therefore, there exists at least one discriminator D i that can't perfectly predict labels (real/generated) of given data samples. The generator will then be provided with sufficient information, e.g., information or features that can be extracted from S dis , to progress. This property helps us address the vanishing gradients issue. As for the mode collapse issue, suppose the over-optimized generator is able to find plausible outputs for both discriminators in the next generation. However, note that optimization is implemented on mini-batches in practice, the randomly selected samples x p1 , x p2 in Duel-D as well as the dynamically changing weights ?, ? can bring a certain degree of randomness in the next generation. Thus, rotating through this subset of the generator's output types could not force Term c ? to remain unchanged, so that the discriminators won't maintain a constant disagreement level and they unlikely get stuck in a local optimum. In Section 5.1, we use synthetic experiments to show that DuelGAN addresses mode collapse issues. And we include more empirical observations of the competition introduced by Duel-D in the Appendix B.5, i.e., the stability of the DuelGAN training, and the visualization of agreement levels between D 1 and D 2 due to the introduce of the duel game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stability and Convergence Behavior</head><p>In Section 4.1, we discussed the significant role of the introduced intermediate duel game. Now we discuss the potential downsides of introducing a second discriminator. Particularly, we are interested in understanding if the introduce of a peer discriminator D j will disrupt the training and make the competition game with D i unstable. Suppose D j diverges from the optimum in the max game, in other words, the diverged peer discriminatorD j fails to provide qualified verification label Y * j (given by D * j,G ), and provides? j instead. Mathematically, denote: e data,j := P(? j = 0|Y * j = 1), e g,j := P(? j = 1|Y * j = 0).</p><p>For any peer discriminator D j , D j may be a diverged peer discriminatorD j or an optimal one D * j,G , we denote the Duel Game of D i given her peer discriminator D j as:</p><formula xml:id="formula_17">Duel(D i )| Dj :=E x?pduel ? D i (x), 1 D j (x) &gt; 1 2 ? ? ? ? D i (x p1 ), 1 D j (x p2 ) &gt; 1 2 .<label>(14)</label></formula><p>Theorem 2 explains the condition of stability (for D i ) when its peer discriminator in DuelGAN diverges from the corresponding optimum. Theorem 2. Given G, suppose D i has enough capacity, and at one step of Algorithm 1, if e data,j + e g,j &lt; 1, ? = 1, the duel term of discriminator D i is stable/robust with diverged peer discriminatorD j . Mathematically,</p><formula xml:id="formula_18">max Di Duel(D i )|D j is equivalent with max Di Duel(D i )| D * j,G .<label>(15)</label></formula><p>The above theorem implies that a diverging and degrading peer discriminator D j will not disrupt the training of D i . Remark. Note that assuming uniform prior of real and generated samples, the condition to be stable is merely requiring that the proportion of false/wrong D j 's prediction is less than a half (random guessing). This condition can be easily satisfied in practice. Thus, Theorem 2 provides the stability/robustness guarantee when the peer discriminator diverged from its optimum.</p><p>Build upon Theorem 1, with sufficiently small updates, Theorem 3 presents when p g converges to p data . Theorem 3. If G and D i s have enough capacity, and at each step of Algorithm 1, D i s are allowed to reach its optimum given G, D i is updated so as to improve the criterion in Eqn. <ref type="bibr" target="#b11">(12)</ref>, and p g is updated so as to improve:</p><formula xml:id="formula_19">C(G) =E x?pdata 1 [log D * 1,G (x)] + E x?pg 1 log 1 ? D * 1,G (x) +E x?pdata 2 [log D * 2,G (x)] + E x?pg 2 log 1 ? D * 2,G (x) .<label>(16)</label></formula><p>If ? = 0, we have D * 1,G = D * 2,G , p g converges to p data .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we empirically validate the properties of DuelGAN through a set of datasets, including a synthetic task and several real world datasets ranging from hand-written digits to human faces. Both metrics compare the generated data points to data points drawn from the true target distribution. DuelGAN has the best performance. The right side visualizes generated blue data points and true red p data data points. Note that Vanilla GAN has a clear mode collapse which both D2GAN and DuelGAN avoid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Results on Synthetic Data</head><p>We apply the experiment and model structures proposed in UnrolledGAN <ref type="bibr" target="#b33">[34]</ref> to investigate whether the DuelGAN design can prevent mode collapse. This experiment aims to generate eight 2D Gaussian distributions with a covariance matrix 0.02I, arranged around the same centroid with radius 2.0. Vanilla GAN fails on this example. D2GAN has been shown to outperform UnrolledGAN, so we include it as an alternate method which performs well. <ref type="figure" target="#fig_2">Figure 2</ref> shows symmetric KL-divergence, Wasserstein distance, and a visualization of results with Vanilla GAN, D2GAN, and DuelGAN. Knowing the target distribution p data , we can employ symmetric KL divergence and Wasserstein distance, which calculate the distance between the true p data and the normalized histogram of 10,000 generated points. On the left of <ref type="figure" target="#fig_2">Figure 2</ref>, the plots for symmetric KL-divergence and Wasserstein distance show that DuelGAN has a much better score than Vanilla GAN and slightly better than D2GAN.</p><p>On the right side of <ref type="figure" target="#fig_2">Figure 2</ref> is a visualization of 512 generated blue samples points, together with red data points drawn from the true distribution. Vanilla GAN generates data points around only a single valid mode of the data distribution. D2GAN and DuelGAN distribute data around all eight mixture components, demonstrating the ability to resolve modal collapse in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on Real Image Datasets</head><p>We tested the proposed DuelGAN and baseline methods on MNIST <ref type="bibr" target="#b26">[27]</ref>, FashionMNIST <ref type="bibr" target="#b53">[54]</ref>, CIFAR-10 <ref type="bibr" target="#b25">[26]</ref>, STL-10 <ref type="bibr" target="#b8">[9]</ref>, CelebA <ref type="bibr" target="#b30">[31]</ref> and VGGFace2 <ref type="bibr" target="#b6">[7]</ref>. For quantitative evaluation, we adopt Fr?chet Inception Distance (FID) <ref type="bibr" target="#b17">[18]</ref> and Inception score(IS) <ref type="bibr" target="#b42">[43]</ref> as the evaluation metric. FID summarizes the distance between the Inception features of the generated images and the real images. A lower FID indicates both better accuracy and higher diversity, so that a batch of generated images with good accuracy but identical to each other will have a poor FID score. A higher IS score indicates a higher generated image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>We reproduce/report the performance of a list of existing baseline methods, including: DCGAN <ref type="bibr" target="#b41">[42]</ref>, D2GAN <ref type="bibr" target="#b36">[37]</ref>, WGAN <ref type="bibr" target="#b16">[17]</ref>, DRAGAN <ref type="bibr" target="#b24">[25]</ref>, LSGAN <ref type="bibr" target="#b40">[41]</ref>, MicroBatchGAN <ref type="bibr" target="#b35">[36]</ref>, Dist-GAN <ref type="bibr" target="#b46">[47]</ref>, PresGAN <ref type="bibr" target="#b10">[11]</ref>, and QSNGAN <ref type="bibr" target="#b15">[16]</ref>. We used the same generator and discriminator backbone for all the comparison methods in each dataset unless specified by the original author. We recorded the best performing checkpoints when evaluating each method.</p><p>Grey-Scale Images MNIST <ref type="bibr" target="#b26">[27]</ref> and FashionMNIST <ref type="bibr" target="#b53">[54]</ref> are small grey-scale image datasets including 60,000 training and 10,000 testing 28?28 gray-scale images of hand-written digits and clothing. Since they are of small-scale, we adopt the shallow version of the generator and discriminators to generate the grey-scale images. We firstly give the performance comparisons between DuelGAN and baseline methods that only adopted the Inception score in the original paper. We then include a comprehensive comparison via FID score in <ref type="table" target="#tab_1">Table 2</ref>. And the first two columns in <ref type="table" target="#tab_1">Table 2</ref> show our method has the best FID score among all tested methods. <ref type="figure" target="#fig_3">Figure 3</ref> (left) shows FashionMNIST image results.  Natural Scene Images CIFAR-10 <ref type="bibr" target="#b25">[26]</ref> and STL-10 <ref type="bibr" target="#b8">[9]</ref> are natural scene RGB image datasets. CIFAR-10 includes 50,000 training and 10,000 testing 32?32 images with ten unique categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. STL-10 is sub-sampled from ImageNet, and has more diverse samples than CIFAR-10, containing about 100,000 96?96 images. We adopt the deep version of the generator and discriminator to generate 32?32 RGB images. <ref type="table" target="#tab_0">Table 2 middle two columns show FID score results and Table 1</ref> shows the inception score results. Note that the introduce of competitive Duel Game in two discriminator GAN setup, brings performance boost in all the experiments. <ref type="figure" target="#fig_3">Figure 3 (middle)</ref> shows STL-10 image results. Human Face Images CelebA <ref type="bibr" target="#b30">[31]</ref> and VGGFace2 <ref type="bibr" target="#b6">[7]</ref> are large-scale face datasets. CelebA includes 162,770 training and 19,962 testing images of celebrity faces. VGGFace2 contains more than 3.3 million face images of celebrities caught in the 'wild'. There are different lighting conditions, emotions, and viewing angles. We randomly choose 200 categories from VGGFace2 and trained on the reduced dataset. We adopt the deep version of the generator and discriminators to generate 32?32 RGB images on CelebA and 64?64 RGB images on VGGFace2. <ref type="table" target="#tab_1">Table 2</ref> last two columns show our method has the best FID score among tested methods. <ref type="figure" target="#fig_3">Figure 3 (right)</ref> shows CelebA image results. Implementation Details Our model architecture adopts the same generator and discriminator backbone as DC-GAN <ref type="bibr" target="#b41">[42]</ref>. In DuelGAN, the newly introduced discriminator is a duplicate of the first one. DuelGAN achieves low FID scores and high IS scores when ? and ? are simply set to constant values. However we found that we could obtain an approximately 10% improvement through dynamic tuning. The parameter ? controls the overall weight of Duel-D, while ? punishes the condition when D 1 over-agrees with D 2 . In the early training phase, when we have an unstable generator and discriminator, we set ? and ? to 0. As training progresses, we gradually increase these parameters to a max value, which helps with vanishing gradients. After the midpoint of training we decrease these parameters to help the discriminators converge, until the parameters reach approximately 0 at the end of the training process. We adopt 0.3, 0.5 as the max value for ? and ?, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Duel Game as a Regularizer</head><p>Intuitively, the introduced duel game could be well applied to a large family of GAN variants defined w.r.t a single discriminator D 1 and a generator G. This is due to the fact that Eqn.(3) could be denoted by:</p><formula xml:id="formula_20">min G max D1,D2 L(D 1 , D 2 , G) = min G max D1,D2 [GAN(D 1 ) + ? ? Duel-D + GAN(D 2 )] ,<label>(17)</label></formula><p>where GAN(</p><formula xml:id="formula_21">D i ) := E x?pdata [log D i (x)]+E z?pz [log (1 ? D i (G(z)))]</formula><p>. Thus, if we substitute the GAN loss GAN(D i ) by a state-of-the-art GAN variant, i.e., StyleGAN-ADA <ref type="bibr" target="#b22">[23]</ref>, one could view the duel game Duel-D as a regularizer.</p><p>We take the higher resolution version (256?256 RGB images) of CelebA <ref type="bibr" target="#b30">[31]</ref> for illustration. Clearly in <ref type="table" target="#tab_2">Table 3</ref>, StyleGAN-ADA reaches the state-of-the-art result on this task. And the introduced Duel-D regularizer could further improve its performance. <ref type="figure" target="#fig_4">Figure 4</ref> shows the corresponding generated images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose DuelGAN which introduces a peer discriminator to Vanilla GAN. The role of the peer discriminator is to allow an intermediate game (duel game) between discriminators. Theoretical analysis demonstrates that the introduced duel game incentivizes incremental improvement, addresses vanishing gradients and mode collapse issues, punishes over-agreements among discriminators and is stable with diverged peer discriminator. Experimental results on a synthetic dataset and multiple real world datasets validate that DuelGAN produces high quality images, with lower error than competing techniques.</p><p>Proof. Denote by f (y) := a log(y) + b log(1 ? y), clearly, when y = 0 or y = 1, f (y) = ??. For y ? (0, 1), we have: Now we proceed to prove Proposition 1.</p><formula xml:id="formula_22">f ? (y) = 0 ?? a y ? b 1 ? y = 0 ?? y = a a + b .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 1</head><p>Proof. The trainer criterion for the discriminator D i , given any generator G, is to maximize the quantity L(D 1 , D 2 , G).</p><p>Remember that:</p><formula xml:id="formula_23">L(D 1 , D 2 , G) = E x?pdata [log D 1 (x)] + E x?pdata [log D 2 (x)] + E z?pz [log (1 ? D 1 (G(z)))] + E z?pz [log (1 ? D 2 (G(z)))] + ? ? E x?pduel ? D 1 (x), 1 D 2 (x) &gt; 1 2 ? ? ? ? D 1 (x p1 ), 1 D 2 (x p2 ) &gt; 1 2 + ? ? E x?pduel ? D 2 (x), 1 D 1 (x) &gt; 1 2 ? ? ? ? D 2 (x p1 ), 1 D 1 (x p2 ) &gt; 1 2 .<label>(19)</label></formula><p>=? Note that 2 ? E x?pdata i [? log 2] + E x?pg i [? log 2] = ? log 16, by subtracting this expression from C(G), we have:</p><formula xml:id="formula_24">C(G) = ? log 16 + 2 ? KL p g + ? 2 ? p duel p data + p g + ? ? p duel 2 + 2 ? KL p data + ? 2 ? p duel p data + p g + ? ? p duel 2 ,<label>(26)</label></formula><p>where KL is the Kullback-Leibler divergence. Note that:</p><formula xml:id="formula_25">C(G) = ? log 16 + 2 ? JSD p data + ? 2 ? p duel p g + ? 2 ? p duel ,<label>(27)</label></formula><p>and the Jensen-Shannon divergence between two distributions is always non-negative and zero only when they are equal, we have shown that C(G) * = ? log 16 is the global minimum of C(G). Thus, we need</p><formula xml:id="formula_26">p data + ? 2 ? p duel = p g + ? 2 ? p duel ? p data = p g .</formula><p>?= Given that p data = p g , we have:</p><formula xml:id="formula_27">C(G) = max D L(G, D 1 , D 2 ) =2 ? E x?pdata i ? ? ?log p data (x) + ? 2 ? p duel (x) p data (x) + p g (x) + ? ? p duel (x) ? ? ? + 2 ? E x?pg i ? ? ?log p g (x) + ? 2 ? p duel (x) p data (x) + p g (x) + ? ? p duel (x) ? ? ? =2 ? log 1 2 + log 1 2 = ? log 16.<label>(28)</label></formula><p>A.3 Proof of Theorem 2</p><p>Proof. Ignoring the weight ?, the duel term of discriminator D i w.r.t. its diverged peer discriminatorD j becomes:</p><formula xml:id="formula_28">Duel(D i )|D j := E x?pduel ? D i (x), 1 D j (x) &gt; 1 2 ? ? ? ? D i (x p1 ), 1 D j (x p2 ) &gt; 1 2 = E x?pduel,Y * j =1 P(? j = 1|Y * j = 1) ? ? D i (x), 1 + P(? j = 0|Y * j = 1) ? ? D i (x), 0 + E x?pduel,Y * j =0 P(? j = 1|Y * j = 0) ? ? D i (x), 1 + P(? j = 0|Y * j = 0) ? ? D i (x), 0 ? ? ? E xp 1 ?pduel P(? j = 1) ? ? D i (x p1 ), 1 + P(? j = 0) ? ? D i (x p1 ), 0 = E x?pduel,Y * j =1 (1 ? e data,j ) ? ?(D i (x), 1) + e data,j ? ?(D i (x), 0) + E x?pduel,Y * j =0 e g,j ? ?(D i (x), 1) + (1 ? e g,j ) ? ?(D i (x), 0) ? ? ? E xp 1 ?pduel P(Y * j = 1) ? (1 ? e data,j ) + P(Y * j = 0) ? e g,j ? ? D i (x p1 ), 1 ? ? ? E xp 1 ?pduel P(Y * j = 1) ? e data,j + P(Y * j = 0) ? (1 ? e g,j ) ? ? D i (x p1 ), 0 = E x?pduel,Y * j =1 (1 ? e data,j ? e g,j ) ? ?(D i (x), 1) + e data,j ? ?(D i (x), 0) + e g,j ? ?(D i (x), 1) + E x?pduel,Y * j =0 (1 ? e data,j ? e g,j ) ? ?(D i (x), 0) + e data,j ? ?(D i (x), 0) + e g,j ? ?(D i (x), 1) ? ? ? E xp 1 ?pduel c 1 ? ? D i (x p1 ), 1 ? ? ? E xp 1 ?pduel c 2 ? ? D i (x p1 ), 0 ,</formula><p>where we define: c 1 := P(Y * j = 1) ? (1 ? e data,j ? e g,j ) + P(Y * j = 0) ? e g,j + P(Y * j = 1) ? e g,j , c 2 := P(Y * j = 0) ? (1 ? e data,j ? e g,j ) + P(Y * j = 1) ? e data,j + P(Y * j = 0) ? e data,j , for a clear presentation. Proceeding the previous deduction, we then have:</p><formula xml:id="formula_29">Duel(D i )|D j = (1 ? e data,j ? e g,j ) ? E x?pduel ? D i (x), Y * j + E x?pduel e data,j ? ? D i (x), 0 + e g,j ? ? D i (x), 1 ? ? ? (1 ? e data,j ? e g,j ) ? E x?pduel ? D i (x p1 ), Y * j ? ? ? E x?pduel e data,j ? ? D i (x), 0 + e g,j ? ? D i (x), 1 . (29) Thus, Duel(D i )|D j =(1 ? e data,j ? e g,j ) ? Duel(D i )| D * j,G + (1 ? ?) ? E x?pduel e data,j ? ? D i (x), 0 + e g,j ? ? D i (x), 1 Bias .<label>(30)</label></formula><p>Note that:</p><formula xml:id="formula_30">Bias = (1 ? ?) ? E x?pduel e data,j ? log 1 ? D i (x) + e g,j ? log D i (x) .<label>(31)</label></formula><p>Thus, given ? = 1, the Bias term is cancelled out. When e data,j + e g,j &lt; 1, we have:</p><formula xml:id="formula_31">Duel(D i )|D j =(1 ? e data,j ? e g,j ) ? Duel(D i )| D * j,G ,<label>(32)</label></formula><p>and we further have: max</p><formula xml:id="formula_32">Di Duel(D i )|D j = max Di Duel(D i )| D * j,G .<label>(33)</label></formula><p>A.4 Proof of Theorem 3</p><p>Proof. When ? = 0, the overall min-max game becomes: min</p><formula xml:id="formula_33">G max D1,D2 L(D 1 , D 2 , G) = min G max D1,D2 E x?pdata log D 1 (x) + E z?pz log 1 ? D 1 G(z) + E x?pdata log D 2 (x) + E z?pz log 1 ? D 2 G(z) .<label>(34)</label></formula><p>Since we assume enough capacity, the inner max game is achieved if and only if:</p><formula xml:id="formula_34">D 1 (x) = D 2 (x) = pdata(x)</formula><p>pdata(x)+pg(x) . To prove p g converges to p data , only need to reproduce the proof of proposition 2 in <ref type="bibr" target="#b14">[15]</ref>. We omit the details here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Details and Additional Results</head><p>Model Architectures For the small-scale datasets, we used a shallow version of generator and discriminator: three convolution layers in the generator and four layers in the discriminators. We use a deep version of generator and discriminator for natural scene and human face image generation, which have three convolution layers in the generator and seven layers in the discriminators. The deep version is the original design of DCGAN <ref type="bibr" target="#b41">[42]</ref>. The peer discriminator uses the duplicate version of the first one. <ref type="figure" target="#fig_6">Figure 5</ref> shows the architecture designs of single discriminator, dual discriminator, and our proposed DuelGAN. Compared with Vanilla GAN, DuelGAN has one more identical discriminator and a competitive Duel Game between two discriminators. The introduced Duel Game induces diversified generated samples by discouraging the agreement between D 1 and D 2 . In D2GAN, although both discriminators are trained with different loss functions, they do not interfere with each other in the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Architecture Comparison Between GAN, D2GAN and DuelGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Additional Experiment Results</head><p>StyleGAN-ADA <ref type="bibr" target="#b22">[23]</ref> is the state-of-the-art method in image generation. We applied our duel game to StyleGAN-ADA and further improves its performance. On CelebA <ref type="bibr" target="#b30">[31]</ref> dataset, we improved FID from 4.85 to 4.52, and FFHQ-10k <ref type="bibr" target="#b23">[24]</ref> dataset improved FID from 7.24 to 6.01. We show the generated image results (trained on CelebA) in <ref type="figure" target="#fig_7">Figure 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Additional Experiment Details</head><p>Model Architectures For the small-scale datasets, we used a shallow version of generator and discriminator: three convolution layers in the generator and four layers in the discriminators. We use a deep version of generator and discriminator for natural scene and human face image generation, which have three convolution layers in the generator and seven layers in the discriminators. The deep version is the original design of DCGAN <ref type="bibr" target="#b41">[42]</ref>. The peer discriminator uses the duplicate version of the first one.</p><p>Hyper-Parameters DuelGAN achieves low FID scores and high IS scores when ? and ? are simply set to constant values. However we found that we could obtain an approximately 10% improvement through dynamic tuning. The parameter ? controls the overall weight of Duel-D, while ? punishes the condition when D 1 over-agrees with D 2 . In the early training phase when we have an unstable generator and discriminator, we set ? and ? to 0. As training progresses, we gradually increase these parameters to a max value, which helps with vanishing gradients. After the midpoint of training we decrease these parameters to help the discriminators converge, until the parameters reach approximately 0 at the end of the training process. We adopt 0.3, 0.5 as the max value for ? and ?, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Ablation Study of DuelGAN</head><p>During training, We initialize the ? and ? as 0, and gradually increase to the set maximum value. We experimentally discover ?=0.3 and ?=0.5 can achieve the best FID score in the datasets we tested on. <ref type="table" target="#tab_2">Table 3</ref> shows an thorough ablation of different hyper-parameter setting on STL-10 dataset. The bold text are the best ? setting when beta is fixed.     <ref type="table">Table 4</ref>: Ablation study of max ? and max ? value tuning on STL-10 dataset (evaluate with FID score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Stability of Training</head><p>In this section, we empirically show the stability of DuelGAN training procedure. We adopt STL-10 dataset and ? = 0.25 for illustration. In <ref type="figure" target="#fig_11">Figure 8</ref> and 9, we visualize the loss of two discriminators during the training procedure of STL-10 dataset. The red lines indicate the smoothed trend of the loss evaluated on the generated images and real images. Real losses are represented by the shaded red lines. Although there exists certain unstable episodes (the difference between smoothed loss and the real loss is large) for both discriminators, the overall trend of both discriminators are stable. What is more, we do observe that D 1 and D 2 hardly experience unstable episodes at the same time. This phenomenon further validates our conclusion in Theorem 2: an unstable/diverged discriminator hardly disrupts the training of its peer discriminator!   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agreements Between Two Discriminators</head><p>We also empirically estimate the agreement level between two discriminators while training. In <ref type="figure" target="#fig_0">Figure 10</ref>, the y?axis denotes the percentage of predictions that reach a consensus by D 1 and D 2 . The smoothed curve depicts the overall change of the agreement level. At the initial stage, D i is not encouraged to agree overly on its peer discriminator D j . As the training progresses, the agreement level gradually increases to a high value to help the convergence of the whole training process. The shaded red line means that the practical agreement level fluctuates around the smoothed line, incurs a certain degree of randomness and prevents discriminators from getting stuck in a local optimum. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 .</head><label>1</label><figDesc>When ? = 0, r j,G (x) = 1 2 , the global minimum of the virtual training criterion C(G) is achieved if and only if p data = p g . At this point, C(G) achieves the value of ? log 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 1 : 2 :</head><label>112</label><figDesc>DuelGAN Input: two discriminators D 1 , D 2 , generator G, training samples {x i } n i=1 , weights ?, ?. For number of training iterations do For 1 to k steps do ? Sample mini-batch of m noise samples Z = {z 1 , ..., z m } from noise prior p z . ? Sample mini-batch of m samples X = {x 1 , ..., x m } from data generating distribution p data (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of Vanilla GAN, D2GAN, and proposed DuelGAN on 2D synthesized data. The top-left graph shows the symmetric KL divergence over the training iterations, while the bottom left graph shows the Wasserstein distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Image results generated by proposed DuelGAN. Left: FashionMNIST, grey-scale clothing images; Middle: STL-10, natural scene images; Right: CelebA, large-scale celebrate face images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Image results generated by proposed DuelGAN. (Trained on CelebA 256?256 RGB images. More generated images are deferred to the Appendix B.) More Experiment Results We defer more experiment results to the Appendix B, including: an ablation study of hyper-parameters tuning; experiment validations about the stability of training; the visualization of the duel game between D 1 and D 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Note that f ? (y) &gt; 0 if 0 &lt; y &lt; a a+b and f ? (y) &lt; 0 if 1 &gt; y &gt; a a+b . Thus, the maximum of f (y) should be max(f (a), f ( a a+b ), f (b)) = f ( a a+b ). And f (y) achieves its maximum in [0, 1] at a a+b .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Architecture comparisons between GAN based method (first row), dual discriminators GAN based method (second row) and DuelGAN (third row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>More CelebA image generation results of applying duel game on StyleGAN-ADA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>The trend of ?, ? in the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>?=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4 -</head><label>4</label><figDesc>2k 0 2k 4k 6k 8k 10k 12k 14k 16k 18k 20k 22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>The loss of D 1 in DuelGAN with ? = 0.25 on STL-10 dataset, left: ? = 0.3; middle: 0.5; right: ? = 0.7. 4k 6k 8k 10k 12k 14k 16k 18k 20k 22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>The loss of D 2 in DuelGAN with ? = 0.25 on STL-10 dataset, left: ? = 0.3; middle: 0.5; right: ? = 0.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>4k 6k 8k 10k 12k 14k 16k 18k 20k 22 Figure 10 :</head><label>2210</label><figDesc>The agreement level between D 1 and D 2 in DuelGAN with ? = 0.25 on STL-10 dataset, left: ? = 0.3; middle: 0.5; right: ? = 0.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Inception score results of CIFAR-10 and STL-10.</figDesc><table><row><cell></cell><cell cols="2">CIFAR10 STL-10</cell></row><row><cell>WGAN</cell><cell>3.82</cell><cell>3.97</cell></row><row><cell>GAN</cell><cell>2.61</cell><cell>2.17</cell></row><row><cell>MicroBatchGAN</cell><cell>6.77</cell><cell>7.23</cell></row><row><cell>DCGAN</cell><cell>6.40</cell><cell>5.87</cell></row><row><cell>D2GAN</cell><cell>7.15</cell><cell>6.15</cell></row><row><cell>DuelGAN (ours)</cell><cell>7.45</cell><cell>6.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experiment FID score results of grey-scale image dataset: MNIST and FashionMNIST; natural scene image dataset: CIFAR-10 and STL-10; human face image dataset: CelebA and VGGFace2. Baseline results denoted with (*) were extracted from the original paper report, not independently run in our experiments.</figDesc><table><row><cell></cell><cell cols="4">MNIST FasionMNIST CIFAR10 STL-10</cell><cell>CelebA</cell><cell>VGG</cell></row><row><cell>DCGAN [42]</cell><cell>19.86</cell><cell>24.78</cell><cell>27.45</cell><cell>59.79</cell><cell>17.38</cell><cell>49.99</cell></row><row><cell>WGAN* [17]</cell><cell>14.07</cell><cell>28.24</cell><cell>35.37</cell><cell>60.21</cell><cell>15.23</cell><cell>39.24</cell></row><row><cell>DRAGAN [25]</cell><cell>66.96</cell><cell>62.64</cell><cell>36.49</cell><cell>91.07</cell><cell>14.57</cell><cell>50.20</cell></row><row><cell>D2GAN [37]</cell><cell>22.20</cell><cell>29.33</cell><cell>27.38</cell><cell>54.12</cell><cell>17.30</cell><cell>20.67</cell></row><row><cell>Dist-GAN* [47]</cell><cell>-</cell><cell>-</cell><cell>22.95</cell><cell>36.19</cell><cell>23.7</cell><cell>-</cell></row><row><cell>PresGAN* [11]</cell><cell>42.02</cell><cell>-</cell><cell>52.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LSGAN [41]</cell><cell>23.80</cell><cell>43.00</cell><cell>51.42</cell><cell>70.37</cell><cell>15.35</cell><cell>55.96</cell></row><row><cell cols="2">MicroBatchGAN* [36] 17.10</cell><cell>-</cell><cell>77.70</cell><cell>-</cell><cell>34.50</cell><cell>-</cell></row><row><cell>QSNGAN* [16]</cell><cell>-</cell><cell>-</cell><cell>31.97</cell><cell>59.61</cell><cell>-</cell><cell>-</cell></row><row><cell>DuelGAN (ours)</cell><cell>7.87</cell><cell>21.73</cell><cell>21.55</cell><cell>51.37</cell><cell>13.95</cell><cell>19.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experiment FID score results of CelebA (256?256 RGB images). Baseline results denoted with (*) were obtained from the original paper report.</figDesc><table><row><cell cols="7">Method GLF* [55] MSP* [29] NCP-VAE* [2] LSGM* [49] StyleGAN-ADA [23] StyleGAN-ADA+Duel-D</cell></row><row><cell>FID</cell><cell>41.80</cell><cell>35.00</cell><cell>24.79</cell><cell>7.22</cell><cell>4.85</cell><cell>4.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>60.88 56.01 51.86 58.17 60.91 ?=0.50 58.77 51.37 58.45 55.16 57.75 ?=0.75 55.07 59.58 58.58 58.22 57.75</figDesc><table><row><cell>.1 ?=0.3 ?=0.5</cell><cell>?=0.7</cell><cell>?=0.9</cell></row><row><cell>?=0.25</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The appendix is organized as follows:</p><p>? Section A includes the omitted proofs for all theoretical conclusions in the main paper. ? Section B includes experiment details and additional experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Omitted Proofs</head><p>A.1 Proof of Proposition 1</p><p>We firstly introduce Lemma 1 which helps with the proof of Proposition 1.</p><p>We then have:</p><p>For D 1 , D 2 , according to Lemma 1, the above objective function respectively achieves its maximum in [0, 1], [0, 1] at:</p><p>With the introduce of Duel Game, the distributions p data and p g in the Vanilla GAN got changed due to the appearance of p duel . Thus, we define the corresponding updated distributions in DuelGAN w.r.t. discriminator D i as p datai and p gi , respectively:</p><p>A.2 Proof of Theorem 1</p><p>Proof. When ? = 0, r j,G (x) = 1 2 , for ? = 0, i = 1, 2, we have:</p><p>This allows us to rewrite C(G) 2 as:</p><p>C(G) 2 =E x?pdata i log p data (x) + ? 2 ? p duel (x) p data (x) + p g (x) + ? ? p duel (x) + E x?pg i log p g (x) + ? 2 ? p duel (x) p data (x) + p g (x) + ? ? p duel (x)</p><p>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Isabela Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breandan</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Considine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08680</idno>
		<title level="m">Tiago Falk, and Ioannis Mitliagkas. Multi-objective training of generative adversarial networks with multiple discriminators</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A contrastive learning approach for training variational autoencoder priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face aging with conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigory</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2089" to="2093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On gradient regularizers for mmd gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6700" to="6710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Analysis of Single Layer Networks in Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf" />
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayushman</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Cristian Borges</forename><surname>Gamboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheraz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad Zeshan</forename><surname>Afzal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06412</idno>
		<title level="m">Tac-gan-text conditioned auxiliary classifier generative adversarial network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis K</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titsias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04302</idno>
		<title level="m">Prescribed generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Mahadevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01673</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Generative multi-adversarial networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-agent diverse generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viveka</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet K</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8513" to="8521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3224" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Comminiello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09630</idno>
		<title level="m">Quaternion generative adversarial networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Tu Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02556</idno>
		<title level="m">Multi-generator generative adversarial nets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5077" to="5086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards the automatic anime characters creation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiakai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05509</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07215</idno>
		<title level="m">On convergence and stability of gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Convolutional deep belief networks on cifar-10. Unpublished manuscript</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent space factorisation and manipulation via matrix subspace projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaozheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Guerin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5916" to="5926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3911" to="3919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Large-scale celebfaces attributes (celeba) dataset. Retrieved</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="406" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">microbatchgan: Stimulating diversity with multiadversarial discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gon?alo</forename><surname>Mordido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>?lvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06355</idno>
		<title level="m">Invertible conditional gans for image editing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Loss-sensitive generative adversarial networks on lipschitz densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1118" to="1140" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dist-gan: An improved gan using distance constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Anh</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dist-gan: An improved gan using distance constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Anh</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised gan: Analysis and improvement with multi-class minimax game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Ngoc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13253" to="13264" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Stabilizing generative adversarial networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Wiatrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nystrom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00927</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gp-gan: Towards realistic high-resolution image blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2487" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Amit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10485</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Generative latent flow. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teck Yian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5485" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12027</idno>
		<title level="m">Consistency regularization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
