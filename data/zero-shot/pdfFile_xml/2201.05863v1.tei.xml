<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONVMIXER: FEATURE INTERACTIVE CONVOLUTION WITH CURRICULUM LEARNING FOR SMALL FOOTPRINT AND NOISY FAR-FIELD KEYWORD SPOTTING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianwen</forename><surname>Ng</surname></persName>
							<email>dianwen.ng@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Tian</surname></persName>
							<email>tianbiao.tb@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng</forename><forename type="middle">Siong</forename><surname>Chng</surname></persName>
							<email>aseschng@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CONVMIXER: FEATURE INTERACTIVE CONVOLUTION WITH CURRICULUM LEARNING FOR SMALL FOOTPRINT AND NOISY FAR-FIELD KEYWORD SPOTTING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-keyword spotting</term>
					<term>small footprint</term>
					<term>noisy far-field</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Building efficient architecture in neural speech processing is paramount to success in keyword spotting deployment. However, it is very challenging for lightweight models to achieve noise robustness with concise neural operations. In a realworld application, the user environment is typically noisy and may contain reverberations. We proposed a novel feature interactive convolutional model with merely 100K parameters to tackle this under the noisy far-field condition. The interactive unit is proposed in place of the attention module that promotes the flow of information with more efficient computations. Moreover, curriculum-based multi-condition training is adopted to attain better noise robustness. Our model achieves 98.2% top-1 accuracy on Google Speech Command V2-12 and is competitive against large transformer models under the designed noise condition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>generalizing noisy signals. As a result, the accuracy of the system is likely to deteriorate causing bad user experience when devices get less responsive or subjected to a higher false alarm rate.</p><p>Prior works on improving the overall performance and noise robustness include using an attention-based module to boost the efficiency of the audio networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. This provides the ability to selectively focus on valuable segments of the audio sequence. Furthermore, self-attention such as the audio transformer has shown to outperform the convolutional networks-attention hybrid <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Nevertheless, the huge computational and memory complexity eminently discounts its usability on small devices and becomes less desirable .</p><p>In this paper, we focus on the actual application scenario of a noisy far-field environment. We attempt to optimize the performance of a small KWS system by constructing a novel convolutional networks (CNN) encoder with a mixer module that offers a strong alternative to attention. The mixer unit computes the weighted feature interaction of the global channel to allow the flow of information with varying importance. Prominently, the CNN encoder has a light memory footprint and is highly effective at smaller model sizes. Furthermore, we proposed a learning strategy with curriculum-based multicondition training that surpasses the vanilla multi-condition learning to achieve better noise robustness. We have shown from our experiments that our system outperforms the existing state-of-the-art (SOTA) solutions for small footprint KWS under the noisy far-field condition. Besides, the performance of our proposed system is comparable to models of its size 50 times larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Small Footprint Keyword Spotting -Deep neural networks (DNN) has been proven to be effective in KWS task <ref type="bibr" target="#b8">[9]</ref>. With the rapid development of CNN to automatically learns the encoding of spatial information given a sequence, it has become increasingly popular in acoustic modelling. Earlier work <ref type="bibr" target="#b9">[10]</ref> has demonstrated the use of CNN to execute small footprint KWS. Subsequently, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> have extensively reduced the memory footprint with depthwise separable convolution and achieved the best model size accuracy tradeoff. Noise Robust Speech Model -Multi-condition training has emerged as the method of choice for its simple strategy for noise robustness in small footprint model. However, it gets incompetent when the model learns from a broader range of noises, i.e. from a very low SNR such as -10 dB to clean <ref type="bibr" target="#b12">[13]</ref>. Recently, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> have proposed a more effective method with curriculum learning. In short, they train the model starting with clean or high SNR audio and then gradually increases the noise level to lower SNR. This progressive training is more effective than the conventional method in obtaining noise robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>Our ConvMixer networks consist of three main sections, i.e. pre-convolutional block, convolution-mixer block and postconvolutional block. Similar to the previous work, we built our model encoder based on depthwise separable (DWS) convolution as it provides the most efficient computation using a small number of model parameters. We designed our pre and post convolutional blocks with the same neural layers of a 1dimensional DWS, batch normalization followed by the swish activation <ref type="bibr" target="#b14">[15]</ref>. All of the following blocks are convolved with different kernel sizes as listed in <ref type="figure" target="#fig_0">Fig. 1</ref> and padded to preserve the dimension from the previous time frame. However, <ref type="bibr" target="#b15">[16]</ref> discussed that the property of translation equivariance for the convolutional operation in 1D is not preserved in the frequency domain. This would compromise the learning of some spatial information along with the frequency channel. Hence, we consider introducing 2-dimensional DWS, specifically in our ConvMixer block.</p><p>The ConvMixer block takes the previous channel ? time feature and passes it through the 2D convolutional sub-block for frequency domain extraction. This creates a third dimension that expresses the rich information from the frequency domain. To maintain the shape from the previous input, we employed a pointwise convolution that compresses it back to fit the shape. Then, we implemented the temporal domain feature extraction with a 1-dimensional DWS block. The product from these two operations will result in frequency and temporal rich embeddings. Following that, we built a mixer layer to allow the flow of information over the global feature channel. Lastly, we added skip connections from the previous output and the 2D feature connecting to the output of the block. We express our ConvMixer block in the following equations: </p><formula xml:id="formula_0">z = ? ? f 1 (? ? f (x)) y 1 = ? ? BatchNorm(f (z))<label>(1)</label></formula><formula xml:id="formula_1">y 2 = ? ? BatchNorm(f 2 (y 1 ))<label>(2)</label></formula><formula xml:id="formula_2">y = x + y 1 + f 3 (y 2 )<label>(3)</label></formula><p>where eq (1) computes the frequency domain features with f 1 as the 2d-DWS, 2D convolution function f . Eq (2) computes the temporal domain features with f 2 as the 1d-DWS. Eq <ref type="formula" target="#formula_2">(3)</ref> computes the output of the block with f 3 as the mixer layer and ? as the swish activation for eq (1-3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mixer Layer</head><p>The attention layer is trendy for its strength to allow networks to focus on useful spatial information. Nonetheless, this requires heavy linear computation. Instead of weighing the relevance of an element to every other token, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> suggested mixing the token channel-wise as an alternative approach to feature communication. Therefore, we proposed to utilize two types of multi-layer perceptrons (MLP), namely temporal channel mixing and frequency channel mixing, to induce the interaction between the feature space. Each MLP mixing involves two linear layers and a GELU activation unit independent of each temporal and frequency channel. This is defined as</p><formula xml:id="formula_3">u * ,i = x * ,i + W 2 ? ?(W 1 ? LayerNorm(x) * ,i ) y j, * = u j, * + W 4 ? ?(W 3 ? LayerNorm(u) j, * )<label>(4)</label></formula><p>where ? represents the GELU unit. W 1 and W 2 are the learnable weights of the linear layers for temporal channel shared across all frequency i, for i ? {1, I}. W 3 and W 4 are the learnable weights of the linear layers for frequency channel shared across all j, for j ? {1, J}.</p><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we only learn the weights that connect the channel feature with the weighted coefficient shared across the other domain. For convenience, we transpose the latent feature for frequency channel mix so that the arithmetic stays the same as temporal channel mix. Following that, another transpose will be done to recover its original frequency ? time arrangement. The learned coefficient value facilitate the distribution of information with different significance similar to the attention but to be much more computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Curriculum Based Multi-condition Training</head><p>To enhance the noise robustness of our model, the aforementioned curriculum learning based on the SNR level is employed as a training strategy. To execute, we divide the training process into five progressively harder steps. At the start, we conditioned the model on clean samples without noise. In the following three steps, noises will be introduced to the fixed N samples in increments of -5dB, and all the conditions in N samples is uniformly distributed, i.e. [clean, 0], [clean, 0, -5], [clean, 0, -5, -10]. Lastly, we include far-field audio by augmenting half of our dataset with room impulse response (RIR) data.</p><p>In every epoch of each stage, we record the learning progress with the validation accuracy and the loss. Next, the progression step criterion c is defined as the difference between the normalized validation accuracy and loss. Normalization is based on the accuracy and loss of previous epochs. Eq <ref type="bibr" target="#b4">(5)</ref> depicts the general arithmetic for computing the m th epoch value of the normalized accuracy and loss. Note that the normalization result is zero if m is equal to zero. Subsequently, if c is not higher than the current best criterion for a consecutive of 10 epochs, the model with the latest best criterion will be loaded and progressed to the next stage of difficulty for training. The complete training strategy is shown in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Dataset for Far-field Keyword Spotting</head><p>We evaluate our proposed system on the Google Speech Commands V2 <ref type="bibr" target="#b19">[20]</ref>. It contains 105,000 utterances of 35 Load best model from previous stage; <ref type="bibr" target="#b12">13</ref> Augment noise with next level of difficulty; unique words, each of 1 second long, sampled at 16 kHz. We use the official train, validation and test split provided for the 12 labels classification task. This covers the words: 'up', 'down', 'left', 'right', 'yes', 'no', 'on', 'off', 'go' and 'stop' together with 'silence' and 'unknown' classes. The latter class is treated from the remaining words in the dataset.</p><p>To simulate our noisy far-field environment, we have employed two additional datasets. We apply the noise samples from MUSAN <ref type="bibr" target="#b20">[21]</ref>, where it contains 930 files of assorted noises sampled at 16kHz, with a total duration of about 6 hours. These carry various technical and non-technical noises such as DTMF tones, thunder and car horns and we add them to our commands to mimic the audio under different noisy conditions. Far-field speech is generated using the reverberation from BUT Speech@FIT Reverberation Database <ref type="bibr" target="#b21">[22]</ref>. The dataset holds the RIR data from nine rooms of different sizes (large, middle and small sizes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Implementation Details</head><p>Input Feature -We use the input features of a 64-dimensional log Mel filterbank (FBank) with a 25ms window size and a 10ms shift. We fixed the resolution of our FBank at 98 ? 64, equivalent to 1s of the utterance. Commands that are shorter than 1s will be zero-padded to the right. During training, data augmentation is performed with a time shift in the range of -100 to 100ms. Furthermore, spectrogram masking with both the time and frequency masking parameters of max length 25 is adopted. We generate our noisy data with SNR chosen from the list of set [0, -5, -10] dB as detailed in section 3.3. Then, for stronger learning regularization, input mixup is executed with a mixup ratio of 0.5 on the training samples.  binary cross-entropy loss are used in the optimization process. We trained our model for 200 epochs with early stopping criteria defined as in the progression step criterion in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We compare the performance of the ConvMixer with previously proposed SOTA models. Models are retrained from the official source code provided with our designed data environment. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. From the table, we observed that our proposed model achieved the SOTA accuracy among small models when tested on the official V2-12. Furthermore, it has a noticeable drop in the number of model parameters and MACs that signify lower memory and computation resources. Most importantly, when evaluated on the noisy far-field condition, we scored an absolute improvement of 3% against MatchboxNet with a similar memory footprint of the same multi-condition training. This is extended to 7.4% for our curriculum-based training. Finally, we show that the proposed model is competitive against the larger transformerbased model (KWT-3, AST-Tiny) under the challenging noisy far-field conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We further investigate the importance of the feature interactive structure: MLP mixer under noisy far-field conditions. Using the same curriculum based multi-condition training method, we removed the MLP mixer in the ConvMixer block of our model and obtained the results as shown in <ref type="table">Table 2</ref>.The addition of the Mixer layers provide a substantial boost in the accuracy of approximately 7%, indicating the usefulness of this feature interactive structure in making the model more robust.</p><p>We also explored the performance gains from curriculum based multi-condition training on the transformer based AST-Tiny and the results are shown in <ref type="figure" target="#fig_4">Fig 2.</ref> Curriculum learning on AST-Tiny ? leads the chart with an improvement in accuracy of about 3% compared to multi-condition training.  <ref type="table">Table 2</ref>. Comparison with/without MLP mixer layer This is in agreement with the capability of curriculum learning to improve the performance of the model. Despite that, our proposed model only lags less than 2% behind AST-Tiny ?. Also, the chart shows that curriculum learning is more effective on ConvMixer ? with smaller model parameters, especially in lower SNRs, boosting accuracy by about 5.5%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we introduce a novel small footprint model ConvMixer with the feature interactive structure MLP mixer. Curriculum based multi-condition training method is applied to improve noise robustness. The performance of our Con-vMixer exceeds the existing SOTA KWS in clean and noisy far-field conditions on Command V2-12. Furthermore, it also matches the performance of the transformer-based KWS that uses 50 times more memory consumption and computing resources. The results highlight the potential of ConvMixer used in deployment at the endpoint and application in realworld scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of our ConvMixer model architectur?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>N</head><label></label><figDesc>orm(a m ) = a m ? min(A) max(A) ? min(A) , A = {a 1 , a 2 ..., a m }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Curriculum Based Multi-condition Input: clean audio utterances, D = {x i , y i } N i=1 1 Initialize: bst crit = 0; stage = 0; model parameters, F (?); 2 while stage &lt; 5 do 3 for epoch, m = 1, 2, . . . M do 4? m = Forward(F m (x, ?)); 5 loss m = BCE(? m , y); 6 acc m = Accuracy Score(?, y); 7 compute c = N orm(acc m ) ? N orm(loss m ); 8 update bst crit ? max(bst crit, c) 9 Saving best model if c == bst crit; 10 if c &lt; bst crit for 10 epochs then 11 update stage ? stage +1;12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Model Training -Model is trained with a batch size of 128 and an initial learning rate of 6e-3 factored by 0.85 on every four epoch intervals after the fifth epoch. Adam optimizer and Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Performance gains from curriculum learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the SOTA models ( ?: proposed model with curriculum learning). MACs computed with 1 .</figDesc><table><row><cell cols="5">Accuracy of Far-field Test Command, SNR in dB (%)</cell></row><row><cell>Clean</cell><cell>20 dB</cell><cell>0 dB</cell><cell>-5 dB</cell><cell>-10 dB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>https://github.com/sovrasov/flops-counter.pytorch</figDesc><table><row><cell>ConvMixer  ?</cell><cell cols="2">Accuracy of Far-field Test Command (%)</cell></row><row><cell></cell><cell>Clean 20dB 0dB -5dB</cell><cell>-10dB</cell></row><row><cell>With MLP Mixer</cell><cell>93.16 90.83 83.04 78.39</cell><cell>71.88</cell></row><row><cell cols="2">Without MLP Mixer 85.77 83.52 76.56 72.60</cell><cell>66.26</cell></row></table><note>1</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Match-boxNet: 1d time-channel separable convolutional neural network architecture for speech commands recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somshubra</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Streaming Keyword Spotting on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirk?</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Laurenzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2277" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sercan?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Ar?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1606" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic gain control and multi-style training for robust smallfootprint keyword spotting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4704" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end attention based textdependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Shi-Xiong Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-Task Network for Noise-Robust Keyword Spotting and Speaker Verification Using CTC-Based Soft VAD and Global Query Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunghun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmoon</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahyun</forename><surname>Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoirin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="931" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AST: Audio Spectrogram Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="571" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Keyword Transformer: A Self-Attention Model for Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel Tairum</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="4249" to="4253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1478" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hello edge: Keyword spotting on microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yundong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07128</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A curriculum learning method for improved noise robustness in automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="548" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Curriculum learning based approaches for noise robust speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Broadcasted Residual Learning for Efficient Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeonggeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simyung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dooyong</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="4538" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<title level="m">FNet: Mixing Tokens with Fourier Transforms</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">MLP-Mixer: An all-MLP Architecture for Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5484" to="5488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snyder</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Guoguo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Povey</forename><surname>Daniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08484</idno>
		<title level="m">MU-SAN: A Music, Speech, and Noise Corpus</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building and evaluation of a real room impulse response dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Sz?ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Sk?cel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Mo?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Paliesek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan?ernock?</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="863" to="876" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
