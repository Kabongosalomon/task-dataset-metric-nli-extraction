<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-20">20 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
						</author>
						<title level="a" type="main">LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-20">20 Apr 2022</date>
						</imprint>
					</monogr>
					<note>Both L. Qiao and Z. Li contributed equally. Z. Cheng is the corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Table Structure Recognition ? Aligned Bounding Box ? Empty Cell</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table structure recognition is a challenging task due to the various structures and complicated cell spanning relations. Previous methods handled the problem starting from elements in different granularities (rows/columns, text regions), which somehow fell into the issues like lossy heuristic rules or neglect of empty cell division. Based on table structure characteristics, we find that obtaining the aligned bounding boxes of text region can effectively maintain the entire relevant range of different cells. However, the aligned bounding boxes are hard to be accurately predicted due to the visual ambiguities. In this paper, we aim to obtain more reliable aligned bounding boxes by fully utilizing the visual information from both text regions in proposed local features and cell relations in global features. Specifically, we propose the framework of Local and Global Pyramid Mask Alignment, which adopts the soft pyramid mask learning mechanism in both the local and global feature maps. It allows the predicted boundaries of bounding boxes to break through the limitation of original proposals. A pyramid mask re-scoring module is then integrated to compromise the local and global information and refine the predicted boundaries. Finally, we propose a robust table structure recovery pipeline to obtain the final structure, in which we also effectively solve the problems of empty cells locating and division. Experimental results show that the proposed method achieves competitive and even new state-of-the-art performance on several public benchmarks. The code is available in https://github.com/hikopensource/ DAVAR-Lab-OCR/tree/main/demo/table_recognition/lgpma.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Table structure recognition is a challenging task due to the various structures and complicated cell spanning relations. Previous methods handled the problem starting from elements in different granularities (rows/columns, text regions), which somehow fell into the issues like lossy heuristic rules or neglect of empty cell division. Based on table structure characteristics, we find that obtaining the aligned bounding boxes of text region can effectively maintain the entire relevant range of different cells. However, the aligned bounding boxes are hard to be accurately predicted due to the visual ambiguities. In this paper, we aim to obtain more reliable aligned bounding boxes by fully utilizing the visual information from both text regions in proposed local features and cell relations in global features. Specifically, we propose the framework of Local and Global Pyramid Mask Alignment, which adopts the soft pyramid mask learning mechanism in both the local and global feature maps. It allows the predicted boundaries of bounding boxes to break through the limitation of original proposals. A pyramid mask re-scoring module is then integrated to compromise the local and global information and refine the predicted boundaries. Finally, we propose a robust table structure recovery pipeline to obtain the final structure, in which we also effectively solve the problems of empty cells locating and division. Experimental results show that the proposed method achieves competitive and even new state-of-the-art performance on several public benchmarks. The code is available in https://github.com/hikopensource/ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Table is one of the rich-information data formats in many real documents like financial statements, scientific literature, purchasing lists, etc. Besides the text content, the table structure is vital for people to do the key information extraction. Thus, table structure recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b38">39]</ref> becomes one of the important techniques in current document understanding systems. From the global perspective, early table structure recognition processes usually depend on the detection of the grid's boundaries <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. However, these methods can not handle tables without grid boundaries, such as three-line tables. Though recent works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31]</ref> attempt to predict row/column regions or even invisible grid lines <ref type="bibr" target="#b32">[33]</ref>, they are limited to handle tables that cross span multiple rows/columns. The row/column splitting operation might also cut cells that contain text in multiple lines.</p><p>Another group of methods solves the above problems in a bottom-up way to firstly detect the text blocks' positions and then recover the bounding-boxes' relations by heuristic rules <ref type="bibr" target="#b37">[38]</ref> or GNN(Graph Neural Networks) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. However, rules designed based on bounding boxes of text regions are vulnerable to handling complicated matching situations. GNN-based methods not only bring extra network cost but also depend on more expensive training cost such as the data volume. Another issue is that these methods are difficult to obtain the empty cells because they usually fall into the visual ambiguity problem with the cross-row/column cells. The prediction of empty cells directly affects the correctness of table structure, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(a). Moreover, how to split or merge these empty regions is still a challenging problem that cannot be neglected, because different division results will generate different editable areas when the image is transferred into digital format.</p><p>Notice that the structure of the table itself is a human-made rule-based data form. Under the situation that tables are without visual rotation or perspective transformation, if we could obtain all of the perfect aligned cell regions rather than the text regions <ref type="bibr" target="#b25">[26]</ref>, the structure inference will be easy and almost lossless, as illustrated in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. Nevertheless, acquiring such information is not easy. On the one hand, the annotations of text regions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39]</ref> are much easier to get than cell regions. On the other hand, the aligned boxes are difficult to be accurately learned since there is usually no visible texture of boundaries at the region's periphery. Multi-row/column cells are easy to be confused with the empty cell regions. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>(c), the network usually falls into the situation that the predicted aligned boxes are not large enough and results in the wrong cell matching. Although <ref type="bibr" target="#b25">[26]</ref> designs an alignment loss to assist the bounding boxes learning, it only considers the relative relations between boxes and fails to capture the cell's absolute coverage area.</p><p>In this paper, we aim to train the network to obtain more reliable aligned cell regions and solve the problems of empty cell generation and partition in one model. Observing that people perceive visual information from both local text region and global layout when they read, we propose a uniform table structure recognition framework to compromise the benefits from both local and global information, called LGPMA (Local and Global Pyramid Mask Alignment) Network. Specifically, the model simultaneously learns a local Mask-RCNN-based <ref type="bibr" target="#b5">[6]</ref> aligned bounding boxes detection task and a global segmentation task. In both tasks, we adopt the pyramid soft mask supervision <ref type="bibr" target="#b16">[17]</ref> to help obtain more accurate aligned bounding boxes. In LGPMA, the local branch (LPMA) acquires more reliable text region information through visible texture perceptron, while the global branch (GPMA) can learn more legible spatial information of cells' range or division. The two branches help the network learn better-fused features via jointly learning and effectively refine the detected aligned bounding boxes through a proposed mask re-scoring strategy. Based on the refined results, we design a robust and straightforward table structure recovery pipeline, which can effectively locate empty cells and precisely merge them according to the guidance of global segmentation.</p><p>The major contributions of this paper are as follows: (1) We propose a novel framework called LGPMA Network that compromises the visual features from both local and global perspectives. The model makes full use of the information from the local and global features through a proposed mask re-scoring strategy, which can obtain more reliable aligned cell regions. <ref type="bibr" target="#b1">(2)</ref> We introduce a uniform table structure recovering pipeline, including cell matching, empty cell searching, and empty cell merging. Both non-empty cells and empty cells can be located and split efficaciously. (3) Extensive experiments show that our method achieves competitive and even state-of-the-art results on several popular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Traditional table recognition researches mainly worked with hand-crafted features and heuristic rules <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3]</ref>. These methods are mostly applied to simple table structures or specific data formats, such as PDFs. The early techniques about table detection and recognition can be found in the comprehensive survey <ref type="bibr" target="#b36">[37]</ref>. With the great success of deep neural network in computer vision field, works began to focus on the image-based table with more general structures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>. According to the basic components granularities, we roughly divide previous methods into two types: global-object-based methods and local-object-based methods.</p><p>Global-object-based methods mainly focus on the characteristics of global table components and mostly started from row/column or grid boundaries detection. Works of <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> firstly obtain the rows and columns regions using the detection or segmentation models and then intersect these two regions to obtain the grids of cells. <ref type="bibr" target="#b21">[22]</ref> handles the table detection and table recognition tasks in an end-to-end manner by the table region mask learning and table's row/column mask learning. <ref type="bibr" target="#b32">[33]</ref> detects the rows and columns by learning the interval areas' segmentation between rows/columns and then predicting the indicator to merge the separated cells.</p><p>There also exist some methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref> that directly perceive the whole image information and output table structures as text sequence in an encoder-decoder framework. Although these methods look graceful and entirely avoid human being involved, the models are usually challenging to be trained and rely on a large amount of training data. Global-object-based methods usually have difficulties in handling various complicated table structures, such as cells spanning multiple rows/columns or containing text in multi-lines.</p><p>Local-object-based methods begin from the smallest fundamental element, cells. Given the cell-level text region annotation, the text detection task is relatively easy to finish by the general detection methods like Yolo <ref type="bibr" target="#b26">[27]</ref>, Faster R-CNN <ref type="bibr" target="#b27">[28]</ref>, etc. After that, a group of methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref> tries to recover the cell relations based on some heuristic rules and algorithms. Another type of methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> treat the detected boxes as nodes in a graph and attempt to predict the relations based on techniques of Graph Neural Networks <ref type="bibr" target="#b28">[29]</ref>. <ref type="bibr" target="#b13">[14]</ref> predicts the relations between nodes in three classes (the horizontal connection, the vertical connection, no connection) using several features such as visual features, text positions, word embedding, etc. <ref type="bibr" target="#b1">[2]</ref> adopts graph attention mechanism to enhance the predicting accuracy. <ref type="bibr" target="#b23">[24]</ref> alleviates the problem of large graph nodes numbers by the pair sampling strategy. The above three works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref> also published new table datasets for this research area. Since there is no empty cell detected, local-object based-methods usually fall into empty cell ambiguity.</p><p>In this paper, we try to compromise the advantages of both global and local features. Based on the local detection results, we integrate the global information to refine the detected bounding boxes and provide a straightforward guide for empty cell division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We propose the model LGPMA, whose overall workflow is shown in <ref type="figure">Figure 2</ref>. The workflow of LGPMA. The network simultaneously learns a local aligned bounding boxes detection task (LPMA) and a global segmentation task (GPMA). We adopt the pyramid mask learning mechanisms in both branches and use a mask re-scoring strategy to refine the predicted bounding boxes. Finally, the table structure can be uniformly recovered by a pipeline, including cell matching, empty cell searching, and empty cell merging.</p><p>The model is built based on the existing Mask-RCNN <ref type="bibr" target="#b5">[6]</ref>. The bounding box branch directly learns the detection task of aligned bounding boxes for nonempty cells. The network simultaneously learns a Local Pyramid Mask Alignment (LPMA) task based on the local feature extracted by the RoI-Align operation and a Global Pyramid Mask Alignment (GPMA) task based on the global feature map.</p><p>In LPMA, in addition to the binary segmentation task that learns the text region mask, the network is also trained with the pyramid soft mask supervision in both horizontal and vertical directions.</p><p>In GPMA, the network learns a global pyramid mask for all aligned bounding boxes of non-empty cells. To obtain more information about empty cell splitting, the network also learns the global binary segmentation task that considers both non-empty and empty cells.</p><p>A pyramid mask re-scoring module is then adopted to refine the predicted pyramid labels. The accurate aligned bounding boxes can be obtained by the process of plane clustering. Finally, a uniform structure recovering pipeline containing cell matching, empty cell searching, empty cell merging is integrated to obtain the final table structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Aligned Bounding Box Detection</head><p>The difficulty of accurate text region matching mainly comes from the covered range gap between text regions and the real cell regions. Real cell regions may  contain empty spaces for row/column alignment, especially for those cells crossing span multiple rows/columns. Inspired by <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref>, with the annotations of text regions and row/column indices, we can easily generate the aligned bounding box annotations according to the maximum box height/width in each row/column. The regions of aligned bounding boxes approximately equal to that of real cells. For the table images in print format and without visual rotation or perspective transformation, if we could obtain the aligned cell regions and assume there is no empty cell, it is easy to infer the cell relations according to the coordinate overlapping information in horizontal and vertical directions.</p><p>We adopt Mask-RCNN <ref type="bibr" target="#b5">[6]</ref> as the base model. In the bounding box branch, the network is trained based on the aligned bounding box supervision. However, the aligned bounding box learning is not easy because cells are easy to be confused with empty regions. Motivated by the advanced pyramid mask text detector <ref type="bibr" target="#b16">[17]</ref>, we find that using the soft-label segmentation may break through the proposed bounding box's limitation and provide more accurate aligned bounding boxes. To fully utilize the visual features from both local texture and global layout, we propose to learn the pyramid mask alignment information in these two folds simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Local Pyramid Mask Alignment</head><p>In the mask branch, the model is trained to learn both a binary segmentation task and a pyramid mask regression task, which we call Local Pyramid Mask Alignment (LPMA).</p><p>The binary segmentation task is the same as the original model, in which only the text region is labeled as 1 and others are labeled as 0. The detected mask regions can be used in the following text recognition task.</p><p>For the pyramid mask regression, we assign the pixels in the proposal bounding box regions with the soft-label in both horizontal and vertical directions, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The middle point of text will have the largest regressed target 1. Specifically, we assume the proposed aligned bounding box has the shape of H ? W . The top-left point and bottom right point of the text region are denoted as {(x 1 , y 1 ), (x 2 , y 2 )}, respectively, where 0?x 1 &lt;x 2 ? W and 0?y 1 &lt;y 2 ? H. Therefore, the target of the pyramid mask is in shape R 2?H?W ?[0, 1], in which the two channels represent the target map of the horizontal mask and vertical mask, respectively. For every pixel (h, w), these two targets can be formed as:</p><formula xml:id="formula_0">t (w,h) h = w/x mid w ? x mid W ?w W ?x mid w &gt; x mid , t (w,h) v = h/y mid h ? y mid H?h H?y mid h &gt; y mid ,<label>(1)</label></formula><p>where 0?w&lt;W , 0?h&lt;H, and x mid = x1+x2 2 , y mid = y1+y2 2 . In this way, every pixel in the proposal region takes part in predicting the boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Global Pyramid Mask Alignment</head><p>Although LPMA allows the predicted mask to break through the proposal bounding boxes, the local region's receptive fields are limited. To determine the accurate coverage area of a cell, the global feature might also provide some visual clues. Inspired by <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b24">25]</ref>, learning the offsets of each pixel from a global view could help locate more accurate boundaries. However, bounding boxes in celllevel might be varied in width-height ratios, which leads to the unbalance problem in regression learning. Therefore, we use the pyramid labels as the regressing targets for each pixel, named Global Pyramid Mask Alignment (GPMA).</p><p>Like LPMA, the GPMA learns two tasks simultaneously: a global segmentation task and a global pyramid mask regression task. In the global segmentation task, we directly segment all aligned cells, including non-empty and empty cells. The ground-truth of empty cells are generated according to the maximum height/width of the non-empty cells in the same row/column. Notice that only this task learns empty cell division information since empty cells don't have visible text texture that might influence the region proposal networks to some extent. We want the model to capture the most reasonable cell division pattern during the global boundary segmentation according to the human's reading habit, which is reflected by the manually labeled annotations. For the global pyramid mask regression, since only the text region could provide the information of distinct 'mountain top,' all non-empty cells will be assigned with the soft labels similar to LPMA. All of the ground-truths of aligned bounding boxes in GPMA will be shrunk by 5% to prevent boxes from overlapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimization</head><p>The proposed network is trained end-to-end with multiple optimization tasks. The global optimization can be written as:</p><formula xml:id="formula_1">L = L rpn + ? 1 (L cls + L box ) + ? 2 (L mask + L LP M A ) + ? 3 (L seg + L GP M A ),<label>(2)</label></formula><p>where L rpn , L cls , L box , L mask are the same losses with that of Mask-RCNN, which represent the region proposal network loss, the bounding box classification loss, the bounding boxes regression loss and the segmentation loss of mask in proposals, respectively. L seg is the global binary segmentation loss that is implemented in Dice coefficient loss <ref type="bibr" target="#b19">[20]</ref>, L LP M A and L GP M A are the pyramid label regression losses which are optimized by pixel-wise L1 loss. ? 1 , ? 2 , ? 3 are weighted parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Inference</head><p>The inference process can be described in two stages. We first obtain the refined aligned bounding boxes according to the pyramid mask prediction and then generate the final table structure by the proposed structure recovery pipeline.</p><p>Aligned Bounding Box Refine. In addition to the benefits generated via joint training, the local and global features also exhibit various advantages in object perceiving <ref type="bibr" target="#b34">[35]</ref>. In our setting, we find that local features predict more reliable text region masks, while global prediction can provide more credible long-distance visual information. To compromise both levels' merits, we propose a pyramid mask re-scoring strategy to compromise predictions from LPMA and GPMA. For any proposal region with local pyramid mask prediction, we add the information that comes from the global pyramid mask to adjust these scores. We use some dynamic weights to balance the impacts from LPMA and GPMA.</p><p>Specifically, for a predicted aligned bounding box B={(x 1 , y 1 ), (x 2 , y 2 )}, we firstly obtain the bounding box of the text region mask, denoted as B t ={(x 1 , y 1 ), (x 2 , y 2 )}. Then, we can find a matched connected region P ={p 1 ,p 2 ,...,p n } in the global segmentation map, where p=(x, y) represents a pixel. We use P o = {p|x 1 ? p.x ? x 2 , y 1 ? p.y ? y 2 , ?p ? P } to represent the overlap region. Then the predicted pyramid label of point (x, y) ? P o can be re-scored as follows.</p><formula xml:id="formula_2">F (x) = x?x1 x mid ?x1 F (L) hor (x, y) + x mid ?x x mid ?x1 F (G)</formula><p>hor (x, y) </p><formula xml:id="formula_3">x 1 ? x ? x mid x?x2 x mid ?x2 F (L) hor (x, y) + x mid ?x x mid ?x2 F (G) hor (x, y) x mid &lt; x ? x 2 ,<label>(3)</label></formula><p>where x mid = ver (x, y) are the local horizontal, global horizontal, local vertical and global vertical pyramid label prediction, respectively.</p><p>Next, for any proposal region, the horizontal and vertical pyramid mask labels (corresponding to the z-coordinate) can be used to fit two planes in the 3-dimensional space, respectively. All the four planes' intersection lines with the zero plane are the refined boundaries. For example, to refine the right boundary of the aligned box, we select all pixels that P r = {p|x mid ? p.x ? x 2 , p ? P o } with the refined pyramid mask prediction F (x, y) to fit the plane. If we formed the plane as ax + by + c ? z = 0, using the least square method, the problem is equal to minimize the equation of:   The parameters of a, b, c can be calculated by the matrix as follows:</p><formula xml:id="formula_5">? ? a b c ? ? = ? ? x 2 i x i y i x i x i y i y 2 i y i x i y i ||P o || ? ? ?1 ? ? x i F (x i , y i ) y i F (x i , y i ) F (x i , y i ) ? ? ,<label>(6)</label></formula><p>where ||.|| is the set size. Then we calculate the intersection line between the fitting plane with the plane of z = 0. Given that the bounding boxes are axisaligned, we calculate the refined x-coordinate as the average value:</p><formula xml:id="formula_6">x ref ine = ? 1 y 2 ? y 1 + 1 y2 yi=y1 by i + c a<label>(7)</label></formula><p>Similarly, we can obtain the other three refined boundaries. Notice that the refining process can optionally be conducted iteratively refer to <ref type="bibr" target="#b16">[17]</ref>. <ref type="table">Table Structure</ref> Recovery. Based on the refined aligned bounding boxes, the table structure recovery pipeline aims to obtain the final table structure, including three steps: cell matching, empty cell searching and empty cell merging, as illustrated in <ref type="figure" target="#fig_8">Figure 4</ref>.</p><p>Cell Matching. In the situation that all of the aligned bounding boxes are axis-aligned, the cells matching process is pretty simple but robust. Following the same naming convention with <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref>, the connecting relations can be divided into horizontal and vertical types. The main idea is that if two aligned bounding boxes has enough overlap in x/y-coordinate, we will match them in vertical/horizontal direction. Mathematically, for every two aligned bounding boxes, {(x 1 , y 1 ), (x 2 , y 2 )} and {(x 1 , y 1 ), (x 2 , y 2 )}, they will be horizontally connected if y 1 ? y1+y2 2 ?y 2 or y 1 ? y 1 +y 2 2 ?y 2 . Similarly, they will be vertically connected if</p><formula xml:id="formula_7">x 1 ? x1+x2 2 ? x 2 or x 1 ? x 1 +x 2 2</formula><p>?x 2 . Empty Cell Searching. After obtaining the relations between the detected aligned bounding boxes, we treat them as nodes in a graph, and the connected relations are edges. All of the nodes in the same row/column make up a complete subgraph. Inspired by <ref type="bibr" target="#b23">[24]</ref>, we adopt the algorithm of Maximum Clique Search <ref type="bibr" target="#b0">[1]</ref> to find all maximum cliques in the graph. Take the row searching process as an example, every node that belongs to the same row will be in the same clique. For the cell that crosses span multiple rows, the corresponding node will appear multiple times in different cliques. After sorting these cliques by the average ycoordinate, we can easily label each node with its row index. Nodes that appear in multiple cliques will be labeled with multiple row indices. We can easily find those vacant positions, which are corresponding to the empty cells.</p><p>Empty Cell Merging. By now, we have obtained the empty cells at the smallest level (occupies 1 row and 1 column). To merge these cells more feasibly, we first assign the single empty cells with the aligned bounding box shape as the cell's maximum height/width in the same row/column. Thanks to the visual clues learned by the global segmentation task, we can design the simple merging strategy following the segmentation result. We compute the ratio of pixels that are predicted as 1 in the interval region for every two neighbor empty cells, as the red region illustrated in <ref type="figure" target="#fig_8">Figure 4</ref>. If the ratio is larger than the preset threshold, we will merge these two cells. As we can see, the empty regions' visual ambiguity always exists, and the segmentation task can hardly be learned perfectly. That is why many segmentation-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref> struggle with complicated post-processing, such as fracture completion and threshold setting. The proposed method straightforwardly adopts the original visual clue provided by global segmentation and uses pixel voting to obtain a more reliable result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our proposed framework on following popular benchmarks that contain the annotations of both text content bounding boxes and cell relations.</p><p>ICDAR 2013 <ref type="bibr" target="#b4">[5]</ref>. This dataset contains 98 training samples and 156 testing samples that cropped from the PDF of government reports.</p><p>SciTSR <ref type="bibr" target="#b1">[2]</ref>. This dataset contains 12,000 training images and 3,000 testing images cropped from PDF of scientific literature. Authors also select a subset of complicated samples that contains 2,885 training images and 716 testing images, called SciTSR-COMP.</p><p>PubTabNet <ref type="bibr" target="#b38">[39]</ref>. It is a large-scale complicated table collection that contains 500,777 training images, 9,115 validating images and 9,138 testing images. This dataset contains a large amount of three-lines tables with multi-row/column cells, empty cells, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All experiments are implemented in Pytorch with 8?32 GB-Tesla-V100 GPUs. The deep features are extracted and aggregated through the backbone of ResNet-50 <ref type="bibr" target="#b6">[7]</ref> with Feature Pyramid Network (FPN) <ref type="bibr" target="#b14">[15]</ref>. The weights of the backbone For all benchmarks, the model is trained by the SGD optimizer with batch-size=4, momentum=0.9, and weight-decay=1 ? 10 ?4 . The initial learning ratio of 1 ? 10 ?2 is divided by 10 every 5 epochs. The model's training on SciTSR and PubTabNet lasts for 12 epochs, and the fine-tuning process on ICDAR 2013 lasts for 25 epochs. We also randomly scale the longer side of the input images to the lengths in the range <ref type="bibr">[480,</ref><ref type="bibr">1080]</ref> for all training processes. In the testing phase, we set the longer side of the input image as 768. We empirically set all weight parameters as ? 1 =? 2 =? 3 =1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Table Structure Recognition Benchmarks</head><p>We first conduct experiments on the datasets of ICDAR 2013 and SciTSR, and the evaluation metric follows <ref type="bibr" target="#b4">[5]</ref> (counting the micro-averaged correctness of neighboring relations). Notice that since ICDAR 2013 has very few samples, many previous works used different training or pre-training data. To be comparable to <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b25">[26]</ref>, our model is only trained by the training set of SciTSR. We also report the result of the model that is then fine-tuned on the training set of ICDAR2013, labeled by ?. The results of DeepDeSRT on SciTSR come from <ref type="bibr" target="#b1">[2]</ref>. The results are demonstrated in <ref type="table" target="#tab_0">Table 1</ref>, from which we can see that the proposed LGP-TabNet vastly surpasses previous advances on these three benchmarks by 4.4%, 3.5%, 2.5%, respectively. Beside, LGPMA shows no performance decline on the complicated testing dataset SciTSR-COMP, which demonstrates its powerful ability to perceive spatial relations.</p><p>We also test our model in a more challenging benchmark of PubTabNet, whose results are demonstrated in <ref type="table" target="#tab_1">Table 2</ref>. Since the corresponding evaluation metric of TEDS <ref type="bibr" target="#b38">[39]</ref> considers both table structure and text content, we simply adopt an attention-based model <ref type="bibr" target="#b11">[12]</ref> to recognize the text recognition. In the results, our method surpasses the previous SOTA by 1.6 in TEDS. We also report the results that only considers the table structure, denoted as TEDS-Struc. The performance gap between TEDS-Struc and TEDS mainly comes from the recognition error and annotation ambiguities.</p><p>Visualization Results. We demonstrate some of the visualization results in <ref type="figure">Figure 5</ref>, in which the green boxes denote the predicted non-empty boxes, and blue boxes are the recovered empty boxes. We can see that our model can predict cells' accurate boundaries, even for those cross span multiple rows/columns. <ref type="figure" target="#fig_9">Figure 6</ref> demonstrates an example that is successfully predicted due to the correct refinement. We only show the LPMA and GPMA maps in the horizontal direction, where the LPMA map is generated by overlapping all proposals' local maps. In the initially proposed bounding boxes, some boxes do not have enough breadth, which would lead to wrong matching results. After refining by LPMA and GPMA, these boundaries can reach the more feasible positions. The empty cells can also be merged feasibly according to the predicted segmentation map during the table structure recovery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We randomly select 60,000 training images and 1,000 validation images from PubTabNet to conduct the subsequent ablation studies.</p><p>Effectiveness of Aligned Bounding Box Detection. To verify the effectiveness of the designed modules, we conduct a group of ablation experiments, as shown in <ref type="table" target="#tab_2">Table 3</ref>. Besides the TEDS-Struc metric, we also report the text region's detection results and aligned bounding boxes, where the detection IoU threshold is 0.7, and empty cells are ignored. From the results, we can easily find that both LPMA and GPMA can vastly enhance the aligned bounding box detection performance, which is also proportional to the performance of TEDS-Struc. Although these modules are designed for aligned bounding boxes, they also slightly enhance the performance of text region detection results. The performance gap between text region detection and aligned bounding box detection again demonstrates the latter task is much more difficult and challenging. We also evaluate the effectiveness of Alignment Loss (abbr. AL) proposed by <ref type="bibr" target="#b25">[26]</ref>. Although solely adopting AL achieves better performance than the original Mask-RCNN, the performance is even lower than the best results of LGPMA when compromising all three modules. It means AL might bring adverse impact on LGPMA. Compared to AL, our proposed LGPMA can obtain more performance gain by 3.1% in aligned bounding box detection and 0.59 in TEDS-Struc. <ref type="table">Table Structure</ref> Recovery. To verify the proposed table recovery pipeline's effectiveness, we conduct experiments to compare different empty cell merging strategies, as illustrated in <ref type="table" target="#tab_3">Table 4</ref>. The evaluations contain the detection results that only consider empty cells, the detection results of all aligned bounding boxes, and TEDS-Struc. The strategy of using minimum cells means the direct results after Empty Cell Searching, and using maximum cells  means merging all neighboring empty cells with the same height/width. From the results, we can see that our strategy using the visual information from GPMA can correctly merge many empty cells and obtain the highest performances in both detection and TED-Struc metrics. Compared with Strategy of Minimum empty cells , the promotion of Empty Cell Merging by LGPMA on TEDS-Struc is relatively small. This is because the number of empty cells minority, and most of them in this dataset are labeled in the smallest shapes. Nevertheless, our proposed empty cell merging strategy is more robust to adapt to any possibility. Suppose the aligned bounding boxes of non-empty cells are detected perfectly, which equals the situation given ground-truths, as shown in the bottom part of <ref type="table" target="#tab_3">Table 4</ref>. In this case, we can easily find that using the strategy of whether Minimum cells or the proposed LGPMA can almost achieve 100% accuracy in table structure recovery, and many errors come from the noisy labels. It demonstrates the robustness of our table structure recovery pipeline, and the performance mainly depends on the correctness of aligned bounding box detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a novel framework for table structure recognition named LGPMA. We adopt the local and global pyramid mask learning to compromises advantages from both local texture and global layout information. In the inference stage, fusing the two levels' predictions via a mask re-scoring strategy, the network generates more reliable aligned bounding boxes. Finally, we propose a uniform table structure recovery pipeline to get the final results, which can also predict the feasible empty cell partition. Experimental results demonstrate our method has achieved the new state-of-the-art in three public benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) The visualized results without considering empty cells. (b) The groundtruth of aligned bounding boxes and nodes relations. (c) A false example due to the ambiguity between the empty cell and cross-column cell. The cells and their relations are represented as nodes and connected lines (red: vertical, green: horizontal). Empty cells are displayed in dashed circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. The workflow of LGPMA. The network simultaneously learns a local aligned bounding boxes detection task (LPMA) and a global segmentation task (GPMA). We adopt the pyramid mask learning mechanisms in both branches and use a mask re-scoring strategy to refine the predicted bounding boxes. Finally, the table structure can be uniformly recovered by a pipeline, including cell matching, empty cell searching, and empty cell merging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(a) shows the original aligned bounding box (blue) and text region box (red). (b) shows the pyramid mask labels in horizontal and vertical direction, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F</head><label></label><figDesc>ver (x, y) + y mid ?y y mid ?y1 F (G) ver (x, y) y 1 ? y ? y mid y?y2 y mid ?y2 F (L) ver (x, y) + y mid ?y y mid ?y2 F (G) ver (x, y) y mid &lt; y ? y 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>x 1 +x 2 2 , y mid = y 1 +y 2 2</head><label>22</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc>ax i + by i + c ? F (x i , y i )) 2 , ?p = (x i , y i ) ? P r . (5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>The illustration of table structure recovery pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of an example that is successfully refined. (a) The aligned bounding boxes before refinement. (b) LPMA (in horizontal). (c) GPMA (in horizontal). (d) Global binary segmentation. (e) Final result after refinement and empty cell merging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on ICDAR 2013, SciTSR, SciTSR-COMP datasets. P, R, F1 represent Precision, Recall, F1-Score, respectively. Symbol of ? means pre-trained data are used.</figDesc><table><row><cell>Methods</cell><cell>Training Dataset</cell><cell>ICDAR 2013 P R F1</cell><cell>P</cell><cell>SciTSR R</cell><cell>F1</cell><cell cols="3">SciTSR-COMP P R F1</cell></row><row><cell>DeepDeSRT [30]</cell><cell>-</cell><cell cols="7">0.959 0.874 0.914 0.906 0.887 0.890 0.863 0.831 0.846</cell></row><row><cell>Split [33]</cell><cell>Private</cell><cell>0.869 0.866 0.868</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepTabStR [31]</cell><cell cols="2">ICDAR 2013 0.931 0.930 0.930</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Siddiqui et al. [32] Synthetic 500k 0.934 0.934 0.934</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ReS2TIM [36]</cell><cell cols="2">ICDAR 2013 ? 0.734 0.747 0.740</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GTE [38]</cell><cell cols="2">ICDAR 2013 ? 0.944 0.927 0.935</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GraphTSR [2]</cell><cell>SciTSR</cell><cell cols="7">0.885 0.860 0.872 0.959 0.948 0.953 0.964 0.945 0.955</cell></row><row><cell>TabStruct-Net [26]</cell><cell>SciTSR</cell><cell cols="7">0.915 0.897 0.906 0.927 0.913 0.920 0.909 0.882 0.895</cell></row><row><cell>LGPMA</cell><cell>SciTSR</cell><cell cols="7">0.930 0.977 0.953 0.982 0.993 0.988 0.973 0.987 0.980</cell></row><row><cell>LGPMA</cell><cell cols="2">ICDAR 2013 ? 0.967 0.991 0.979</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="9">are initialized from the pre-trained model of MS-COCO [16]. In LPMA, the</cell></row><row><cell cols="9">model generates anchors in six different ratios [1/20, 1/10, 1/5, 1/2, 1, 2] for cap-</cell></row><row><cell cols="9">turing the different shapes of bounding boxes. The Non-Maximal suppression</cell></row><row><cell cols="6">(NMS) IoU threshold of RCNN is 0.1 in the testing phase.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on PubTabNet. TESDS-Struc only considers the table structures. Visualization results on ICDAR2013, SciTSR, PubTabNet. Green boxes are detected aligned bounding boxes, and blue boxes are empty cells generated by the proposed table structure recovery pipeline.</figDesc><table><row><cell>Methods</cell><cell>Training Dataset</cell><cell>Tesing Dataset</cell><cell>TEDS (All)</cell><cell>TEDS-Struc. (All)</cell></row><row><cell>EDD [39]</cell><cell cols="3">PTN-train PTN-val 88.3</cell><cell>-</cell></row><row><cell cols="4">TabStruct-Net [26] SciTSR PTN-val 90.1</cell><cell>-</cell></row><row><cell>GTE [38]</cell><cell cols="3">PTN-train PTN-val 93.0</cell><cell>-</cell></row><row><cell>LGPMA (ours)</cell><cell cols="3">PTN-train PTN-val 94.6</cell><cell>96.7</cell></row><row><cell>Fig. 5.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiments on that how different modules effect the aligned bounding box detection 83.33 95.04 92.27 91.86 92.06 85.14 84.77 84.95 95.53</figDesc><table><row><cell>Models</cell><cell cols="5">Modules LPMA GPMA AL[26] Precision Recall Hmean Precision Rrecall Hmean Det of text regions Det of non-empty aligned bounding boxes</cell><cell>TEDS-Struc.</cell></row><row><cell>Faster R-CNN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.32</cell><cell cols="2">81.31 81.31 94.63</cell></row><row><cell></cell><cell cols="3">91.71 91.53 91.62</cell><cell>81.83</cell><cell cols="2">81.82 81.83 94.65</cell></row><row><cell cols="7">91.92 91.66 91.79 91.98 91.50 91.74 92.11 91.85 91.98 83.18 Mask R-CNN 84.29 84.10 84.20 95.22 Mask R-CNN 83.48 81.91 81.79 81.85 94.94 92.05 91.65 91.85 84.87 84.50 84.68 95.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation experiments on different empty cells merging strategy. The bottom part shows the results given non-empty aligned bounding boxes ground-truth. 63.04 59.40 82.36 82.74 82.55 95.50 Maximum empty cells 23.19 10.76 14.70 82.30 77.39 79.77 92.57 Proposed LGPMA 68.12 72.43 70.21 82.69 83.17 82.93 95.53 Minimum cells 96.14 97.00 96.56 99.60 99.69 99.64 99.52 Maximum cells 41.13 15.22 22.22 97.58 91.19 94.28 95.60 Proposed LGPMA 97.26 97.68 97.47 99.85 99.88 99.87 99.77</figDesc><table><row><cell>Structure recovery strategies</cell><cell>with non-empty box GT?</cell><cell>Det of empty aligned bounding boxes Precision Recall Hmean Precision Recall Hmean Det of all aligned bounding boxes</cell><cell>TEDS-Struc.</cell></row><row><cell>Minimum empty cells</cell><cell></cell><cell>56.17</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding all cliques of an undirected graph (algorithm 457)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kerbosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="575" to="576" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Complicated table structure recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<idno>abs/1908.04729</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Detecting and recognizing tables in spreadsheets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Doush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pontelli</surname></persName>
		</author>
		<editor>IAPR. pp</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>D?jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kleber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Lang</surname></persName>
		</author>
		<title level="m">ICDAR 2019 competition on table detection and recognition (ctdar). In: ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1510" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orsi</surname></persName>
		</author>
		<title level="m">ICDAR 2013 table competition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1449" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Table structure recognition based on textblock arrangement and ruled line position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Itonori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDAR. pp</title>
		<imprint>
			<biblScope unit="page" from="765" to="768" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Table structure extraction with bi-directional gated recurrent unit networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M D</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Table structure recognition based on robust block segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kieninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Recognition V</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">3305</biblScope>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Table recognition in spreadsheets via a graph representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for OCR in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tablebank: Table benchmark for image-based table detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1918" to="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GFTE: graph-based financial table extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12662</biblScope>
			<biblScope unit="page" from="644" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pyramid mask text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1903.11800</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving the table boundary detection in pdfs by fixing the sequence error of the sparse lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1006" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifying table boundaries in digital documents via sparse line detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1311" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Understanding the semantic structures of tables with a hybrid deep neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sadamitsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="168" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="128" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cascadetabnet: An approach for end to end table detection and structure recognition from imagebased documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gadpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapadni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sultanpure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2439" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rethinking table recognition using graph neural networks. In: ICDAR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Text perceptron: Towards end-to-end arbitrary-shaped text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="11899" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Table structure recognition using top-down and bottom-up cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">12373</biblScope>
			<biblScope unit="page" from="70" to="86" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1162" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deeptabstr: Deep learning based table structure recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Fateh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T R</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation for table structure recognition in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1397" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep splitting and merging for table structure decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Table structure understanding and its performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1479" to="1497" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Scene text detection with supervised pyramid context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="9038" to="9045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Res2tim: Reconstruct syntactic structures from table images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="749" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey of table recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Cordy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Global table extractor (GTE): A framework for joint table identification and cell structure recognition using visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">X R</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2005.00589</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image-based table recognition: Data, model, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shafieibavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="volume">12366</biblScope>
			<biblScope unit="page" from="564" to="580" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">EAST: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
