<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><forename type="middle">Abu</forename><surname>Farha</surname></persName>
							<email>abufarha@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporally locating and classifying action segments in long untrimmed videos is of particular interest to many applications like surveillance and robotics. While traditional approaches follow a two-step pipeline, by generating framewise probabilities and then feeding them to high-level temporal models, recent approaches use temporal convolutions to directly classify the video frames. In this paper, we introduce a multi-stage architecture for the temporal action segmentation task. Each stage features a set of dilated temporal convolutions to generate an initial prediction that is refined by the next one. This architecture is trained using a combination of a classification loss and a proposed smoothing loss that penalizes over-segmentation errors. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our model achieves state-of-the-art results on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Analyzing activities in videos is of significant importance for many applications ranging from video indexing to surveillance. While methods for classifying short trimmed videos have been very successful <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, detecting and temporally locating action segments in long untrimmed videos is still challenging.</p><p>Earlier approaches for action segmentation can be grouped into two categories:</p><p>sliding window approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>, that use temporal windows of different scales to detect action segments, and hybrid approaches that apply a coarse temporal modeling using Markov models on top of frame-wise classifiers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>. While these approaches achieve good results, they are very slow as they require solving a maximization problem over very long sequences.</p><p>Motivated by the advances in speech synthesis, recent  <ref type="figure">Figure 1</ref>. Overview of the multi-stage temporal convolutional network. Each stage generates an initial prediction that is refined by the next stage. At each stage, several dilated 1D convolutions are applied on the activations of the previous layer. A loss layer is added after each stage.</p><p>approaches rely on temporal convolutions to capture long range dependencies between the video frames <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5]</ref>. In these models, a series of temporal convolutions and pooling layers are adapted in an encoder-decoder architecture for the temporal action segmentation. Despite the success of such temporal models, these approaches operate on a very low temporal resolution of a few frames per second.</p><p>In this paper, we propose a new model that also uses temporal convolutions which we call Multi-Stage Temporal Convolutional Network (MS-TCN). In contrast to previous approaches, the proposed model operates on the full temporal resolution of the videos and thus achieves better results. Our model consists of multiple stages where each stage out-puts an initial prediction that is refined by the next one. In each stage, we apply a series of dilated 1D convolutions, which enables the model to have a large temporal receptive field with less parameters. <ref type="figure">Figure 1</ref> shows an overview of the proposed multi-stage model. While this architecture already performs well, we further employ a smoothing loss during training which penalizes over-segmentation errors in the predictions. Extensive evaluation on three datasets shows the effectiveness of our model in capturing long range dependencies between action classes and producing high quality predictions. Our contribution is thus two folded: First, we propose a multi-stage temporal convolutional architecture for the action segmentation task that operates on the full temporal resolution. Second, we introduce a smoothing loss to enhance the quality of the predictions. Our approach achieves state-of-the-art results on three challenging benchmarks for action segmentation: 50Salads <ref type="bibr" target="#b24">[25]</ref>, Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b7">[8]</ref>, and the Breakfast dataset <ref type="bibr" target="#b11">[12]</ref>. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Detecting actions and temporally segmenting long untrimmed videos has been studied by many researchers. While traditional approaches use a sliding window approach with non-maximum suppression <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11]</ref>, Fathi and Rehg <ref type="bibr" target="#b6">[7]</ref> model actions based on the change in the state of objects and materials. In <ref type="bibr" target="#b5">[6]</ref>, actions are represented based on the interactions between hands and objects. These representations are used to learn sets of temporally-consistent actions. Bhattacharya et al. <ref type="bibr" target="#b0">[1]</ref> use a vector time series representation of videos to model the temporal dynamics of complex actions using methods from linear dynamical systems theory. The representation is based on the output of pretrained concept detectors applied on overlapping temporal windows. Cheng et al. <ref type="bibr" target="#b3">[4]</ref> represent videos as a sequence of visual words, and model the temporal dependency by employing a Bayesian non-parametric model of discrete sequences to jointly classify and segment video sequences.</p><p>Other approaches employ high level temporal modeling over frame-wise classifiers. Kuehne et al. <ref type="bibr" target="#b12">[13]</ref> represent the frames of a video using Fisher vectors of improved dense trajectories, and then each action is modeled with a hidden Markov model (HMM). These HMMs are combined with a context-free grammar for recognition to determine the most probable sequence of actions. A hidden Markov model is also used in <ref type="bibr" target="#b25">[26]</ref> to model both transitions between states and their durations. Vo and Bobick <ref type="bibr" target="#b27">[28]</ref> use a Bayes network to segment activities. They represent compositions of actions using a stochastic context-free grammar with AND-OR operations. <ref type="bibr" target="#b19">[20]</ref> propose a model for temporal action detection that consists of three components: an action model that maps features extracted from the video frames into action probabilities, a language model that describes the probability of actions at sequence level, and finally a length model that models the length of different action segments. To get the video segmentation, they use dynamic programming to find the solution that maximizes the joint probability of the three models. Singh et al. <ref type="bibr" target="#b22">[23]</ref> use a two-stream network to learn representations of short video chunks. These representations are then passed to a bidirectional LSTM to capture dependencies between different chunks. However, their approach is very slow due to the sequential prediction. In <ref type="bibr" target="#b23">[24]</ref>, a three-stream architecture that operates on spatial, temporal and egocentric streams is introduced to learn egocentric-specific features. These features are then classified using a multi-class SVM.</p><p>Inspired by the success of temporal convolution in speech synthesis <ref type="bibr" target="#b26">[27]</ref>, researchers have tried to use similar ideas for the temporal action segmentation task. Lea et al. <ref type="bibr" target="#b14">[15]</ref> propose a temporal convolutional network for action segmentation and detection. Their approach follows an encoder-decoder architecture with a temporal convolution and pooling in the encoder, and upsampling followed by deconvolution in the decoder. While using temporal pooling enables the model to capture long-range dependencies, it might result in a loss of fine-grained information that is necessary for fine-grained recognition. Lei and Todorovic <ref type="bibr" target="#b16">[17]</ref> build on top of <ref type="bibr" target="#b14">[15]</ref> and use deformable convolutions instead of the normal convolution and add a residual stream to the encoder-decoder model. Both approaches in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref> operate on downsampled videos with a temporal resolution of 1-3 frames per second. In contrast to these approaches, we operate on the full temporal resolution and use dilated convolutions to capture long-range dependencies.</p><p>There is a huge line of research that addresses the action segmentation task in a weakly supervised setup <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref>. Kuehne et al. <ref type="bibr" target="#b13">[14]</ref> train a model for action segmentation from video transcripts. In their approach, an HMM is learned for each action and a Gaussian mixture model (GMM) is used to model observations. However, since frame-wise classifiers do not capture enough context to detect action classes, Richard et al. <ref type="bibr" target="#b20">[21]</ref> use a GRU instead of the GMM that is used in <ref type="bibr" target="#b13">[14]</ref>, and they further divide each action into multiple sub-actions to better detect complex actions. Both of these models are trained in an iterative procedure starting from a linear alignment based on the video transcript. Similarly, Ding and Xu <ref type="bibr" target="#b4">[5]</ref> train a temporal convolutional feature pyramid network in an iterative manner starting from a linear alignment. Instead of using hard labels, they introduce a soft labeling mechanism at the boundaries, which results in a better convergence. In contrast to these approaches, we address the temporal action segmentation task in a fully supervised setup and the weakly supervised case is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Action Segmentation</head><p>We introduce a multi-stage temporal convolutional network for the temporal action segmentation task. Given the frames of a video x 1:T = (x 1 , . . . , x T ), our goal is to infer the class label for each frame c 1:T = (c 1 , . . . , c T ), where T is the video length. First, we describe the single-stage approach in Section 3.1, then we discuss the multi-stage model in Section 3.2. Finally, we describe the proposed loss function in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Single-Stage TCN</head><p>Our single stage model consists of only temporal convolutional layers. We do not use pooling layers, which reduce the temporal resolution, or fully connected layers, which force the model to operate on inputs of fixed size and massively increase the number of parameters. We call this model a single-stage temporal convolutional network (SS-TCN). The first layer of a single-stage TCN is a 1 ? 1 convolutional layer, that adjusts the dimension of the input features to match the number of feature maps in the network. Then, this layer is followed by several layers of dilated 1D convolution. Inspired by the wavenet <ref type="bibr" target="#b26">[27]</ref> architecture, we use a dilation factor that is doubled at each layer, i.e. 1, 2, 4, ...., 512. All these layers have the same number of convolutional filters. However, instead of the causal convolution that is used in wavenet, we use acausal convolutions with kernel size 3. Each layer applies a dilated convolution with ReLU activation to the output of the previous layer. We further use residual connections to facilitate gradients flow. The set of operations at each layer can be formally described as follow?</p><formula xml:id="formula_0">H l = ReLU (W 1 * H l?1 + b 1 ),<label>(1)</label></formula><formula xml:id="formula_1">H l = H l?1 + W 2 * ? l + b 2 ,<label>(2)</label></formula><p>where H l is the output of layer l, * denotes the convolution operator, W 1 ? R 3?D?D are the weights of the dilated convolution filters with kernel size 3 and D is the number of convolutional filters, W 2 ? R 1?D?D are the weights of a 1 ? 1 convolution, and b 1 , b 2 ? R D are bias vectors. These operations are illustrated in <ref type="figure">Figure 2</ref>. Using dilated convolution increases the receptive field without the need to increase the number of parameters by increasing the number of layers or the kernel size. Since the receptive field grows exponentially with the number of layers, we can achieve a very large receptive field with a few layers, which helps in preventing the model from over-fitting the training data.</p><p>The receptive field at each layer is determined using this formula</p><formula xml:id="formula_2">ReceptiveF ield(l) = 2 l+1 ? 1,<label>(3)</label></formula><p>where l ? [1, L] is the layer number. Note that this formula is only valid for a kernel of size 3. To get the probabilities for the output class, we apply a 1 ? 1 convolution over the output of the last dilated convolution layer followed by a softmax activation, i.e.</p><formula xml:id="formula_3">Y t = Sof tmax(W h L,t + b),<label>(4)</label></formula><p>where Y t contains the class probabilities at time t, h L,t is the output of the last dilated convolution layer at time t, W ? R C?D and b ? R C are the weights and bias for the 1 ? 1 convolution layer, where C is the number of classes and D is the number of convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Stage TCN</head><p>Stacking several predictors sequentially has shown significant improvements in many tasks like human pose estimation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b17">18]</ref>. The idea of these stacked or multi-stage architectures is composing several models sequentially such that each model operates directly on the output of the previous one. The effect of such composition is an incremental refinement of the predictions from the previous stages.</p><p>Motivated by the success of such architectures, we introduce a multi-stage temporal convolutional network for the temporal action segmentation task. In this multi-stage model, each stage takes an initial prediction from the previous stage and refines it. The input of the first stage is the frame-wise features of the video as follows</p><formula xml:id="formula_4">Y 0 = x 1:T ,<label>(5)</label></formula><formula xml:id="formula_5">Y s = F(Y s?1 ),<label>(6)</label></formula><p>where Y s is the output at stage s and F is the single-stage TCN discussed in Section 3.1. Using such a multi-stage architecture helps in providing more context to predict the class label at each frame. Furthermore, since the output of each stage is an initial prediction, the network is able to capture dependencies between action classes and learn plausible action sequences, which helps in reducing the oversegmentation errors.</p><p>Note that the input to the next stage is just the frame-wise probabilities without any additional features. We will show in the experiments how adding features to the input of next stages affects the quality of the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>As a loss function, we use a combination of a classification loss and a smoothing loss. For the classification loss, we use a cross entropy loss</p><formula xml:id="formula_6">L cls = 1 T t ?log(y t,c ),<label>(7)</label></formula><p>where y t,c is the the predicted probability for the ground truth label at time t.</p><p>While the cross entropy loss already performs well, we found that the predictions for some of the videos contain a few over-segmentation errors. To further improve the quality of the predictions, we use an additional smoothing loss to reduce such over-segmentation errors. For this loss, we use a truncated mean squared error over the frame-wise logprobabilities</p><formula xml:id="formula_7">L T ?M SE = 1 T C t,c? 2 t,c ,<label>(8)</label></formula><formula xml:id="formula_8">? t,c = ? t,c : ? t,c ? ? ? : otherwise ,<label>(9)</label></formula><formula xml:id="formula_9">? t,c = |log y t,c ? log y t?1,c | ,<label>(10)</label></formula><p>where T is the video length, C is the number of classes, and y t,c is the probability of class c at time t.</p><p>Note that the gradients are only computed with respect to y t,c , whereas y t?1,c is not considered as a function of the model's parameters. This loss is similar to the Kullback-Leibler (KL) divergence loss where</p><formula xml:id="formula_10">L KL = 1 T t,c y t?1,c (log y t?1,c ? log y t,c ).<label>(11)</label></formula><p>However, we found that the truncated mean squared error (L T ?M SE ) (8) reduces the over-segmentation errors more. We will compare the KL loss and the proposed loss in the experiments. The final loss function for a single stage is a combination of the above mentioned losses</p><formula xml:id="formula_11">L s = L cls + ?L T ?M SE ,<label>(12)</label></formula><p>where ? is a model hyper-parameter to determine the contribution of the different losses. Finally to train the complete model, we minimize the sum of the losses over all stages</p><formula xml:id="formula_12">L = s L s .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>We use a multi-stage architecture with four stages, each stage contains ten dilated convolution layers, where the dilation factor is doubled at each layer and dropout is used after each layer. We set the number of filters to 64 in all the layers of the model and the filter size is 3. For the loss function, we set ? = 4 and ? = 0.15. In all experiments, we use Adam optimizer with a learning rate of 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We evaluate the proposed model on three challenging datasets: 50Salads <ref type="bibr" target="#b24">[25]</ref>, Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b7">[8]</ref>, and the Breakfast dataset <ref type="bibr" target="#b11">[12]</ref>.</p><p>The 50Salads dataset contains 50 videos with 17 action classes. On average, each video contains 20 action instances and is 6.4 minutes long. As the name of the dataset indicates, the videos depict salad preparation activities. These activities were performed by 25 actors where each actor prepared two different salads. For evaluation, we use five-fold cross-validation and report the average as in <ref type="bibr" target="#b24">[25]</ref>.</p><p>The GTEA dataset contains 28 videos corresponding to 7 different activities, like preparing coffee or cheese sandwich, performed by 4 subjects. All the videos were recorded by a camera that is mounted on the actor's head. The frames of the videos are annotated with 11 action classes including background. On average, each video has 20 action instances. We use cross-validation for evaluation by leaving one subject out.</p><p>The Breakfast dataset is the largest among the three datasets with 1, 712 videos. The videos were recorded in 18 different kitchens showing breakfast preparation related activities. Overall, there are 48 different actions where each video contains 6 action instances on average. For evaluation, we use the standard 4 splits as proposed in <ref type="bibr" target="#b11">[12]</ref> and report the average.</p><p>For all datasets, we extract I3D <ref type="bibr" target="#b2">[3]</ref> features for the video frames and use these features as input to our model. For GTEA and Breakfast datasets we use the videos temporal resolution at 15 fps, while for 50Salads we downsampled the features from 30 fps to 15 fps to be consistent with the other datasets.</p><p>Evaluation Metrics. For evaluation, we report the framewise accuracy (Acc), segmental edit distance and the segmental F1 score at overlapping thresholds 10%, 25% and 50%, denoted by F 1@{10, 25, 50}. The overlapping threshold is determined based on the intersection over union (IoU) ratio. While the frame-wise accuracy is the most commonly used metric for action segmentation, long action classes have a higher impact than short action classes on this metric and over-segmentation errors have a very low  <ref type="table">Table 1</ref>. Effect of the number of stages on the 50Salads dataset. impact. For that reason, we use the segmental F1 score as a measure of the quality of the prediction as proposed by <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effect of the Number of Stages</head><p>We start our evaluation by showing the effect of using a multi-stage architecture. <ref type="table">Table 1</ref> shows the results of a single-stage model compared to multi-stage models with different number of stages. As shown in the table, all of these models achieve a comparable frame-wise accuracy. Nevertheless, the quality of the predictions is very different. Looking at the segmental edit distance and F1 scores of these models, we can see that the single-stage model produces a lot of over-segmentation errors, as indicated by the low F1 score. On the other hand, using a multi-stage architecture reduces these errors and increases the F1 score. This effect is clearly visible when we use two or three stages, which gives a huge boost to the accuracy. Adding the fourth stage still improves the results but not as significant as the previous stages. However, by adding the fifth stage, we can see that the performance starts to degrade. This might be an over-fitting problem as a result of increasing the number of parameters. The effect of the multi-stage architecture can also be seen in the qualitative results shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Adding more stages results in an incremental refinement of the predictions. For the rest of the experiments we use a multi-stage TCN with four stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Stage TCN vs. Deeper Single-Stage TCN</head><p>In the previous section, we have seen that our multi-stage architecture is better than a single-stage one. However, that comparison does not show whether the improvement is because of the multi-stage architecture or due to the increase in the number of parameters when adding more stages. For a fair comparison, we train a single-stage model that has the same number of parameters as the multi-stage one. As each   <ref type="table">Table 3</ref>. Comparing different loss functions on the 50Salads dataset. stage in our MS-TCN contains 12 layers (ten dilated convolutional layers, one 1 ? 1 convolutional layer and a softmax layer), we train a single-stage TCN with 48 layers, which is the number of layers in a MS-TCN with four stages. For the dilated convolutions, we use similar dilation factors as in our MS-TCN. I.e. we start with a dilation factor of 1 and double it at every layer up to a factor of 512, and then we start again from 1. As shown in <ref type="table" target="#tab_0">Table 2</ref>, our multi-stage architecture outperforms its single-stage counterpart with a large margin of up to 27%. This highlights the impact of the proposed architecture in improving the quality of the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparing Different Loss Functions</head><p>As a loss function, we use a combination of a crossentropy loss, which is common practice for classification tasks, and a truncated mean squared loss over the framewise log-probabilities to ensure smooth predictions. While the smoothing loss slightly improves the frame-wise accuracy compared to the cross entropy loss alone, we found that this loss produces much less over-segmentation errors. <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_5">Figure 4</ref> show a comparison of these losses. As shown in <ref type="table">Table 3</ref>, the proposed loss achieves better F1 and edit scores with an absolute improvement of 5%. This indicates that our loss produces less over-segmentation errors compared to cross entropy since it forces consecutive frames to have similar class probabilities, which results in a smoother output.</p><p>Penalizing the difference in log-probabilities is similar to the Kullback-Leibler (KL) divergence loss, which measures the difference between two probability distributions. However, the results show that the proposed loss produces better results than the KL loss as shown in <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_5">Figure 4</ref>. The reason behind this is the fact that the KL divergence loss does not penalize cases where the difference between the target probability and the predicted probability is very small. Whereas the proposed loss penalizes small differences as well. Note that, in contrast to the KL loss, the proposed loss is symmetric. <ref type="figure" target="#fig_6">Figure 5</ref> shows the surface for both the KL loss and the proposed truncated mean squared loss for the case of two classes. We also tried a symmetric version of the KL loss but it performed worse than the proposed loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Impact of ? and ?</head><p>The effect of the proposed smoothing loss is controlled by two hyper-parameters: ? and ? . In this section, we study the impact of these parameters and see how they affect the performance of the proposed model. Impact of ?: In all experiments, we set ? = 0.15. To analyze the effect of this parameter, we train different models with different values of ?. As shown in <ref type="table" target="#tab_1">Table 4</ref>, the impact of ? is very small on the performance. Reducing ? to 0.05 still improves the performance but not as good as the default value of ? = 0.15. Increasing its value to ? = 0.25 also causes a degradation in performance. This drop in performance is due to the fact that the smoothing loss penalizes heavily changes in frame-wise labels, which affects the detected boundaries between action segments. Impact of ? : This hyper-parameter defines the threshold to truncate the smoothing loss. Our default value is ? = 4. While reducing the value to ? = 3 still gives an improvement over the cross entropy baseline, setting ? = 5 results in a huge drop in performance. This is mainly because when ? is too high, the smoothing loss penalizes cases where the model is very confident that the consecutive frames belong to two different classes, which indeed reduces the capability of the model in detecting the true boundaries between action segments.   <ref type="table">Table 5</ref>. Effect of passing features to higher stages on the 50Salads dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Effect of Passing Features to Higher Stages</head><p>In the proposed multi-stage TCN, the input to higher stages are the frame-wise probabilities only. However, in the multi-stage architectures that are used for human pose estimation, additional features are usually concatenated to the output heat-maps of the previous stage. In this experiment, we therefore analyze the effect of combining additional features to the input probabilities of higher stages. To this end, we trained two multi-stage TCNs: one with only the predicted frame-wise probabilities as input to the next stage, and for the second model, we concatenated the output of the last dilated convolutional layer in each stage to the input probabilities of the next stage. As shown in Table 5, concatenating the features to the input probabilities results in a huge drop of the F1 score and the segmental edit distance (around 20%). We argue that the reason behind this degradation in performance is that a lot of action classes share similar appearance and motion. By adding the features of such classes at each stage, the model is confused and produces small separated falsely detected action segments that correspond to an over-segmentation effect. Passing only the probabilities forces the model to focus on the context of neighboring labels, which are explicitly represented by the probabilities. This effect can also be seen in the qualitative results shown in <ref type="figure" target="#fig_8">Figure 6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Impact of Temporal Resolution</head><p>Previous temporal models operate on a low temporal resolution of 1-3 frames per second <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5]</ref>. On the con-  <ref type="table">Table 6</ref>. Impact of temporal resolution on the 50Salads dataset. trary, our approach is able to handle higher resolution of 15 fps. In this experiment, we evaluate our model in a low temporal resolution of 1 fps. As shown in <ref type="table">Table 6</ref>, the proposed model is able to handle both low and high temporal resolutions. While reducing the temporal resolution results in a better edit distance and segmental F1 score, using high resolution gives better frame-wise accuracy. Operating on a low temporal resolution makes the model less prune to the over-segmentation problem, which is reflected in the better edit and F1 scores. Nevertheless, this comes with the cost of losing the precise location of the boundaries between action segments, or even missing small action segments.  <ref type="table">Table 7</ref>. Effect of the number of layers (L) in each stage on the 50Salads dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Impact of the Number of Layers</head><p>In our experiments, we fix the number of layers (L) in each stage to 10 Layers. <ref type="table">Table 7</ref> shows the impact of this parameter on the 50Salads dataset. Increasing L form 6 to 10 significantly improves the performance. This is mainly due to the increase in the receptive field. Using more than 10 layers (L = 12) does not improve the frame-wise accuracy but slightly increases the F1 scores.</p><p>To study the impact of the large receptive field on short videos, we evaluate our model on three groups of videos  <ref type="table">Table 8</ref>. Evaluation of three groups of videos based on their durations on the GTEA dataset.</p><p>based on their durations. For this evaluation, we use the GTEA dataset since it contains shorter videos compared to the others. As shown in <ref type="table">Table 8</ref>, our model performs well on both short and long videos. Nevertheless, the performance is slightly worse on longer videos due to the limited receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Impact of Fine-tuning the Features</head><p>In our experiments, we use the I3D features without finetuning. <ref type="table">Table 9</ref> shows the effect of fine-tuning on the GTEA dataset. Our multi-stage architecture significantly outperforms the single stage architecture -with and without finetuning. Fine-tuning improves the results, but the effect of fine-tuning for action segmentation is lower than for action recognition. This is expected since the temporal model is by far more important for segmentation than for recognition.  <ref type="table">Table 9</ref>. Effect of fine-tuning on the GTEA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Comparison with the State-of-the-Art</head><p>In this section, we compare the proposed model to the state-of-the-art methods on three datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and Breakfast datasets. The results are presented in <ref type="table">Table 10</ref>. As shown in the table, our model outperforms the state-of-the-art methods on the three datasets and with respect to three evaluation metrics: F1 score, segmental edit distance, and frame-wise accuracy (Acc) with a large margin (up to 12.6% for the frame-wise accuracy on the 50Salads dataset). Qualitative results on the three datasets are shown in <ref type="figure" target="#fig_10">Figure 7</ref>. Note that all the reported results are obtained using the I3D features. To analyze the effect of using a different type of features, we evaluated our model on the Breakfast dataset using the improved dense trajectories (IDT) features, which are the standard used features for the Breakfast dataset. As shown in <ref type="table">Table 10</ref>, the impact of the features is very small. While the frame-wise accuracy and edit distance are slightly better using the I3D features, the model achieves a better F1 score when using the IDT features compared to I3D. This is mainly because I3D features encode both motion and appearance, whereas the IDT features encode only motion. For datasets like Breakfast, using appearance information does not help the performance since the appearance does 50Salads F1@{10,25,50} Edit Acc IDT+LM <ref type="bibr" target="#b19">[20]</ref> 44.4 38.9 27.8 45.8 48.7 Bi-LSTM <ref type="bibr" target="#b22">[23]</ref> 62.6 58.3 47.0 55.6 55.7 ED-TCN <ref type="bibr" target="#b14">[15]</ref> 68.0 63.9 52.6 59.8 64.7 TDRN <ref type="bibr" target="#b16">[17]</ref> 72.9 68.5 57.  <ref type="table">Table 10</ref>. Comparison with the state-of-the-art on 50Salads, GTEA, and the Breakfast dataset. (* obtained from <ref type="bibr" target="#b4">[5]</ref>). not give a strong evidence about the action that is carried out. This can be seen in the qualitative results shown in <ref type="figure" target="#fig_10">Figure 7</ref>. The video frames share a very similar appearance. Additional appearance features therefore do not help in recognizing the activity.</p><p>As our model does not use any recurrent layers, it is very fast both during training and testing. Training our fourstages MS-TCN for 50 epochs on the 50Salads dataset is four times faster than training a single cell of Bi-LSTM with a 64-dimensional hidden state on a single GTX 1080 Ti GPU. This is due to the sequential prediction of the LSTM, where the activations at any time step depend on the activations from the previous steps. For the MS-TCN, activations at all time steps are computed in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a multi-stage architecture for the temporal action segmentation task. Instead of the commonly used temporal pooling, we used dilated convolutions to increase the temporal receptive field. The experimental evaluation demonstrated the capability of our architecture in capturing temporal dependencies between action classes and reducing over-segmentation errors. We further introduced a smoothing loss that gives an additional improvement of the predictions quality. Our model outperforms the state-of-the-art methods on three challenging datasets with a large margin. Since our model is fully convolutional, it is very efficient and fast both during training and testing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 +Figure 2 .</head><label>12</label><figDesc>Overview of the dilated residual layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F1@{10</head><label></label><figDesc>(2 stages) 55.5 52.9 47.3 47.9 79.8 MS-TCN (3 stages) 71.5 68.6 61.1 64.0 78.6 MS-TCN (4 stages) 76.3 74.0 64.5 67.9 80.7 MS-TCN (5 stages) 76.4 73.4 63.6 69.2 79.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative result from the 50Salads dataset for comparing different number of stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F1@{10, 25</head><label>25</label><figDesc>,50} Edit Acc SS-TCN (48 layers) 49.0 46.4 40.2 40.7 78.0 MS-TCN 76.3 74.0 64.5 67.9 80.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative result from the 50Salads dataset for comparing different loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Loss surface for the Kullback-Leibler (KL) divergence loss (LKL) and the proposed truncated mean squared loss (LT ?M SE ) for the case of two classes. yt,c is the predicted probability for class c and yt?1,c is the target probability corresponding to that class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(? = 0.05, ? = 4) 74.1 71.7 62.4 66.6 80.0 MS-TCN (? = 0.15, ? = 4) 76.3 74.0 64.5 67.9 80.7 MS-TCN (? = 0.25, ? = 4) 74.7 72.4 63.7 68.1 78.9 Impact of ? F1@{10,25,50} Edit Acc MS-TCN (? = 0.15, ? = 3) 74.2 72.1 62.2 67.1 79.4 MS-TCN (? = 0.15, ? = 4) 76.3 74.0 64.5 67.9 80.7 MS-TCN (? = 0.15, ? = 5) 66.6 63.7 54.7 60.0 74.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results for two videos from the 50Salads dataset for showing the effect of passing features to higher stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results for the temporal action segmentation task on (a) 50Salads (b) GTEA, and (c) Breakfast dataset. F1@{10,25,50} Edit Acc MS-TCN (1 fps) 77.8 74.9 64.0 70.7 78.6 MS-TCN (15 fps) 76.3 74.0 64.5 67.9 80.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>F1@{10</head><label></label><figDesc>76.3 74.0 64.5 67.9 80.7 L = 12 77.8 75.2 66.9 69.6 80.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>F1@{10, 25</head><label>25</label><figDesc>,50} Edit Acc w/o FT SS-TCN 62.8 60.0 48.1 55.0 73.3 MS-TCN (4 stages) 85.8 83.4 69.8 79.0 76.3 with FT SS-TCN 69.5 64.9 55.8 61.1 75.3 MS-TCN (4 stages) 87.5 85.4 74.6 81.4 79.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Comparing a multi-stage TCN with a deep single-stage TCN on the 50Salads dataset. L cls + ?L KL 71.9 69.3 60.1 64.6 80.2 L cls + ?L T ?M SE 76.3 74.0 64.5 67.9 80.7</figDesc><table><row><cell></cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>L cls</cell><cell cols="2">71.3 69.7 60.7 64.2 79.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Impact of ? and ? on the 50Salads dataset.</figDesc><table><row><cell></cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell cols="3">Probabilities and features 56.2 53.7 45.8 47.6 76.8</cell></row><row><cell>Probabilities only</cell><cell cols="2">76.3 74.0 64.5 67.9 80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>87.9 77.0 82.5 76.6 1 ? 1.5 min 85.9 84.3 71.9 80.7 76.4 ? 1.5 min 81.2 76.5 58.4 71.8 75.9</figDesc><table><row><cell>Duration</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>&lt; 1 min</cell><cell>89.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>TCN (IDT) 58.2 52.9 40.8 61.4 65.1 MS-TCN (I3D) 52.6 48.1 37.9 61.7 66.3</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">2 66.0 68.1</cell></row><row><cell>MS-TCN</cell><cell cols="5">76.3 74.0 64.5 67.9 80.7</cell></row><row><cell>GTEA</cell><cell cols="3">F1@{10,25,50}</cell><cell cols="2">Edit Acc</cell></row><row><cell>Bi-LSTM [23]</cell><cell cols="3">66.5 59.0 43.6</cell><cell>-</cell><cell>55.5</cell></row><row><cell>ED-TCN [15]</cell><cell cols="3">72.2 69.3 56.0</cell><cell>-</cell><cell>64.0</cell></row><row><cell>TDRN [17]</cell><cell cols="5">79.2 74.4 62.7 74.1 70.1</cell></row><row><cell>MS-TCN</cell><cell cols="5">85.8 83.4 69.8 79.0 76.3</cell></row><row><cell>MS-TCN (FT)</cell><cell cols="5">87.5 85.4 74.6 81.4 79.2</cell></row><row><cell>Breakfast</cell><cell cols="3">F1@{10,25,50}</cell><cell cols="2">Edit Acc</cell></row><row><cell>ED-TCN [15]*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.3</cell></row><row><cell>HTK [14]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.7</cell></row><row><cell>TCFPN [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.0</cell></row><row><cell>HTK(64) [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.3</cell></row><row><cell>GRU [21]*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.6</cell></row><row><cell>MS-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code for our model is publicly available at https:// github.com/yabufarha/ms-tcn.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognition of complex events: Exploiting temporal dynamics between underlying concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2243" to="2250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sharath Pankanti, and Alok Choudhary. Temporal sequence modeling for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2227" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling actions through state changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James M Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2579" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An end-toend generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal CNNs for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The lear submission at THUMOS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with RNN based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suriya</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Speech Synthesis Workshop</title>
		<imprint>
			<publisher>SSW</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">F</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
