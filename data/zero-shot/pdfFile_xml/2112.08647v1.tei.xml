<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QAHOI: Query-Based Anchors for Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Chen</surname></persName>
							<email>chen-j@mm.inf.uec.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">The University of Electro-Communications</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
							<email>yanai@cs.uec.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">The University of Electro-Communications</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">QAHOI: Query-Based Anchors for Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-object interaction (HOI) detection as a downstream of object detection tasks requires localizing pairs of humans and objects and extracting the semantic relationships between humans and objects from an image. Recently, one-stage approaches have become a new trend for this task due to their high efficiency. However, these approaches focus on detecting possible interaction points or filtering human-object pairs, ignoring the variability in the location and size of different objects at spatial scales. To address this problem, we propose a transformer-based method, QAHOI (Query-Based Anchors for Human-Object Interaction detection), which leverages a multi-scale architecture to extract features from different spatial scales and uses query-based anchors to predict all the elements of an HOI instance. We further investigate that a powerful backbone significantly increases accuracy for QAHOI, and QAHOI with a transformer-based backbone outperforms recent state-of-the-art methods by large margins on the HICO-DET benchmark. The source code is available at https://github.com/cjw2021/QAHOI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-object interaction (HOI) detection has recently received increasing attention as a field with great potential applications. HOI detection approaches need to extract the semantic relationships between humans and objects and predict a set of human, object, action triplets within an image. Specifically, an HOI instance is a pair of human and object bounding boxes, and a corresponding action class represents the relationship between them. HOI detection can be seen as a combination of two parts: object detection and human-object interaction recognition. According to the inference process of these two parts, existing HOI detection approaches can be divided into two-stage and one-stage.</p><p>From the beginning to the present, two-stage approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> as an intuitive approach build the method upon the off-the-shelf object detector <ref type="bibr" target="#b24">[25]</ref>. In the first stage, the object detector selects (a) Larger Area (b) Center Distance <ref type="figure" target="#fig_3">Figure 1</ref>. The spatial distribution of the HOI instances in the HICO-DET dataset <ref type="bibr" target="#b2">[3]</ref>. The larger area and center distance indicate the larger area of the human and object bounding box and the distance between the center of the human and object bounding box in an HOI instance, respectively. The bounding boxes are normalized according to the image size. some of high scoring human and object bounding boxes from detection results and extracts appearance features or crops images of the selected bounding boxes. In the second stage, for each of the human-object pairs, the human and object appearance features are used to predict the action class scores separately or fused with supplementary semantic information as a pairwise interaction feature to predict action class scores directly. For the two-stage approaches, the difficulty lies in the integration of semantic information of human-object pairs. With the split human-object pairs, spatial information can be extracted to enhance the appearance features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Furthermore, fine-grained information like human pose can be used as supplementary semantic information <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>, and graph-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> are also well suited to the understanding of complex semantic relationships between humans and objects. Without considering the object detection process, the two-stage approach can design a complex model to fuse abundant semantic information of human-object pairs and achieve high accuracy. However, processing all of the human-object pairs is time-consuming, and the appearance features limited in the bounding box are lack contextual information when the human and the object are far apart. As shown in <ref type="figure" target="#fig_3">Figure 1b</ref>, the spatial distribution of a widely used HOI detection dataset, HICO-DET <ref type="bibr" target="#b2">[3]</ref>, HOI instances with the center distance between the human and object bounding box more than a third of the image size commonly exist.</p><p>To achieve high efficiency, one-stage approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> detect human-object pairs and recognize the corresponding action class in parallel. A commonly adopted way is to make use of the interaction point, which is between the human-object pair <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. A key-point heatmap prediction network such as Hourglass-104 <ref type="bibr" target="#b22">[23]</ref>, or DLA-34 <ref type="bibr" target="#b31">[32]</ref> is applied to extract appearance features at first. Then the interaction points, the human and object offset vectors from the interaction points, the action class located at the interaction point, and the human and object boxes are predicted through a multi-branch network. The interaction points plus the human and object offset vectors indicate where the center points of the human and object are located. To match the human and object boxes, a matching process is required. Although interaction points converge the HOI instance detection and recognition together, there are mainly two drawbacks such as the semantic features are ambiguous when the interaction point is far apart from the human and object, and the lack of a multi-scale architecture which is commonly used in object detection.</p><p>Both the two-stage and one-stage approaches suffer from the problem of poorly extracted semantic features due to the locality of convolution neural networks (CNNs). On the other hand, the transformer <ref type="bibr" target="#b30">[31]</ref> is well utilized in vision tasks such as image classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>, object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>, and semantic segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>. The self-attention mechanism in the transformer shows encouraging potential for the extraction of contextual visual features, which is beneficial for the HOI detection task. To extract the semantic features between the human-object pairs with more contextual information and less irrelevant local information, transformer-based HOI detection methods are proposed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>. As query embeddings in the transformer decoder represent HOI instances and incorporate object detection and interaction recognition together, the transformer-based HOI detection methods also can be seen as query-based methods which belong to the one-stage approach. By introducing the transformer as a powerful feature extractor in the HOI detection task, promising results are observed. However, the transformer-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref> are built upon the CNN backbone, and the multi-head attention used in transformer suffers from a quadratic complexity with the growth of the feature map size. Thus, these transformer-based methods only use the low-resolution feature map from the CNN backbone and leave a burden for the transformer encoder to extract spa-tial semantic information. Besides, the training of the high complexity transformer suffers from slow convergence, and pre-training the model in object detection task and finetuning in HOI detection task are always used to obtain a fine result.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 1a</ref>, most HOI instances consist of boundary boxes of humans and objects with an area less than 0.1? image size. The recent transformer-based onestage approaches lack a multi-scale architecture to detect the HOI instances containing small objects and are hard to train. To address these problems above, we proposed a transformer-based method, which leverages a hierarchical backbone to extract multi-scale visual features, and a deformable transformer <ref type="bibr" target="#b36">[37]</ref> to encode the multi-scale semantic features and decode the HOI instances. The deformable transformer decoder generates the reference points to calculate the multi-scale deformable attention. In our case, the reference points in the deformable transformer decoder act as the anchors for aggregating the multi-scale semantic feature from the deformable transformer encoder to the HOI embeddings. With the anchors, an interaction detection head predicts the HOI instances with the HOI embeddings directly. As the anchors are used throughout the HOI embeddings' decoding and the HOI instances prediction process, we call our method Query-Based Anchors for HOI detection, QAHOI.</p><p>To summarize, our contributions are three-fold:</p><p>? We propose a multi-scale transformer-based method, QAHOI for HOI detection, which leverages querybased anchors to extract the HOI embeddings and predict the HOI instances.</p><p>? We combine a hierarchical backbone with a deformable transformer encoder to build a powerful multi-scale feature extractor, beneficial for the HOI detection task. In addition, we first study and implement the transformer-based backbone on the one-stage HOI detection method and find its great potential for the HOI detection task.</p><p>? By leveraging the multi-scale architecture, the attention mechanism of the transformer in the whole model, and the flexible query-based anchors, our method outperforms recent state-of-the-art methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-stage Approaches. The two-stage approach simplifies the HOI detection to an interaction recognition problem with prepared human-object pairs. The multistream architecture was first introduced in the Human-Object Region-based Convolutional Neural Networks (HO-RCNN) <ref type="bibr" target="#b2">[3]</ref>, which extracts human appearance features, ob- <ref type="figure">Figure 2</ref>. This figure illustrates the overall architecture of the proposed method, QAHOI. QAHOI uses a hierarchical backbone and the deformable transformer encoder to extract the semantic feature in a multi-scale manner. The deformable transformer decoder is used to decode the HOI embeddings according to the HOI query embeddings and anchors derived from the query embeddings. On top of the decoder is an interaction detection head to predict the HOI instance for each anchor with the corresponding HOI embedding. ject appearance features, and spatial semantic features separately. Based on the HO-RCNN, subsequent methods improve the multi-stream architecture by incorporating the advanced feature extracting module <ref type="bibr" target="#b7">[8]</ref> or the graph module <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> and fusing the human pose information <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>One-stage Approaches. The one-stage approach is proposed with creative designs and adopts a two-branch architecture in general. The Union-level Detector (UnionDet) <ref type="bibr" target="#b12">[13]</ref> builds a two-branch architecture to detect the union regions of the human-object pairs and the localization of instances in interactions. The Parallel Point Detection and Matching (PPDM) <ref type="bibr" target="#b16">[17]</ref> defines the interaction point as the midpoint of the human and object center points and matches the human and object instances via interaction points. The Glance and Gaze Network (GGNet) <ref type="bibr" target="#b34">[35]</ref> extends the idea of the interaction point by inferring a set of action-aware points (ActPoints) around each pixel of the feature map. However, the methods using interaction points <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> or union regions <ref type="bibr" target="#b12">[13]</ref> require a matching or gathering process to clarify the HOI instance. On the other hand, the transformer-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref> using the selfattention mechanism of the transformer to extract contextual semantic information and the embeddings to represent the HOI instance become a new trend of the HOI detection task. Tamura et al. <ref type="bibr" target="#b25">[26]</ref> transform the object detection head of the transformer-based object detector DETR <ref type="bibr" target="#b1">[2]</ref> into a simple interaction detection head to predict all of the elements of the HOI instance directly. Similarly, Zou et al. <ref type="bibr" target="#b37">[38]</ref> combine the CNN backbone and the transformer to predict HOI instances directly from the query embeddings. Both Chen et al. <ref type="bibr" target="#b3">[4]</ref> and Kim et al. <ref type="bibr" target="#b13">[14]</ref> propose a transformedbased two-branch architecture, which constructs an instance decoder and an interaction decoder to decode the boxes and action classes of the HOI instances in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our purpose is to address the drawbacks in the recent one-stage approaches that lack a multi-scale architecture and suffers from a poor CNN backbone for the HOI detection task. The deformable DETR <ref type="bibr" target="#b36">[37]</ref> develops the deformable multi-scale attention module to reduce the complexity of attention in DETR to the linear complexity with the spatial size, which achieves a multi-scale transformerbased object detector. Our proposed method, QAHOI, further improves this idea to solve HOI detection as a dense prediction problem. QAHOI adapts the deformable transformer decoder to an HOI instance detector by using the query embeddings to generate anchors and decode the HOI information. The overall architecture of QAHOI is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Scale Feature Extractor</head><p>For the one-stage approaches based on interaction points or composed with the transformer, a CNN backbone such as Hourglass-104 <ref type="bibr" target="#b22">[23]</ref>, DLA-34 <ref type="bibr" target="#b31">[32]</ref>, ResNet-50 and ResNet-101 <ref type="bibr" target="#b10">[11]</ref> is an ordinary setting. However, these methods ignore two factors of using CNN backbones. First, the CNN is poor at capturing non-local semantic features like the relationships between humans and objects. And the way of using the low-resolution feature map with the large receptive field neglects the spatial information on a small scale. The transformer with the attention mechanism is powerful at extracting semantic information from the image. Thus, recent one-stage methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref> normally build a feature extractor consisting of a CNN backbone and a transformer encoder. To improve the model's expression ability, QA-HOI constructs a multi-scale feature extractor by combining a hierarchical backbone and a deformable transformer encoder <ref type="bibr" target="#b36">[37]</ref> as shown in <ref type="figure">Figure 2</ref>. The hierarchical back- bone extracts four stages' feature maps for the deformable transformer encoder, which is well designed for processing multi-scale feature maps. Specifically, given an image of size 3 ? H ? W , QAHOI uses the last three stages' feature maps</p><formula xml:id="formula_0">x 1 ? R 2Cs? H 8 ? W 8 , x 2 ? R 4Cs? H 16 ? W 16 and x 3 ? R 8Cs? H 32 ? W 32 of the backbone. The 1 ? 1 convolution is used to project the feature map x 1 , x 2 and x 3 from di- mension C s to dimension C d .</formula><p>Then, the multi-scale feature maps x 1 , x 2 and x 3 are flattened and concatenated to N S vectors with C d dimensions as the input of the deformable transformer encoder, and the fixed positional encoding indicates the scale level of the input. N S is the sum of pixel numbers of the three feature maps from the backbone. The deformable transformer encoder extracts the semantic feature S ? R N S ?C d in a multi-scale manner and provides it for the deformable transform decoder to decode the HOI instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Predicting HOI with Query-Based Anchors</head><p>According to the deformable DETR, the query embeddings of the deformable transformer decoder in QAHOI are split equally into two parts, one as the HOI query embeddings Q HOI ? R Nq?C d and the other as the positional embeddings Q P os ? R Nq?C d , and the anchors P ? R Nq?2 are generated from positional embeddings Q P os via a linear layer. With the HOI query embeddings and the anchors, the HOI embeddings E ? R Nq?C d are decoded by the deformable transformer decoder's attention mechanism with the source of the encoded semantic feature from the deformable transformer encoder. The decoding process of the deformable transformer decoder is shown in <ref type="figure" target="#fig_0">Figure 3</ref>. The self-attention of the HOI query embeddings are calculated by the multi-head attention module <ref type="bibr" target="#b30">[31]</ref> with the positional embeddings, and the anchors aggregate the semantic feature from the output of the deformable transform encoder to calculate the multi-scale deformable attention <ref type="bibr" target="#b36">[37]</ref> with the HOI query embeddings. Besides, after the calculation of the multi-scale deformable attention, a feed-forward net- work (FFN) composed of linear layers is used to process the output embeddings. The self-attention and the multi-scale attention are calculated in the stacked decode layer for N L times, and the last layer outputs the HOI embeddings for the interaction detection head to predict the HOI instances.</p><p>QAHOI implements a simple interaction head which is similar to the QPIC <ref type="bibr" target="#b25">[26]</ref>, and the difference is that QA-HOI combines each HOI embedding with a certain anchor. Hence, QAHOI feeds the decoded HOI embeddings into the interaction head to predict the HOI instances based on the anchors. <ref type="figure" target="#fig_1">Figure 4</ref> shows the predicting process of the interaction head in QAHOI. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>Because the human and object are predicted in pairs, the matching process, which is important in the method based on interaction points, is not needed. Following the training procedure of the QPIC <ref type="bibr" target="#b25">[26]</ref>, the ground-truth set is padded with ? (no pairs) to the size of N q , and the Hungarian algorithm <ref type="bibr" target="#b14">[15]</ref> is used to match all of the N q predictions with the ground-truth set. For the loss calculated on the matched pairs, the QPIC's loss function is based on the DETR <ref type="bibr" target="#b1">[2]</ref>, and because QAHOI implements the deformable DETR <ref type="bibr" target="#b36">[37]</ref>, we follow the Deformable DETR to calculate the Focal Loss <ref type="bibr" target="#b18">[19]</ref> of the object class, which is different from the QPIC. For the anchors derived from the query  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Top K Scores and HOI NMS</head><p>QAHOI requires sufficient anchors to extract multi-scale features. In general, the number of anchors far exceeds the number of HOI instances in an image. For the HICO-DET dataset, 96% of the images contains less than 10 HOI instances. QAHOI filters the results in two steps. Firstly, the HOI instances with the top N t object class scores are selected. Then, an HOI Non-Maximal Suppression (NMS) is used to filter out the final results. The HOI NMS is calculated based on the IoU of humans and objects between HOI instances and the HOI score. The HOI score is obtained by multiplying the object score and the action score, c HOI = c o ? c a . And a combined IoU of human and object between an HOI instance i and j is calculated as:</p><formula xml:id="formula_1">IoU(i, j) = IoU(B (h) i , B (h) j ) ? IoU(B (o) j , B (o) j ) (1)</formula><p>The same as the object detection task, a threshold ? is used to remove HOI instances with low scores for each action category based on the IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Dataset. We conduct the experiments on the HICO-DET <ref type="bibr" target="#b2">[3]</ref> dataset, which contains 47,776 images <ref type="bibr" target="#b37">(38,</ref><ref type="bibr">118</ref> in the training set and 9,658 in the test set). HICO-DET has 117 action classes and 80 object classes (the object classes same as the MS-COCO <ref type="bibr" target="#b19">[20]</ref> dataset), and the action classes and the object classes constitute 600 HOI classes. Based on the number of instances of the 600 HOI classes in the dataset, these HOI classes are divided into three categories: Full (all of the HOI classes), Rare (138 classes with less than 10 instances), and Non-Rare (462 classes with 10 or more than 10 instances). We report the results (in <ref type="table">Table 1</ref>) on the Default setting (with unknown objects) and the Known Object setting (without unknown objects) of the HICO-DET. Metric. The mean average precious (mAP) is used to evaluate the predicted HOI instances. For a true positive HOI instance, the intersection over union (IoU) between the predicted human bounding box and the ground-truth human bounding box is higher than 0.5, and the IoU between the predicted object and the ground-truth object bounding box is also higher than 0.5. As usual, we report the mAP on the Full, Rare, and Non-Rare categories of the HICO-DET. Implementation Details. For the backbone, we train QA-  HOI with Swin-Transformer <ref type="bibr" target="#b20">[21]</ref> pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as our best model. Specifically, we use Swin-Tiny and Swin-Base pre-trained on ImageNet-1K, and Swin-Base and Swin-Large pre-trained on ImageNet-22K. Following the setting of the Deformable DETR, the deformable transformer encoder and decoder both have 6 layers (N L = 6), the number of the query embeddings is N q = 300, and top N t = 100 HOI instances are selected by object scores. In the NMS process, ? = 0.5 is used to filter the HOI instances by the combined IoU. For the Swin-Tiny, Swin-Base and Swin-Large as the backbone, the first stage's feature map's dimensions are C s = 96, C s = 128 and C s = 192. Following the deformable DETR's setting, the dimension of the embeddings in the deformable transformer is C d = 256. We use the AdamW <ref type="bibr" target="#b21">[22]</ref> optimizer with the backbone's learning rate of 10 ?5 and other's 10 ?4 , and the weight decay of 10 ?4 . We train the model for 150 epochs with a batch size of 16 (two images per GPU, 8 GPUs), and the learning rates of the backbone and others are decayed at 120 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Arts</head><p>The results compared with the state-of-the-art methods on the HICO-DET are shown in <ref type="table">Table 1</ref>. We use QAHOI with the Swin Transformer as our best model to compare with other state-of-the-art methods. Compared with the two-stage approaches, by leveraging the attention mechanism to extract semantic information, QAHOI with Swin-Large exceeds the state-of-the-art graph-based method <ref type="bibr" target="#b32">[33]</ref> by 4.45 mAP (relatively 14.2%). Compared with the onestage approaches, with the multi-scale feature maps and multi-scale deformable attention, even we do not train a detector on the MS-COCO dataset, which is beneficial for the object detection part of the model, QAHOI with Swin-Large backbone still outperforms the state-of-the-art onestage method, QPIC with 5.88 mAP (relatively 19.7%). We found that the better the performance of the pre-trained backbone in the classification task became, the further improvement in accuracy we achieved in the HOI detection. The mAP of QAHOI with Swin-Base backbone pre-trained on ImageNet-20K is 4.1 (relatively 13.9%) higher than the same backbone pre-trained on ImageNet-1K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We conduct ablation studies using CNN-based and Transformer-based backbones. For the CNN-based backbone, we use ResNet-50 and investigate the performance of two training strategies, starting from scratch and fine-tuning the weights of the detector. Training strategies. The same as QPIC, we use the deformable DETR's weight which is trained on the MS-COCO dataset, to initialize QAHOI and then fine-tune QA-HOI on the HICO-DET dataset. Following the deformable DETR's implementation, An additional low-resolution feature map</p><formula xml:id="formula_2">x 4 ? R C d ? H 64 ? W 64</formula><p>is generated by using a 3 ? 3 convolution on the feature map x 3 . The additional feature map x 4 has the dimension C d = 256, which is the same as the embeddings in the deformable transformer. We also train QAHOI and QPIC with ResNet-50 and Swin-Tiny from scratch, respectively. From the results in <ref type="table">Table 2</ref>, without training a detector, (4) QAHOI with ResNet-50 or <ref type="bibr" target="#b6">(7)</ref> Swin-Tiny achieves better results on the Full and Non-Rare categories compared with (1) QPIC with ResNet-50 or (3) Swin-Tiny. Multi-scale feature maps. We use the Swin-Tiny backbone  to investigate the effect of different combinations of feature maps on the accuracy of the proposed method. From the results in <ref type="table">Table 2</ref> <ref type="bibr" target="#b5">(6)</ref>, the additional feature map does not improve the accuracy. For methods (7)(8)(9) of QAHOI, the accuracy decreases with the removal of multi-scale feature maps. Comparing (9) to <ref type="bibr" target="#b6">(7)</ref>, using the feature maps of three stages gives a model accuracy improvement of 1.82 mAP (relatively 6.8%) on the Full category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-based backbone vs Transformer-based backbone.</head><p>The Swin-Tiny has the model size and the computation complexity similar to ResNet-50, but the accuracy on Im-ageNet is higher than ResNet-50. Without training an object detector, compared with the model trained with ResNet-50 in <ref type="table">Table 2</ref>(1)(4), the transformer-based backbone Swin-Tiny improves the accuracy of both (3) QPIC (2.98 mAP, relatively 12.3%) and (7) QAHOI (4.12 mAP, relatively 16.9%), and (7) QAHOI with Swin-Tiny is better than (3) QPIC with Swin-Tiny both of the accuracy and improvement, which means our method has a great potential based on well-designed backbones. The results of QAHOI trained with Swin-Base and Swin-Large in <ref type="table">Table 1</ref> also show that using a backbone with higher accuracy on classification tasks can improve the accuracy of HOI detection significantly. The result of (5) QAHOI fine-tuned from Deformable DETR is lower than (2) QPIC fine-tuned from DETR. One of the reasons is that QPIC uses the DETR with 500 epochs of training, while we use the deformable DETR with only 50 epochs of training. QAHOI would have achieved better results if we have fine-tuned the deformable DETR with more epochs. Top K scores and HOI NMS. The filtering process is important to QAHOI, in <ref type="table">Table 3</ref>, the top K scores step and NMS step improve the accuracy on the Full category by 1.83 mAP. To optimize the top K scores step, we test different kinds of scores and top K numbers. The results in <ref type="table" target="#tab_2">Table 4a</ref> show that using the object score is better than using the action score, and the results of c a and c a ? c o are the same, which means the action score is not sensitive to the number of top K. The best result is obtained under the condition that the top 100 HOI instances of the object score are used as output. And we further test the IoU calculation and threshold of the NMS process. In <ref type="table" target="#tab_2">Table 4b</ref>, the IoU h and IoU o indicate using the human or object bounding boxes between two HOI instances to calculate an IoU, which is similar to the object detection task. From the results of IoU h and IoU o , when using only human or object bounding boxes to filter overlapping HOI instances, the human bounding box can achieve better results. By using the combined IoU, IoU h ? IoU o to represent the degree of overlap of two HOI instances, the best result can be obtained by setting an IoU threshold of ? = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Contribution at Different Spatial Scales</head><p>The multi-scale architecture of QAHOI should be more advantageous for the detection of small targets. In order to investigate the contribution of QAHOI to the detection capability of targets at different spatial scales and compare it with the state-of-the-art transformer-based method, the same as the evaluation method of QPIC, we evaluate both of different center distances and larger areas of HOI instances in different scales. The results are shown in <ref type="figure" target="#fig_5">Figure 5a</ref> and 5b, the ground-truth HOI instances in the test set of HICO-DET is divided into 10 bins, and we also select the bins with more than 1,000 instances to display the AP results.</p><p>While the human and object areas are small, it is difficult to extract the features of the area containing interactive information with a low-resolution feature map. In <ref type="figure" target="#fig_5">Figure 5a</ref>, QAHOI with transformer-based backbones outperform QPIC with ResNet-101 on the detection of small HOI instances in the first three bins. QAHOI with ResNet-50, which is fine-tuned from deformable DETR, outperforms the QPIC with ResNet-101 in the first bin and obtains comparable results on the second and third bins. Besides, QA-HOI with Swin-Large and Swin-Base can also perform well on large instances.</p><p>The shorter the distance between the human and object of an HOI instance, the harder it is to distinguish features from each other. In <ref type="figure" target="#fig_5">Figure 5b</ref>, while the distance between the center of the human and object bounding box is less than 0.3? image size, QAHOI performs better than QPIC. Although the accuracy of both QPIC and QAHOI decreases as the distance between the person and the object increases, a better backbone can alleviate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>The flexibility of Query-Based anchors. The query-based anchors are able to extract features from multi-scale feature maps, which allows anchors to detect HOI instances regardless of their location. As shown in <ref type="figure">Figure 6a</ref> and 6b, two women are riding the elephant, different from the method based on interaction points, the anchor that detects the action with the highest confidence can be far from the center of the human-object pair but close to the human. The <ref type="figure">Figure 6c, 6d</ref> and 6e display the anchors' results with the top 3 action class scores in order. In this scene, the human and the object are far from each other, in 6c the top 1 anchor in the middle of the human-object pair detects the human well but does not capture the object well, in 6d the top 2 anchor is close to the object but far from the human, and it detects the object well but does not capture the human well, in 6e the anchor is far from both the human and object, and it can locate the human-object pair but does not well detect each of them. <ref type="figure">Figure 7a</ref>,7b,7c and 7d further illustrate the distribution of the top 100 anchors for the images with a single HOI instance. The anchors with high object scores are these located close to the center of the human and object, however, the anchors with high confidence of the action class are not limited to the center of the human and object, such as the man holding the tennis racket in 7a and 7c. The quantitative results above show that the query-based anchor is a powerful representation for HOI instances which is flexible in localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we propose a transformer-based one-stage method for HOI detection, which leverages a hierarchical backbone and transformer encoder to extract the multi-scale semantic feature, a transformer decoder to decode the HOI embeddings and an interaction head to predict the HOI instances. The transformer decoder and the interaction head leverage the query-based anchors to decode the HOI embeddings and predict the HOI instances. Transformer-based backbones with the attention mechanism show a great advance for HOI detection, and the query-based anchors are also flexible in detecting the HOI instances.</p><p>Because our method has a multi-scale architecture and leverages the anchors to detect HOI instances like object detection, there are several improvements that can be added, such as the Feature Pyramid Networks (FPN) <ref type="bibr" target="#b17">[18]</ref> can be added to enhance the multi-scale features, and the anchors' predictions can be used as the HOI proposals and refining the predictions in a two-stage manner like the two-stage deformable DETR. Furthermore, without training a detector, our method can train a large model from scratch and achieve a state-of-the-art result. We hope our method based on query-based anchors can be further developed with the techniques that are used in modern object detectors and used as a strong baseline for the HOI detection task in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>The decoding process of the deformable transformer decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The interaction head predicts the HOI instances based on the anchors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Following the deformable DETR, each anchor (p x , p y ) of the anchor set P ? R Nq?2 acts as the base point for the bounding boxes of a pair of a human and an object. Thus, the human and object boxes B h , B o ? R Nq?4 predicted by the FFN in the interaction head are composed of {d x , d y , w, h}, where d x and d y denote the offsets between the anchor and the box's center, and w and h denote the box's width and height. Then, the final bounding boxesB h ,B o are composed of {d x +p x , d y +p y , w, h}. Finally, the object class of the object boxes O ? R Nq?Ko and the action class of the HOI instances A ? R Nq?Ka are combined with the human and object bounding boxesB h , B o to construct the output HOI instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art on HICO-DET. The features of 'A', 'S', 'P' and 'L' represent the appearance feature, spatial feature, human pose feature, and language feature, respectively. For two-stage approaches, using fine-tuned detection means using a detector<ref type="bibr" target="#b24">[25]</ref> first trained on the MS-COCO dataset, and then fine-tuned on the HICO-DET dataset. For one-stage approaches, using finetuned detection means initializing the weights of the detection part from a model pre-trained on the MS-COCO dataset and fine-tuning the whole network on the HICO-DET dataset. The Swin-Base and Swin-Large backbone with the * and + are pre-trained on ImageNet-22K with 384 ? 384 input resolution. embeddings, because the query embeddings are learnable parameters, the positions of the anchors are learned during training and fixed during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) AP results on different large areas.(b) AP results on different center distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Evaluations on different spatial scales of HOI instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>The flexibility of the anchors. The blue and gray points represent the selected anchors with object class scores of the top 100 and the unselected anchors, and green points represent the anchors with the highest action class scores for each detected HOI instance. The distribution of the anchors with top 100 object class scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Evaluations of the training strategies and the effect of multi-scale feature maps and transformer-based backbone. Ablation study of the filtering steps. QAHOI with Swin-Tiny is used as the base method. 26.63 26.63 26.63 c o 26.69 26.70 26.64 c a ? c o 26.63 26.63 26.63 (a) Ablation study of the top K scores. Three kinds of top k scores with three top k numbers are tested without the NMS process. IoU o 28.41 28.47 28.37 28.07 (b) Ablation study of the NMS method. Three kinds of IoU calculation methods with four thresholds are tested, the top k score co is used, and Nt = 100.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Fine-tuned</cell><cell></cell><cell></cell><cell></cell><cell>Default</cell><cell></cell><cell></cell><cell>N t</cell></row><row><cell>Arch.</cell><cell cols="2">Method Backbone</cell><cell>Detection</cell><cell cols="2">Multi-scale</cell><cell>Full</cell><cell cols="2">Rare Non-Rare</cell><cell>topk scores</cell><cell>50</cell><cell>100</cell><cell>150</cell></row><row><cell>QPIC</cell><cell>(1) (2)</cell><cell>ResNet-50 ResNet-50</cell><cell></cell><cell>x 3 x 3</cell><cell></cell><cell cols="2">24.21 17.51 29.07 21.85</cell><cell>26.21 31.23</cell><cell>c a</cell></row><row><cell></cell><cell>(3)</cell><cell>Swin-Tiny</cell><cell></cell><cell>x 3</cell><cell></cell><cell cols="2">27.19 21.32</cell><cell>28.95</cell><cell></cell></row><row><cell></cell><cell>(4)</cell><cell>ResNet-50</cell><cell></cell><cell cols="4">x 1 , x 2 , x 3 , x 4 24.35 16.18</cell><cell>26.80</cell><cell></cell></row><row><cell></cell><cell>(5)</cell><cell>ResNet-50</cell><cell></cell><cell cols="4">x 1 , x 2 , x 3 , x 4 26.18 18.06</cell><cell>28.61</cell><cell></cell></row><row><cell>QAHOI</cell><cell>(6) (7)</cell><cell>Swin-Tiny Swin-Tiny</cell><cell></cell><cell cols="4">x 1 , x 2 , x 3 , x 4 28.09 21.65 x 1 , x 2 , x 3 28.47 22.44</cell><cell>30.01 30.27</cell><cell></cell></row><row><cell></cell><cell>(8)</cell><cell>Swin-Tiny</cell><cell></cell><cell cols="2">x 2 , x 3</cell><cell cols="2">28.12 20.43</cell><cell>30.41</cell><cell></cell></row><row><cell></cell><cell>(9)</cell><cell>Swin-Tiny</cell><cell></cell><cell>x 3</cell><cell></cell><cell cols="2">26.65 19.13</cell><cell>28.89</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IoU threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IoU</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IoU h</cell><cell>27.85 27.93 27.96 27.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IoU o</cell><cell>26.69 26.77 26.84 26.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IoU h ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Default</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>method</cell><cell></cell><cell>Full</cell><cell cols="3">Rare Non-Rare</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>base</cell><cell></cell><cell cols="2">26.64 20.62</cell><cell>28.44</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">+ topk scores (N t = 100) 26.70 20.89</cell><cell>28.43</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">+ NMS (? = 0.5)</cell><cell cols="2">28.47 22.44</cell><cell>30.27</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies of top K scores and NMS methods. QAHOI with Swin-Tiny is used to conduct results on the Full category of the HICO-DET dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reformulating hoi detection as adaptive set prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DRG: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">iCAN: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Piotr Doll?r, and Kaiming He</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">UnionDet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HOTR: End-to-end human-object interaction detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eun-Sol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PPDM: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">QPIC: Query-based pairwise human-object interaction detection with image-wide contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoaki</forename><surname>Yoshinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">VSGNet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatially conditioned graphs for detecting human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Frederic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for robust human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glance and Gaze: Inferring action-aware points for onestage human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Changxing Ding, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
