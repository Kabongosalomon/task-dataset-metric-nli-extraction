<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context Matters: Self-Attention for Sign Language Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Fares</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Quebec at Montreal Montreal</orgName>
								<address>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slimane</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Quebec at Montreal Montreal</orgName>
								<address>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bouguessa</surname></persName>
							<email>bouguessa.mohamed@uqam.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Quebec at Montreal Montreal</orgName>
								<address>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context Matters: Self-Attention for Sign Language Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes an attentional network for the task of Continuous Sign Language Recognition. The proposed approach exploits co-independent streams of data to model the sign language modalities. These different channels of information can share a complex temporal structure between each other. For that reason, we apply attention to synchronize and help capture entangled dependencies between the different sign language components. Even though Sign Language is multi-channel, handshapes represent the central entities in sign interpretation. Seeing handshapes in their correct context defines the meaning of a sign. Taking that into account, we utilize the attention mechanism to efficiently aggregate the hand features with their appropriate spatio-temporal context for better sign recognition. We found that by doing so the model is able to identify the essential Sign Language components that revolve around the dominant hand and the face areas. We test our model on the benchmark dataset RWTH-PHOENIX-Weather 2014, yielding competitive results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sign languages are often defined as manual languages. However, besides the hand articulations, non-manual components like facial expressions, arm, head, body movements and positions play a crucial part in Sign Languages <ref type="bibr" target="#b0">[1]</ref>. Any change in one of these components can alter the meaning of a sign. Usually, the handshape performed by the dominant hand carries most of the meaning of the sign <ref type="bibr" target="#b1">[2]</ref>. As for the other components, they provide a semantic context to it. For example, the signs for "man" and "woman" are very close as they both use the same handshape and body movement, although, as mentioned in <ref type="bibr" target="#b2">[3]</ref>, male words tend to be signed close to the forehead while female signs are usually performed around the chin. Similarly, for the signs "sit" and "chair", they both have the same handshape but slightly different hand movements, in which "chair" is the sign "sit" but twice <ref type="bibr" target="#b3">[4]</ref>. Facial expressions can also greatly affect the meaning of a handshape, and are mostly used to ask questions or express emotion. For instance, when signing a "you" sign, by raising the eyebrows, it becomes a question: "Is it you?" or by using a different facial expression it turns into "It's you?!?!" meaning that "I am surprised that it is you" <ref type="bibr" target="#b4">[5]</ref>. These examples highlight the idea that interpreting a sign often requires recognizing the handshape accompanied by its contextual information. This context is not limited to the spatial information that resides around the handshape for a fixed point in time, but also the temporal information that consists of hand and body movements.</p><p>Much of the current literature in Continuous Sign Language Recognition (CSLR) ignores the notion of incorporating contextual information to the handshape. Instead, they either exclusively use the handshape features from the cropped hand images <ref type="bibr" target="#b5">[6]</ref>, or simply use global features by trying to learn a complete spatial representation of full-body images <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Nevertheless, there have been some works that investigated the effect of combining information of the handshape with the other Sign Language modalities. For instance, <ref type="bibr" target="#b8">[9]</ref> use a two-stream 3D-CNN to extract global (full-body) and local (hand) features and then fuse the representations in the last fully connected layers. Another work <ref type="bibr" target="#b9">[10]</ref> involves the use of specialized sub-networks (SubUNets) to learn fullframe and handshape information and then to synchronize the modalities through a recurrent BLSTM layer. The authors found that combining modalities in this manner outperforms the use of isolated SubUNets. However, it is important to note that sign language modalities mostly share complex nonlinear relations. Because of that, these approaches may fail to successfully capture and aggregate the required handshape dependencies. This is especially true when using recurrent networks, as they are more biased by close contexts, only considering short-term dependencies rather than the global long-term dependencies <ref type="bibr" target="#b10">[11]</ref>. In this paper, we propose a superior method that efficiently combines handshape features with their appropriate global context, and which yields better recognition results than the latter studies.</p><p>Virtually all progress in CSLR has operated under the assumption that this is a gesture recognition task and that the main challenge is to simply learn the mapping of signs to their respective glosses. The authors in <ref type="bibr" target="#b11">[12]</ref> argue that this is not the case, as Sign Language has its own linguistic and grammatical properties, and that we should take into account the word orders and grammar. That being the case, they introduce a new problem setup: Sign Language Translation (SLT) 1 , in which they approach it as a machine translation task, generating spoken language translations from sign language <ref type="bibr" target="#b0">1</ref> We draw the attention of the reader that in this study we tackle the specific problem of Continuous Sign Language Recognition (CSLR). The task of Sign Language Translation (SLT) is beyond the scope of this paper. videos. And they, accordingly, employ an encoder-decoder RNN with attention which is commonly used for machine translation tasks. Alternatively, it has been shown that using an attention network by itself can also be efficient in machine translation. The Transformer Network from <ref type="bibr" target="#b12">[13]</ref>, which is based solely on attention, has been proven to be very effective in machine translation tasks. Just recently, Camgoz et al. <ref type="bibr" target="#b13">[14]</ref> proposed a transformer-based encoder-decoder architecture for the task of SLT which significantly exceeds the previous stateof-the-art performance from <ref type="bibr" target="#b11">[12]</ref> and sets a new baseline result for SLT. This proves the efficiency of such architecture for capturing temporal dependencies.</p><p>Unlike spoken languages, sign language is undoubtedly multi-channel. Information is carried through handshapes, body movements, and facial expressions. Contrary to <ref type="bibr" target="#b13">[14]</ref> and instead of relying on a unique channel of information from full-frame representations, we exploit the attention mechanism to effectively combine different modality information. When signing, the handshape usually depends on some relevant contexts, rather than all context information. As a result, the transformer network is a more suitable choice for our problem setup, as it can efficiently combine hand features with their appropriate full-body information, since it is explicitly built to detect important dependencies, as opposed to their recurrent counterpart.</p><p>Accordingly, in this paper, we propose an attention-based approach for sequence to sequence sign language alignment and recognition. Unlike previous works, the originality of our approach lies in explicitly picking up and aggregating contextual information from the non-manual sign language components. Without any domain annotation, our approach is able to exclusively identify the most relevant features associated with the handshape when predicting a sign. The main contributions of this paper can be summarized as follows:</p><p>? Devising an end-to-end framework for sequence to sequence Sign Language Recognition that utilizes selfattention for temporal modeling. ? Elaborating a more efficient method to incorporate handshapes with their spatiotemporal context for Sign Language Recognition. ? Achieving competitive results, in terms of Word Error Rate, on the RWTH-PHOENIX-Weather 2014 benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Most work in CSLR can be mainly divided into two stages: the extraction of spatial representations from the input images and then learning the correspondence between the representations' sequence and the target sequence. There exists a fair amount of literature in the field that utilizes handcrafted feature representations <ref type="bibr" target="#b14">[15]</ref> and classical models like HMMs for sequence learning <ref type="bibr" target="#b5">[6]</ref>. However, due to the revolutionary advances in deep learning, recent research has started to shift interest to using CNNs <ref type="bibr" target="#b5">[6]</ref>, and 3D-CNNs <ref type="bibr" target="#b8">[9]</ref> as feature extractors, and recurrent networks for temporal modelling <ref type="bibr" target="#b9">[10]</ref>. For instance, the authors in <ref type="bibr" target="#b6">[7]</ref> propose a CNN-RNN-HMM architecture for the task of CSLR. On top of their CNN-RNN-HMM model, <ref type="bibr" target="#b6">[7]</ref> employ an iterative forced alignment algorithm that refines the label-to-image alignment and that broadly outperforms the previous work by a large margin. Instead of manual labelling, forced alignment is able to provide useful training targets, which are re-fined through iterative alignment. The model will be trained on these new labels and re-align them for the next iteration. The training process is repeated until the resulting model stops getting better recognition. A similar work <ref type="bibr" target="#b15">[16]</ref> employ the same forced alignment technique to maximize the target likelihood estimation. The authors adopt a multi-stream HMM approach to synchronize and learn from parallel and different sub-problems (sign language, mouth shape, and hand shape recognition). Respectively, using a parallel alignment approach produces better recognition performance than <ref type="bibr" target="#b6">[7]</ref> which follows a similar CNN-LSTM-HMM setup but uses a single stream input.</p><p>As a rule, CSLR is considered a weakly supervised problem since gloss annotations usually lack the exact temporal location <ref type="bibr" target="#b16">[17]</ref>. Besides HMMs, a common strategy used to solve this problem is the use of Connectionist Temporal Classification (CTC) loss function <ref type="bibr" target="#b17">[18]</ref>, since it considers all possible alignments between input and output sequences. CTC has been successfully applied for CSLR <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref> and also for various other sequence to sequence problems including speech recognition <ref type="bibr" target="#b18">[19]</ref> and handwriting recognition <ref type="bibr" target="#b19">[20]</ref>. Nonetheless, the CTC algorithm is found to be incompatible for machine translation since it only allows monotonic alignment between source and target sequences <ref type="bibr" target="#b20">[21]</ref>. Instead, the encoder-decoder architecture is usually applied for machine translation. As described in <ref type="bibr" target="#b21">[22]</ref>, the encoder-decoder network consists of learning a latent representation that maps the input sequence to another target sequence. However, this usually results in having a bottleneck state that fails to capture longrange dependencies, especially when dealing with long input sequences. Giving a network an attention mechanism is widely considered to be a good way to solve this problem, as it allows the decoder to focus on relevant hidden states, and avoids squeezing all input information into the last hidden state <ref type="bibr" target="#b22">[23]</ref>. The authors in <ref type="bibr" target="#b11">[12]</ref> adopt this strategy for Sign Language Translation. They found that having an attention mechanism <ref type="bibr" target="#b22">[23]</ref> exceptionally improved the translation performance.</p><p>Recent machine translation research <ref type="bibr" target="#b12">[13]</ref> achieved state of the art performance by merely using self-attention, entirely replacing recurrent models. This attention mechanism has been successfully used in a variety of other tasks like video captioning <ref type="bibr" target="#b23">[24]</ref> in which the authors use an encoder-decoder framework with the transformer network backbone to generate a text description of a given video. Another interesting work in the field of reading comprehension <ref type="bibr" target="#b24">[25]</ref>, utilizes the selfattention mechanism to compute the similarities between a pair of context and query words.</p><p>A separate study in action recognition <ref type="bibr" target="#b25">[26]</ref> proposes a transformer-based model. They argue that human actions are recognizable from the state of the environment, apart from their own pose. For that, they use self-attention to aggregate features from the spatio-temporal context around the person to correctly classify a person's actions. This inspires us to exploit, in this paper, self-attention to incorporate handshape features with their spatio-temporal dependencies. Doing so would add context from the global information to the handshape and ultimately contribute to improving sign recognition. To the best of our knowledge, no previous study has considered doing this for Sign Language Recognition.</p><p>The field of Continuous Sign Language Recognition only really took off in recent years, as labeled datasets became more publicly accessible for researchers. For instance, RWTH-PHOENIX-Weather 2014 <ref type="bibr" target="#b26">[27]</ref> that quickly became a popular baseline for CSLR. The dataset offers sign language gloss annotation for a German weather forecast. In this paper, we demonstrate the effectiveness of our approach through the use of the latter dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>In this section, we describe the approaches that we have devised for the task of Continuous Sign Language Recognition (CSLR). First, in Section 3.B, we introduce our proposed approach which we refer to as Sign Attention Network (SAN). The considered model is based on the Transformer architecture that we briefly describe in Section 3.A. Our model first extracts spatial features from the sign clip frames using 2D CNNs. Then, it employs self-attention for temporal modeling. Finally, the proposed model learns sequence alignment through a CTC layer. In Section 3.C, we add a secondary stream for the cropped handshape sequences and we combine the hand features with their spatio-temporal full-body context. In Section 3.D, instead of considering the entire context information, we merely attend to information from the handshape local surroundings. This will allow the model to only focus on the required context, discarding unnecessary distant information. We explain the motivation behind such an approach and demonstrate its effectiveness in the experimentation section through quantitative and qualitative findings. Implementation and Training details are given in Sections 3.E and 3.F respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transformer Network</head><p>This architecture was firstly proposed in <ref type="bibr" target="#b12">[13]</ref> as an alternative for the traditional recurrent models. It relies on self-attention to compute sequence representation, entirely repealing recurrence and convolutional operations, in which it helped reduce computational cost. Instead of performing attention on all the feature space, features are first linearly projected to query Q, key K, and value V embeddings with lower dimensionality, h times. According to the authors <ref type="bibr" target="#b12">[13]</ref>, this allows the model to attend to information from different representation subspaces. The architecture then uses the scaled dot product (see Equation 1) to calculate the attention scores (similarity) of each feature with the rest of the features in the sequence by multiplying Q with K. The output is computed as the weighted sum of values V conditionally to the attention weights.</p><formula xml:id="formula_0">Attention(Q, K, V ) = sof tmax QK T ? d k V<label>(1)</label></formula><p>Moving away from recurrent and convolutional models, we end up losing positional information. Accordingly, to preserve the sequence order, the authors add positional encodings to the feature representations through the use of a sinusoidwave-based function. A detailed explanation of the overall architecture can be found in the original paper <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sequence Alignment with CTC</head><p>We propose an attention-based model for Sign Recognition. Our model is designed to accept a clip as input and accordingly produce the output words in a spoken language, making it a sequence to sequence task. Subsequently, our model is trained to recognize and time-align the target glosses from the sign language clip.</p><p>In recognition tasks, framewise aligning each timestep in the input sequence X = {x 1 , x 2 , ..x T } with its corresponding ground-truth annotation can be time-consuming and computationally expensive to train. This is especially true in the case of CSLR as the input sequence's length can be much more superior than that of the target sequence. Connectionist Temporal Classification (CTC) proposed by <ref type="bibr" target="#b17">[18]</ref>, is a popular solution to overcome such a problem. The CTC considers all possible alignment in each timestep. It introduces an extended target vocabulary L = L ? { } where represents the blank label that accounts for silence and which is to be removed from the output while decoding.</p><p>The CTC objective loss function is defined as L CT C = ? log P (Y |X) where P (Y |X) is the conditional probability and can be described as:</p><formula xml:id="formula_1">P (Y |X) = a?L 1?t?T P t (a t |X)<label>(2)</label></formula><p>Computing the score for each alignment can be computationally intensive; to that end, CTC uses a dynamic programming algorithm to efficiently and promptly compute L CT C <ref type="bibr" target="#b20">[21]</ref>.</p><p>Recurrent networks are usually used to estimate posteriors P t (a t |X) for each timestep t 1?t?T . However, RNNs normally fail to capture global dependencies, especially when dealing with long sequences. Therefore, for sequence to sequence modeling, we replace traditional recurrent methods with selfattention, as they have proven to be more resilient to the vanishing gradient problem. Similarly, as used in the encoder side of the Transformer network, we use stacked self-attention layers to learn temporal dependencies and map the input frame features viewed as X ? R T xD to another sequence of equal length, in which D represents the feature dimensionality. In summary, our model produces image representations through a 2D CNN on the clip individual frames of a given sign clip. The resulting matrix is passed to succeeding Ax Attention <ref type="figure">Fig. 1</ref>. Overview of our Sign Attention Network that takes a sequence of full-frame images and outputs the target word glosses. The Ax Unit represents the Attention stack as introduced in <ref type="bibr" target="#b12">[13]</ref>, which is composed of a multi-head self-attention mechanism followed by a fully connected layer. We apply a layer Norm <ref type="bibr" target="#b27">[28]</ref> and then a residual connection for each as opposed to the original Transformer paper. units followed by a linear projection and a Softmax to output the word probabilities and finally a CTC layer in order to generate the gloss words, as highlighted in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Context-Hand Attention Layer</head><p>Although our model may be able to learn all sign language modalities simply from the full-frame sequences, it would be of special interest to investigate the impact of fusing hand and global features. Handshapes require spatio-temporal information; as a result, each hand feature needs to attend to context across time and not just the current context frame. Respectively, we design two-stream sub-networks: a Context Stream that is trained on the full-frame sequences and a Hand Stream which is trained on the cropped hand images. This will allow the first to learn to recognize global information and as a result the overall context of the signs. The second will be equipped to merely learn the handshape information. We combine the two through a third module that we refer to as a Context-Hand Attention Layer, in which the key and value features of shape (T xd k ) are computed as linear projections of the full-frame sequence representation, while the Query features Q hand of shape (T xd k ) are obtained through the hand sequence projections. We apply the dotproduct to get the attention values of the Q hand features over the K context features and then a weighted averaging of the resulting matrix over the V context features to get the updated hand representations. We apply a Layer-Norm operation on the hand query and then add it to the original hand features. The resulting feature is passed to another Norm Layer and a 2 layer Feed-Forward Network, to eventually produce the final hand query. The updated hand features are passed to a linear layer followed by a Softmax and then to a CTC to generate the word glosses. Similarly to <ref type="bibr" target="#b9">[10]</ref>, the overall network is trained end-to-end using the three loss layers: L hand , L context and L combine . The described process and the overall architecture is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Relative Local Context Masking</head><p>Typically, self-attention allows the model to observe the entire sequence when measuring the attention at each sequence step. This can be very helpful for recognition, as Sign Language possesses a complex temporal structure and having this type of attention can help capture entangled dependencies. However, while handshapes expect temporal context, it is unclear to what extent this context may span. Required information can be limited to a local temporal window. So, instead of taking information from the entire sequence, we set a relative window and we only attend to the context around the current handshape frame. A similar idea has been explored for Acoustic Models <ref type="bibr" target="#b28">[29]</ref>. This can be especially beneficial when dealing with long sequences where early handshapes don't usually require information from distant context frames. We set a relative window of size r and we apply a mask M rel ? R T xT to the attention scores to mask out unwanted values in which:</p><formula xml:id="formula_2">M rel = 1 if |j ? k| &lt; r 0 else</formula><p>Having attention restricted to a local region may result in missing some required long-range dependencies. Therefore, we avoid using such a mechanism on self-attention layers in both the context and hand streams as they are responsible for learning global information from the overall sequences. And we solely use it in the Context-Hand attention layer to incorporate handshapes to their local related contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>We start by extracting T frames (mostly 64) from the original video clip, in order to reduce computational complexity and discard frame redundancy. We resize the input full-frame images to a spatial resolution of 224 x 224. We also resize the hand patches to a lower resolution of 112 x 112. We normalize both full-frame and hand images by subtracting the dataset's image mean. For spatial image embeddings, we utilize the MobileNetV2 <ref type="bibr" target="#b29">[30]</ref> architecture, due to its low-latency and high efficiency as a feature extractor. We drop its last fully connected layers and use the rest of the layers for feature embeddings. All our networks use a feature dimensionality of d k = 128, 10 attention heads, and 2 layers of self-attention. The rest follow the default recommended setup provided by <ref type="bibr" target="#b12">[13]</ref>: A 2 layer position-wise feed-forward network with a dimensionality of d f f = 2, 048 and a positional encoding sinusoidal function that encodes relative and absolute information. We based our networks' implementations on <ref type="bibr" target="#b30">[31]</ref> and they are made publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Training Details</head><p>We initialize all of our networks' layers using Xavier <ref type="bibr" target="#b31">[32]</ref>, except for the image embedding layers which have been pretrained on ImageNet <ref type="bibr" target="#b32">[33]</ref>. We use the Adam <ref type="bibr" target="#b33">[34]</ref> optimization method with its default parameters and a learning rate of 10 ?4 . We also use gradient clipping with a threshold of 1. We avoid overfitting by employing data augmentation through random x-y translation and using a dropout probability of 0.3. We train all our networks using a small batch size of 2. Since we are performing batch training and considering that input clips may vary in size, sequences in every batch are padded to equal lengths (maximum sequence length in the batch). As a result, we utilize a mask on the input sequences to avoid attending on padded elements. In the case where we use relative local masking, we merge both the padding mask with M rel . So that, besides the distant elements, we also avoid looking at the padded elements. We train our networks for 100 epochs or until train perplexity convergence. We evaluate our model every epoch on the validation set and report the best performing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate our models on the task of Continuous Sign Language Recognition and we compare our findings with state-of-the-art previous works on the RWTH-PHOENIX-Weather 2014 corpus. The dataset offers sign clips 2 https://github.com/faresbs/slrt (sequence of frames) and their corresponding gloss annotation. It contains 9 different signers and a train set of 5,672 sequences, thereby helping with model generalization. The textual annotation consists of a vocabulary L of 1,231, with only 410 singletons (words that appear only once in the training set). The dataset provides full-frames alongside the cropped dominant hand (right hand), which provide a favourable test bed for our approach. Accordingly, we empirically demonstrate the effectiveness of combining the handshape features with their proper context using our proposed method, and we show that it significantly improves recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Results</head><p>As shown in <ref type="table" target="#tab_0">Table I</ref>, we start by comparing our approaches' results for the RWTH-PHOENIX-Weather 2014 dataset on both the validation and test sets and using the Word Error Rate:</p><p>W ER = #deletions + #insertions + #substitutions #number of reference observations</p><p>Instead of using a greedy approach for decoding, in which we simply take the word with the highest probability, we consider using a CTC beam decoder with a beam width of 10 to get our final output sequence in both training and evaluation. We found that having a higher beam value did not improve performance and rather tremendously increased decoding time. Taking this into account, we use the same beam search strategy for all our experiments.</p><p>First, we study the effect of incorporating handshape features with full-frame context information by adding a hand stream module and a Context-Hand attention layer. This leads to a considerable boost in performance as shown in <ref type="table" target="#tab_0">Table I</ref>.  But more importantly and as manifested in <ref type="figure" target="#fig_1">Figure 3</ref>, adding handshape features also improves training and accelerates model convergence. This empirically showcases the usefulness of combining the dominant hand with the overall context derived from the nonmanual components of the sign. Note that both hand and full-frame sequences have equal lengths (T = T ). Next and as demonstrated in <ref type="table" target="#tab_0">Table I</ref>, limiting attention and fusing handshapes with their local related context leads to a significant improvement in performance, surpassing all our previous approaches. This supports the idea that the handshape, when accompanied by its proper context, can notably help in recognition.</p><p>In view of the significant complexity of our SAN architecture and the training dataset being too limited, our proposed model may not be able to generalize well for the task in hand and can as a result perform badly in the evaluation set. One way to tackle this problem is to provide a better initialization scheme for our model by firstly training the spatial feature extractor (CNN) on the same dataset. The CNN module will be at first initialized on ImageNet and then trained on the RWTH-PHOENIX-Weather 2014 dataset. As can be seen in <ref type="table" target="#tab_0">Table II</ref>, by pre-training the CNN module of our SAN model on the same dataset, we significantly outperform the equivalent model when merely pre-trained on ImageNet. <ref type="table">Table 3</ref> shows a comparison of our best performing model with previous publications. For a fair comparison, we only compare with competing methods that are trained on ground truth target glosses and we don't account for approaches that rely on forced alignment to train on per-frame labels. Accordingly, we opted to compare in this study with methods that employ similar training configurations, in order to showcase the efficiency of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Analysis</head><p>Apart from obtaining promising results, it is of interest to visualize the areas on which the model focuses for sign prediction. Accordingly, we use Grad-Cam <ref type="bibr" target="#b35">[36]</ref> to produce a localization heatmap in which bright pixels represent positive influence and have great importance on the predicted sign. As can be seen in <ref type="figure">Figure 4</ref>, our models primarily focus on the dominant hand (right hand) and the face area which reinforces the intuition that our model is able to identify the essential components for sign interpretations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this work, we have proposed a novel method that exploits attention to effectively combines hand query features with their respective temporal full-body context without the need for any additional supervision. We have proven the efficiency of such an approach to the task of Continuous Sign Language Recognition. For future studies, it will be of interest to investigate the effect of using a forced alignment algorithm on top of our architecture, similarly to <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Relying on forced alignment may significantly improve recognition as shown in <ref type="bibr" target="#b6">[7]</ref> and it is a popular solution to overcome weak supervision by iteratively refining and training on labelto-image prediction. We can also employ HMMs instead of CTC for sequence alignment, as they have been proven to be superior in <ref type="bibr" target="#b9">[10]</ref>. Another important venue to explore is to furthermore extend this work by applying our architecture for the task of Sign Language Translation (SLT) similarly to <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b13">[14]</ref> and investigating the effect of combining hand features with their global non-manual context through the use of the attention mechanism. <ref type="figure">Fig. 4</ref>. Heatmap Localization on the frame embedding activations, highlighting important regions which the model uses to predict a particular sign. The top sequence is the output results of our SAN network. The middle is for SAN with the hand stream and the bottom is for SAN with hand stream and the local context masking. Note that this example is randomly chosen and not cherry-picked.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Combination of both the full-frame and the handshape streams through a Context-Hand Attention layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The Word Error Rate learning curve of our SAN variants for the task of CSLR on the RWTH-PHOENIX-Weather 2014 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF OUR SIGN ATTENTION NETWORK VARIANTS ON RWTH-PHOENIX-WEATHER 2014 IN WORD ERROR RATE % (THE LOWER THE BETTER).</figDesc><table><row><cell></cell><cell>Dev</cell><cell>Test</cell></row><row><cell>SAN</cell><cell>35.33</cell><cell>35.45</cell></row><row><cell>+ Hand Stream</cell><cell>33.68</cell><cell>34.12</cell></row><row><cell>+ Relative Local Masking</cell><cell>32.74</cell><cell>33.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II SAN</head><label>II</label><figDesc>RECOGNITION PERFORMANCE IN WORD ERROR RATE WHEN PRE-TRAINING ITS CNN MODULE ON IMAGENET AND RWTH-PHOENIX-WEATHER 2014 DATASET % (THE LOWER THE BETTER).</figDesc><table><row><cell>Pre-Training</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>ImageNet</cell><cell>32.74</cell><cell>33.29</cell></row><row><cell>RWTH-PHOENIX-Weather 2014</cell><cell>29.02</cell><cell>29.78</cell></row><row><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="3">COMPARATIVE RESULTS ON THE RWTH-PHOENIX-WEATHER 2014</cell></row><row><cell cols="3">DATASET IN WORD ERROR RATE % (THE LOWER THE BETTER).</cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell></row><row><cell>SAN</cell><cell>29</cell><cell>29.7</cell></row><row><cell>Koller et al. (CNN-2BLSTM) [7]</cell><cell>32.7</cell><cell>32.9</cell></row><row><cell>Koller et al. (CNN) [7]</cell><cell>33.7</cell><cell>33.3</cell></row><row><cell>Huang et al. [9]</cell><cell>-</cell><cell>38.3</cell></row><row><cell>Cui et al. [17]</cell><cell>39.4</cell><cell>38.7</cell></row><row><cell>Koller et al. [6]</cell><cell>38.3</cell><cell>38.8</cell></row><row><cell>Camgoz et al. (HMM-LM) [10]</cell><cell>40.8</cell><cell>40.7</cell></row><row><cell>Camgoz et al. (CTC) [10]</cell><cell>43.1</cell><cell>42.1</cell></row><row><cell>Koller et al. [35]</cell><cell>47.1</cell><cell>45.1</cell></row><row><cell>Koller et al. [27]</cell><cell>57.3</cell><cell>55.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors gratefully thank the reviewers and the associate editor for their valuable comments and important suggestions. This work is supported by research grants from the Natural Sciences and Engineering Research Council of Canada (NSERC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonmanual structures in sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Crasborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Language and Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recovering the linguistic components of the manual signs in american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="447" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">American sign language -more than just hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weronika</forename><surname>Lass</surname></persName>
		</author>
		<ptr target="https://www.omniglot.com/language/articles/morethanjusthands.htm" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Linguistics of American sign language: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Gallaudet University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">American sign language: Nonmanual markers (NMMs)</title>
		<ptr target="http://www.lifeprint.com/asl101/pages-layout/nonmanualmarkers.htm" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep sign: hybrid CNN-HMM for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent cnn-hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3416" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep dynamic neural networks for multimodal gesture segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1583" to="1597" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video-based sign language recognition without temporal segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SubUNets: End-to-end hand shape and continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7784" to="7793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sign language transformers: Joint end-to-end sign language recognition and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A multi-scale boosted detector for efficient and robust gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="491" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly supervised learning with multi-stream CNN-LSTM-HMMs to discover sequential parallelism in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="4" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for continuous sign language recognition by staged optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1610" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sequence modeling with CTC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<ptr target="https://distill.pub/2017/ctc" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TVT: Two-view transformer network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="847" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and accurate reading comprehension by combining self-attention and convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B14TlG-RW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="108" to="125" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Selfattentional acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09519</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mo-bileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">OpenNMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-4012</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-4012" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep hand: How to train a CNN on 1 million hand images when your data is continuous and weakly labelled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="3793" to="3802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
