<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">English Conversational Telephone Speech Recognition by Humans and Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research -Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Dimitriadis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Picheny</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn-Li</forename><surname>Lim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bergul</forename><surname>Roomi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><forename type="middle">Hall</forename><surname>Appen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Australia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Watson</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">English Conversational Telephone Speech Recognition by Humans and Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: LSTM</term>
					<term>ResNet</term>
					<term>dilated convolutions</term>
					<term>conversa- tional speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues -what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which -at least at the writing of this paper -is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multitask learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the performance of ASR systems inching ever closer to that of humans, it is important to benchmark human performance accurately. In <ref type="bibr" target="#b0">[1]</ref>, the authors claim a human word error rate (WER) of 5.9%/11.3% on the Switchboard/CallHome subsets (SWB/CH) of the NIST Hub5 2000 evaluation testset. When compared with <ref type="bibr" target="#b1">[2]</ref> which quotes a WER of 4%, the 5.9% estimate seemed rather high (albeit measured on different data). This intriguing discrepancy prompted us to launch our own human transcription effort in order to confirm (or disconfirm) the estimates from <ref type="bibr" target="#b0">[1]</ref>. The findings from this effort were doubly surprising. First, we were expecting the SWB measurement to be closer to the Lippmann estimate of 4% but could only get down to 5.1% for the best transcriber after quality checks. Second, the same transcriber achieved a surprisingly low 6.8% WER for CallHome (we were expecting a much higher number based on the 11.3% estimate).</p><p>For comparison, our latest ASR system achieves 5.5%/10.3% WER on SWB/CH. This means that "human parity" is attainable on this particular Switchboard subset (although not achieved yet) but is a distant dream for the CallHome task. What makes the Switchboard and CallHome testsets so different one might ask? The biggest problem with the SWB testset is that 36 out of 40 test speakers appear in the training data, some in as many as 8 different conversations <ref type="bibr" target="#b2">[3]</ref>, and our acoustic models are very good at memorizing speech patterns seen during training. The second problem is that the SWB and CH tasks differ in the style of conversational speech: SWB consists of conversations between strangers while CH consists of calls between friends and family members. Speaking style between strangers tends to be more formal whereas the CallHome style is more casual making CH a harder task. The training data collected by LDC under the Switchboard and Fisher protocols is almost entirely Switchboard-like meaning that testing on CallHome is a mismatched scenario for ASR systems. Since ASR systems are generally not robust to mismatched training and testing conditions, it comes as no surprise that the degradation in performance from SWB to CH for ASR systems is larger than that of expert transcribers.</p><p>On the system side, we have simplified and improved our acoustic models considerably and experimented with more sophisticated language models such as LSTM and WaveNet LMs. Most of the AM improvement comes from LSTMs that operate on multiple features or use a different training criterion such as speaker-adversarial multi-task learning. Additionally, replacing the VGG convolutional nets <ref type="bibr" target="#b3">[4]</ref> that we had in our last year's system <ref type="bibr" target="#b4">[5]</ref> with ResNets <ref type="bibr" target="#b5">[6]</ref> turned out to be beneficial for performance. On the LM side, adding LSTM word and characterbased LMs resulted in substantial accuracy gains.</p><p>The rest of the paper is organized as follows: in section 2 we talk about the human transcription experiments; in section 3 we describe a series of system improvements pertaining to both acoustic and language modeling and in section 4 we summarize our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Human transcription experiments</head><p>These experiments were carried out by IBM's preferred speech transcription provider, Appen. The transcription protocol that was agreed upon was to have three independent transcribers provide transcripts which were quality checked by a fourth senior transcriber. All four transcribers are native US English speakers and were selected based on the quality of their work on past transcription projects. The transcribers were familiarized with the LDC transcription guidelines which cover hyphenations, spelled abbreviations, contractions, partial words, nonspeech sounds, etc.</p><p>The transcription time was estimated at 12-14 times realtime (xRT) for the first pass for Transcribers 1-3 and an additional 1.7-2xRT for the second quality checking pass (by Transcriber 4). Both passes involved listening to the audio multiple times: around 3-4 times for the first pass and 1-2 times for the second. After receiving the transcripts, the following filtering rules were aplied: ? Other markers such as '...', '-', '(( ))' were eliminated prior to scoring. ? All partial words ending in '-' were marked as nonlexical items. ? All punctuation marks such as '.', ',', '!' and '?' were eliminated prior to scoring.</p><formula xml:id="formula_0">?</formula><p>In order to use NIST's scoring tool sclite, we had to convert the transcripts into CTM files which have time-marked word boundary information. This was done by splitting the duration of the utterance uniformly across the number of words.</p><p>In <ref type="table">Table 1</ref> we show the error rates of the three transcribers before and after quality checking by the fourth transcriber as well as the human WER reported in <ref type="bibr" target="#b0">[1]</ref>. Unsurprisingly, there is some variation among transcriber performance and the quality checking pass reduces the error rate across all transcribers.  <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WER SWB WER CH</head><p>Additionally, in <ref type="table" target="#tab_3">Tables 2 and 3</ref>, we take a closer look at the most frequent substitution, deletion and insertion errors for our system output and the best human transcript after quality checking. While many of the errors look similar to those reported in <ref type="bibr" target="#b0">[1]</ref>, there is a glaring discrepancy in the frequency of top deletions for CallHome between our human transcript and theirs. This suggests that the very different estimates for the human error rate for CallHome (6.8% versus 11.3%) can be attributed to a much lower deletion rate for our best human transcript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System improvements</head><p>In this section we discuss the training data and testsets that were used as well as improvements in acoustic and language modeling. The training set for our acoustic models consists of 262    <ref type="table">Table 4</ref>: Testsets that are used to report experimental results.</p><p>In <ref type="bibr" target="#b6">[7]</ref>, we have shown that convolutional and nonconvolutional AMs have comparable performance and good complementarity. Hence, the strategy for our previous systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5]</ref> was to use a combination of recurrent and convolutional nets. For example, in last year's system we used a score fusion of three models which share the same decision tree: unfolded RNNs with maxout activations, LSTMs and VGG nets. This year, in order to simplify and enhance the overall architecture, we eliminated the maxout RNN, we improved the LSTMs and we replaced the VGG nets with residual nets (ResNets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">LSTM acoustic models</head><p>All models presented here share the following characteristics. Their architecture consists in 4-6 bidirectional layers with 1024 cells per layer (512 per direction), one linear bot-tleneck layer with 256 units and an output layer with 32K units corresponding to as many context-dependent HMM states (shown on the left side of <ref type="figure" target="#fig_0">Figure 1</ref>). Training is done on nonoverlapping subsequences of 21 frames where each frame consists of 40-dimensional FMLLR features to which we append 100-dimensional i-vectors. We group subsequences from different utterances into minibatches of size 128 for processing speed and reliable gradient estimates. The training consists of 14 passes of cross-entropy followed by 1 pass of SGD sequence training using the boosted MMI criterion <ref type="bibr" target="#b8">[9]</ref> smoothed by adding the scaled gradient of the cross-entropy loss <ref type="bibr" target="#b9">[10]</ref>. Implementation of the LSTM was done in Torch <ref type="bibr" target="#b10">[11]</ref> with cuDNN v5.0 backend. Cross-entropy training for each model took about 2 weeks for 700M samples/epoch, on a single Nvidia K80 GPU device.</p><p>The first two improvements are fairly banal and consist in increasing the number of layers from 4 (like in our previous model <ref type="bibr" target="#b4">[5]</ref>) to 6 and in realigning the training data with a 6-layer LSTM and retraining another LSTM. The effect of these steps is shown in the first three rows of <ref type="table" target="#tab_6">Table 5</ref>   The second set of experiments was centered around the use of speaker-adversarial multi-task learning (SA-MTL). In <ref type="bibr" target="#b11">[12]</ref>, the authors introduce domain-adversarial neural networks which are models that are trained to not distiguish between in-domain, labeled data and out-of-domain, unlabeled data. This is achieved by training a domain classifier in parallel with the main classifier and by subtracting the gradient component from the domain classifier when estimating the parameters of the main classifier. This idea has been successfully applied in speech by <ref type="bibr" target="#b12">[13]</ref> in the context of noise robustness where the author proposes noise-adversarial MTL to suppress the effects of noise. Here, we experiment with training a speaker classifier in addition to the main CD-HMM state classifier in order to suppress the effects of speaker variability on ASR performance. Since i-vectors are a good low-dimensional representation of a speaker, we decided to train the speaker classifier to predict the i-vector inputs using an MSE loss function. The speaker classifier has one sigmoid layer and one hyperbolic tangent layer as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>If we denote by ?, ?c, ?s the parameters of the common LSTM, the main classifier (weights of linear layer before softmax) and the speaker classifier, the SGD update is done according to:?</p><formula xml:id="formula_1">c = ?c ? ?LCE(x) ??? ?s = ?s ? ?LMSE(x) ??? ? = ? ? ?LCE(x) ?? ? ? ?LMSE(x) ??</formula><p>where x denotes a minibatch, LCE, LMSE denote respectively the cross-entropy loss of the main classifier and the meansquared error loss of the i-vector classifier, ? is a scaling parameter (typically set to 0.1), and is the learning rate. After the model is trained, the i-vector classifier branch is discarded at test time. As can be seen from <ref type="table" target="#tab_6">Table 5</ref> rows 3 and 4, we observe some small gains across all testsets which are also due in part to reestimating the VTLN warp factors and FMLLR transforms using an LSTM decoding output (old factors and transforms were based off a GMM decoding). Last but not least, the largest improvement in LSTM modeling was achieved through feature fusion. The thought process leading to this experiment was that we wanted to add utterance-level information to our models which were only looking at a window of 21 consecutive frames. One possibility was to train an end-to-end LSTM using CTC as in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and append the features from the last LSTM layer before the softmax to our existing features. This experiment worked quite well however, upon closer inspection, it turned out that the CTC model used a different set of input features: Logmel+?+?? instead of PLP followed by LDA and FM-LLR. The question then naturally arose whether the gains came from CTC modeling or from the different input representations. To answer this question, we built an LSTM trained on fused FMLLR+i-vector+Logmel+?+?? features the standard way (without speaker-adversarial MTL). The WER improvement from adding the Logmel features, indicated in <ref type="table" target="#tab_6">Table 5</ref> rows 3 and 5, is the same as with CTC features meaning that the CTC modeling step was not needed. Finally, we note that the feature fusion LSTM compares favorably with other single acoustic models from the literature as mentioned in <ref type="bibr" target="#b16">[17]</ref>  <ref type="table">(Table  4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ResNet acoustic models</head><p>On the convolutional network acoustic modeling side, we trained residual networks with pre-activation identity shortcut connections. Residual Networks were introduced for image recognition in <ref type="bibr" target="#b17">[18]</ref> and used in speech recognition in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>. The novelty of residual networks is to introduce shortcut connections between so-called "blocks" of convolutional layers, which was argued to improve the flow of information and gradients, and allows training even deeper CNNs without the optimization problem occuring without the residual connections.  and their performance on the testsets with small LM. We achieved best results with basic residual blocks without bottleneck, similar to the observations from <ref type="bibr" target="#b19">[20]</ref> on CIFAR and SVHN experiments. However, bottleneck residual blocks could possibly be optimal with a larger computational budget. The input to our network are vtln-warped logmel features with 64 mel bins. We perform data-balancing according to <ref type="bibr" target="#b21">[22]</ref> with exponent ? = 0.8. We use full pre-activation identity shortcut connections which keep a clean information path <ref type="bibr" target="#b20">[21]</ref> without nonlinearity after addition. For batch normalization the statistics are accumulated per feature map and per frequency bin following <ref type="bibr" target="#b23">[24]</ref>. In order to use residual networks for acoustic modeling, we need to adapt the residual blocks (see <ref type="figure" target="#fig_1">Figure 2</ref>), while taking efficient convolution on sequences into account. In ResNets for image classification the convolutional pathway only includes padded convolutions, so does not reduce the size of the feature maps. The addition with the shortcut pathway is trivial, since both feature maps have the same size. In contrast, for convolutions on sequences we can not pad along the time directions. Padding along the time direction would modify the values on the edges based on the input sliding window location, thus making efficient convolution over a full utterance impossible (see <ref type="bibr" target="#b22">[23]</ref>). So we do not pad the convolutions in time and as a consequence, the convolutional pathway reduces the size of the feature maps along the time direction. In this case, we need to crop on the shortcut connection to match the size of the feature maps coming out of the convolutional pathway. It is important to note that this does not impact the ability to convolve the residual net over full utterances at once: since the values at the edges are computed the same as everywhere else, they are independent of the position of the input window.</p><p>Let us now consider how to use strided pooling and strided convolutions, and the relation to time-dilated convolutions. First off, in the frequency direction, similar as for images, convolutions are padded so they do not reduce the size. Rather, the size is reduced by a factor of 2 through convolutions with stride 2. In <ref type="table" target="#tab_7">Table 6</ref>, the "initStride" field on the first line of each stage indicates the (frequency x time) stride for the first block of that stage, where the number of feature maps is increased. This stride applies to both the first 3 ? 3 convolution of the block, and the 1 ? 1 convolution in the projection shortcut. The output feature map size is indicated in the left column for each stage. Secondly, along the time direction, strided convolutions and strided pooling is optional, but was found to improve performance <ref type="bibr" target="#b23">[24]</ref>. In <ref type="table" target="#tab_7">Table 6</ref>, Stage 4, column (c) and (d), bold indicates striding in time. Note that, when adding time-strided conv and pool to an architecture, we need to increase the input context window to compensate for the additional size reduction. For residual networks, similar as for VGG-style networks, we indeed observe that time-strided time-pooling improves performance, see column (b) vs (d).</p><p>When transitioning from cross-entropy (XE) to sequence training (ST), we want to modify our network to do dense prediction efficiently <ref type="bibr" target="#b23">[24]</ref>. This means the intermediate states of the convolutional layers and output of the ResNet should maintain inputs full time-resolution, i.e. it should produce an output CD state distribution for each input frame. We can achieve this by using time-dilated convolutions according to the same recipe as in <ref type="bibr" target="#b23">[24]</ref>: for each layer which originally strides in time with factor 2, set time-stride to 1 and dilate with factor 2 all consecutive convolutions, maxpooling and fully connected layers. This includes the projection shortcut in the first block of each stage, though dilation for these 1 ? 1 convolutions is irrelevant. After these modifications, the residual net can be used for dense prediction on sequences.</p><p>The ResNet which we will use in further sections is in Table 6 (d). It has 12 residual blocks, 30 weight layers and 67.1 M parameters. We trained this model using Nesterov accelerated gradient with learningrate 0.03 and momentum 0.99. Implementation of the CNN was also done in Torch with cuDNN v5.0 backend. Cross-entropy training took about 80 days for 1.5 billion samples, on 2 Nvidia K80 GPU's (4 devices) with batch size 64 per GPU and full synchronization between every minibatch. We sequence trained this model for 200M frames with the boosted MMI criterion <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model combination</head><p>In <ref type="table">Table 7</ref> we report the performance of the best individual models described in the previous paragraphs as well as the results after frame-level score fusion across all testsets. All decodings are done with an 85K word vocabulary and a 4-gram language model with 36M n-grams. We note that LSTMs and ResNets exhibit a strong complementarity which improves the WER for all testsets.  <ref type="table">Table 7</ref>: Word error rates for LSTMs and ResNet and framelevel score fusion results across all testsets (36M n-gram LM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Language modeling improvements</head><p>In addition to n-gram and model-M used in our previous system <ref type="bibr" target="#b4">[5]</ref>, we introduced LSTM-based as well as convolutionbased LMs in this paper. We experimented with four LSTM LMs, namely Word-LSTM, Char-LSTM, Word-LSTM-MTL, and Char-LSTM-MTL. The Word-LSTM had one word-embeddings layer, two LSTM layers, one fully-connected layer, and one softmax layer, as shown in <ref type="figure">Figure 3</ref>. The upper LSTM layer and the fullyconnected layer were wrapped by residual connections <ref type="bibr" target="#b5">[6]</ref>. Dropout was only applied to the vertical dimension and not applied to the time dimension <ref type="bibr" target="#b24">[25]</ref>. The Char-LSTM added an additional LSTM layer to estimate word-embeddings from character sequences as illustrated in <ref type="figure">Figure 4</ref>  <ref type="bibr" target="#b25">[26]</ref>. Both Word-LSTM and Char-LSTM used the cross-entropy loss of predicting the next word given its history as objective function, similar to conventional LMs. In addition, we introduced multi-task learning (MTL) in Word-LSTM-MTL and Char-LSTM-MTL. We first clustered the vocabulary using Brown clustering <ref type="bibr" target="#b26">[27]</ref>. When training Word-LSTM-MTL and Char-LSTM-MTL, weighted summation of cross-entropy of predicting next word given its history and next class given its history was used as objective function.</p><p>Inspired by the complementarity of convolutional and non-convolutional acoustic models, we experimented with a convolution-based LM in the form of dilated causal convolution as used in WAVENET <ref type="bibr" target="#b27">[28]</ref>. The resulting model is called Word-DCC and consists of word-embeddings layer, causal convolution layers with dilation, convolution layers, fully-connected layers, softmax layer, and residual connections. The actual number of layers and dilation/window sizes were determined using heldout data ( <ref type="figure">Figure 5</ref> has a simple configuration for illustration purposes).</p><p>For these five LMs, the training data and training procedures are common and described below:</p><p>? We used the same vocabulary of 85K words from <ref type="bibr" target="#b4">[5]</ref>.</p><p>?  ? We controlled the leaning rate by ADAM <ref type="bibr" target="#b28">[29]</ref> and introduced a self-stabilization term to coordinate the layerwise learning rates <ref type="bibr" target="#b29">[30]</ref>.</p><p>? For all models, we tuned the hyper-parameters based on the perplexity of the heldout data which is a subset of the acoustic transcripts. The approximate number of parameters for each model was 90M to 130M.</p><p>We first generated word lattices with the n-gram LM and our best acoustic model consisting of ResNet and two LSTMs. Then we rescored the word lattices with the model-M and generated n-best lists from the rescored lattice. Finally, we applied the four LSTM-based LMs and the convolution-based LM. Note that LM probabilities were linearly interpolated and the interpolation weights of all LMs were estimated using the heldout data. <ref type="table" target="#tab_11">Table 8</ref> shows WER on SWB and CH with various LM configurations. The LSTM-based LMs show significant improvements over the strong n-gram + model-M results. The Word-DCC also has a marginal improvement over the n-gram + model-M. The effect of multi-task learning was confirmed especially on CH. Among the five LSTM-based and convolutionbased LMs, word-LSTM-MTL achieved the best WER of 5.6% and 10.3% on SWB and CH respectively. By combining five LMs on top of n-gram + model-M, we achieved 5.5% and 10.3% WER for SWB and CH respectively. Lastly, we summarize the improvements due to the various language model rescoring steps across all testsets in <ref type="table">Table 9</ref>. We noticed that the testset references have inconsistent transcription conventions with regards to spellings which are not followed by periods for SWB and CH (e.g. T V) and followed by periods for the other testsets (such as T. V.). The last line of <ref type="table">Table 9</ref> shows the WERs when periods are removed from both the references and system outputs by adding the filtering rules A. =&gt; A ... Z. =&gt; Z to the GLM file.</p><p>More details about the language modeling are given in a companion paper <ref type="bibr" target="#b30">[31]</ref>.  <ref type="table">Table 9</ref>: Word error rates for the different LM rescoring steps across all testsets. Last line shows WERs after '.' removal from the references and system outputs. Conv. <ref type="figure">Figure 5</ref>: Word-DCC</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We have presented a set of acoustic and language modeling improvements to our English Switchboard system that resulted in a new record word error rate on this task. On the acoustic side, two things were instrumental in reaching this level of performance. The first one is a steady improvement in bidirectional LSTM modeling, chief among them being a simple feature fusion experiment. The second one is the replacement of VGG nets with residual nets which are a more effective architecture on the ImageNet classification task. When combined together, these recurrent and convolutional nets show good complementarity and enhanced accuracy on a variety of testsets. On the language modeling side, we exploited the same complementarity between recurrent and convolutional architectures by adding word and character-based LSTM LMs and a convolutional WaveNet LM. The second and perhaps more important point made in this paper is that, unlike what was claimed in <ref type="bibr" target="#b0">[1]</ref>, we do not believe that human parity has been reached on this task. The reasons why we came to the opposite conclusion are twofold. First, the Hub5'00 SWB testset has a large overlap between training and test speakers which results in ASR systems having deceptively good performance. A more realistic level of ASR performance is the average WER across all testsets which is around 8% for our system. The second and more direct argument is that the human WER of expert transcribers that were asked to do a highquality job is simply lower than what was previously reported. On an optimistic note, this means that the future of research in conversational speech recognition looks bright for at least a few more years.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>LSTM with speaker-adversarial MTL architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Residual connections on sequences. The convolutions are unpadded and reduce the size of the feature maps in the time direction (indicated with red dashed lines). To match this reduction, we simply crop the edges along the time on the shortcut connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>WordChar</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Most frequent substitution errors for humans and ASR system on SWB and CH.</figDesc><table><row><cell></cell><cell cols="2">Deletions</cell><cell></cell><cell></cell><cell cols="2">Insertions</cell><cell></cell></row><row><cell>SWB</cell><cell></cell><cell>CH</cell><cell></cell><cell></cell><cell>SWB</cell><cell></cell><cell>CH</cell></row><row><cell>ASR</cell><cell>Human</cell><cell>ASR</cell><cell>Human</cell><cell>ASR</cell><cell>Human</cell><cell>ASR</cell><cell>Human</cell></row><row><cell>30: it</cell><cell>19: i</cell><cell>46: i</cell><cell>20: i</cell><cell>13: i</cell><cell>16: is</cell><cell>23: a</cell><cell>17: is</cell></row><row><cell>20: i</cell><cell>17: it</cell><cell>46: it</cell><cell>18: and</cell><cell>10: a</cell><cell cols="2">14: %hes 14: is</cell><cell>17: it</cell></row><row><cell>17: that</cell><cell cols="2">16: and 39: and</cell><cell>15: it</cell><cell>7: and</cell><cell>12: i</cell><cell>11: i</cell><cell>16: and</cell></row><row><cell>16: a</cell><cell cols="2">14: that 32: is</cell><cell>15: the</cell><cell>7: of</cell><cell>11: and</cell><cell>10: are</cell><cell>14: have</cell></row><row><cell>14: and</cell><cell cols="2">14: you 26: oh</cell><cell>14: is</cell><cell>6: you</cell><cell>9: it</cell><cell cols="2">10: you 13: a</cell></row><row><cell>14: oh</cell><cell>12: is</cell><cell>25: a</cell><cell>13: not</cell><cell>5: do</cell><cell>6: do</cell><cell>9: the</cell><cell>13: that</cell></row><row><cell>14: you</cell><cell>12: the</cell><cell>20: to</cell><cell>10: a</cell><cell>5: the</cell><cell>5: have</cell><cell>8: have</cell><cell>12: i</cell></row><row><cell cols="2">12: %bcack 11: a</cell><cell>19: that</cell><cell>10: in</cell><cell cols="2">5: yeah 5: yeah</cell><cell>8: that</cell><cell>11: %hes</cell></row><row><cell>12: the</cell><cell>10: of</cell><cell>19: the</cell><cell>10: that</cell><cell>4: air</cell><cell>5: you</cell><cell>7: and</cell><cell>10: not</cell></row><row><cell>11: to</cell><cell>9: have</cell><cell cols="2">18: %bcack 10: to</cell><cell>4: in</cell><cell>4: are</cell><cell>7: it</cell><cell>9: oh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Testset</cell><cell cols="3">Duration Nb. speakers Nb. words</cell></row><row><cell>Hub5'00 SWB</cell><cell>2.1h</cell><cell>40</cell><cell>21.4K</cell></row><row><cell>Hub5'00 CH</cell><cell>1.6h</cell><cell>40</cell><cell>21.6K</cell></row><row><cell>RT'02</cell><cell>6.4h</cell><cell>120</cell><cell>64.0K</cell></row><row><cell>RT'03</cell><cell>7.2h</cell><cell>144</cell><cell>76.0K</cell></row><row><cell>RT'04</cell><cell>3.4h</cell><cell>72</cell><cell>36.7K</cell></row><row><cell>DEV'04f</cell><cell>3.2h</cell><cell>72</cell><cell>37.8K</cell></row></table><note>Most frequent deletion and insertion errors for humans and ASR system on SWB and CH. hours of Switchboard 1 audio with transcripts provided by Mis- sissippi State University, 1698 hours from the Fisher data col- lection and 15 hours of CallHome audio. In order to allay fears that we may be overfitting to the Hub5 2000 testsets by exten- sively testing on them, we have decided to report results on a va- riety of testsets. Since the RT'02, RT'03, RT'04 and DEV'04f testsets have not been used in more than a decade, we are fairly confident that performance improvements on these testsets are indicative of real progress. Statistics about all the testsets used in the experiments are given in Table 4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>across all testsets.</figDesc><table><row><cell>LSTM</cell><cell>SWB</cell><cell>CH</cell><cell cols="4">RT'02 RT'03 RT'04 DEV'04f</cell></row><row><cell>4-layer</cell><cell>8.0</cell><cell>14.3</cell><cell>12.2</cell><cell>11.6</cell><cell>11.0</cell><cell>10.8</cell></row><row><cell>6-layer</cell><cell>7.7</cell><cell>14.0</cell><cell>11.8</cell><cell>11.4</cell><cell>10.8</cell><cell>10.4</cell></row><row><cell>Realigned</cell><cell>7.7</cell><cell>13.8</cell><cell>11.7</cell><cell>11.2</cell><cell>10.8</cell><cell>10.2</cell></row><row><cell>SA-MTL</cell><cell>7.6</cell><cell>13.6</cell><cell>11.5</cell><cell>11.0</cell><cell>10.7</cell><cell>10.1</cell></row><row><cell>Feat. fusion</cell><cell>7.2</cell><cell>12.7</cell><cell>10.7</cell><cell>10.2</cell><cell>10.1</cell><cell>9.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Word error rates for LSTM AMs across all testsets (36M n-gram LM).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows four residual network model architectures</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>ResNet architectures and results. Decoding with small LM (4M n-grams). In the bottom rows (results on test-sets). XE-300 indicates the network was cross-entropy trained on the 300h SWB corpus only, XE and ST for training on the 2000h SWB+Fisher corpus. Column (d) has best performance, com- pared against 3 different ablation variants: (a) with bottleneck blocks and without pooling, (b) without pooling, and (c) less depth. The size of the output of the 3 ? 3 convolutions is indi- cated for each stage.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>We first train the LM with a corpus of 560M words consisting of publicly available text data from LDC, including Switchboard, Fisher, Gigaword, and Brodcast News and Conversations. Then, starting from the trained model, we further train the LM with only the transcripts of the 1975 hours audio data used to train the acoustic model, consisting of 24M words.</figDesc><table><row><cell></cell><cell cols="2">WER [%]</cell></row><row><cell></cell><cell>SWB</cell><cell>CH</cell></row><row><cell>n-gram</cell><cell>6.7</cell><cell>12.1</cell></row><row><cell>n-gram + model-M</cell><cell>6.1</cell><cell>11.2</cell></row><row><cell>n-gram + model-M + Word-LSTM</cell><cell>5.6</cell><cell>10.4</cell></row><row><cell>n-gram + model-M + Char-LSTM</cell><cell>5.7</cell><cell>10.6</cell></row><row><cell>n-gram + model-M + Word-LSTM-MTL</cell><cell>5.6</cell><cell>10.3</cell></row><row><cell>n-gram + model-M + Char-LSTM-MTL</cell><cell>5.6</cell><cell>10.4</cell></row><row><cell>n-gram + model-M + Word-DCC</cell><cell>5.8</cell><cell>10.8</cell></row><row><cell>n-gram + model-M + 4 LSTMs + DCC</cell><cell>5.5</cell><cell>10.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>WER on SWB and CH with various LM configurations.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Achieving human parity in conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05256</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech recognition by machines and humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2000 nist evaluation of conversational speech recognition over the telephone: English and mandarin performance results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Speech Transcription Workshop</title>
		<meeting>Speech Transcription Workshop</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The IBM 2016 English conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventeenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint training of convolutional and non-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">to Proc. ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The IBM 2015 English conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boosted MMI for model and feature-space discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Visweswariah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4057" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial multi-task learning of deep neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shinohara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2369" to="2372" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08240</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning acoustic frame labeling for speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4280" to="4284" />
		</imprint>
	</monogr>
	<note>2015 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09975</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gram-ctc: Automatic unit selection and target decomposition for sequence labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hairong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00096</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for lvcsr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<title level="m">Advances in very deep convolutional neural networks for lvcsr</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense prediction on sequences with time-dilated convolutions for speech recognition</title>
	</analytic>
	<monogr>
		<title level="m">NIPS End-to-end Learning for Speech and Audio Processing Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02096</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-stabilized deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Empirical exploration of LSTM and CNN language models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Submitted to Interspeech</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
