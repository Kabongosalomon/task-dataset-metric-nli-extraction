<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Yeong</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Inha University</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Wan</forename><surname>Choi</surname></persName>
							<email>dchoi@inha.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Inha University</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Continual learning has been a major problem in the deep learning community, where the main challenge is how to effectively learn a series of newly arriving tasks without forgetting the knowledge of previous tasks. Initiated by Learning without Forgetting (LwF), many of the existing works report that knowledge distillation is effective to preserve the previous knowledge, and hence they commonly use a soft label for the old task, namely a knowledge distillation (KD) loss, together with a class label for the new task, namely a cross entropy (CE) loss, to form a composite loss for a single neural network. However, this approach suffers from learning the knowledge by a CE loss as a KD loss often more strongly influences the objective function when they are in a competitive situation within a single network. This could be a critical problem particularly in a class incremental scenario, where the knowledge across tasks as well as within the new task, both of which can only be acquired by a CE loss, is essentially learned due to the existence of a unified classifier. In this paper, we propose a novel continual learning method, called Split-and-Bridge, which can successfully address the above problem by partially splitting a neural network into two partitions for training the new task separated from the old task and re-connecting them for learning the knowledge across tasks. In our thorough experimental analysis, our Split-and-Bridge method outperforms the state-of-the-art competitors in KD-based continual learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, deep neural networks (DNNs) have performed remarkably well in many practical applications like image or voice recognition <ref type="bibr" target="#b10">(He et al. 2016)</ref>, <ref type="bibr" target="#b8">(Graves, Mohamed, and Hinton 2013)</ref>, object detection <ref type="bibr" target="#b9">(He et al. 2020)</ref>, and language translation <ref type="bibr" target="#b35">(Sutskever, Vinyals, and Le 2014)</ref>. A typical DNN learns the entire data for a fixed number of target tasks at once. However, in real-life applications encountering a dynamic stream of samples such as autonomous robots and unmanned vehicles, it is necessary to continuously incorporate a series of new tasks into the model being trained.</p><p>This problem is known as continual learning <ref type="bibr" target="#b29">(Parisi et al. 2019)</ref>, which aims to incrementally train a model so that it can perform well on both previous tasks and new tasks. A major difficulty of continual learning in DNNs lies in the fact that the knowledge previously learned for old classes can Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-old knowledge</head><p>Intra-new knowledge Cross-task knowledge <ref type="figure">Figure 1</ref>: Three types of knowledge in CIL severely be lost during the training process of new classes, often referred to as catastrophic forgetting <ref type="bibr" target="#b28">(McCloskey and Cohen 1989;</ref><ref type="bibr" target="#b27">McClelland, McNaughton, and O'Reilly 1995)</ref>. In addition, such a loss of previous knowledge can be even more aggravated in class incremental learning (CIL) where a single classifier should be incrementally unified. This is unlike task incremental learning (TIL) where an inference is usually made with a task identity, i.e., the prior knowledge of which task each sample belongs to, for as many classifiers as tasks that have been trained.</p><p>In general, we can think of CIL as the problem of learning three types of knowledge, namely intra-old, intra-new, and cross-task, as shown in <ref type="figure">Figure 1</ref>. Either of the intra-old and the intra-new knowledge indicates how to discriminate classes only within the old task or the new task. On the other hand, the cross-task knowledge is about distinguishing every class in a particular task from the ones in the other task. Thus, we need all of the intra-old, intra-new, and cross-task knowledge for a unified classifier in CIL, but the cross-task knowledge is not necessary in TIL with a task identity provided at inference time. In this context, the goal of CIL is (i) to newly learn the intra-new knowledge as well as (ii) to incrementally update the cross-task knowledge (iii) without forgetting the previous intra-old knowledge.</p><p>As firstly introduced by Learning without Forgetting (LwF) <ref type="bibr">Hoiem 2016, 2018)</ref>, it has been reported that knowledge distillation (KD) <ref type="bibr" target="#b11">(Hinton, Vinyals, and Dean 2015)</ref> is an effective strategy to preserve the intra-old knowledge. Also, the rehearsal technique (Lopez-Paz and Ranzato 2017; <ref type="bibr" target="#b30">Rebuffi et al. 2017</ref>) is often supplementary used to acquire the cross-task knowledge as well as to give an additional opportunity for learning the intra-old knowledge. The intra-new  <ref type="figure">Figure 2</ref>: Standard KD-based CIL vs. Split-and-Bridge knowledge can obviously be learned by a given task-specific dataset. Many of the KD-based continual learning methods follow this standard training scheme shown in <ref type="figure">Figure 2</ref>(a) even though they propose their own strategies to improve the way of making inference on the model already trained <ref type="bibr" target="#b30">(Rebuffi et al. 2017;</ref><ref type="bibr" target="#b37">Wu et al. 2019;</ref><ref type="bibr" target="#b41">Zhao et al. 2020</ref>). More specifically, for the intra-old knowledge, they mainly use a KD loss with the soft label from the previous model along with a cross entropy (CE) loss with the class label from stored exemplar samples, and learn the cross-task knowledge and the intra-new knowledge from a CE loss with the class labels from exemplar and new samples.</p><p>Unfortunately, to be shown in our experiments, training a neural network is often more strongly influenced by a KD loss than by a CE loss. This is intuitively because the KD loss is not intended to newly learn additional information but for the network to stay as it is. Also, in terms of the quantity of information, a soft label used by the KD loss carries a larger amount of information than a one-hot encoded class label in the CE loss. Consequently, the aforementioned KD-based method suffers from learning the intra-new and cross-task knowledge, both of which can only be acquired by a CE loss, due to the interference from a KD loss for the intra-old knowledge.</p><p>One natural solution for this problem would be to train only samples of new classes separately in another neural network, and then combine it into the model previously trained for old classes. To this end, we need to minimize two KD losses, each of which transfers the knowledge from either model being integrated. This way seems to effectively learn the intranew knowledge without any intervention. At the same time, however, two KD losses for the integration would doubly disrupt learning the cross-task knowledge from a CE loss, not to mention that we need such an extra model only temporarily used for new classes.</p><p>In this paper, we propose a novel CIL method, called Splitand-Bridge, which can effectively learn all three types of knowledge in a way that learning each type of knowledge is much less interfered by learning the others even within a single neural network. As presented in <ref type="figure">Figure 2</ref>(b), Splitand-Bridge works in the following two phases, that is, split and bridge. In the split phase, we partially split a previously trained network into two partitions exclusive in upper layers yet sharing the same component in lower layers. In the mean time, for each exclusive partition, we separately learn either the intra-new knowledge or the intra-old knowledge without interrupting each other. Then, in the bridge phase, we re-connect (i.e., bridge) those two partitions so that the crosstask knowledge can also be effectively learned without using two KD losses to integrate two pre-trained networks, i.e., the old model and new model.</p><p>In our experimental results, we show that Split-and-Bridge is superior to the state-of-the-art methods based on KD in CIL. In particular, our method turns out to be more adaptable to new classes than the existing methods, which is observed by the fact that the accuracy on the intra-new knowledge is fairly improved without loss of the knowledge previously learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Continual learning in DNNs. Continual learning with DNNs has been mainly studied in the following two problem settings: class incremental learning (CIL) and task incremental learning (TIL). In CIL, a task identity should also be inferred for a unified classifier, whereas it is provided at test time in TIL. Both in CIL and TIL with DNNs, most of the existing works focus on how to overcome catastrophic forgetting, that is, the problem of preserving the previous knowledge when learning the new knowledge. The followings are the major branches of works on this problem strongly related to ours. Rehearsal methods. Rehearsal methods (Lopez-Paz and Ranzato 2017; <ref type="bibr" target="#b30">Rebuffi et al. 2017</ref>) store a subset of previous samples, and train them together with samples for a new task. This approach is known to be most effective to mitigate catastrophic forgetting in the CIL problem in which a single neural network needs to learn a sequence of tasks (van de Ven and Tolias 2019), and therefore many state-of-the-art methods complementary leverage a rehearsal method and so does our Split-and-Bridge method. Parameter regularization. An alternative approach <ref type="bibr" target="#b16">(Kirkpatrick et al. 2017;</ref><ref type="bibr" target="#b39">Zenke, Poole, and Ganguli 2017;</ref><ref type="bibr">Chaudhry et al. 2018;</ref><ref type="bibr" target="#b2">Aljundi et al. 2018)</ref> to address catastrophic forgetting is to regularize the parameters of a neural network so that more important parameters with respect to the previous task can be protected during training on each new task. The main drawback of this approach lies in the fact that it suffers from learning a long sequence of tasks as its main strategy is not to further change parameters from themselves that have been already trained. Consequently, it is reported that these methods do not work well especially in CIL, compared to the methods based on knowledge distillation (KD) (van de Ven and Tolias 2019). Parameter isolation. Mostly in a TIL scenario, we can also prevent catastrophic forgetting by separating model parameters for each task from the others. This idea motivates parameter isolation methods like PNN <ref type="bibr">(Rusu et al. 2016)</ref>, DEN <ref type="bibr" target="#b38">(Yoon et al. 2018)</ref>, PackNet , Piggyback <ref type="bibr" target="#b24">(Mallya, Davis, and Lazebnik 2018)</ref>, HAT <ref type="bibr" target="#b34">(Serr? et al. 2018)</ref>, CGATE <ref type="bibr" target="#b0">(Abati et al. 2020)</ref>, all of which are designed to learn each task in an isolated part of the network and to make inference by using only the task-specific parameters selected by a task identity given at test time or inferred by an extra task classifier <ref type="bibr" target="#b0">(Abati et al. 2020)</ref>. Another type of parameter isolation is to temporarily learn a new task in an extra network and then combine it into the previously learned model, which covers P&amp;C <ref type="bibr" target="#b33">(Schwarz et al. 2018)</ref>, DR <ref type="bibr" target="#b12">(Hou et al. 2018)</ref>, DMC , and GD <ref type="bibr" target="#b19">(Lee et al. 2019</ref>). Our method is somewhat inspired by parameter isolation in the sense that we also temporarily split a network and then perform a unification process. However, none of the works do not deal with how to solve the CIL problem within a single neural network, which is the main goal of this work. KD-based methods. Similar to parameter regularization, KD-based methods basically aim to retain a pretrained model by transferring the previous knowledge distilled from the model. However, they differ from parameter regularization in that KD allows parameters to be well updated during training on a new task to find a better optima for both the previous and new task. Since this approach was firstly introduced by LwF ( <ref type="bibr">Hoiem 2016, 2018)</ref> for the TIL problem, there have been many variants following this standard LwF training scheme. iCaRL <ref type="bibr" target="#b30">(Rebuffi et al. 2017</ref>) first proposes a combination approach of rehearsal and KD. More recent approaches tend to focus on data imbalance problem between old classes and new classes in CIL, which includes WA <ref type="bibr" target="#b41">(Zhao et al. 2020)</ref>, Bic <ref type="bibr" target="#b37">(Wu et al. 2019)</ref>, LUCIR <ref type="bibr" target="#b13">(Hou et al. 2019)</ref>, and EEIL <ref type="bibr" target="#b4">(Castro et al. 2018)</ref>. Although all these KD-based methods work effectively well to transfer the previous knowledge, we claim that these approaches could have undervalued the importance of the intra-new and cross-task knowledge, which should also be highly valued in the CIL problem we focus on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary</head><p>This section first formally defines the class incremental learning (CIL) problem, and then describes the standard KD-based training method in the context of three essential types of knowledge in CIL as mentioned in the introduction. Class incremental learning problem. We consider a sequence of tasks T 1 , T 2 , ..., T n , where each T i at the i-th time step is a set of classes such that T i ?T j = ? for any i = j, and carries its task-specific sample set D i . Each D i consists of pairs (x, y) of input sample x and its one-hot encoded label y. At any t-th step, we are given the new sample set D t as well as an exemplar set M t of a fixed size, which is a subset of previously observed samples, i.e., M t ? D 1 ? D 2 ? ? ? ? ? D t?1 . Due to the limitation of memory space, it is usually assumed that |M t | |D 1 ? ? ? ? ? D t?1 | in practice. Let ? i denote a neural network that have been trained from T 1 to T i . Then, for each arrival of a new task T t , the goal of the CIL problem is to newly train D t with the help of M t on the previously trained model ? t?1 so that the resulting model ? t can work well with respect to all the classes in T 1 ? ? ? ? ? T t .</p><p>As mentioned in the introduction, we can categorize the information required for each ? t into the three types of knowledge, namely intra-new, intra-old, and cross-task. The intranew knowledge is essential to identify the class of each new sample in D t from the other classes within T t . Similarly, the intra-old knowledge represents how to discriminate samples within the set of old classes, i.e., T 1 ? ? ? ? ? T t?1 . On the other hand, the cross-task knowledge is necessary to distinguish  Standard KD-based incremental learning. In the literature, KD-based CIL methods mostly adopt the same strategy to learn each of the above three knowledge types. To illustrate, we first denote o(x) as the output logit from a neural network for a given input sample x. Then, we can define a cross entropy (CE) loss on a model ? for a given dataset D as:</p><formula xml:id="formula_0">? ce (D, ?) = ? (x,y)?D y log p(x),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">p(x) is the softmax probability vector of o(x) such that p(x) i = e o(x) i j e o(x) j .</formula><p>Similarly, we define a knowledge distillation (KD) loss as follows:</p><formula xml:id="formula_2">? kd (D, ?) = ? (x,y)?Dq (x) log q(x),<label>(2)</label></formula><p>where q(x) is the softened probability of o(x) such that q(x) i = e o(x) i /? j e o(x) j /? for a temperature variable ? , andq(x) indicates the soft label of x from the referenced model of ?, which is ? t?1 at the t-th time step.</p><p>The standard KD-based method learns a new task T t by minimizing the following composite loss function:</p><formula xml:id="formula_3">? ? kd (D t ? M t , ? t ) + (1 ? ?) ? ce (D t ? M t , ? t ), (3)</formula><p>where ? is a hyperparameter balancing between two losses, and usually set to</p><formula xml:id="formula_4">C old C old +Cnew such that C old = |T 1 ? ? ? ? ? T t?1 | and C new = |T t |.</formula><p>We can further identify which part of this loss function is utilized to acquire each type of knowledge, and our understanding is as follows:</p><formula xml:id="formula_5">? Intra-old knowledge: ? kd (D t ? M t , ? t ) + ? ce (M t , ? t ) ? Intra-new knowledge: ? ce (D t , ? t ) ? Cross-task knowledge: ? ce (D t ? M t , ? t )</formula><p>Thus, both the intra-new and the cross-task knowledge have to be learned by a CE loss, but this could not be very effective as a KD loss for the intra-old knowledge is likely to dominate the final loss function. This is a reasonable conjecture in that the soft labelq(x) will carry a larger amount of information than the one-hot encoded label y, leading to more updates by ? kd than by ? ce .</p><p>To examine our conjecture, we conduct a simple experiment on learning CIFAR-100 <ref type="bibr" target="#b17">(Krizhevsky, Hinton et al. 2009</ref>) on ResNet-18 <ref type="bibr" target="#b10">(He et al. 2016)</ref> with two incremental tasks, <ref type="figure">Figure 3</ref>: Two-phase learning of Split-and-Bridge each consisting of 20 classes. Then, we compare the resulting accuracy of using ? kd + ? ce to that of using only ? ce , where the accuracy is further divided into four categories; old, new, intra-old, and intra-new, to show how well each type of knowledge is learned by each loss function. When measuring either the intra-old or intra-new accuracy, we locally compare only the output probabilities corresponding to either the first (i.e., old) task or the second (i.e., new) task. As shown in <ref type="table" target="#tab_2">Table 1</ref>, it is well observed that the new accuracy and the intra-new accuracy of using ? kd + ? ce gets lower than those of using only ? ce . Thus, even though the standard KD-based method (i.e., ? kd + ? ce ) improves the overall accuracy partly because of the preservation of the intra-old knowledge, it is achieved at the sacrifice of the adaptability to the new task.</p><formula xml:id="formula_6">Split phase Bridge phase ? s ? o ? n ? t ? t ? ? ? L?? L?? L??? L?? M t D t M t D t ? ? ? ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Adaptable Incremental Learning</head><p>In this section, we propose our Split-and-Bridge method for the CIL problem, which can effectively learn all three types of knowledge. Motivation. As described in the preliminary section, the standard KD-based method suffers from learning the new knowledge by a CE loss due to the interference from the KD loss. With respect to the intra-new knowledge, a possible solution would be to separately learn a new task with an extra network, and then integrate its intra-new knowledge with the previous knowledge by distilling from two teacher models (i.e., the old model and the new model). However, at this time, the cross-task knowledge to be learned by another CE loss is doubly disturbed by these two KD losses.</p><p>This motivates us to propose a two phase learning method within a single network, where we first (i) partially split the network into two partitions for a separated learning, namely split phase, and then (ii) re-connect these trained partitions to additionally learn the cross-task knowledge, namely bridge phase. By doing so, we do not only learn the intra-task knowledge in an isolated partition without any competition between losses, but also acquire the cross-task knowledge without having to use double KD losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split Phase</head><p>Separated learning within a single network. The goal of split phase is to learn the intra-new knowledge as independently as possible from the task of preserving the intra-old knowledge without using an extra neural network. To this end, we need to re-organize the given network ? t to have two disjoint branches at upper layers, denoted by ? o and ? n , coming from a shared common feature extractor spanning lower layers, denoted by ? s , as shown in <ref type="figure">Figure 3</ref> <ref type="figure">Figure 4</ref>: Sparsification across tasks is successfully transformed into the branched network, denoted by ? s , [? o , ? n ] t , we can learn both the intra-new and intra-old knowledge by the following loss function:</p><formula xml:id="formula_7">. Once ? t Wn?o Wo?n Wo Wn vn (??1) v? (??1) W (?) (?) (?) (?) (?) v? (?) vn (?) v? (?) vn (?) vn (??1) v? (??1) W (?) Wo (?) Wn (?)</formula><formula xml:id="formula_8">? kd (D t ? M t , ? s , ? o t ) + ? lce (D t , ? s , ? n t ). (4)</formula><p>Through ? kd , we distill the intra-old knowledge from the previous model, i.e., ? t?1 , into a part of the model consisting of the shared component followed by the disjoint partition for the old task, i.e., ? s , ? o t . Separately, the intra-new knowledge is learned on the other part of the model consisting of the shared component and the other partition, i.e., ? s , ? n t . Thus, with respect to ? n and ? o , each learning is completely independent, while both of learning processes are performed on ? s . It is reasonable to have this shared component as the features at lower levels are usually applicable to every task and therefore such common features can effectively help to learn either task as well.</p><p>Note that we use a different type of loss, called localized cross entropy (LCE) and denoted by ? lce , for learning the intra-new knowledge in this branched network. This loss is similar to the normal CE loss, but it only takes into account the local probabilities within the new task in order to focus on the intra-new knowledge as defined:</p><formula xml:id="formula_9">? lce (D t , ?) = ? (x,y)?Dt y t log p t (x),<label>(5)</label></formula><p>where p t (x) represents a softmax output locally computed using only the sub-logit corresponding to the task T t , and y t is similarly a sliced hard label corresponding to T t . Recall that p(x) and y are defined for the all the classes in Eq.</p><p>(1), but p t (x) and y t are differently defined as p t (x) i = e o(x) i j e o(x) j and y ti = y i such that i, j ? [C old + 1, C old + C new ], where C old is the number of old classes and C new is the number of new classes. Weight sparsification across tasks. Then, how do we get a given network ? t to have a partially separated structure ? s , [? o , ? n ] t ? In a simple way, we can randomly select two disjoint partitions and disconnect all the weights between them. However, such a simple method can cause a considerable loss of the previously trained knowledge. Our solution is instead to make those weights across partitions as sparse as possible during learning the intra-new knowledge as well as distilling the intra-old knowledge. This idea is inspired by SplitNet <ref type="bibr" target="#b15">(Kim et al. 2017)</ref>, which is originally introduced to build a tree-structured network with multiple branches such that similar classes are assigned to the same part of the network parameters. In the CIL problem setting, our goal is to prevent mutual interference between tasks during learning a new task, instead of grouping similar classes.</p><p>As illustrated in <ref type="figure">Figure 4</ref>, let us first consider a weight matrix W ( ) in layer , which takes the output v ( ?1) of the ( ? 1)-th layer as its input vector and produces its output v ( ) to be the input of the ( + 1)-th layer. Also, let S denote the index of the last layer covered by ? s , and then we have to split each W ( ) into two partitions for each layer ? [S + 1, L], where L indicates the final layer of the network. Splitting each W ( ) means that we divide its input nodes v ( ?1) and output nodes v ( ) into two disjoint groups with a particular ratio (e.g., 1:1), namely v  <ref type="figure">Figure 4</ref>. As mentioned above, to preserve the previous knowledge as much as possible in such a splitting process, we train a single network ? t by the following loss function, which can make weights to be disconnected as sparse as possible, while simultaneously trying to put each of the intra-old and the intra-new knowledge into two separated partitions:</p><formula xml:id="formula_10">? kd (D t ? M t , ? t ) + ? lce (D t , ? t ) + ? L =S+1 (||W ( ) o,n || 2 + ||W ( ) n,o || 2 ),<label>(6)</label></formula><p>where W ( ) o,n and W ( ) n,o indicate two sub-matrices whose weights should be disconnected, as defined: <ref type="bibr">By Eq. (6)</ref>, ? t is jointly optimized by ? kd , ? lce , and a regularization term for sparsification. Note that the regularization term imposes l 2 norm on two sub-matrices to be disconnected, and therefore minimizing this loss makes those weights as small as possible. ? controls the strength of this sparse regularization. Once the training of weight sparsification is performed enough, we explicitly disconnect all W ( ) 's into two partitions W ( ) o 's and W ( ) n 's, all of which together constitute ? o and ? n , respectively, for further training on the resulting branched network by minimizing Eq. (4).</p><formula xml:id="formula_11">W ( ) o,n = {w ij ? W ( ) | i ? v ( ?1) o ? j ? v ( ) n } W ( ) n,o = {w ij ? W ( ) | i ? v ( ?1) n ? j ? v ( ) o }.</formula><p>Note that this sparsification process can also help to prevent overfitting in earlier incremental steps. If we want our model to accept as many tasks as possible, it is somewhat unavoidable to start with a high-capacity model. However, such a large model tends to overfit by memorizing patterns of samples belonging to a few classes in earlier steps. Our regularization term for sparsification can mitigate this overfitting problem particularly when we train initial tasks consisting of only a few classes on a model with high capacity.</p><p>Adaptive split ratio. Now, the question is how much we allocate the partition of each task. In the final layer (i.e., when = L), there is no choice but to assign the final outputs of old classes to v  Re-connect ? o and ? n to form ? t ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12</head><p>Train ? t by minimizing Eq. <ref type="formula">(3)</ref>;</p><formula xml:id="formula_12">13 M t+1 ? random sample from M t ? D t ;</formula><p>old task, which depends on models, total number of steps, etc. Once |v ( ) n | &lt; 1, we consider to be a layer shared by the intra-old and intra-new knowledge. In our experiments, we set ? to a value between 1.0 and 1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bridge Phase</head><p>Given ? s , [? o , ? n ] t that has separately learned the intra-old and intra-new knowledge, we re-connect two partitions ? o and ? n in order to learn the cross-task knowledge between them in the bridge phase. To this end, we first initialize all the weights that have been removed in the split phase to be zero. This is intuitively because a random initialization can yield erroneous cross-task information and this would happen to break the model ? s , [? o , ? n ] t trained well in the split phase. Our intention in the bridge phase is to learn any new information across tasks on these zero-initialized bridge weights if necessary.</p><p>In order to train this re-connected network, we minimize the same loss function of Eq. (3) as the standard KD-based method. At this time, however, we train the model that has already learned the intra-new knowledge as well as the intraold knowledge in the split phase, not a model trained only for the old task. Thus, since the intra-new knowledge is already there in the target model being trained, learning by Eq.</p><p>(3) is like an auxiliary training process as for the intra-new knowledge.</p><p>In addition, the KD loss now transfers not only the intraold knowledge but also the common knowledge between old and new classes as its referenced model is now ? s , ? o t containing the shared component ? s , not ? t?1 owning only the intra-old knowledge. As a result, the KD loss would even help to learn the cross-task knowledge, which is mainly learned by the overall CE loss. Overall algorithm. In summary, Algorithm 1 outlines how Split-and-Bridge trains a given neural network for each incremental task. Learning the first task is typical training by a CE loss (Lines 5-6). After that, we first train ? t to have a partially split network (Lines 8-9), and then perform a sep-arated learning on it (Line 10). Finally, we re-connect the split partitions for further learning the cross-task knowledge . It is noteworthy that the KD loss in Lines 8 and 10 uses ? t?1 as its reference model, but the reference model is changed to ? s , ? o t for Line 12 once the branched network is successfully trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Environment Datasets and base model. In our experiments, we train two benchmark datasets, CIFAR-100 <ref type="bibr" target="#b17">(Krizhevsky, Hinton et al. 2009</ref>) and Tiny-ImageNet <ref type="bibr" target="#b18">(Le and Yang 2015)</ref>, on ResNet-18 <ref type="bibr" target="#b10">(He et al. 2016)</ref>. CIFAR-100 consists of 50K training images, 500 per class, and 10K test images, 100 per class, from 100 classes in total, where every image is of size 32 ? 32. We randomly arrange and divide the classes into a particular number of groups of the same size for each group to be an incremental task. Tiny-ImageNet includes 100K training images and 10K validation images for 200 classes, each of which is of size 64 ? 64. As a labeled test set of Tiny-ImageNet is not available, we use its validation set as a test set. Similar to CIFAR-100, we randomly split the classes into a certain number groups of the same size to form a series of as many incremental tasks. As a base network, we use ResNet-18 for both datasets as it is widely used for various benchmark datasets including CIFAR-100 and Tiny-ImageNet in the literature <ref type="bibr" target="#b7">(Devries and Taylor 2017;</ref><ref type="bibr" target="#b14">Keshari, Singh, and Vatsa 2019)</ref>. Compared methods. In order to evaluate the performance of Split-and-Bridge, we test the following KD-based class incremental learning methods: iCaRL <ref type="bibr" target="#b30">(Rebuffi et al. 2017)</ref>, WA <ref type="bibr" target="#b41">(Zhao et al. 2020)</ref>, and Bic <ref type="bibr" target="#b37">(Wu et al. 2019)</ref>. All three methods basically adopt the standard KD-based training method described in the preliminary section, which we name STD 1 . Their difference lies in their way of making inference. iCaRL makes a prediction by adopting k-NN classification on the exemplar set. Both WA and Bic, which are the state-of-theart methods in CIL, propose new balancing techniques to overcome the data imbalance problem between old classes and new classes, yet commonly follow STD for their training scheme.</p><p>In addition to STD, we consider another baseline training method, called double distillation (DD), which is the way of combining two neural networks separately trained for either the old task and the new task as described in the motivation of our proposed method. Note that any inference techniques are orthogonal to the training scheme itself this work focuses on, and therefore we apply WA to DD as well as Split-and-Bridge. Finally, we measure the performance of oracle as an upper bound, which is jointly trained at once using the entire dataset. Implementation details. We implement all the methods 2 in PyTorch, and train each model on a machine with an NVIDIA 1 The original version of iCaRL is slightly different from STD, but this STD version is often reported to perform even better than the original one by the recent works such as <ref type="bibr" target="#b1">(Ahn and Moon 2020</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Overall performance. <ref type="table" target="#tab_5">Table 2</ref> summarizes the overall performance of all the compared methods, where we present the average accuracy of each method over all the incremental steps other than the first. It is clearly observed that our Splitand-Bridge method outperforms the other learning methods in most of the results. As also shown in <ref type="figure">Figures 5 and 6</ref>, Splitand-Bridge is not only the best on the average accuracy, but also consistently achieves the highest accuracy throughout the incremental steps in both CIFAR-100 and Tiny-ImageNet. The DD method occasionally performs better than STD probably because of learning the intra-new knowledge better, but its effectiveness turns out to be marginal in the sense that STD with WA or iCaRL often shows higher accuracies. This confirms our claim that a simple separated learning method with two neural networks does not work well in CIL. Among the STD based methods, WA seems the best inference technique as it almost always shows the highest accuracy except for DD and Split-and-Bridge, and Bic is generally better than iCaRL in many cases. Interestingly, iCaRL performs quite well in the case of learning 20 tasks (i.e., 5 classes per task) in CIFAR-100, but both Bic and WA seem to struggle with learning this series of 20 tiny tasks as their performance gets abruptly worse than they used to be in the other cases. As observed in <ref type="figure">Figure 5(d)</ref>, this is due to the fact that the accuracy of the methods except for iCaRL and Split-and-Bridge is far lower than expected in the first couple steps, and these inaccurately trained initial models badly influences learning the next sequence of tasks. As mentioned earlier, starting with a high-capacity model can inevitably cause an overfitting problem in earlier steps especially when each task consists of a few classes such as one of 20 tasks in CIFAR-100. Since iCaRL does not involve any FC layer to make an inference, such an overfitting problem can be alleviated. Also, our Split-and-Bridge can mitigate overfitting with the help of regularization of the split phase, and hence still remains the best out of all the compared methods. For all the other details, please refer to our supplementary material.</p><p>Accuracy on the intra-new and the intra-old knowledge. As shown in <ref type="figure">Figure 7</ref>, we also compare the intra-new accuracy and the intra old accuracy of each method on the average. To focus on the effectiveness of each training scheme regardless of a balancing technique, we conduct this analysis using only the methods with WA. As expected, Split-and-Bridge always shows the best intra-new accuracy in both datasets, which tells our separated training scheme is quite effective to learn the intra-new knowledge. DD is also more adaptive to new tasks than STD when learning a small number of large tasks, but it gets less effective as the number of steps increases probably due to as many merging processes of two neural networks. Another observation is that our Split-and-Bridge method is not only highly adaptive to new classes, but also good at preserving the intra-old knowledge as shown in Figures 7(b) and 7(d). This is contrast to the result of <ref type="table" target="#tab_2">Table 1</ref>, where learning by a CE loss without any KD losses is also adaptive but prone to forgetting the previous knowledge. Through this analysis, it is confirmed that Split-and-Bridge is able to achieve a good placement between stability and plasticity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In class incremental learning, we need to learn three types of essential knowledge, intra-new, intra-old, and cross-task, but the standard KD-based method has more focused on preserving the intra-old knowledge by sacrificing plasticity of a model. Motivated by this, we proposed a novel class incremental learning method, called Split-and-Bridge, which is highly adaptive to new classes yet stable enough not to forget too much of the previous knowledge. Through the extensive experiments on CIFAR-100 and Tiny-ImageNet, we confirmed that our Split-and-Bridge method can be an effective solution for the stability-plasticity dilemma in neural networks. As a future work, we hope to see our proposed training scheme is applied to a more complex deep learning problem such as object detection and sequence generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>n</head><label></label><figDesc>to those of new classes. Other than the final layer, we propose an adaptive splitting scheme such that |v( ) o | : |v ( ) n | = ? C old : (1??) C old +C new , where? is a hyperparameter that controls the allocation rate for the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Comparison on the accuracy for each incremental task using CIFAR-100 Comparison on the accuracy for each incremental task using Tiny-ImageNet S&amp;B with WA (ours) Comparison on the average intra-new and intra-old accuracy over the incremental tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Four types of accuracy after incrementally learning two tasks, each consisting of 20 classes, on ResNet-18 with CIFAR-100, where the accuracy for the first task is 85.05% samples across tasks, which informs how each new sample is different from old samples, and vice versa.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Split-and-Bridge Incremental Learning1 ? 0 ? a neural network randomly initialized; 2 M 1 ? ?; 3 foreach incremental task T t do Input: model ? t?1 , the task-specific data D t Output: model ? t 4 ? t ? ? t?1 ; Train ? t by minimizing ? ce (D t , ? t ); Explicitly disconnect ? t into ? s , [? o , ? n ] t ; 10Train ? s , [? o , ? n ] t by minimizing Eq. (4);</figDesc><table><row><cell>5</cell><cell>if t = 1 then</cell></row><row><cell>7</cell><cell>else</cell></row><row><cell>8</cell><cell>Train ? t by minimizing Eq. (6);</cell></row><row><cell>11</cell><cell></cell></row></table><note>69</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Average accuracies over all the incremental tasks of</cell></row><row><cell>ResNet-18 using CIFAR-100 and Tiny-ImageNet</cell></row><row><cell>TITAN RTX and Intel Core Xeon Gold 5122. In the split</cell></row><row><cell>phase, we divide two last residual blocks along with the final</cell></row><row><cell>fully-connected (FC) layer of ResNet-18 into two disjoint</cell></row><row><cell>partitions, i.e., ?</cell></row></table><note>o and ? n , implying S = 13 and L = 18. Full details are covered in our supplementary material.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2018R1D1A1B07049934) and in part by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grants funded by the Korea government (MSIT) (2019-0-00240, 2019-0-00064, and 2020-0-01389, Artificial Intelligence Convergence Research Center(Inha University)).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional Channel Gated Networks for Task-Aware Continual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3930" to="3939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Simple Class Decision Balancing for Incremental Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<idno>abs/2003.13947</idno>
		<ptr target="https://arxiv.org/abs/2003.13947" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Memory Aware Synapses: Learning What (not) to Forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<editor>Ferrari, V.</editor>
		<editor>Hebert, M.</editor>
		<editor>Sminchisescu, C.</editor>
		<editor>and Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings, Part III</title>
		<meeting>Part III<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="144" to="161" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-End Incremental Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-08" />
			<biblScope unit="volume">11216</biblScope>
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XII</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11215</biblScope>
			<biblScope unit="page" from="556" to="572" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved Regularization of Convolutional Neural Networks with Cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-05-26" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lifelong Learning via Progressive Distillation and Retrospection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-08" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="452" to="467" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning a Unified Classifier Incrementally via Rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Guided Dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Keshari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27" />
			<biblScope unit="page" from="4065" to="4072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D.</editor>
		<editor>and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1866" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="58" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Without Forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-10-11" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="614" to="629" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning without Forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient Episodic Memory for Continual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>; I.; Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
	<note>Guyon</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision</title>
		<editor>Ferrari, V.</editor>
		<editor>Hebert, M.</editor>
		<editor>Sminchisescu, C.</editor>
		<editor>and Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings, Part IV</title>
		<meeting>Part IV<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="72" to="88" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7765" to="7773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">419</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">iCaRL: Incremental Classifier and Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5533" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<idno>abs/1606.04671</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Networks. CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progress &amp; Compress: A scalable framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. G.</editor>
		<editor>and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4535" to="4544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overcoming Catastrophic Forgetting with Hard Attention to the Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. G.</editor>
		<editor>and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V. ; Z</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>Ghahramani</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Tolias</surname></persName>
		</author>
		<idno>abs/1904.07734</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large Scale Incremental Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lifelong Learning with Dynamically Expandable Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Continual Learning Through Synaptic Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D.</editor>
		<editor>and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Class-incremental Learning via Deep Model Consolidation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tasci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting><address><addrLine>Snowmass Village, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-03-01" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1120" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Maintaining Discrimination and Fairness in Class Incremental Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13205" to="13214" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
