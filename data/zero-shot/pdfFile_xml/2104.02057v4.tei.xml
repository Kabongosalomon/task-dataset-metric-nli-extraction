<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study of Training Self-Supervised Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kaiming He Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kaiming He Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Study of Training Self-Supervised Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code: https://github.com/facebookresearch/moco-v3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: selfsupervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other selfsupervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised pre-training has revolutionized natural language processing (NLP) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4]</ref>. In computer vision, the un-/self-supervised pre-training paradigms differ from their NLP counterparts in at least two aspects: (i) the learners in NLP are masked auto-encoders, while in vision the recently popular choices are Siamese networks (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>); (ii) the backbone architectures in NLP are self-attentional Transformers <ref type="bibr" target="#b42">[43]</ref>, while in vision the common choice is convolutional <ref type="bibr" target="#b27">[28]</ref>-yet non-attentionaldeep residual networks (ResNets) <ref type="bibr" target="#b20">[21]</ref>. To complete the big picture of self-supervised learning in vision, and towards closing the gap of pre-training methodology between vision and language, it is of scientific merit to investigate these differences.</p><p>This work focuses on training Transformers with the leading self-supervised frameworks in vision. This investigation is a straightforward extension given the recent progress on Vision Transformers (ViT) <ref type="bibr" target="#b15">[16]</ref>. In contrast to prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> that train self-supervised Transformers with masked auto-encoding, we study the frameworks that are based on Siamese networks, including MoCo <ref type="bibr" target="#b19">[20]</ref> and *: equal contribution. framework model params acc. (%) linear probing: iGPT <ref type="bibr" target="#b8">[9]</ref> iGPT-L 1362M 69.0 iGPT <ref type="bibr" target="#b8">[9]</ref> iGPT-XL 6801M 72.0 MoCo v3</p><p>ViT-B 86M 76.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo v3</head><p>ViT-L 304M 77. <ref type="bibr" target="#b5">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo v3</head><p>ViT-H 632M 78.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo v3</head><p>ViT-BN-H 632M 79.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo v3</head><p>ViT-BN-L/7 304M 81.0 end-to-end fine-tuning: masked patch pred. <ref type="bibr" target="#b15">[16]</ref> ViT-B 86M 79.9 ? MoCo v3</p><p>ViT-B 86M 83.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo v3</head><p>ViT-L 304M 84.1 <ref type="table">Table 1</ref>.</p><p>State-of-the-art Self-supervised Transformers in ImageNet classification, evaluated by linear probing (top panel) or end-to-end fine-tuning (bottom panel). Both iGPT <ref type="bibr" target="#b8">[9]</ref> and masked patch prediction <ref type="bibr" target="#b15">[16]</ref> belong to the masked auto-encoding paradigm. MoCo v3 is a contrastive learning method that compares two (224?224) crops. ViT-B, -L, -H are the Vision Transformers proposed in <ref type="bibr" target="#b15">[16]</ref>. ViT-BN is modified with BatchNorm, and "/7" denotes a patch size of 7?7. ? : pre-trained in JFT-300M. others <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Unlike standard convolutional networks whose training practice has been extensively studied thanks to continuous community effort, ViT models are new and their recipes are yet to be established. In this work, we go back to basics and investigate the fundamental components of training deep neural networks: the batch size, learning rate, and optimizer. We find that under various cases, instability is a major issue that impacts self-supervised ViT training.</p><p>Interestingly, we observe that unstable ViT training may not result in catastrophic failure (e.g., divergence); instead, it can cause mild degradation in accuracy (e.g., 1?3%). Such a degree of degradation may not be noticeable, unless a more stable counterpart is available for comparison. To the best of our knowledge, this phenomena is rare in the literature of training convolutional networks 1 , and we believe this problem and its hidden degradation are worth noticing.</p><p>To demonstrate the possible harm of instability, we investigate a simple trick that can improve stability in practice. Based on an empirical observation on gradient changes, we freeze the patch projection layer in ViT, i.e., we use fixed random patch projection. We empirically show that this trick alleviates the instability issue in several scenarios and consistently increases accuracy.</p><p>We benchmark and ablate self-supervised ViT in a variety of cases. We provide ViT results in several selfsupervised frameworks. We conduct ablations on architecture designs and discuss the implications. Furthermore, we explore scaling up the ViT models, including the non-trivial ViT-Large and ViT-Huge <ref type="bibr" target="#b15">[16]</ref> -the latter has 40? more computation than ResNet-50 <ref type="bibr" target="#b20">[21]</ref>. Based on these experimental results, we discuss both the currently positive evidence as well as the challenges and open questions.</p><p>We report that self-supervised Transformers can achieve strong results using a contrastive learning framework, compared against masked auto-encoding <ref type="table">(Table 1</ref>). This behavior of Transformers differs from the existing trend in NLP. Moreover, as a promising signal, our bigger self-supervised ViT can achieve better accuracy, unlike the ImageNetsupervised ViT in <ref type="bibr" target="#b15">[16]</ref> whose accuracy degrades if getting bigger. For instance, for the very big ViT-Large, our self-supervised pre-training can outperform its supervised pre-training counterpart for transfer learning in certain cases. This presents a proof-of-concept scenario where selfsupervised pre-training is needed.</p><p>In addition, we report that our self-supervised ViT models have competitive results vs. the big convolutional ResNets in prior art <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. On one hand, this comparison shows the potential of ViT, especially considering that it achieves these results using relatively "fewer inductive biases" <ref type="bibr" target="#b15">[16]</ref>. On the other hand, we suggest that there could be room for self-supervised ViT models to further improve. As one example, we observe that removing the position embedding in ViT only degrades accuracy by a small margin. This reveals that self-supervised ViT can learn strong representations without the positional inductive bias, but it also implies that the positional information has not been sufficiently exploited.</p><p>In summary, we believe that the evidence, challenges, and open questions in this study are worth knowing, if selfsupervised Transformers will close the gap in pre-training between vision and language. We hope our data points and experience will be useful to push this frontier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised visual representation learning. In computer vision, contrastive learning <ref type="bibr" target="#b18">[19]</ref> has become increasingly successful for self-supervised learning, e.g., <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>. The methodology is to learn representations that attract similar (positive) samples and dispel different (negative) samples. The representations from contrastive self-supervised pre-training can outperform their supervised counterparts in certain tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Contrastive learning is commonly instantiated as some forms of Siamese networks <ref type="bibr" target="#b2">[3]</ref>. Recently, a series of works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref> retain the Siamese architectures but eliminate the requirement of negative samples. The success of these methods suggest that it is of central importance to learn invariant features by matching positive samples.</p><p>Transformers. Transformers <ref type="bibr" target="#b42">[43]</ref> were originally introduced for machine translation and later became a dominant backbone in NLP <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4]</ref>. The long-range, selfattentional behavior makes Transformers an effective tool given the non-local, relational nature of languages.</p><p>There have been continuous efforts on generalizing Transformers to computer vision <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref>. The recent work on Vision Transformers (ViT) <ref type="bibr" target="#b15">[16]</ref> greatly pushes this frontier. ViT is purely Transformer-based, rather than interlaced with non-degenerated (i.e., non-1?1) convolutions. <ref type="bibr" target="#b1">2</ref> This largely closes the architectural gap between NLP and vision. ViT achieves compelling accuracy in supervised learning, especially with large-scale data and highcapacity models. Given these properties, we believe ViT is a must-study baseline for self-supervised learning in computer vision.</p><p>Self-supervised Transformers for vision. In pioneering works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>, training self-supervised Transformers for vision problems in general follows the masked auto-encoding paradigm in NLP <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15]</ref>  <ref type="table">(Table 1</ref>). iGPT <ref type="bibr" target="#b8">[9]</ref> masks and reconstructs pixels, and the self-supervised variant of ViT in <ref type="bibr" target="#b15">[16]</ref> masks and reconstructs patches. In this work, we focus on training Transformers in the contrastive/Siamese paradigm, in which the loss is not defined for reconstructing the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MoCo v3</head><p>We introduce a "MoCo v3" framework that facilitates our study. MoCo v3 is an incremental improvement of MoCo v1/2 <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref>, and we strike for a better balance between simplicity, accuracy, and scalability. The pseudocode of MoCo v3 is in Alg. 1, described next.</p><p>As common practice (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref>), we take two crops for each image under random data augmentation. They are encoded by two encoders, f q and f k , with output vectors q and k. Intuitively, q behaves like a "query" <ref type="bibr" target="#b19">[20]</ref>, and the goal of learning is to retrieve the corresponding "key". This is formulated as minimizing a contrastive loss function <ref type="bibr" target="#b18">[19]</ref>. We adopt the form of InfoNCE <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_0">Lq = ? log exp(q?k + /? ) exp(q?k + /? ) + k ? exp(q?k ? /? ) .<label>(1)</label></formula><p>Here k + is f k 's output on the same image as q, known as from other images, known as q's negative samples. ? is a temperature hyper-parameter <ref type="bibr" target="#b44">[45]</ref> for 2 -normalized q, k.</p><p>Following <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, in MoCo v3 we use the keys that naturally co-exist in the same batch. We abandon the memory queue <ref type="bibr" target="#b19">[20]</ref>, which we find has diminishing gain if the batch is sufficiently large (e.g., 4096). With this simplification, the contrastive loss in (1) can be implemented by a few lines of code: see ctr(q, k) in Alg. 1. We adopt a symmetrized loss <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>: ctr(q1, k2)+ctr(q2, k1).</p><p>Our encoder f q consists of a backbone (e.g., ResNet, ViT), a projection head <ref type="bibr" target="#b9">[10]</ref>, and an extra prediction head <ref type="bibr" target="#b17">[18]</ref>; the encoder f k has the backbone and projection head, but not the prediction head. f k is updated by the movingaverage of f q <ref type="bibr" target="#b19">[20]</ref>, excluding the prediction head.</p><p>As a reference, we examine the MoCo v3 accuracy with ResNet-50 (R50) (detailed in appendix). This The improvement here is mainly due to the extra prediction head and large-batch (4096) training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stability of Self-Supervised ViT Training</head><p>In principle, it is straightforward to replace a ResNet backbone with a ViT backbone in the contrastive/Siamese self-supervised frameworks. But in practice, a main challenge we have met is the instability of training.</p><p>We observe that the instability problem can not be simply reflected by accuracy numbers. In fact, as we will show, the training is "apparently good" and provides decent results, even when it is potentially unstable. To reveal the instability, we monitor the kNN curves <ref type="bibr" target="#b44">[45]</ref>   training. In Sec. 4.1, we study how the basic factors influence stability. The curves suggest that the training can be "partially successful", or in other words, "partially failed". In Sec. 4.2, we explore a simple trick that can improve stability. As a result, the accuracy is improved in various cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Empirical Observations on Basic Factors</head><p>Batch size. ViT models in <ref type="bibr" target="#b15">[16]</ref> are by design computationally heavy (see <ref type="table">Table 2</ref> and 3), and large-batch training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> is a desirable solution to big ViT models.</p><p>A large batch is also beneficial for accuracy in recent selfsupervised learning methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>. <ref type="figure" target="#fig_0">Fig. 1</ref> presents the training curves with different batch sizes. A batch of 1k and 2k produces reasonably smooth curves, with 71.5% and 72.6% linear probing accuracy. In this regime, the larger batch improves accuracy thanks to more negative samples <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref>. The curve of a 4k batch becomes noticeably unstable: see the "dips" in <ref type="figure" target="#fig_0">Fig. 1</ref>. It has 72.2% linear probing accuracy. Although this seems to be a marginal decrease vs. the 2k batch, its accuracy is harmed by the instability, as we will show in the next subsection.</p><p>The curve of a 6k batch has worse failure patterns (big dips in <ref type="figure" target="#fig_0">Fig. 1</ref>). We hypothesize that the training is partially restarted and jumps out of the current local optimum, then seeks a new trajectory. As a consequence, the training does not diverge, but the accuracy depends on how good the local restart is. When this partial failure happens, it still provides an apparently decent result (69.7%). This behavior is harmful to explorative research: unlike catastrophic failure that is easily noticeable, the small degradation can be fully hidden.</p><p>We also find that the mild instability does not result in a noticeably large variation. In many of our ablations, running the same configuration for a second trial often results in a small difference of 0.1?0.3%. This also makes it difficult to notice the potential degradation caused by instability.</p><p>Learning rate. In practice, the learning rate is often scaled when the batch size increases <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref>. In all experiments in this paper, we adopt the linear scaling rule <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref>: we set the learning rate as lr?BatchSize/256, where lr is a "base" learning rate. lr is the hyper-parameter being set <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>. In <ref type="figure">Fig. 2</ref> we study the influence of lr. When lr is smaller, the training is more stable, but it is prone to under-fitting. In <ref type="figure">Fig. 2</ref>, lr=0.5e-4 has 1.8% worse accuracy than lr=1.0e-4 (70.4 vs. 72.2). In this regime, the accuracy is determined by fitting vs. under-fitting. Training with a larger lr becomes less stable. <ref type="figure">Fig. 2</ref> shows that lr=1.5e-4 for this setting has more dips in the curve, and its accuracy is lower. In this regime, the accuracy is determined by stability.</p><p>Optimizer. By default, we use AdamW <ref type="bibr" target="#b30">[31]</ref> as the optimizer, which is the common choice for training ViT models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b35">36]</ref>. <ref type="bibr" target="#b2">3</ref> On the other hand, recent self-supervised methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref> are based on the LARS optimizer <ref type="bibr" target="#b46">[47]</ref> for large-batch training. In <ref type="figure">Fig. 3</ref>, we study the LAMB optimizer <ref type="bibr" target="#b47">[48]</ref>, which is an AdamW-counterpart of LARS.</p><p>Given an appropriate learning rate (lr=5e-4, <ref type="figure">Fig. 3</ref>), LAMB achieves slightly better accuracy (72.5%) than AdamW. But the accuracy drops rapidly when lr is larger than the optimal value. LAMB with lr=6e-4 and 8e-4 has 1.6% and 6.0% lower accuracy. Interestingly, the training curves are still smooth, but they degrade gradually in the middle. We hypothesize that although LAMB can avoid sudden change in the gradients, the negative impact of unreliable gradients is accumulated.</p><p>During our exploration, we find that LAMB can achieve comparable accuracy with AdamW, if lr is appropriately chosen. But the sensitivity to lr makes it difficult to ablate <ref type="figure">Figure 4</ref>. We monitor the gradient magnitude, shown as relative values for the layer. A "spike" in the gradient causes a "dip" in the training curve. We observe that a spike happens earlier in the first layer, and are delayed by tens of iterations in the last layers.  different architecture designs without extra lr search. As a result, we opt to use AdamW in other parts of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A Trick for Improving Stability</head><p>All these experiments suggest that instability is a major issue. Next we describe a simple trick that can improve stability in various cases in our experiments.</p><p>During training, we notice that a sudden change of gradients (a "spike" in <ref type="figure">Fig. 4</ref>) causes a "dip" in the training curve, which is as expected. By comparing all layers' gradients, we observe that the gradient spikes happen earlier in the first layer (patch projection), and are delayed by couples of iterations in the last layers (see <ref type="figure">Fig. 4</ref>). Based on this observation, we hypothesize that the instability happens earlier in the shallower layers. Motivated by this, we explore freezing the patch projection layer during training. In other words, we use a fixed random patch projection layer to embed the patches, which is not learned. This can be easily done by applying a stop-gradient operation right after this layer.</p><p>Comparisons. In <ref type="figure" target="#fig_3">Fig. 5</ref> we show the MoCo v3 results with learnable vs. random patch projection. Random patch projection stabilizes training, with smoother and better training curves. This stability benefits the final accuracy, boosting the accuracy by 1.7% to 73.4% at lr=1.5e-4. The improvement is bigger for a larger lr (0.4%, 0.6%, 1.7%). This comparison confirms that the training instability is a main issue that impacts accuracy. Besides MoCo, we find that other related methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref> can also be unstable. <ref type="figure" target="#fig_4">Fig. 6</ref> presents the training curves of ViT in SimCLR <ref type="bibr" target="#b9">[10]</ref> and BYOL <ref type="bibr" target="#b17">[18]</ref>. Random patch projection improves stability in both SimCLR and BYOL, and increases the accuracy by 0.8% and 1.3%. We also observe the instability issue for SwAV <ref type="bibr" target="#b6">[7]</ref>, in which, however, the loss diverges (NaN) when it is unstable. Random patch projection helps SwAV by enabling a relatively larger lr without diverging, and improves its accuracy from 65.8% to 66.4% when using the largest stable lr. In sum, this trick is effective in all these self-supervised frameworks.</p><p>We have also tried BatchNorm (BN) <ref type="bibr" target="#b23">[24]</ref>, WeightNorm (WN) <ref type="bibr" target="#b39">[40]</ref>, or gradient clip on patch projection. We observe that BN or WN on the learnable patch projection layer does not improve instability, and produces similar results; gradient clip on this layer is useful if given a sufficiently small threshold, which to the extreme becomes freezing the layer.</p><p>Discussions. It is an interesting observation that it is not necessary to train the patch projection layer. For the standard ViT patch size, the patch projection matrix is complete (768-d output for a 3-channel 16?16 patch) or overcomplete. In this case, random projection should be sufficient to preserve the information of the original patches.</p><p>We note that freezing the first layer does not change the architecture, and it actually narrows down the solution space. This indicates that the underlying problem is on optimization. The trick alleviates the issue, but does not solve it. The model can still be unstable if lr is too big. The first layer is unlikely the essential reason for the instability; instead, the issue concerns all layers. The first layer is merely easier to be handled separately, e.g., it is the only non-Transformer layer in the backbone. We hope to see a more fundamental solution in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>This section describes the details of ViT+MoCo v3. More subtleties are described in the appendix.</p><p>Optimizer. By default we use AdamW <ref type="bibr" target="#b30">[31]</ref> and a batch size of 4096 <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>. We search for lr and weight decay wd based on 100-epoch results, and then apply it for longer training. We adopt learning rate warmup <ref type="bibr" target="#b16">[17]</ref> for 40 epochs (as per "warmup of 10k steps", <ref type="table" target="#tab_6">Table 4</ref> in <ref type="bibr" target="#b15">[16]</ref>). This long warmup helps alleviate instability, though all unstable results are already with this warmup. After warmup, lr follows a cosine decay schedule <ref type="bibr" target="#b29">[30]</ref>.</p><p>MLP heads. The projection head <ref type="bibr" target="#b9">[10]</ref> is a 3-layer MLP, following <ref type="bibr" target="#b10">[11]</ref>. The prediction head <ref type="bibr" target="#b17">[18]</ref> is a 2-layer MLP. The hidden layers of both MLPs are 4096-d and are with ReLU <ref type="bibr" target="#b31">[32]</ref>; the output layers of both MLPs are 256-d, without ReLU. In MoCo v3, all layers in both MLPs have BN <ref type="bibr" target="#b22">[23]</ref>, following SimCLR <ref type="bibr" target="#b9">[10]</ref>. The MLP heads of BY-OL/SwAV have different BN designs (see appendix).</p><p>Loss. We scale the contrastive loss in (1) by a constant 2? (see Alg. 1), following <ref type="bibr" target="#b17">[18]</ref>'s appendix. This scale is redundant because it can be absorbed by adjusting lr and wd. But this scale makes it less sensitive to the ? value when lr and wd are fixed. We set ? =0.2 <ref type="bibr" target="#b11">[12]</ref> as the default.</p><p>ViT architecture. We closely follow the designs in <ref type="bibr" target="#b15">[16]</ref>. The input patch size is 16?16 or 14?14 ('/16' or '/14'), and after projection it results in a sequence of length 196 or 256 for a 224?224 input. Position embeddings are added to the sequence, and we use the sine-cosine variant <ref type="bibr" target="#b42">[43]</ref> in 2-D. This sequence is concatenated with a learnable class token. The sequence is then encoded by a stack of Transformer blocks <ref type="bibr" target="#b42">[43]</ref> following the design in <ref type="bibr" target="#b15">[16]</ref>. The class token after the last block (and after the final LayerNorm <ref type="bibr" target="#b0">[1]</ref>) is treated as the output of the backbone, and is the input to the MLP heads.</p><p>Linear probing. Following common practice, we evaluate the representation quality by linear probing. After selfsupervised pre-training, we remove the MLP heads and train a supervised linear classifier on frozen features. We use the SGD optimizer, with a batch size of 4096, wd of 0, and sweep lr for each case. We train this supervised classifier for 90 epochs in the ImageNet training set, using only random resized cropping and flipping augmentation. We evaluate single-crop top-1 accuracy in the validation set.  <ref type="table">Table 2</ref>. Configurations of ViT models in our experiments. Here "blocks" is the number of Transformer blocks, "dim" is the input/output channel dimension of all blocks, and "heads" is the number of heads in multi-head attention. The MLP hidden dimension is 4?dim.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section we benchmark and ablate self-supervised ViT. We perform self-supervised training on the 1.28M Im-ageNet training set <ref type="bibr" target="#b13">[14]</ref>, and evaluate by linear probing. <ref type="table">Table 2</ref> summarizes the ViT configurations we study. ViT-B/L/H follow <ref type="bibr" target="#b15">[16]</ref>, and ViT-S is similar to that in <ref type="bibr" target="#b41">[42]</ref>. We use ViT-B by default in our ablations.</p><p>Training time. We train our models in TPUs (v3) that are publicly available in Google Cloud Platform (GCP). <ref type="table" target="#tab_5">Table 3</ref> summarizes the training time (per 100 epochs). It takes 2.1 hours training ViT-B for 100 epochs, and our ablations typically take 6.3 hours each (300 epochs). This is a competitive performance, as it enables us to ablate many design decisions. The TPU implemenataion also makes it possible to explore the ViT-H model, which takes 9.8 hours per 100 epochs using 512 TPUs. This is a gigantic scale of training: for the 300-epoch ViT-H, this amounts to ?625 TPU?days, or ?1.7 TPU?years of training.</p><p>We also verify our models in GPUs using PyTorch. It takes 24 hours for ViT-B in 128 GPUs (vs. 2.1 hours in 256 TPUs). With an increasing number of devices, we observe that TPUs scale up more favorably than GPUs. While further engineering optimization could speed up our GPU system, we opt to use the TPU system for the ease of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Self-supervised Learning Frameworks</head><p>We benchmark self-supervised ViT in four frameworks: MoCo v3, SimCLR <ref type="bibr" target="#b9">[10]</ref>, BYOL <ref type="bibr" target="#b17">[18]</ref>, and SwAV <ref type="bibr" target="#b6">[7]</ref>.We use the same random projection trick in all cases. We sweep lr and wd for each individual framework for fair comparisons.  <ref type="table" target="#tab_6">Table 4</ref>. ViT-S/16 and ViT-B/16 in different self-supervised learning frameworks (ImageNet, linear probing). R-50 results of other frameworks are from the improved implementation in <ref type="bibr" target="#b12">[13]</ref>. For fair comparisons, all are pre-trained with two 224?224 crops for each image (multi-crop training <ref type="bibr" target="#b6">[7]</ref> could improve results, which is beyond the focus of this work). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablations of ViT + MoCo v3</head><p>Next we ablate the designs of the ViT + MoCo v3 system. We use random patch projection in all ablations. The learned version works well, but not better than sin-cos. Surprisingly, the model works decently even with no position embedding (74.9%). The capability to encode positions contributes only 1.6%. We believe this data point reveals both strengths and limitations of the current model. On the positive side, it suggests that the model can learn strong representations just by a set of patches, which are fully permutation-invariant. This is analogous to bag-ofwords models <ref type="bibr" target="#b40">[41]</ref>. This model has no positional inductive bias. On the negative side, it also suggests that the model has not made good use of positions, and the gesture of the object contributes relatively little to the representation. We hope this data point will draw attention to future study.</p><p>Class token. This comparison indicates that the class token is not essential for the system to work. It also suggests that the choice of normalization layers can make a difference. Removing the prediction MLP head has a decent result of 75.5%. While this extra head boosts accuracy, MoCo as a contrastive method does not need the predictor MLP to work, unlike the negative-free methods in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Momentum encoder. The optimal value is m=0.99 (our default). The case of m=0 is analogous to SimCLR (plus the prediction head and stop-gradient on the keys), and its accuracy of 74.3% is similar to SimCLR's (73.9%, <ref type="table" target="#tab_6">Table 4</ref>). The usage of the momentum encoder leads to 2.2% increase.</p><p>Training length. In the following The smaller ViT-S enjoys the benefit of training longer, and improves by 0.9% when extending to 600 epochs. This is similar to the behavior of R50, which was typically trained for 800 epochs <ref type="bibr" target="#b9">[10]</ref>. But the gain of training longer is diminishing on ViT-B. Based on this ablation, we train the bigger ViT-L/H for 300 epochs presented next <ref type="table">(Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparisons with Prior Art</head><p>Self-supervised Transformers. <ref type="table">Table 1</ref> in Sec. 1 presents MoCo v3 results with different ViT models, compared with state-of-the-art self-supervised Transformers. Both iGPT <ref type="bibr" target="#b8">[9]</ref> and the masked patch prediction in <ref type="bibr" target="#b15">[16]</ref> can be categorized as the masked auto-encoding paradigm (e.g., GPT <ref type="bibr" target="#b36">[37]</ref>   <ref type="table">Table 1</ref> in <ref type="bibr" target="#b10">[11]</ref>, and BYOL results are from <ref type="table">Table 1</ref> in <ref type="bibr" target="#b17">[18]</ref>.</p><p>and BERT <ref type="bibr" target="#b14">[15]</ref>). Our MoCo-based ViT has higher accuracy and smaller models than iGPT, under the same linear probing protocol and training data. The mask patch prediction in <ref type="bibr" target="#b15">[16]</ref> is pre-trained on JFT-300M and end-to-end fine-tuned in ImageNet, which we append as a reference. Our self-supervised ViT models have higher accuracy when the models are bigger. This is in contrast to the supervised results in <ref type="bibr" target="#b15">[16]</ref>, where ViT-L has lower accuracy than ViT-B when pre-trained in ImageNet-1k/21k. Actually, for ViT-L, our self-supervised pre-training with linear probing (77.6%) is better than the supervised counterpart in <ref type="bibr" target="#b15">[16]</ref> (76.53%) when trained in ImageNet-1k. <ref type="bibr" target="#b3">4</ref> These comparisons suggest that self-supervised learning as a tool for generic representation learning is less prone to over-fitting.</p><p>Comparisons with big ResNets. <ref type="bibr" target="#b4">5</ref> In <ref type="figure" target="#fig_7">Fig. 8</ref> we compare with the state-of-the-art big ResNets reported by SimCLR v2 <ref type="bibr" target="#b10">[11]</ref> and BYOL <ref type="bibr" target="#b17">[18]</ref>. We note that both SimCLR v2 and BYOL use a momentum encoder. Our baseline ViT MoCo (the curve of "ViT, MoCo v3") is slightly better than ResNet SimCLR v2 in the small-model regime, but the envelopes become just comparable for larger models. SimCLR v2 with SK-ResNet (Selective Kernel <ref type="bibr" target="#b28">[29]</ref>, a form of attention) has a higher envelope. BYOL also has a higher envelope with wider ResNets (1-4?), and has an outstanding point with a deeper ResNet (R200-2?).  CIFAR-100 <ref type="bibr" target="#b25">[26]</ref> Oxford Flowers-102 <ref type="bibr" target="#b32">[33]</ref> Oxford-IIIT-Pets <ref type="bibr" target="#b34">[35]</ref>   <ref type="table">Table 5</ref>. End-to-end fine-tuning accuracy (%) in ImageNet-1k.</p><p>We notice that this comparison concerns a composition of many choices. As one example, the default ViT backbone in <ref type="bibr" target="#b15">[16]</ref> uses LayerNorm (LN), while the default ResNet <ref type="bibr" target="#b20">[21]</ref> uses BatchNorm (BN). These design choices can lead to a systematic gap. In our preliminary experiments, we explore replacing LN with BN in the ViT backbone's MLP blocks (i.e., excluding self-attention blocks). <ref type="bibr" target="#b5">6</ref> We simply refer to this as a "ViT-BN" backbone. It leads to ?1% improvement consistently (see <ref type="figure" target="#fig_7">Fig. 8</ref>).</p><p>In iGPT <ref type="bibr" target="#b8">[9]</ref>, accuracy can be improved by using longer sequences in the pixel domain. Here we explore longer sequences by reducing the patch size to 7?7 ("/7" in <ref type="figure" target="#fig_7">Fig. 8)</ref>. This keeps the model size unchanged, but increases FLOPs to ?6?. It can improve the accuracy by ?2-3%. The gain of using small patches is also observed in <ref type="bibr" target="#b7">[8]</ref>. MoCo v3 achieves 81.0% with ViT-BN-L/7. <ref type="bibr" target="#b6">7</ref> As a comparison, the previous best results under the linear probing protocol are 79.8% with SimCLR v2 (SK-ResNet152-3?), and 79.6% with BYOL (ResNet200-2?).</p><p>Discussion. While the bigger self-supervised ViT can achieve better accuracy, the results are saturated. This is unlike the trend in NLP, where bigger Transformers learn better representations (e.g., <ref type="bibr" target="#b3">[4]</ref>). A potential solution is to use more data. The saturation can also be caused by the limited power of the existing instance-based pretext task <ref type="bibr" target="#b44">[45]</ref>. It may be desired to design more difficult pretext tasks.</p><p>Our self-supervised ViT models are competitive with the big convolutional ResNets. It suggests that ViT can learn strong representations with "fewer inductive biases" <ref type="bibr" target="#b15">[16]</ref>. However, we also find that the accuracy only decreases by a bit even if removing the only positional inductive bias (position embedding), suggesting that in our method ViT relies less on positional information than convolutional networks.</p><p>End-to-end fine-tuning. <ref type="table">Table 5</ref> reports end-to-end finetuning results. We use the DeiT codebase <ref type="bibr" target="#b41">[42]</ref> and all its default settings unless specified. MoCo v3 achieves 83.2% with ViT-B under 150-epoch fine-tuning, substantially bet- <ref type="bibr" target="#b5">6</ref> We are trying to replace every LN with BN in the ViT backbone. In preliminary experiments, doing so leads to convergence problems. <ref type="bibr" target="#b6">7</ref> ViT-BN-H/7 is out of memory in our unoptimized implementation.</p><p>ter than DeiT's 81.8% at 300 epochs. In addition, MoCo v3 has 84.1% with ViT-L when finetuned for only 100 epochs with a drop path rate of 0.5. This short schedule demonstrates the effectiveness of MoCo pretraining. We have also found DeiT-L diverges under its default settings, and a different solution may be needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Transfer Learning</head><p>In <ref type="table" target="#tab_11">Table 6</ref> we evaluate transfer learning. We study the four downstream datasets as in <ref type="bibr" target="#b15">[16]</ref>. We fine-tune the models end-to-end, also following <ref type="bibr" target="#b15">[16]</ref>.</p><p>Our self-supervised ViT has better transfer learning accuracy when the model size increases from ViT-B to ViT-L, yet it gets saturated when increased to ViT-H. As a comparison, the ImageNet-supervised ViT in <ref type="bibr" target="#b15">[16]</ref> becomes saturated or overfitted starting at ViT-L. Our self-supervised ViT achieves better results than its ImageNet-supervised counterparts in three of these four datasets.</p><p>The overfitting is more prominent when training the big ViT models from scratch in these small datasets: the accuracy in general decreases with bigger ViT. We also find that the from-scratch ViT results are much worse than their ResNet-counterparts (c.f ., <ref type="table">Table 8</ref> in <ref type="bibr" target="#b9">[10]</ref>) in these small datasets. This suggests that if data are not enough, it is difficult for ViT to learn representations in the lack of inductive biases. Self-supervised pre-training can close this gap and largely reduce overfitting in small datasets.</p><p>Finally, we note that with supervised pre-training in bigger datasets (ImageNet-21k or JFT-300M), the ViT results in <ref type="bibr" target="#b15">[16]</ref> can be better than ours when transferring to these small datasets. A potential future work is to perform self-supervised pre-training for big ViT models in bigger data. This is analogous to the trajectory of unsupervised pre-training in NLP in the past years <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4]</ref>, i.e., both models and datasets are scaled up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have explored training ViT in the recently popular self-supervised frameworks. Our comparisons concern several aspects, including ViT vs. convolutional networks, supervised vs. self-supervised, and contrastive learning vs. masked auto-encoding. We report positive evidence as well as challenges, open questions, and opportunities. We hope our empirical study will be useful for the community to close the gap of pre-training between vision and language.</p><p>Postscript. After the first version of this manuscript, an author of BiT <ref type="bibr" target="#b24">[25]</ref> and ViT <ref type="bibr" target="#b15">[16]</ref>, Lucas Beyer, echoed that "the exact same behaviour" was observed for supervised BiT-ResNet in ImageNet-21k. The instability problem can be more general than the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Implementation Details</head><p>Data augmentation. We follow the good practice in existing works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>. Our augmentation policy includes random resized cropping, horizontal flipping, color jittering <ref type="bibr" target="#b44">[45]</ref>, grayscale conversion <ref type="bibr" target="#b44">[45]</ref>, blurring <ref type="bibr" target="#b9">[10]</ref>, and solarization <ref type="bibr" target="#b17">[18]</ref>. We take two 224?224 crops for each image in each iteration.</p><p>BatchNorm. We use SyncBN as our default BatchNorm implementation, following <ref type="bibr" target="#b9">[10]</ref>. When BN is used, there are two options on batching: (i) all samples and crops are in the same batch, i.e., BN is over 4096?2 crops for 4096 images; and (ii) only different images are in the same batch, i.e., the two crops of the same image are separately forwarded in two 4096 batches. We notice that the code of SimCLR <ref type="bibr" target="#b9">[10]</ref> adopts the former option, while the code in BYOL <ref type="bibr" target="#b17">[18]</ref> adopts the latter. The pseudo-code in our Alg. 1 implies that we adopt the latter. The BN batching size influences the gradient variance, and the two implementations should lead to different results.</p><p>AdamW implementation. We notice that in PyTorch and JAX, the weight decay in AdamW is implemented as "-lr * wd * weight'' (consistent with <ref type="bibr" target="#b30">[31]</ref>), but in TensorFlow it is implemented as "-wd * weight", and wd needs to be scaled beforehand. <ref type="bibr" target="#b7">8</ref> In our TPU/TensorFlow code, we follow the version consistent with <ref type="bibr" target="#b30">[31]</ref>.</p><p>MLP heads in BYOL and SwAV. In our BYOL+ViT implementation, the projection/prediction MLP heads have BN in their hidden layers, but not in their output layers, which faithfully follow <ref type="bibr" target="#b17">[18]</ref>. In our SwAV+ViT implementation, we use no BN in the MLP heads, which is a configuration that performs the best in our SwAV experiments. kNN monitor. The kNN monitor <ref type="bibr" target="#b44">[45]</ref> is a widely used tool in self-supervised learning research. The kNN evaluation was often performed sparsely, e.g., once per epoch. We notice that this may hide the sudden "dips" in the curves.</p><p>To better reveal the sudden changes, we monitor the kNN performance more densely, e.g., every tens of iterations. This is prohibitive even though the kNN classifier does not need training. We adopt a few approximations to make it feasible. We maintain a small memory bank <ref type="bibr" target="#b44">[45]</ref> (whose length is 10% of ImageNet) for the purpose of kNN search. This memory bank is updated per iteration by the features from the training samples (which are augmented images). This memory bank is maintained as a queue similar to <ref type="bibr" target="#b19">[20]</ref>,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>q's positive sample. The set {k ? } consists of f k 's outputs MoCo v3: PyTorch-like Pseudocode # f_q: encoder: backbone + proj mlp + pred mlp # f_k: momentum encoder: backbone + proj mlp # m: momentum coefficient # tau: temperature for x in loader: # load a minibatch x with N samples x1, x2 = aug(x), aug(x) # augmentation q1, q2 = f_q(x1), f_q(x2) # queries: [N, C] each k1, k2 = f_k(x1), f_k(x2) # keys: [N, C] each loss = ctr(q1, k2) + ctr(q2, k1) # symmetrized loss.backward() update(f_q) # optimizer update: f_q f_k = m * f_k + (1-m) * f_q # momentum update: f_k # contrastive loss def ctr(q, k): logits = mm(q, k.t()) # [N, N] pairs labels = range(N) # positives are in diagonal loss = CrossEntropyLoss(logits/tau, labels) return 2 * tau * loss Notes: mm is matrix multiplication. k.t() is k's transpose. The prediction head is excluded from f k (and thus the momentum update).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Training curves of different batch sizes (MoCo v3, ViT-B/16, 100-epoch ImageNet, AdamW, lr=1.0e-4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Training curves of different learning rates (MoCo v3, ViT-B/16, 100-epoch ImageNet, AdamW, batch 4096). Training curves of LAMB optimizer (MoCo v3, ViT-B/16, 100-epoch ImageNet, wd=1e-3, batch 4096).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Random vs. learned patch projection (MoCo v3, ViT-B/16, 100-epoch ImageNet, AdamW, batch 4096). Top: lr=1.0e-4. Bottom: lr=1.5e-4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Random vs. learned patch projection (ViT-B/16, 100epoch ImageNet, AdamW, batch 4096). Top: SimCLR: lr=2e-4, wd=0.1. Bottom: BYOL: lr=1e-4, wd=0.03.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Different self-supervised learning frameworks perform differently between R-50 [21] (x-axis) and ViT-B [16] (y-axis). The numbers are ImageNet linear probing accuracy from Table 4. ferent between ViT-B and R50: see Fig 7. MoCo v3 and SimCLR are more favorable for ViT-B than R50 (above the diagonal line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Position embedding. The following table compares the choice of position embedding (our default is sin-cos):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Comparisons with state-of-the-art big ResNets, presented as parameters-vs.-accuracy trade-off. All entries are pretrained with two 224?224 crops, and are evaluated by linear probing. SimCLR v2 results are from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table com</head><label>com</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>-</cell></row><row><cell cols="4">pares the linear probing accuracy in ImageNet:</cell></row><row><cell>R50, 800-ep</cell><cell cols="2">MoCo v2 [12] MoCo v2+ [13]</cell><cell>MoCo v3</cell></row><row><cell>linear acc.</cell><cell>71.1</cell><cell>72.2</cell><cell>73.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Training</figDesc><table /><note>time of ViT + MoCo v3, per 100 ImageNet- epochs, in our TensorFlow implementation. The FLOPs number (in multiply-adds) is per 224?224 crop, and "vs. R50" is the rela- tive FLOPs vs. ResNet-50 (4.1G).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 reports</head><label>4</label><figDesc>MoCo v3 has better accuracy on ViT than other frameworks. The relative accuracy among these methods is dif-</figDesc><table><row><cell>model</cell><cell cols="2">MoCo v3 SimCLR</cell><cell>BYOL</cell><cell>SwAV</cell></row><row><cell>R-50, 800-ep</cell><cell>73.8</cell><cell>70.4</cell><cell>74.3</cell><cell>71.8</cell></row><row><cell>ViT-S, 300-ep</cell><cell>72.5</cell><cell>69.0</cell><cell>71.0</cell><cell>67.1</cell></row><row><cell>ViT-B, 300-ep</cell><cell>76.5</cell><cell>73.9</cell><cell>73.9</cell><cell>71.6</cell></row></table><note>the results of ViT-S/16 and ViT-B/16.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The following table ablates the role of the class token [CLS] in ViT:</figDesc><table><row><cell>ViT-B, 300-ep</cell><cell cols="3">w/ [CLS] w/o [CLS]; LN+pool w/o [CLS]; pool</cell></row><row><cell>linear acc.</cell><cell>76.5</cell><cell>69.7</cell><cell>76.3</cell></row><row><cell cols="4">Global average pooling is used right after the final block if</cell></row></table><note>[CLS] is not used. ViT has an extra LayerNorm (LN) af- ter the final block [16], and if we keep this LN and remove [CLS], the result is much worse (69.7%). But if we remove this LN and [CLS], the result is nearly unchanged (76.3%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>BatchNorm in MLP heads. Unlike the standard ResNets<ref type="bibr" target="#b20">[21]</ref>, ViT models by default have no BN, and thus all BN layers are in the MLP heads. The following table compares with vs. without BN in the heads:</figDesc><table><row><cell>ViT-B, 300-ep</cell><cell>heads w/ BN</cell><cell>heads w/o BN</cell></row><row><cell>linear acc.</cell><cell>76.5</cell><cell>74.4</cell></row><row><cell cols="3">We have to set the batch size as 2048 when removing BN,</cell></row><row><cell cols="3">otherwise it does not converge. Removing BN reduces ac-</cell></row><row><cell cols="3">curacy by 2.1%. Despite the decrease, this is a completely</cell></row><row><cell cols="3">BN-free system. This data point suggests that BN is not</cell></row><row><cell cols="3">necessary for contrastive learning to work, yet appropriate</cell></row><row><cell cols="2">usage of BN can improve accuracy.</cell><cell></cell></row><row><cell cols="3">Prediction head. MoCo v3 uses a prediction MLP head as</cell></row><row><cell cols="3">per [18]. The next table ablates this design:</cell></row><row><cell>ViT-B, 300-ep</cell><cell>w/ pred. MLP</cell><cell>w/o pred. MLP</cell></row><row><cell>linear acc.</cell><cell>76.5</cell><cell>75.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>table we</head><label>we</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>report ViT-S/B</cell></row><row><cell cols="2">+ MoCo v3 vs. training length:</cell><cell></cell></row><row><cell></cell><cell>300-ep</cell><cell>600-ep</cell></row><row><cell>ViT-S/16</cell><cell>72.5</cell><cell>73.4</cell></row><row><cell>ViT-B/16</cell><cell>76.5</cell><cell>76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>ImNet self-sup., MoCo v3 98.9 ?0.8 99.1 ?1.2 99.1 90.5 ?3.4 91.1 ?4.7 91.2 97.7 ?8.2 98.6 ?8.9 98.8 93.2 ?0.6 93.7 ?0.1 94.2 Transfer learning accuracy (%) in four datasets. All entries are end-to-end fine-tuned [16]. Pre-training are performed in the ImageNet-1k training set. The models are ViT-B/16, ViT-L/16, and ViT-H/14. Results of ImageNet-supervised pre-training are fromTable 3in<ref type="bibr" target="#b15">[16]</ref>. The arrows indicate the changes w.r.t. the ImageNet-supervised counterparts.</figDesc><table><row><cell>pre-train</cell><cell>ViT-B</cell><cell>ViT-L</cell><cell>ViT-H</cell><cell cols="2">ViT-B</cell><cell>ViT-L</cell><cell>ViT-H</cell><cell>ViT-B</cell><cell>ViT-L</cell><cell>ViT-H</cell><cell>ViT-B</cell><cell>ViT-L</cell><cell>ViT-H</cell></row><row><cell>random init.</cell><cell>77.8</cell><cell>77.1</cell><cell>75.9</cell><cell>48.5</cell><cell cols="2">48.3</cell><cell>48.0</cell><cell>54.4</cell><cell>54.3</cell><cell>52.8</cell><cell>40.1</cell><cell>42.8</cell><cell>40.4</cell></row><row><cell>ImNet supervised [16]</cell><cell>98.1</cell><cell>97.9</cell><cell>n/a</cell><cell>87.1</cell><cell cols="2">86.4</cell><cell>n/a</cell><cell>89.5</cell><cell>89.7</cell><cell>n/a</cell><cell>93.8</cell><cell>93.6</cell><cell>n/a</cell></row><row><cell>case</cell><cell cols="2">pre-train</cell><cell cols="4">ViT-S ViT-B ViT-L</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>masked patch pred. [16]</cell><cell cols="2">JFT-300M</cell><cell>-</cell><cell>79.9</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT [42]</cell><cell>-</cell><cell></cell><cell>79.9</cell><cell>81.8</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo v3</cell><cell cols="2">ImageNet-1k</cell><cell>81.4</cell><cell>83.2</cell><cell>84.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See also postscript on a related discussion. arXiv:2104.02057v4 [cs.CV] 16 Aug 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We argue that it is imprecise to simply compare self-attention against "convolutions". Convolutions<ref type="bibr" target="#b27">[28]</ref> by definition have several properties: weight-sharing, locally-connected, translation-equivariant. All projection layers in a self-attention block have all these properties of convolutions, and are equivalent to 1?1 convolutions. The counterpart of self-attention is more appropriately the non-degenerated (e.g., 3?3) convolutions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In original ViT<ref type="bibr" target="#b15">[16]</ref> in JAX, the weight decay is "AdamW style":https://github.com/google/flax/blob/master/flax/optim/adam.py 6000 7000 # iter. gradient, ?-norm first layer last layer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Stronger regularization could reduce over-fitting for supervised ViT<ref type="bibr" target="#b41">[42]</ref>, though regularizing the very big ViT-L/H is yet to be explored.<ref type="bibr" target="#b4">5</ref> Transformers<ref type="bibr" target="#b42">[43]</ref> by design consist of residual blocks<ref type="bibr" target="#b20">[21]</ref>, and thus are a form of residual networks. In the literature on "Transformer vs. ResNet", precisely speaking, the term of "ResNet" refers to the specific design that has non-degenerated (e.g., 3?3) convolutions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW so it requires no extra feature extraction. We use the features</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>from the class token for kNN monitoring, so the monitor is independent of the choice of the head. Other details follow the kNN implementation in <ref type="bibr" target="#b44">[45]</ref>. We find that this approximate kNN monitor is sufficient to reflect the stability of training.</p><p>MoCo v3 for ResNet-50. The implementation follows the good practice in recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. It uses the LARS optimizer <ref type="bibr" target="#b46">[47]</ref> with a 4096 batch <ref type="bibr" target="#b9">[10]</ref>, lr=0.3, wd=1.5e-6. The temperature is ? =1.0. The encoder f k 's momentum coefficient is m=0.996 and increases to 1 with a cosine schedule <ref type="bibr" target="#b17">[18]</ref>. The augmentation is the same as described above.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;Siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner; Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Sam Mc-Candlish</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end object detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Big Transfer (BiT): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video Google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">and Illia Polosukhin. Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
