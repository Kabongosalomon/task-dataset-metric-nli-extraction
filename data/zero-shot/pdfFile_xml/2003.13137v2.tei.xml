<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection of 3D Bounding Boxes of Vehicles Using Perspective Transformation for Accurate Speed Measurement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08">Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Kocur</surname></persName>
							<email>viktor.kocur@fmph.uniba.sk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Applied Informatics</orgName>
								<orgName type="department" key="dep2">Faculty of Mathe-mathics, Physics and Informatics</orgName>
								<orgName type="institution">Comenius University</orgName>
								<address>
									<addrLine>Mlynsk Dolina</addrLine>
									<postCode>841 01</postCode>
									<settlement>Bratislava, Bratislava</settlement>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Ft??nik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Applied Informatics</orgName>
								<orgName type="department" key="dep2">Faculty of Mathe-mathics, Physics and Informatics</orgName>
								<orgName type="institution">Comenius University</orgName>
								<address>
									<addrLine>Mlynsk Dolina</addrLine>
									<postCode>841 01</postCode>
									<settlement>Bratislava, Bratislava</settlement>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kocur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Applied Informatics</orgName>
								<orgName type="department" key="dep2">Faculty of Mathe-mathics, Physics and Informatics</orgName>
								<orgName type="institution">Comenius University</orgName>
								<address>
									<addrLine>Mlynsk Dolina</addrLine>
									<postCode>841 01</postCode>
									<settlement>Bratislava, Bratislava</settlement>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection of 3D Bounding Boxes of Vehicles Using Perspective Transformation for Accurate Speed Measurement</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08">Aug 2020</date>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Machine Vision and Applications manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Traffic Surveillance ? 3D Object Detection ? Deep Learning ? Perspective Transformation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detection and tracking of vehicles captured by traffic surveillance cameras is a key component of intelligent transportation systems. We present an improved version of our algorithm for detection of 3D bounding boxes of vehicles, their tracking and subsequent speed estimation. Our algorithm utilizes the known geometry of vanishing points in the surveilled scene to construct a perspective transformation. The transformation enables an intuitive simplification of the problem of detecting 3D bounding boxes to detection of 2D bounding boxes with one additional parameter using a standard 2D object detector. Main contribution of this paper is an improved construction of the perspective transformation which is more robust and fully automatic and an extended experimental evaluation of speed estimation. We test our algorithm on the speed estimation task of the BrnoCompSpeed dataset. We evaluate our approach with different configurations to gauge the relationship between accuracy and computational costs and benefits of 3D bounding box detection over 2D detection. All of the tested configurations run in real-time and are fully automatic. Compared to other published state-of-the-art fully automatic results our algorithm reduces the mean absolute speed measurement error by 32% (1.10 km/h to 0.75 km/h) and the absolute median error by 40% (0.97 km/h to 0.58 km/h).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent development in commercially available cameras has increased the quality of recorded images and decreased the costs required to install cameras in traffic surveillance scenarios. Automatic traffic surveillance aims to provide information about the surveilled vehicles such as their speed, type and dimensions and as such is an important aspect of intelligent transportation system design. Automatic traffic surveillance system requires an accurate way of detecting the vehicles in the image and an accurate calibration of the recording equipment.</p><p>Standard procedures of camera calibration require a calibration pattern or measurement of distances on the road plane. Dubsk? et al. <ref type="bibr" target="#b8">[8]</ref> proposed a fully automated camera calibration for the traffic surveillance scenario. We use an improved version <ref type="bibr" target="#b36">[33]</ref> of this method to obtain the camera calibration and focus on the accuracy of vehicle detection.</p><p>Object detection is one of the fundamental tasks of computer vision. Recent deep learning techniques have successfully been applied to this task. Deep convolutional neural networks are used to extract features from images and a supplementary structure utilizes these features to detect objects. We opt to use the object detector RetinaNet <ref type="bibr" target="#b21">[21]</ref> as a base framework for object detection as it offers good tradeoff between accuracy and low inference times. RetinaNet uses a structure of so-called anchor boxes for object detection and our method could therefore utilize any other widely used object detection framework based on anchor boxes <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b33">30]</ref>. With minor modifications our method could also utilize emerging object detection frameworks based on keypoint detection <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b42">39]</ref>.</p><p>In this paper we extend our previous work <ref type="bibr" target="#b17">[17]</ref> where we proposed a perspective image transformation which utilizes the geometry of vanishing points in a standard traffic surveillance scenario. The perspective transformation enables us to rectify the image which has two significant effects. The first effect is that this aids the object detection accuracy. The second one is that this enables an intuitive parametrization of the 3D bounding box of a vehicle as a 2D bounding box with one additional parameter. Our method has surpassed the existing state-of-the-art approaches in terms of speed measurement accuracy while being computationally cheaper. The method was mostly automatic, but the construction of the perspective transformation was not robust enough resulting in a need for manual adjustments for some camera angles. Now we propose a new approach which remedies this problem and enables two different transformations to be constructed for a single traffic scene. We also provide an extended study of performance of our method with different configurations to gauge their effects on the speed measurements accuracy and computational costs to offer various options for different computational constraints. We also show that the improved transformation brings improvements in speed measurement accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Measuring speeds of vehicles captured by a monocular camera requires their detection and subsequent tracking followed by measurement of the distance they passed utilizing camera calibration. Connecting these subtasks into a single pipeline is usually trivial so in the last subsection we focus on the available means of evaluating the accuracy of the whole pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Object Detection</head><p>Recent advent of convolutional neural networks had a significant impact on the task of object detection. Two stage approaches such as Faster R-CNN <ref type="bibr" target="#b33">[30]</ref> use a convolutional neural network to generate proposals for objects in image. In the second stage the network determines which of these proposed regions contain objects and regress the boundaries of their bounding boxes.</p><p>Single stage approaches <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b32">29]</ref> work by using a structure of anchor boxes as the output of the network. Each anchor box represents a possible bounding box. Each anchor box has a classification output to determine which object, if any, is in the anchor box and a regression output to align the bounding box to the object. In this approach one object can be covered by mul-tiple anchor boxes so a technique such as non-maximum suppression must be used to leave only one bounding box per object. Current state of the art approaches forego the use of anchor boxes completely and rely on detecting keypoints in the image via heatmaps on the output of the network. CornerNet <ref type="bibr" target="#b19">[19]</ref> detects the two opposite corners of a bounding box and pairs them using an embedding. CenterNet <ref type="bibr" target="#b7">[7]</ref> detects the center of the object and uses regression to determine the dimensions of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vehicle Detection</head><p>In our work we focus on detecting vehicles via their 3D bounding boxes as this approach has been shown to be also beneficial for subsequent tasks such as fine-grained vehicle classification <ref type="bibr" target="#b38">[35,</ref><ref type="bibr" target="#b41">38]</ref> and re-identification <ref type="bibr" target="#b40">[37]</ref>. In the evaluation section of this paper we also show that detecting 3D bounding boxes as opposed to 2D bounding boxes is beneficial to speed measurement accuracy.</p><p>Background subtraction is a common method of detecting vehicles as the traffic surveillance cameras are static. Corral-Soto and Elder <ref type="bibr" target="#b5">[5]</ref> fit a mixture model for the distribution of vehicle dimensions on labeled data. The model is used together with the known geometry of the scene to estimate the vehicle configuration for blobs of vehicles obtained via background subtraction. Similarly, Dubsk? et al. use background subtraction to obtain masks of vehicles. 3D bounding boxes aligned with the vanishing points of the scene are then constructed tangent to these masks. The order of construction of the edges of the bounding box is important and the process may not be stable. This approach has been slightly improved <ref type="bibr" target="#b36">[33]</ref> by using Faster R-CNN object detector <ref type="bibr" target="#b33">[30]</ref> before the background subtraction to determine which blobs are cars. Approaches relying on background subtraction can be sensitive to changing light conditions or vibrations of traffic cameras and may thus not be suitable for some traffic surveillance scenarios.</p><p>Zeng et al. <ref type="bibr" target="#b41">[38]</ref> use a combination of two networks to determine the 3D bounding boxes of vehicles, which are subsequently used to aid in the task of fine-grained vehicle classification. The first network is based on Reti-naNet object detector <ref type="bibr" target="#b21">[21]</ref>. The second network is given the position of 2D bounding boxes obtained by the first network to perform a ROIAlign operation <ref type="bibr" target="#b12">[12]</ref> on a feature map from a separate ResNet network <ref type="bibr" target="#b13">[13]</ref>. This second network then outputs the positions of the vertices of the 3D bounding boxes and is trained as standard regression task with a regularization term which ensures that the bounding box conforms to perspective Detection of 3D Bounding Boxes of Vehicles Using Perspective Transformation for Accurate Speed Measurement 3 geometry. The obtained geometry of the vehicle is then used to extract features for a fusion network which is trained on the task of fine-grained vehicle classification. The whole system is trained on the BoxCars116k <ref type="bibr" target="#b38">[35]</ref> dataset, which contains over 116 thousand images each with one vehicle with annotations containing its 2D and 3D bounding box as well as its make and model. Since the dataset contains 3D bounding box annotations we also utilize it for training.</p><p>Multiple approaches for detecting 3D bounding boxes have been published and evaluated on the KITTI dataset <ref type="bibr" target="#b11">[11]</ref>. This dataset contains videos from the ego-centric view from a vehicle driving in various urban environments. The videos are annotated with 3D bounding boxes of relevant objects such as cars, cyclists and pedestrians.</p><p>Many published approaches rely on modified 2D object detectors. The authors of CenterNet object detector published <ref type="bibr" target="#b7">[7]</ref> an evaluation of a slightly modified version of their detector on the KITTI dataset. Mousavian et al. <ref type="bibr" target="#b30">[28]</ref> use a 2D bounding box and regress orientation and dimensions of vehicles separately and combine them with geometry constraints to obtain a final 3D bounding box. MonoDIS <ref type="bibr" target="#b35">[32]</ref> works by adding a 3D detection head on top of a RetinaNet object detector <ref type="bibr" target="#b21">[21]</ref>. The detection head is trained to regress 10 parameters of the 3D bounding box in a special regime where in each step some parameters are fixed to the ground truth for loss computation. <ref type="bibr">Kim and Kum [16]</ref> propose to use perspective transformation on the image to create a rectified birds eye view of the road plane and find the bounding boxes of vehicles in the transformed image. GS3D <ref type="bibr" target="#b20">[20]</ref> uses the Faster R-CNN <ref type="bibr" target="#b33">[30]</ref> object detector to find a 2D bounding box of a vehicle. Based on statistics of the test set and some geometrical observations of the common self-driving scenario a rough estimate of the 3D bounding box is generated. Information from the rough 3D bounding box is then used to guide further feature extraction with the use of perspective transformation. The extracted features are then used to refine the 3D bounding box. SMOKE <ref type="bibr" target="#b25">[24]</ref> uses a structure similar to CenterNet to detect a keypoint in the projected center of the 3D bounding box and regress the parameters of the 3D bounding box. The network is trained end-to-end using a disentanglement loss similar to MonoDIS. MonoPair <ref type="bibr" target="#b4">[4]</ref> also utilizes a network inspired by CenterNet to detect the 2D and 3D bounding boxes of vehicles. Additionally, the network also detects keypoints which represent the middlepoints between pairs of neighboring vehicles along with regression of the 3D distance of the two vehicles. The network is then trained end-to-end in an uncertaintyaware manner <ref type="bibr" target="#b15">[15]</ref> with the loss incorporating a pair-wise spatial constraint imposed on the detected vehicles and middlepoints.</p><p>The traffic surveillance scenario is significantly different from the autonomous vehicle scenario of the KITTI dataset due to lack of constancy of scene and thus its geometry as well as a different vantage point of the camera. It is therefore not possible to compare approaches for these two tasks directly. Our method shares similarities to the presented works by using a 2D object detector, while exploiting additional constraints that can be assumed in a geometry of a standard traffic surveillance scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Camera Calibration</head><p>In the context of traffic surveillance, camera calibration is necessary to enable measurement of real world distances in the surveilled scene. A review of available methods has been presented by Sochor et al. <ref type="bibr" target="#b37">[34]</ref>. The review found that most published methods are not automatic and require human input such as drawing a calibration pattern on the road <ref type="bibr" target="#b6">[6]</ref>, using positions of line markings on the road <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b27">26]</ref> or some other measured distance related to the scene <ref type="bibr" target="#b34">[31,</ref><ref type="bibr" target="#b39">36]</ref>.</p><p>Ideally, camera calibration can be performed automatically and accurately. Filipiak et al. <ref type="bibr" target="#b10">[10]</ref> proposed an automatic approach based on an evolutionary algorithm, though the approach was validated only on footage zoomed in to obtain clear image of license plates, which is unsuitable for traffic surveillance on multi-lane roads.</p><p>A fully automatic method has been proposed by Dubsk? et al. <ref type="bibr" target="#b8">[8]</ref>. The camera is calibrated by finding the three orthogonal vanishing points related to the road plane. The first vanishing point corresponds to the movement of the vehicles. Relevant keypoints are detected and tracked using the KLT tracker. The tracked lines of motion are then transformed into a so-called diamond space based on parallel coordinates in a fashion similar to the Hough transform. Edges of vehicles which are perpendicular to their movement are used in the same way to determine the position of the second vanishing point. Under the assumption that the principal component is in the center of the image the focal length of the camera is then calculated and subsequently the last vanishing point is determined to be perpendicular to the first two using vector product in homogeneous coordinates. To enable measurements of distances in the road plane a scale factor needs to be determined. The dimensions of the detected vehicles are recorded and their mean is calculated. The mean is compared to statistical data based on typical composition of traffic in the country to obtain the scale. This method has  <ref type="figure">Fig. 1</ref> The diagram of our speed estimation pipeline.</p><p>been further improved by Sochor et al. <ref type="bibr" target="#b36">[33]</ref> by fitting a 3D model of a known common vehicle to its detection in the footage. The detection of the second vanishing point is also improved by using edgelets instead of edges. We opt to use this improved fully-automatic calibration method in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Object Tracking</head><p>To allow for vehicle counting and speed measurement, the vehicles have to be tracked from frame to frame. Since object detection may sometimes fail a robust tracker is necessary. Kalman filter <ref type="bibr">[14]</ref> has been a reliable tool to tackle the task of object tracking in many domains. Bochinksi et al. <ref type="bibr" target="#b0">[1]</ref> have shown that a simple IoU tracker can outperform more complicated trackers when the objects are detected reliably. Based on this we choose to opt for a similar tracking strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Visual Traffic Speed Measurement Datasets</head><p>A review <ref type="bibr" target="#b37">[34]</ref> of existing traffic camera calibration methods, vehicle speed measurement methods and evaluation datasets found that many of the published results are evaluated on small datasets with ground truth known for only few of the surveilled vehicles. Additionally, most of the datasets used in published literature were not publicly available. The authors of the review offer their own dataset called BrnoCompSpeed containing 21 one hour long videos which collectively contain 20 thousand vehicles with known ground truth speeds obtained via laser gates. The authors also provide an evaluation script for this datset. We choose to perform the main evaluation of our method on this dataset. Luviz?n et al. <ref type="bibr" target="#b26">[25]</ref> have published a dataset containing 5 hours of footage from a single intersection. The dataset includes ground truth speeds of vehicles measured by inductive loops installed in the road as well as annotated positions of the license plates of vehicles. The authors provide their own pipeline for speed estimation which is based on detection of license plates.</p><p>The license plates are detected by generating candidate regions around horizontal edges of moving vehicles. These regions are then validated using a T-HOG <ref type="bibr" target="#b29">[27]</ref> descriptor and an SVM classifier. After the license plate is detected a few keypoints located within it are tracked using a pyramidal KLT tracker <ref type="bibr" target="#b1">[2]</ref>. A manually obtained homography matrix is used to determine the real world coordinates of the tracked keypoints and thus also the speed of the vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Algorithm</head><p>The goal of our algorithm is to detect 3D bounding boxes of vehicles recorded with a monocular camera installed above the road plane, track the vehicles and evaluate their speed. The algorithm consists of several stages and it requires the camera to be calibrated as per <ref type="bibr" target="#b36">[33]</ref>. The algorithm is an improvement of our previous work <ref type="bibr" target="#b17">[17]</ref>.</p><p>We will first provide an overview of the whole algorithm and then describe each part in greater detail. The diagram of the whole pipeline can be seen in <ref type="figure">Figure  1</ref>. At first, a perspective transformation is constructed using the positions of the vanishing points of the traffic scene which are known thanks to the calibration. This transformation is applied to every frame of the recording. The transformed frames are used as an input to a RetinaNet object detector which detects vehicles and their 2D bounding boxes with one additional parameter. The output of the object detection network is used for tracking and 3D bounding box construction. Tracking is performed by comparison of the 2D bounding boxes in successive frames using a simple algorithm based on the IoU metric. The 3D bounding boxes are constructed in the transformed frames using the 2D bounding boxes with the additional parameter. Inverse perspective transformation is then used to transform the 3D bounding boxes onto the original scene. The center of the bottom frontal edge of every 3D bounding box is used to provide pixel position of a vehicle for each frame. The calibration is then used to project the pixels onto the road plane and thus enable measurement of the distance traveled between frames by one vehicle. These interframe distances are consequently used to measure the speed of the vehicle over the whole track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Transformation</head><p>To construct the image transformation we require the camera to be calibrated as described in <ref type="bibr" target="#b36">[33]</ref>. This calibration method has very few limitations regarding the camera position. The camera has to be positioned above the road plane and the observed road segment has to be straight. The main parameters obtained by the calibration are the positions of the two relevant vanishing points in the image. Assuming that the principal point is in the center of the image, the position of the third vanishing point as well as focal length of the camera can be calculated. This enables us to project any point in the image onto the road plane. To enable measurements of distances on the road plane one additional parameter, denoted as scale, is determined during calibration.</p><p>The first detected vanishing point (denoted further as VP1 ) corresponds to the lines on the road plane which are parallel to the direction of the moving vehicles. The second detected vanishing point (VP2 ) corresponds to the lines which lie on the road plane but are perpendicular to the the direction of the moving vehicles. The third vanishing point (VP3 ) corresponds to the lines which are perpendicular to the road plane.</p><p>The goal of the transformation is to create a new image in which lines corresponding to one of the vanishing points are parallel to one of the image axes and lines corresponding to another vanishing point are parallel to the other image axis. In the transformed image the two vanishing points will thus be ideal points. We also require that the lines corresponding to the last vanishing point remain lines in the transformed image. In order to fulfill these conditions we use the perspective transformation. Since the orientation of the vehicles is closely related to the positions of the vanishing points the vehicles will be rectified in the transformed image.</p><p>In our previous work <ref type="bibr" target="#b17">[17]</ref> we proposed an algorithm which was able to construct such transformation for the pair of vanishing points VP2 and VP3, however we observed that for some camera positions the results were much worse than for the rest. This was caused by an inadequate perspective transformation which resulted in a very small and distorted part of the transformed image to be relevant for detection. At that time we remedied this by significant manual adjustments, which were not automated and therefore undesirable. Furthermore the previous approach failed completely to construct a reasonable transformation for the pair of vanishing points VP1 and VP2.</p><p>Now we propose to remedy this problem in an automated fashion by setting a condition that the transformed image should contain as much relevant information as possible. To satisfy this we propose two following adjustments. Firstly, we use a mask of the surveilled traffic lanes for the construction of the transformation instead of the whole image. For evaluation we use the BrnoCompSpeed dataset <ref type="bibr" target="#b37">[34]</ref> in which the masks are already provided. In other cases the masks can be obtained automatically by utilizing optical flow <ref type="bibr" target="#b9">[9]</ref>. Secondly, we heuristically set a limit that no more than 20% of the pixels in the transformed image should correspond to pixels which lie outside of the mask in the original image. In the evaluation section we show that this approach not only makes the algorithm fully automatic, but also leads to better accuracy for the speed estimation task.</p><p>The construction algorithm of the transformation has the following steps:</p><p>1. Out of the three vanishing points choose either the pair VP1-VP2 or VP2-VP3. The algorithm is visualised in <ref type="figure" target="#fig_0">Fig. 2</ref>. Note that this algorithm may terminate with failure. This is usually the case when the line connecting the two vanishing points intersects the mask. This is problematic since after the transformation the line would have to be transformed to a line that would be perpendicular to itself. This is theoretically possible by transforming the line to infinity, but such a transformation would not be useful. The only way to resolve this is by reducing the mask so that it does not contain the line connecting the vanishing points. In that case even if the algorithm would output a transformation only a small part of the relevant road segment would be visible in the transformed image and would thus be unusable for traffic surveillance. In the BrnoCompSpeed dataset <ref type="bibr" target="#b37">[34]</ref> this does not occur. Since the calibration method requires that the three vanishing points are orthogonal it is safe to assume that the transformation would work well for at least one of the pairs. The method can theoretically be extended to include the transformation for the pair VP1-VP3, but on the BrnoCompSpeed dataset this results in failure of the construction algorithm for multiple videos due to the fact that the line between the two vanishing points usually intersects the scene. Therefore, we dismiss this approach. Removing this pair also has the benefit of simplifying the parametrization of the 3D bounding box presented in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parametrization of the 3D Bounding Boxes</head><p>We aim to detect 3D bounding boxes aligned with the vanishing points. After performing the image transformation from the previous section, 8 of the 12 edges of the bounding box are aligned with the image axes.</p><p>This enables us to describe the 3D bounding box as a 2D bounding box with one additional parameter in an intuitive way. The 2D bounding box is the rectangle which encloses the 3D bounding box.</p><p>The additional parameter denoted as c c is determined by measuring the vertical distance from the top of the 2D bounding box to the top frontal edge of the 3D bounding box and dividing it by the height of the 2D bounding box. This parameter thus always falls into the [0, 1] interval. The construction of the 2D bounding box and the additional parameter can be seen in <ref type="figure" target="#fig_2">Fig.  3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">3D Bounding Box Reconstruction</head><p>The 2D bounding with the c c parameter can be used to reconstruct the 3D bounding box. Here we apply a similar process to the one described in our previous work <ref type="bibr" target="#b17">[17]</ref>, but we generalize it to accommodate to new cases which emerge from the improved way the perspective transformation is obtained.</p><p>The process of reconstruction depends on the position of the vanishing point which was not used for the transformation. Note that the position of this vanishing point has to be known in the transformed image which is easily obtainable by applying the perspective trans- formation. For simplicity, we will denote this vanishing point in the transformed image as VPU.</p><p>Due to the geometry of the vanishing points there are only two possibilities of relative vertical positions of the 2D bounding box and VPU. The box is either above or below the VPU. Let us first consider that VPU is above the 2D bounding box.</p><p>In that case there are only three possibilities regarding the horizontal positions of VPU and the 2D bounding box. If VPU is to the right of the 2D bounding box then the left end of the line segment representing the c c is a vertex of the 3D bounding box. Knowing this vertex one can construct the 3D bounding box in the transformed image. Similarly, if the VPU is to the left of the 2D bounding box then the right end of the line segment is used. If the VPU is neither to the left or to the right of the 2D bounding box then either of them can be used as a corner to start construction of the 3D bounding box. The process is visualized in <ref type="figure" target="#fig_4">Fig. 4</ref> for the case when VPU is to the left of the 2D bounding box.</p><p>In the case when the VPU is below the 2D bounding box the process is almost identical, the only difference is that when VPU is to the left of the 2D bounding box the left end of the c c line segment is used as a starting vertex and vice versa.</p><p>This process may fail to produce a valid 3D bounding box. This can be easily detected during the reconstruction process as in that case a part of at least one of the edges of the 3D bounding box would lie outside of the area enclosed by the 2D bounding box. This failure indicates that there is no valid 3D bounding box for the given parametrization and perspective geometry. Such a situation may occur as it is impossible to guarantee that a neural network outputs only valid outputs. Since this occurs only rarely a simple solution of regarding these outputs as false positives works well enough in practice.</p><p>After the 3D bounding box is constructed in the transformed image an inverse perspective transformation can be applied to the vertices of the 3D bounding box to obtain the 3D bounding box in the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bounding Box Detection</head><p>As shown in the previous subsection we only need to detect 2D bounding boxes with the parameter c c . For this purpose we utilize the RetinaNet object detector <ref type="bibr" target="#b21">[21]</ref>. This detector outputs 2D bounding boxes for the detected objects. We modify the method to add c c to each of the output boxes.</p><p>The RetinaNet <ref type="bibr" target="#b21">[21]</ref>, as well as other object detecting meta-architectures, uses anchor boxes as default positions of bounding boxes to determine where the objects are. The object detection task is separated into three parts: determining which anchor boxes contain which objects, resizing and moving the anchor boxes to better fit the objects and finally performing non-maximum  suppression to avoid multiple detections of the same object. To train the network a two-part loss (1) is used.</p><formula xml:id="formula_0">L tot = 1 N (L conf + L loc )<label>(1)</label></formula><p>The loss is averaged over all N anchor boxes, L conf is the Focal loss used to train a classifier to determine which objects, if any, are in the bounding box. L loc is the regression loss to train the network how to reshape and offset the anchor boxes. To include the parameter c c we simply add one additional regression loss which results in the total loss:</p><formula xml:id="formula_1">L tot = 1 N (L conf + L loc + L c ) .<label>(2)</label></formula><p>The loss L c (3) is identical in the base structure to the loss used for the four regression parameters in the RetinaNet, which is itself based on the regression loss of the SSD object detector <ref type="bibr" target="#b24">[23]</ref>. The loss is calculated as a sum over all of the N anchor boxes and M ground truth bounding boxes. x i,j determines whether the i-th anchor box corresponds to the j-th ground truth label. We subtract the ground truth value of c c denoted as c g c,j from the predicted value c p c,i and apply the smooth L1 function (s L1 ).</p><formula xml:id="formula_2">L c = 1 N N i=1 M j=1 x i,j s L1 c p c,i ? c g c,j<label>(3)</label></formula><p>Note that this approach could be extended to some of the more recent object detection frameworks which rely on keypoint detection <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b42">39]</ref>. However, we opt to use the anchor box based approach as this makes our method universally transferable between different object detection frameworks with widespread use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Object Detector Training</head><p>To obtain training data we use data from two distinct datasets. The first dataset is BoxCars116k <ref type="bibr" target="#b38">[35]</ref>. The original purpose of this dataset is fine-grained vehicle classification. The dataset contains over 116 thousand images, each containing one car along with make and model labels, information on positions of vanishing points and the 3D bounding box of the car. We transform these images with the proposed transformation and calculate the 2D bounding boxes and the c c parameter based on the provided 3D bounding boxes. Since each image is only of one car we augment the images by randomly rescaling them and placing them on a black background.</p><p>The other used dataset is BrnoCompSpeed <ref type="bibr" target="#b37">[34]</ref>. We use the split C of this dataset providing 9 videos for testing and 12 for validation and training. Each video is approximately one hour long with 50 frames per second (with one exception). For training and validation we use only every 25-th frame of the videos. We use the first 30000 frames for validation and the rest (140000-180000 depending on video length) are used for training. The main purpose of this dataset is to evaluate camera calibration and speed measurement algorithms. The cameras have been manually calibrated and thus the positions of the vanishing points are available, however the dataset does not contain 3D bounding box annotations.</p><p>To obtain the necessary 3D bounding box annotation we run these frames through Mask R-CNN <ref type="bibr" target="#b12">[12]</ref> image segmentation network trained on the COCO dataset <ref type="bibr" target="#b22">[22]</ref>. We transform the masks of detected vehicles and the images using our transformation and create the 2D bounding boxes with c c as labels for training. Obtaining a 2D bounding box from a mask is straightforward. The computation of the c c parameter requires a few steps since the masks may not be perfect and the cars are not commonly box shaped. The process begins with drawing the two lines tangent to the mask from both sides originating in VPU (see subsection 3.3). Each of these lines intersect the edges of the 2D bounding box twice. The intersection closer to the VPU is discarded. Thus we have two points on the edges of the 2D bounding box each corresponding to one tangent line. Calculating the c c parameter for the point which lies on one of the vertical edges is straightforward. In case of a point on one of the horizontal edges of the bounding box a vertical line through this point is drawn. Next a line from VPU is drawn to the closest corner of the 2D bounding box. The vertical position of this intersection is then used to determine the c c parameter. In the end we obtain two values for the c c parameter and use the one which creates a line closer to the VPU thus choosing the wider of the two options. For visual reference see <ref type="figure" target="#fig_5">fig. 5</ref>.</p><p>Based on the development of the validation loss during the training we employ early stopping and train our models for 30 epochs each with 10000 training steps. For each pair of vanishing points we train models of three different sizes dependent on the input size of the transformed image. For the pair VP2-VP3 the sizes of the input image in pixels (width ? height) are 960 ? 540, 640 ? 360 and 480 ? 270. For the pair VP1-VP2 we use the same dimensions we just flip them so the bigger dimension is the height of the image. We use the minibatch size of 16 for the models of the two smaller sizes and due to memory constraints a minibatch size of 8 for the largest model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Tracking</head><p>From the object detector we obtain 2D bounding boxes with the parameter c c for vehicles in each frame of the recording. The tracking algorithm begins in the first frame with no active tracks and continues iterating through frames. For each 2D bounding box detected in the frame its IoU against the last 2D bounding box in each active track is calculated. If IoU of a detection is higher than 0.1 for at least one track, then the bounding box is added to the track with highest IoU score. If no track has at least 0.1 IoU against the detection, then a new active track is created. If a track hasn't had any bounding boxes added to it in the last 10 frames, then the track is no longer considered active and is added to the results. To detect speed we filter out bounding boxes which are less than 10 pixels away from the edges of the images. We also discard tracks which have less than 5 detected bounding boxes within them or smaller distance traveled than 100 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Speed Measurement</head><p>In the previous step the 2D bounding boxes with the parameter c c were grouped and filtered into relevant tracks. 3D bounding boxes are reconstructed (see subsection 3.3) for all detections. Knowing the 3D bounding box position in the original image the speed is determined using a point which is in the middle of the frontal bottom edge of the 3D bounding box (see <ref type="figure">Fig. 6</ref>). Since these points should under normal circumstances lie on the road plane, we can use the camera calibration to easily determine the distances between various positions within a track. To detect the average speeds of the vehicles we employ the same method as <ref type="bibr" target="#b37">[34]</ref> by calculating the median of the interframe speeds of the whole track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>The output 3D bounding boxes for two of our models are showcased in <ref type="figure">Fig. 6</ref> along with some cases where our models fail to detect the vehicle accurately.</p><p>We also provide two videos showcasing our 3D detector on the first five minutes of the video titled ses-sion6 center from the test set of the split C of the BrnoCompSpeed dataset <ref type="bibr" target="#b37">[34]</ref>. In the first video (Online Resource 1) we used the vanishing point pair VP2-VP3 and the input size of 640 ? 360 px. In the second video (Online Resource 2) we used the pair VP1-VP2 and the input size of 360 ? 640 px. Note that in both of these videos we keep all of the detections to showcase some of the false positives, which are later removed during tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Speed Measurement Accuracy</head><p>We evaluate our method on the speed measurement task on the split C of the BrnoCompSpeed dataset <ref type="bibr" target="#b37">[34]</ref>. The evaluation metrics can be seen in <ref type="table" target="#tab_1">Table 1</ref> and we provide files for evaluation for all of our presented variants including the ablation experiments online. <ref type="bibr" target="#b0">1</ref> We compare our results to available published results on the same data. We include the original method by Dubsk? et al. <ref type="bibr" target="#b8">[8]</ref> denoted as DubskaAuto. We also include its improved version by Sochor et al. <ref type="bibr" target="#b36">[33]</ref> in two variants: So-chorAuto, which to our knowledge is the most accurate fully automatic method for speed measurement evaluated on the dataset and SochorManual which is more accurate, but includes a manual adjustment of the scale factor during calibration. We also include our previous work <ref type="bibr" target="#b17">[17]</ref> in two variants: Previous3D, which detects 3D bounding boxes aligned with the vanishing points and Previous2D, which detects 2D bounding boxes aligned with just two of the vanishing points. Both of these methods require a manual adjustment of the perspective image transformation for some camera angles to work properly, therefore they can not be considered fully automatic. To our knowledge the results for the method denoted as Previous2D are the best published so far with respect to the speed measurement accuracy on the dataset.</p><p>We report results for our method denoted as Trans-form3D in its six variants described in subsection 3.5. For all of these we report the rate of frames per second that can be processed on a machine with 8-core AMD Ryzen 7 2700 CPU, 48 GB RAM and Nvidia TITAN V GPU. The results show that the variants with the two bigger input sizes using the pair of vanishing points VP2-VP3 outperform all other published methods. The variants using the other pair of vanishing points show worse performance, but are still comparable to the results of SochorAuto.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>To properly gauge the impact of the perspective transformation we perform two ablation experiments. We train the standard RetinaNet 2D object detector on the same data as the other models, except that the images are not transformed. We refer to this model as Orig2D. We also train the standard RetinaNet 2D object detector on the transformed images. We use the same 2D bounding boxes as in Transform3D, but without the parameter c c . We refer to this method as Trans-form2D. We use the center of the bottom edge of the d) e) f)</p><formula xml:id="formula_3">g h ? i ? j) b) c) a) k) l) Fig. 6</formula><p>3D bounding boxes detected on the videos from the test set of the BrnoCompSpeed <ref type="bibr" target="#b37">[34]</ref> dataset. a-f ) Results for the model using the pair VP2-VP3 and input size 640 ? 360 px are on the left and results for VP1-VP2 and 360 ? 640 px are on the right. In images a-c) we can observe that there are only minor differences between the models and these predictions can be considered accurate, however d-f ) show less accurate results. Inaccuracies for the pair VP1-VP2 are greater. g-l) Results for the pair VP2-VP3. Similar results can be obtained also for the other pair. g-i) Various levels of inaccurate placement of the bounding boxes. j) An accurately detected occluded vehicle. k) Sometimes the two vehicles get grouped into one bounding box. l) Occlusion can also sometimes result in a false positive between the two vehicles, however such false positive usually gets filtered out during tracking. 2D bounidng box to determine the speeds. We train these models with the same hyperparameters as our base model. We perform the ablation experiments only for the image size of 640 ? 360 and 360 ? 640 pixels. We also perform an experiment denoted as MaskRCNN3D where we obtain the 3D bounding boxes of vehicles via their mask obtained by using the Mask R-CNN network <ref type="bibr" target="#b12">[12]</ref> pre-trained on the MS COCO dataset <ref type="bibr" target="#b22">[22]</ref>. We construct the 3D bounding boxes in the same manner as described in subsection 3.5. The results of the ablation experiments can be seen in <ref type="table" target="#tab_1">Table 1</ref>. When compared to the results SochorAuto and So-chorManual which rely on Faster R-CNN in combination with background subtraction for detection it is clear that the use of RetinaNet alone (Orig2D ) for detection of vehicles brings significant improvements. Transforming the image (Transform2D ) also brings a minor improvement for the pair VP2-VP3. It is clear that introducing the construction of the 3D bounding box for this pair improves speed measurement accuracy significantly and is thus beneficial for speed measurement tasks.</p><p>Surprisingly, the transformation for the pair VP1-VP2 increases the mean speed measurement error over the non-transformed variant. This may possibly be caused by the rectification of the image resulting in loss of some visual cues important for object localization.</p><p>Results for MaskRCNN3D are worse than the results of our main approach (Transform3D ) for the pair VP2-VP3, but better than the results for other methods and ablation experiments. This ablation experiment provides further evidence that constructing a 3D bounding box is beneficial for the task of speed measurement, since it performed better than all of the three ablation setups which only used 2D bounding boxes. This ablation experiment required no task-specific training, however this came at a significant hit to the FPS performance and may thus not be a cost-effective option for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluating the influence of recall on the speed measurement accuracy</head><p>The speed measurement accuracy metrics presented in <ref type="table" target="#tab_1">Table 1</ref> could potentially be skewed due to variance in recall of the compared methods. A method with higher recall might be able to correctly detect more difficult instances of vehicles. These difficult examples could then lead to a higher speed measurement error which would not be the case for the methods with lower recall. To verify that this effect is not significant we tested all of the compared methods only on a subset of ground truth vehicle tracks which contained only those tracks that <ref type="table">Table 2</ref> The results of the compared methods on the subset of the split C of the BrnoCompSpeed dataset <ref type="bibr" target="#b37">[34]</ref> which contains only those ground truth vehicle tracks that were correctly detected by all of the compared methods. The original test set contains 13 704 tracks while the subset contains only 7 274 tracks. We present these results to indicate that the results in <ref type="table" target="#tab_1">Table 1</ref>  were correctly detected by all of the compared methods. This is equivalent to the largest subset of the ground truth tracks such that all of the compared methods would achieve 100 % recall on it. This subset contains only 7 274 tracks compared to the standard amount of 13 704 tracks in the split C of the BrnoCompSpeed dataset <ref type="bibr" target="#b37">[34]</ref>. We present the results evaluated on the subset in <ref type="table">Table 2</ref>. It is clear that the subset indeed contains easier examples as the accuracy of almost all of the compared methods on the subset is slightly better than on the original test set. The relative performance of the compared methods remains similar to the results on the full test set. Our method Transform3D for the pair VP2-VP3 still outperforms the other compared methods including the results for the ablation experiments. Interestingly, the variant which uses the input size of 640 ? 360 px outperforms the variant with the input size of 960 ? 640 px. The recall of these two variants on the full test set is very similar so we assume that the variant with smaller input size can perform better on easier examples, but worse in more difficult cases thus leading to its worse accuracy on the full test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computational Costs</head><p>All of our variants run at faster than real-time (25 FPS) speeds, though we have to note that the testing videos of the BrnoCompSpeed dataset were recorded with 50 FPS and the speed measurement accuracy can therefore be worse for footage with lower FPS rates. Results show that increasing the input image size tends to result in increased speed measurement accuracy, while also increasing the computational demands reflected by the FPS rates for models of different sizes. Our method is therefore easily configurable to work under different hardware constraints in real-world applications. We were not able to perform FPS measurement for the other published approaches, however the most significant methods SochorAuto and SochorManual both rely on the Faster R-CNN object detector which, in general, is significantly slower than RetinaNet used in our method <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation on dataset by Luviz?n et al.</head><p>To verify that our method is capable of working in various traffic surveillance scenarios we performed an evaluation on the dataset published along with <ref type="bibr" target="#b26">[25]</ref>. In comparison with the BrnoCompSpeed dataset <ref type="bibr" target="#b37">[34]</ref> there is only one intersection filmed under various weather conditions. The ground truth speeds are measured using inductive loops located in the top portion of the scene and the camera is positioned closer to the road plane.</p><p>Since this dataset contains only vehicles moving away from the camera, whereas our models were mostly trained on vehicles moving towards the camera, we fine-tuned the Transform3D model for the VP2-VP3 pair and the input size of 960 ? 540 on training data obtained from this dataset in the same way we produced training data for the BrnoCompSpeed dataset (see subsection 3.5) and the BoxCars116k dataset <ref type="bibr" target="#b38">[35]</ref> for 10 epochs. The authors of the dataset provide no official split for training and testing so we perform tests on the first half of videos in each subset 2 and use the remaining videos for training.</p><p>The speed measurement method proposed in <ref type="bibr" target="#b26">[25]</ref> uses a separate camera calibration for each of the three surveilled lanes. To make a reasonable comparison we use only one set of vanishing points, but we use a different scale for each of the lanes. Our method achieves recall of 98.9% for the vehicles with valid ground truth measurements with 92.7% of measured speeds falling within the range of -3 to +2 km/h speed measurement error proposed as the evaluation metric for the dataset. In comparison, the method proposed in <ref type="bibr" target="#b26">[25]</ref> achieves better results with 99.2% and 96% respectively on the whole dataset. We consider our results to be competitive since our method has fewer limitations such as not requiring the camera to be so close to the road plane for the license plates to be clearly visible and aligned with the camera as well as not requiring a manual calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed several improvements and extensions to our previously published method <ref type="bibr" target="#b17">[17]</ref> for detection of 3D bounding boxes of vehicles in a traffic surveillance scenario. Our improvements eliminate the need to manually adjust the construction of the perspective transformation for some camera angles. We also extended the transformation method to enable using a different pair of vanishing points.</p><p>We have also extended the experimental analysis of our method providing a range of configurations, which allows for a flexibility of choice with respect to the accuracy-speed tradeoff for real-world applications. All of the models can be run in real-time on commercially available GPUs. Configurations relying on smaller input sizes provide a possibility of processing multiple video streams concurrently.</p><p>Our improved fully automatic approach led to an improvement in speed measurement on the BrnoComp-Speed dataset <ref type="bibr" target="#b37">[34]</ref>. Compared to our previously published non-automatic state of the art method <ref type="bibr" target="#b17">[17]</ref> we reduced the mean speed measurement error by 10% (0.83 km/h to 0.75 km/h), the median speed measurement error by 3% (0.60 km/h to 0.58 km/h) and the the 95-th percentile error by 15% (2.17 km/h to 1.84 km/h). Compared to the state of the art fully automatic method <ref type="bibr" target="#b36">[33]</ref> we reduced the mean absolute speed measurement error by 32% (1.10 km/h to 0.75 km/h), the absolute median error by 40% (0.97 km/h to 0.58 km/h) and the 95-th percentile error by 17% (2.22 km/h to 1.84 km/h).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 .</head><label>2</label><figDesc>For each of the selected vanishing points construct two lines which originate in the vanishing point and are tangent to the mask. Thus creating four lines. 3. Find the four intersections of the lines, for the pairs of lines which originate in different vanisihng points. 4. Pair each of the four intersection points with a corner of a rectangle with the desired dimensions of the transformed image in the way that preserves the vertical direction of the vehicle movement (e.g. vehicles traveling from top-left to bottom-right will be traveling from top to bottom in the transformed image). 5. Use the four pairs of points to obtain the perspective transformation. 6. Apply the transformation on image of the mask. If the area of the transformed mask is less than 80% of the transformed image then the original mask is cropped from the bottom by one pixel (if not possible terminate with failure) and the process is repeated from step 2). Otherwise output the transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>The process of the construction of the perspective transformation for the pair VP1-VP2. a) The original traffic surveillance scene. b) The mask of the desired road segment is applied. c) For both VP1 (dotted red) and VP2 (solid blue) lines which originate in the vanishing point and are tangent to the mask from both sides are constructed. d) The four intersections of these lines are found. e) The four points are paired with the four corners of the rectangle with the desired dimensions of the transformed image. f ) When the transformation is applied the lines corresponding to each of the two vanishing points are parallel to the axes. If the total blank (white) area in the transformed image is more than 20% of the pixels in the transformed image then the mask is cropped by few pixels from the bottom and the process starts again from step b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>The process of constructing 2D bounding box with the c c parameter from a 3D bounding box using the transformation for both pairs for a vehicle from the BoxCars dataset<ref type="bibr" target="#b38">[35]</ref>. a) 3D bounding box (green) which is aligned with VP1 (yellow), VP2 (blue) and VP3 (red). b) 3D bounding box. c) 3D bouding box after the perspective transform for the pair VP2-VP3 is applied. d) The parametrization of the 3D bouding box for the pair VP2-VP3 as a 2D bouding box (green) with the parameter c c is determined as the ratio of the distance from top of the 2D bounding box to the top-front edge of the transformed 3D bounding box (blue) and the height of the 2D bouding box. e) 3D bouding box after the perspective transform for the pair VP1-VP2 is applied. f ) The parametrization of the 3D bouding box for the pair VP1-VP2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>The process of reconstructing the 3D bounding box (solid green) from the known 2D bounding box (dashed black), the line given by the c c parameter (dotted blue) and the position of the VPU (red cross). The process begins in the top left of the figure. Line segments originating in the VPU (red dotted) are used when needed to determine the corners and edges of the 3D bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>The process of creating annotations from provided mask of a vehicle. a) The original image. b) Mask (yellow) is obtained using Mask R-CNN [12]. c) Both the mask and the image is transformed using the transformation for the pair VP2-VP3 (see subsection 3.3). d) The 2D bounding box (green rectangle) and the two lines (dotted red) originating in VPU and tangent to the mask are drawn. e) Intersection of one of the tangents with a vertical edge of the 2D bounding box can be used to determine the first candidate for the c c parameter line (solid blue). f ) Intersection of the other tangent and the bottom edge of the bouding box is used to draw a vertical line (dashed blue) through it. g) A line (dotted red) originating in VPU and going through the top-left corner of the bounding box is drawn. The intersection of this line and the vertical line from the previous step is used to determine the position of the c c parameter line (solid blue). h) Finally, from the two possible c c parameter lines depicted in e) and g) we choose the one constructed in g) as it creates a wider 3D bounding box. The result of this process is a 2D bounding box (green rectangle) with the c c parameter line in the transformed image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>The results of the compared methods on the split C of the BrnoCompSpeed dataset<ref type="bibr" target="#b37">[34]</ref>. Mean, median and 95-th percentile errors are calculated as means of the corresponding error statistics for each video. Recall and precision are averaged over the videos in the test set. FPS values for our methods are calculated on a machine with 8-core AMD Ryzen 7 2700 CPU, 48 GB RAM and Nvidia TITAN V GPU.</figDesc><table><row><cell>Method</cell><cell>VP pair</cell><cell>Input Size (px)</cell><cell>Mean error (km/h)</cell><cell>Median error (km/h)</cell><cell>95-th percentile (km/h)</cell><cell>Mean precision (%)</cell><cell>Mean recall (%)</cell><cell>FPS</cell></row><row><cell>DubskaAuto[8]</cell><cell>-</cell><cell>-</cell><cell>8.22</cell><cell>7.87</cell><cell>10.43</cell><cell>73.48</cell><cell>90.08</cell><cell>-</cell></row><row><cell>SochorAuto[33]</cell><cell>-</cell><cell>-</cell><cell>1.10</cell><cell>0.97</cell><cell>2.22</cell><cell>90.72</cell><cell>83.34</cell><cell>-</cell></row><row><cell>SochorManual[33]</cell><cell>-</cell><cell>-</cell><cell>1.04</cell><cell>0.83</cell><cell>2.35</cell><cell>90.72</cell><cell>83.34</cell><cell>-</cell></row><row><cell>Previous2D[17] Previous3D[17]</cell><cell>VP2-VP3</cell><cell>640 x 360</cell><cell>0.83 0.86</cell><cell>0.60 0.65</cell><cell>2.17 2.17</cell><cell>83.53 87.67</cell><cell>82.06 89.32</cell><cell>62 62</cell></row><row><cell></cell><cell></cell><cell>480 x 270</cell><cell>0.92</cell><cell>0.72</cell><cell>2.35</cell><cell>89.26</cell><cell>79.99</cell><cell>70</cell></row><row><cell></cell><cell>VP2-VP3</cell><cell>640 x 360</cell><cell>0.79</cell><cell>0.60</cell><cell>1.96</cell><cell>87.08</cell><cell>83.32</cell><cell>62</cell></row><row><cell>Transform3D</cell><cell></cell><cell>960 x 540 270 x 480</cell><cell>0.75 1.12</cell><cell>0.58 0.84</cell><cell>1.84 2.84</cell><cell>87.74 87.68</cell><cell>83.21 84.06</cell><cell>43 70</cell></row><row><cell></cell><cell>VP1-VP2</cell><cell>360 x 640</cell><cell>1.17</cell><cell>0.87</cell><cell>2.88</cell><cell>88.32</cell><cell>86.32</cell><cell>62</cell></row><row><cell></cell><cell></cell><cell>540 x 960</cell><cell>1.09</cell><cell>0.84</cell><cell>2.65</cell><cell>88.06</cell><cell>85.30</cell><cell>43</cell></row><row><cell>Transform2D</cell><cell>VP2-VP3 VP1-VP2</cell><cell>640 x 360 360 x 640</cell><cell>0.92 1.11</cell><cell>0.69 0.91</cell><cell>2.18 2.70</cell><cell>84.73 86.96</cell><cell>77.58 79.42</cell><cell>62 62</cell></row><row><cell>Orig2D</cell><cell>-</cell><cell>640 x 360</cell><cell>0.93</cell><cell>0.73</cell><cell>2.53</cell><cell>88.47</cell><cell>85.20</cell><cell>62</cell></row><row><cell>MaskRCNN3D</cell><cell>-</cell><cell>1024 x 576</cell><cell>0.88</cell><cell>0.64</cell><cell>2.19</cell><cell>88.44</cell><cell>81.89</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are not skewed by difficult examples that are not detected by some of the methods with lower recall.</figDesc><table><row><cell>Method</cell><cell>VP pair</cell><cell>Input Size (px)</cell><cell>Mean error (km/h)</cell><cell>Median error (km/h)</cell></row><row><cell>DubskaAuto[8]</cell><cell>-</cell><cell>-</cell><cell>8.16</cell><cell>8.35</cell></row><row><cell>SochorAuto[33]</cell><cell>-</cell><cell>-</cell><cell>1.05</cell><cell>0.90</cell></row><row><cell>SochorManual[33]</cell><cell>-</cell><cell>-</cell><cell>1.08</cell><cell>0.89</cell></row><row><cell>Previous2D[17] Previous3D[17]</cell><cell>VP2-VP3</cell><cell>640 x 360</cell><cell>0.71 0.72</cell><cell>0.56 0.58</cell></row><row><cell></cell><cell></cell><cell>480 x 270</cell><cell>0.84</cell><cell>0.69</cell></row><row><cell></cell><cell>VP2-VP3</cell><cell>640 x 360</cell><cell>0.65</cell><cell>0.53</cell></row><row><cell>Transform3D</cell><cell></cell><cell>960 x 540 270 x 480</cell><cell>0.68 1.16</cell><cell>0.55 0.87</cell></row><row><cell></cell><cell>VP1-VP2</cell><cell>360 x 640</cell><cell>1.07</cell><cell>0.84</cell></row><row><cell></cell><cell></cell><cell>540 x 960</cell><cell>1.05</cell><cell>0.90</cell></row><row><cell>Transform2D</cell><cell>VP2-VP3 VP1-VP2</cell><cell>640 x 360 360 x 640</cell><cell>0.72 1.05</cell><cell>0.59 0.87</cell></row><row><cell>Orig2D</cell><cell>-</cell><cell>640 x 360</cell><cell>0.73</cell><cell>0.62</cell></row><row><cell>MaskRCNN3D</cell><cell>-</cell><cell>1024 x 576</cell><cell>0.76</cell><cell>0.61</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/kocurvik/BCS_results</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The dataset contains five subsets of videos. For the subsets which contain an odd number of videos we use the odd video for testing.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The authors would like to thank Adam Herout for his valuable comments. The authors also gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-speed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Traffic and Street Surveillance for Safety and Security at IEEE AVSS 2017</title>
		<meeting><address><addrLine>Lecce, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramidal implementation of the lucas kanade feature tracker description of the algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Bouguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenCV Document, Intel, Microprocessor Research Labs</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Kocur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Ft??nik</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel technique to dynamically measure vehicle speed using uncalibrated roadway cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cathey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intelligent Vehicles Symposium</title>
		<meeting>the IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12093" to="12102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Slot cars: 3d modelling for improved visual traffic analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Corral-Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple camera calibration method for vehicle velocity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Nghiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Thi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Ngoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology</title>
		<meeting>the 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08189</idno>
		<title level="m">Centernet: Keypoint triplets for object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic camera calibration for traffic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dubsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Scandinavian conference on Image analysis</title>
		<meeting>the Scandinavian conference on Image analysis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nsga-ii based autocalibration of automatic number plate recognition camera for vehicle speed measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Filipiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Golenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dolega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on the Applications of Evolutionary Computation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision? In: Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning based vehicle position and orientation estimation via inverse perspective mapping image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE Intelligent Vehicles Symposium</title>
		<meeting>the 2019 IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="317" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perspective transformation for accurate detection of 3d bounding boxes of vehicles in traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kocur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Computer Vision Winter Workshop</title>
		<meeting>the 24th Computer Vision Winter Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vehicle speed measurement based on gray constraint optical flow algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="295" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Smoke: Single-stage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>T?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="996" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A video-based system for vehicle speed measurement in urban roadways</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Nassu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1393" to="1404" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimation of vehicle velocity and traffic intensity using rectified images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maduro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th IEEE International Conference on Image Processing</title>
		<meeting>the 15th IEEE International Conference on Image Processing</meeting>
		<imprint>
			<biblScope unit="page" from="777" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">T-hog: An effective gradient-based descriptor for single line text regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stolfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1078" to="1090" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic camera calibration of roadside traffic management cameras for vehicle speed estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Schoepflin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Dailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="98" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Traffic surveillance camera calibration by 3d model bounding box alignment for accurate vehicle speed measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jur?nek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Comprehensive data set for automatic single camera visual speed measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jur?nek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?pa?hel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mar??k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>?irok?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zem??k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1633" to="1643" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Boxcars: Improving fine-grained recognition of vehicles using 3-d bounding Detection of 3D Bounding Boxes of Vehicles Using Perspective Transformation for Accurate Speed Measurement 15 boxes in traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?pa?hel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An accurate and practical calibration method for roadside camera using two vanishing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vehicle re-identification for automatic video traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zapletal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07916</idno>
		<title level="m">Geometry-constrained car recognition using a 3d perspective network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
