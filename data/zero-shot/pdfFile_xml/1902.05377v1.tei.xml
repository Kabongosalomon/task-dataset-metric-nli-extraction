<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UrbanFM: Inferring Fine-Grained Urban Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07">July 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Jing</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Ruan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">JD Intelligent City Research &amp; JD Urban Computing Business Unit</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
							<email>liuye@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">JD Intelligent City Research &amp; JD Urban Computing Business Unit</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rosenblum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
							<email>msyuzheng@outlook.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">JD Intelligent City Research &amp; JD Urban Computing Business Unit</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Jing</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Ruan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">JD Intelligent City Research &amp; JD Urban Computing Business Unit</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">JD Intelligent City Research &amp; JD Urban Computing Business Unit</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rosenblum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">ACM Reference format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UrbanFM: Inferring Fine-Grained Urban Flows</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACM Conference</title>
						<meeting>ACM Conference <address><addrLine>Washington, DC, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">11</biblScope>
							<date type="published" when="2017-07">July 2017</date>
						</imprint>
					</monogr>
					<note>Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Conference&apos;17, Washington, DC, USA 123-4567-24-567/08/06. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ?Computing methodologies ? Neural networks</term>
					<term>?Information systems applications ? Spatial-temporal systems</term>
					<term>?Applied computing ? Transportation</term>
					<term>KEYWORDS Urban computing, Deep learning, Spatio-temporal data, Urban ows</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Urban ow monitoring systems play important roles in smart city e orts around the world. However, the ubiquitous deployment of monitoring devices, such as CCTVs, induces a long-lasting and enormous cost for maintenance and operation. is suggests the need for a technology that can reduce the number of deployed devices, while preventing the degeneration of data accuracy and granularity. In this paper, we aim to infer the real-time and ne-grained crowd ows throughout a city based on coarse-grained observations. is task is challenging due to the two essential reasons: the spatial correlations between coarse-and ne-grained urban ows, and the complexities of external impacts. To tackle these issues, we develop a method entitled UrbanFM based on deep neural networks. Our model consists of two major parts: 1) an inference network to generate ne-grained ow distributions from coarse-grained inputs by using a feature extraction module and a novel distributional upsampling module; 2) a general fusion subnet to further boost the performance by considering the in uences of di erent external factors. Extensive experiments on two real-world datasets, namely TaxiBJ and HappyValley, validate the e ectiveness and e ciency of our method compared to seven baselines, demonstrating the state-of-the-art performance of our approach on the ne-grained urban ow inference problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>e ne-grained urban ow monitoring system is a crucial component of the information infrastructure system in smart cities, which lays the foundation for urban planning and various applications such as tra c management. To obtain data at a spatial ne-granularity, the system requires large amounts of sensing devices to be deployed in order to cover a citywide landscape. For example, thousands of piezoelectric sensors and loop detectors are deployed on road segments in a city to monitor vehicle tra c ow volumes in real time; many CCTVs are deployed ubiquitously for surveillance purposes and for obtaining real-time crowd ow data. With a large number of devices deployed, a high cost would be incurred due to the long-term operation (e.g., electricity and communication cost) and maintenance (e.g., on-site maintenance and warranty). A recent study showed that in Anyang, Korea, the annual operation and device maintenance fee for their smart city projects reached 100K USD and 400K USD respectively in 2015 <ref type="bibr" target="#b14">[15]</ref>. With the rapid development of smart cities on a worldwide scale, the cost of manpower and energy will become a prohibitive factor for the further intelligentization of the Earth. To reduce such expense, people require a novel technology which allows cu ing the number of deployed sensors while, most importantly, keeping the original data granularity unchanged. erefore, how to approximate the original ne-grained information from available coarse-grained data (obtained from fewer sensors) becomes an urgent problem.  <ref type="figure">Figure 1</ref>: Tra c ows in two granularities in Beijing, where each grid denotes a region. Our aim is to infer (b) given the coarse-grained data source (a). Take monitoring tra c in a university campus as a regional example. We can reduce the number of the interior loop detectors and keep sensors only at the entrances of the campus to save cost. However, we still desire to recover the ne-grained ow distribution within the campus given only the coarse-grained information. In this paper, our goal is to infer the real-time and spatially negrained ow from observed coarse-grained data on a citywide scale with many other regions (as shown in <ref type="figure">Figure 1(a)</ref>). is Fine-grained Urban Flow Inference (FUFI) problem, however, is very challenging due to the reasons as follows:  e impact of external factors on the regional ow distributions. (a) We obtain Point of Interests (POIs) for different regions, and then categorize regions into di erent semantics according to the POI information. (b)-(d) depict the average ow distribution under various external conditions.</p><p>? Spatial Correlations.</p><p>e ne-grained ow maps preserve spatial and structural correlations with the coarse-grained counterparts. Essentially, the ow volume in a coarse-grained superregion (e.g., the campus), is distributed to the constituent subregions (e.g., libraries) in the ne-grained situation. is implies a crucial structural constraint: the sum of the ow volumes in subregions strictly equals that of the corresponding superregion. Besides, the ow in a region can be a ected by the ows in the nearby regions, which will impact the inference for the negrained ow distributions over subregions. Methods failing to capture these features would lead to a degenerated performance. ? External Factors. e distribution of the ows in a given region is a ected by various external factors, such as local weather, time of day and special events. To see the impact, we present a real-world study in an area of Beijing as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(a). On weekdays, (b) shows more ows occur in the o ce area and a ractions at 10 a.m. as compared to at 8 p.m. where residences witness much higher ow density than the others (see (e)); on weekends, however, (c) demonstrates that people tend to present in a park area in the morning. It corresponds to our common sense that people go out for work in the morning, to a ractions for relaxation in the weekend and return home at night. Besides, (d) shows that people keen to move to indoor areas instead of the outdoor park under storms. ese observations evince that regions with di erent semantics present di erent ow distributions in respect of di erent external factors. Moreover, these external factors can intertwine and thus in uence the actual distribution in a very complicated manner. e FUFI problem can be cast as a mapping problem which maps data of low information entropy to that of high information entropy. Sharing the same nature with FUFI, recent studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref> in image super-resolution (SR) have presented techniques for recovering high-resolution images from low-resolution images, which has motivated applications in other elds, such as meteorology <ref type="bibr" target="#b25">[26]</ref>. Nevertheless, due to the aforementioned challenges, the simple application of these techniques to FUFI is infeasible and thus requires a careful redesign of the model architecture.</p><p>To this end, we present Urban Flow Magni er (UrbanFM), a deep neural network model which learns to infer ne-grained urban ows under the supervised-learning paradigm. Following related techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>, we assume a number of ne-grained data are available to bootstrap our solution <ref type="bibr" target="#b0">1</ref> . e key contributions of this paper lie in the following aspects:</p><p>? We present the rst a empt to formalize the ne-grained urban ow inference problem with identi cation of the problem speci cities and relevant challenges. ? We design an inference network to handle the spatial correlations. e inference network employs a convolutional networkbased feature extraction module to address the nearby region in uence. More importantly, it leverages a distributional upsampling module with a novel and parameter-free layer entitled N 2 -Normalization to impose the structural constraint on the model, by converting the learning focus from directly generating ow volumes (as in related arts) to inferring the actual ow distribution. ? We design an external factor fusion subnet to account for all complex external in uences at once. e subnet generates an integrated, high-level representation for external factors. e hidden features are then fed into di erent levels of the inference network (i.e., coarse-and ne-grained levels) to enhance the inference performance. ? We process, analyze, and experiment in two real-world urban scenarios, including the taxi ows with a metropolitan coverage and the human ows within a touristic district respectively. Our experimental results verify the signi cant advantages of UrbanFM over ve state-of-the-art and two heuristical methods in both e ectiveness and e ciency. Moreover, the experiments from multiple prospectives validate the rationale for di erent components of the model. We have released the code, sample data and demo for public use 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FORMULATION</head><p>In this section, we rst de ne the notations and then formulate the problem of Fine-grained Urban Flow Inference (FUFI). De nition 1 (Region) As shown in <ref type="figure">Figure 1</ref>, we partition an area of interest (e.g., a city) evenly into a I ? grid map based on the longitude and latitude, where a grid denotes a region <ref type="bibr" target="#b28">[29]</ref>. Partitioning the city into smaller regions (i.e., using larger I, ) suggests that we can obtain ow data with more details, which results in a more ne-grained ow map. De nition 2 (Flow Map) Let X ? R I ? + represent a ow map of a particular time, where each entry x i, j ? R + denotes the ow volume of the instances (e.g., vehicle, people, etc.) in region (i, j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>De nition 3 (Superregion &amp; Subregion)</head><p>In our FUFI problem, a coarse-grained grid map indicates the data granularity we can observe upon sensor reduction. It is obtained by integrating nearby grids within an N -by-N range in a ne-grained grid map given a scaling factor N. <ref type="figure">Figure 1</ref> illustrates an example when N = 2. Each coarse-grained grid in <ref type="figure">Figure 1</ref>(a) is composed of 2 ? 2 smaller grids from <ref type="figure">Figure 1</ref>(b). We de ne the aggregated larger grid as superregion, and its constituent smaller regions as subregions. Note that with this se ing, the superregions do not share subregions. Hence, the structure between superregions and the corresponding subregions indicates a special structural constraint in FUFI.  . ? denotes addition and denotes Hadamard product. Note that our framework allows an arbitrary integer upscaling factor, not limited to the power of 2.</p><p>De nition 4 (Structural Constraint) e ow volume x c i, j in a superregion of the coarse-grained grid map and the ows x f i , j in the corresponding subregions of the ne-grained counterpart obey the following equation:</p><formula xml:id="formula_0">x c i, j = i , j x f i , j s.t . i N = i, j N = j.<label>(1)</label></formula><p>For simplicity, i = 1, 2, . . . , I and j = 1, 2, . . . , in our paper unless otherwise speci ed. Problem Statement (Fine-grained Urban Flow Inference) Given a upscaling factor N ? Z and a coarse-grained ow map X c ? R I ? + , infer the ne-grained counterpart X f ? R N I ?N + . <ref type="figure" target="#fig_3">Figure 3</ref> depicts the framework of UrbanFM, which consists of two main components for conducting the structurally constrained inference and capturing complex external in uence, respectively. e inference network takes the coarse-grained ow map X c as input, then extracts high-level features across the whole city by leveraging deep residual networks <ref type="bibr" target="#b6">[7]</ref>. Taking extracted features as a priori knowledge, the distributional upsampling module outputs a ow distribution over subregions with respect to each superregion by introducing a dedicated N 2 -Normalization layer. Finally, the Hadamard product of the inferred distribution with the upsampled coarse-grained ow map gives the ne-grained ow mapX f as the network output. In the external factor fusion branch, we leverage embeddings and a dense network to extract pixel-wise external features in both coarse and ne granularity. e integration of external and ow features enables UrbanFM to exhibit ne-grained ow inference more e ectively. In this section, we articulate the key designs for the two components, as well as the optimization scheme on network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inference Network</head><p>Inference network aims to produce the ne-grained ow distribution over subregions from a coarse-grained input. We follow the general procedure in SR methods, which is composed of two phases: 1) feature extraction; 2) inference upon upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Feature Extraction.</head><p>In the input stage, we use a convolutional layer (with 9 ? 9 lter size, F lters) to extract low-level features from the given coarse-grained ow map X c , and perform the rst stage fusion if external features are provided.</p><p>en M Residual Blocks with identical layout take the (fused) low-level feature maps as input and then construct high-level feature maps. e residual block layout, as shown on the top right of <ref type="figure" target="#fig_3">Figure 3</ref>, follows the guideline in <ref type="bibr" target="#b13">[14]</ref>, which contains two convolutional layers (3 ? 3, F ) followed by Batch Normalization <ref type="bibr" target="#b9">[10]</ref>, with an intermediate ReLU <ref type="bibr" target="#b5">[6]</ref> function to introduce non-linearity.</p><p>Since we utilize a fully convolutional architecture, the reception eld grows larger as we stack the network deeper. In other words, each pixel at the high-level feature map will be able to capture distant or even citywide dependencies. Moreover, we use another convolutional layer (3 ? 3, F ) followed by batch normalization to guarantee feature extraction. Finally, drawing from the intuition that the output ow distribution would exhibit region-to-region dependencies to the original X c , we employ a skip connection to introduce identity mapping <ref type="bibr" target="#b7">[8]</ref> between the low-level features and high-level features, building an information highway skipping over the residual blocks to allow e cient gradient back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Distributional</head><p>Upsampling. In the second phase, the extracted features rst go through n sub-pixel blocks to perform an N = 2 n upscaling operation which produces a hidden feature H f ? R F ? N I ? N . e sub-pixel block, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, leverages a convolutional layer (3 ? 3, F ? 2 2 ) followed by batch normalization to extract features. en it uses a PixelShu e layer <ref type="bibr" target="#b18">[19]</ref> to rearrange and upsample the feature maps to 2? size and applies a ReLU activation at the end. A er each sub-pixel block, the output feature maps grow 2 times larger with the number of channels unchanged. A convolutional layer (9 ? 9, F o ) is applied post-upsampling, which maps H f to a tensor H</p><formula xml:id="formula_1">f o ? R F o ? N I ? N .</formula><p>Note that F o = 1 in our case. In SR tasks, H f o is usually the nal output for the recovered image with super-resolution. However, the structural constraint essential to FUFI has not been considered.</p><p>Conference'17, July 2017, Washington, DC, USA Y. Liang et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: N 2 -Normalization</head><p>Input: x, scale factor, ? Output: out // x: an input feature map // scale factor: the upscaling factor // ?: a small number for numerical stability // out: the structural distributions sum = SumPooling(x, scale factor); sum = NearestNeighborUpsampling(sum, scale factor); out = x (sum+?) // element wise division</p><p>In order to impose the structural constraint on the network, one straightforward manner is to add a structural loss L s as a regularization term to the loss function:</p><formula xml:id="formula_2">L s = i, j x c i, j ? i , j x f i , j F s.t . i N = i, j N = j.<label>(2)</label></formula><p>However, simply applying L s does not improve the model performance, as we will demonstrate in the experiment section. Instead, we design a N 2 -Normalization layer, which outputs a distribution over every patch of N -by-N subregions with regard to the respective superregion. To achieve this, we reformulate Equation 1 as in the following:</p><formula xml:id="formula_3">x c i, j = i , j ? i , j x c i, j s.t . ? i , j = 1, ? ? R + , i N = i, j N = j.<label>(3)</label></formula><p>e ow volume in each subregion is now expressed as a fraction of that in the superregion, i.e., x f i , j = ? i j x c i, j , and we can treat the fraction as a probability. is allows us to interpret the network output in a meaningful way: the value in each pixel states how likely the overall ow will be allocated to the subregion (i , j ). By reformulation, we shi our focus from directly generating the negrained ow to generating the ow distribution. is essentially changes the network learning target and thus diverges from the traditional SR literature. To this end, we present the N 2 -Normalization layer:</p><formula xml:id="formula_4">N 2 -Normalization(H f o ) = H f ? , such that H f ?,(i, j) = H f o,(i, j) / i = i /N * N , j = j /N * N i =( i /N ?1) * N +1, j =( j/N ?1) * N +1 H f o,(i , j )<label>(4)</label></formula><p>N 2 -Normalization layer induces no extra parameters for the network. Moreover, it can be easily implemented within a few lines of code (see Algorithm 1). Also, the operations can be fully paralleled and automatically di erentiated in runtime. Remarkably, this reformulation release the network from concerning varying output scales and enable it to focus on producing a probability within [0, 1] constraint.</p><p>Finally, we upscale X c using nearest-neighbor upsampling <ref type="bibr" target="#b2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">External Factor Fusion</head><p>External factors, such as weather, can have a complicated and vital in uence on the ow distribution over the subregions. For instance, even if the total population in town remains stable over time, under storming weather people tend to move from outdoor regions to indoor regions. When di erent external factors entangle, the actual impact on the ow becomes implicit however unneglectable. ereby, we design a subnet to handle those impacts all at once. Particularly, we rst separate the available external factors into two groups, i.e., continuous and categorical features. Continuous features including temperature and wind speed are directly concatenated to be a vector e con . As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, categorical features include the day of week, the time of the day and weather (e.g, sunny, rainy). Inspired by previous studies <ref type="bibr" target="#b15">[16]</ref>, we transform each categorical a ribute into a low-dimensional vector by feeding them into di erent embedding layers separately, and then concatenate those embeddings to construct the categorical vector e cat . en, the concatenation of e con and e cat gives the nal embeddings for external factors, i.e., e = [e con ; e cat ].</p><p>Once we get the concatenation vector e, we feed it into a feature extraction module, whose structure is depicted in <ref type="figure" target="#fig_3">Figure 3</ref>. By using dense layers, di erent external impacts are compounded to construct a hidden representation, which models the complicated interaction. e module provides two outputs: the coarse-grained feature maps H c e and the ne-grained feature maps H f e , where H f e is obtained by passing X c e through n sub-pixel blocks which are similar to the ones in the inference network. Intuitively, H c e (H f e ) is the spatial encoding for e in coarse-grained ( ne-grained) se ing, modeling how each superregion (subregion) individually responds to the external changes. erefore we concatenate H c e with X c , and H f e with H f to the inference network. e early fusion of H c e and X c allows the network to learn to extract a high-level feature describing not only the citywide ow, but also the external in uences. Besides, the ne-grained H f e carries the external information all the way to the rear of the inference network, playing a similar role as an information highway, which prevents information perishing in the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>UrbanFM provides an end-to-end mapping from coarse-grained input to ne-grained output, which is di erentiable everywhere. erefore, we can learned the network through auto back propagation, by providing training pairs (X c , X f ) and calculating empirical loss between (X f ,X f ), where X f is the ground truth andX f is the outcome inferred by our network. Pixel-wise Mean Square Error (MSE) is a widely used cost function in many tasks, and we employ the same in this work as follows:</p><formula xml:id="formula_5">L(?) = X f ?X f 2 F (5)</formula><p>where ? denotes the set of parameters in UrbanFM. Noted that M and F are the two main hyperparameters controlling the learning ability as well as the parameter size of the network. We experiment with di erent hyperparameter se ings in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>e focus of our experiments is on examining the capacity of our model in a citywide scenario. erefore, we conduct extensive experiments using taxi ows in Beijing to comprehensively test the model from di erent aspects. In addition, we conduct further experiments in a theme park, namely Happy Valley, to show the model adaptivity on a relatively small area. <ref type="table" target="#tab_1">Table 1</ref> details the two datasets we use, namely TaxiBj and HappyValley, where each dataset contains two subdatasets: urban ows and external factors. Since a number of ne-grained ow data are available as ground truth, in this paper, we obtain the coarse-grained ows by aggregating subregion ows from the ne-grained counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><formula xml:id="formula_6">4.1.1 Datasets.</formula><p>? TaxiBJ: is dataset, which is published by Zhang et al. <ref type="bibr" target="#b28">[29]</ref>, indicates the taxi ows traveling throughout Beijing. As depicted in <ref type="figure">Figure 1</ref>, the studied area is split into 32?32 grids, where each grid reports the coarse-grained ow information every 30 minutes within four di erent periods: P1 to P4 (detailed in <ref type="table" target="#tab_1">Table  1</ref>). Here, we utilize the coarse-grained taxi ows to infer negrained ows with 4? resolution (N = 4). In our experiment, we partition the data into non-overlapping training, validation and test data by a ratio of 2:1:1 respectively for each period. For example, in P1 (7/1/2013-10/31/2013), we use the rst two-month data as the training set, the next month as the validation set, and the last month as the test set.  <ref type="figure" target="#fig_7">Figure 4</ref>, we partition this area with 25?50 uniform grids in coarse-grained se ing, and target a ne granularity at 50?100 with an upscaling factor N = 2. Note that in this dataset, one special external factor is the ticket price, including day prices and night prices, which are obtained from the o cial account in WeChat. Regarding the smaller area, crowd ows exhibits large variance across samples given the 1-hour sampling rate. us, we use a ratio of 8:1:1 to split training, validation and test set to provide more training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation</head><p>Metrics. We use three common metrics for urban ow data to evaluate the model performance from di erent facets. Speci cally, Root Mean Square Error (RMSE) is de ned as:</p><formula xml:id="formula_7">RMSE = 1 z z s=1 X f s ?X f s 2 F ,</formula><p>where z is the total number of samples,X   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baselines.</head><p>We compare our proposed model with seven baselines that belong to the following three classes: (1) Heuristics. e last method is the state of the art on statistical downscaling for climate data. We detail them as follows:</p><p>? Mean Partition (Mean): We evenly distribute the ow volume from each superregion in a coarse-grained ow map to the N 2 subregions, where N is the upscaling factor.  is study suggests that large depth is necessary for the task of SR.</p><p>? SRResNet <ref type="bibr" target="#b13">[14]</ref>: SRResNet enhances VDSR by using the residual architecture presented by He et al. <ref type="bibr" target="#b6">[7]</ref>. e residual architecture allows one to stack a much larger number of network layers, which bases many benchmark methods in computer vision tasks. ? DeepSD <ref type="bibr" target="#b25">[26]</ref>: DeepSD is the state-of-the-art method on statistical downscaling 5 for meteorological data. It basically exploits SRCNN for downscaling for an intermediate level, and performs further downscaling by simply stacking more SRCNNs. is method, however, would inherently require much more parameters compared with our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Variants.</head><p>To evaluate each component of our method, we also compare it with di erent variants of UrbanFM:</p><p>? UrbanFM-ne: We simply remove the external factor fusion subnet from our method, which can help reveal the signi cance of this component. ? UrbanFM-sl: Upon removing the external subnet, we further replace distributional upsampling module by using sub-pixel blocks and L s to consider the structural constraint in this variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Training Details &amp; Hyperparameters.</head><p>Our model, as well as the baselines, are completely implemented by PyTorch with one TITAN V GPU. We leverage Adam <ref type="bibr" target="#b11">[12]</ref>, an algorithm for stochastic gradient descent, to perform network training with learning rate lr = 1e ? 4 and batch size being 16. We also apply a staircase-like schedule by halving the learning rate every 20 epochs, which allows smoother search near the convergence point. In the external subnet, there are 128 hidden units in the rst dense layer with dropout rate 0.3, and I ? hidden units in the second dense layer. We embed DayOfWeek to R 2 , HourOfDay to R 3 and weather condition to R 3 . Besides, for VDSR and SRResNet, we use the default se ings in their paper. Since SRCNN, ESPCN and DeepSD perform poorly with default se ings, we test di erent hyperparameters for them and nally use 768 and 384 as the number of lters in their two convolutional layers respectively. See more details in Appendix. <ref type="bibr" target="#b4">5</ref> Downscaling means obtaining higher resolution image in meteorology <ref type="bibr" target="#b25">[26]</ref> while the opposite in computer graphics <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on TaxiBJ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparison</head><p>In this subsection, we compare the model e ectiveness against the baselines. We report the result of UrbanFM with M-F being 16-128 as our default se ing. Further experiments regarding di erent M-F will be discussed later. Likewise, we postpone the result of UrbanFM-sl to the next experiment for a more detailed study. <ref type="table" target="#tab_2">Table 2</ref> summarizes the experimental results on TaxiBJ. We have the following observations: (1) e UrbanFM and its variant outperform all baseline methods in all three metrics over all time spans (P1-P4). Take SRRestNet for example. UrbanFM advances it by 4.5%, 17.0% and 54.1% for RMSE, MAE and MAPE on average, where UrbanFM-ne also advances by 3.0%, 15.6% and 53.6% respectively. e advance of UrbanFM-ne over all baselines indicates that the distribution upsampling in our inference network plays a leading role in improving the inference performance; the advance of UrbanFM over UrbanFM-ne supports that the combination with external subnet indeed enhances the model by incorporating external factors. (2) Image super-resolution methods outdo the heuristic method HA on RMSE while show deteriorate scores on MAE and MAPE. is can be a ributed to two reasons: rst, neural network methods are dedicated to performing well on RMSE as it is the training objective; second, HA preserves the spatial correlation for ne-grained ow maps while the others fail to do so. is again emphasizes the importance of preserving the structural constraint. A piece of further evidence can be seen from the comparison between UrbanFM-ne and SRResNet, where the former model has a similar structure as SRResNet except the distributional upsampling module, which makes it surpass its counterpart. Due to the similarity of model architecture, we select SRResNet as the baseline model for subsequent studies over di erent UrbanFM components.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UrbanFM: Inferring Fine-Grained Urban Flows</head><p>Conference'17, July 2017, Washington, DC, USA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study on Distributional Upsampling</head><p>To examine the e ectiveness of the distributional upsampling module, we compared SRResNet with UrbanFM-ne (using distributional upsampling but no external factors) and UrbanFM-sl (using structural loss instead of distributional upsampling), as shown in <ref type="figure" target="#fig_10">Figure 5</ref>.</p><p>In both M-F se ings, it can be seen that UrbanFM-sl regularized by L s performs very close to the SRResNet which is not constrained at all. ough under the se ing of 16-64, Urban-sl achieves a smaller error than SRResNet in a subtle way, under the 16-128 se ing they behave the opposite. On the contrary, UrbanFM-ne consistently outperforms the others on all three metrics. is result veri es that the distributional upsampling module is the be er choice for imposing the structural constraint than using L s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study on External Factor Fusion</head><p>External impacts, though are complicated, can assist the network for be er inferences when they are properly modeled and integrated, especially in a more di cult situation when there is less data budget.</p><p>ereby, we study the e ectiveness of external factors under four di culties by randomly subsampling di erent ratio of the original training set, which are 10%, 30%, 50% and 100%, corresponding to four levels: hard, semi-hard, medium and easy.</p><p>As shown in <ref type="figure" target="#fig_11">Figure 6</ref>, the gap between UrbanFM and UrbanFMne becomes larger as we reduce the number of training data, indicating that external factor fusion plays a more important role in providing a priori knowledge. When the training size grows, the weight for the priori knowledge decreases, as there exists overlaying information between observed tra c ow and external factors.</p><p>us, the network may learn to capture some external impacts by providing enough data. Moreover, this trend also occurs between UrbanFM and UrbanFM-sl, which illustrates that the N 2 -Normalization layer provides a strong structural prior to facilitate the network training.  <ref type="table" target="#tab_4">Table 3</ref> compares the average performance over P1 to P4. Across all hyperparameter se ings, UrbanFM consistently outperforms SRResNet, advancing by at least 2.6%, 13.7% and 48.6%. Besides, Study on E ciency <ref type="figure">Figure 7</ref> plots the RMSE on the validation set during the training phase using P1-100%. <ref type="figure">Figure 7</ref>(a) and 7(b) delineate that UrbanFM converges much smoother and faster than baselines and the variants. Speci cally, 7(b) suggests such e ciency improvement can be mainly a ributed to the N 2 -Normalization layer since UrbanFM-sl converges much slower and uctuates drastically even it is constrained by L s , when compared with UrbanFM and UrbanFM-ne. is also suggests that learning the spatial correlation is a nontrivial task. Moreover, UrbanFM-ne behaves closely to UrbanFM as external factors fusion a ects the training speed subtly when training data are abundant as suggested by the previous experiments.    <ref type="figure">Figure 8</ref> displays the inference error X f ?X f 1,1 from UrbanFM and the other three baselines for a sample, where a brighter pixel indicates a larger error. Contrast with the other methods, UrbanFM achieves higher delity for totality and in details. For instance, area A and B are "hard area" to be inferred, as A (Sanyuan bridge, the main entrance to downtown) and B (Sihui bridge, a huge yover) are two of the top congestion points in Beijing. Tra c ow of these locations usually uctuates drastically and quickly, resulting in higher inference errors. Nonetheless, UrbanFM remains to produce be er performance in these areas. Another observation is that the SR methods (SRCNN, ESPCN, VDSR and SRResNet) tend to generate blurry images as compared to structural methods (HA and UrbanFM). For instance, even if there is zero ow in area C, SR methods still generate error pixels as they overlap the predicted patches. is suggests the FUFI problem does di er from the ordinary SR problem and requires speci c designs. 2) External in uence. <ref type="figure">Figure 9</ref>(a)-(d) portray that the inferred distribution over subregions varies along with external factor changes. On weekdays, at 10 a.m., people had already owed to the o ce area to start their work (b); at 9 p.m., many people had returned home a er a hard-working day (c). On weekends, most people stayed home at 10 a.m. but some industrial researchers remained working in the university labs. is result proves that UrbanFM indeed captures the external in uence and learns to adjust the inference accordingly.  <ref type="table" target="#tab_5">Table 4</ref> shows model performances using the HappyValley dataset. Note that in this experiment, we do not include DeepSD, since this task contains only 2? upscaling and the DeepSD degrades to SRCNN in this case. One important trait of the HappyValley dataset is that it contains more spikes on the ne-grained ow distribution, which results in a much larger RMSE score versus that in the TaxiBJ task. Nonetheless, UrbanFM remains the winner method outperforming the best baseline by 3.5%, 7.8% and 22%, and the UrbanFM-ne still holds the runner-up position. is proves that UrbanFM not only works on the large-scale scenario, but is also adaptable to smaller areas, which concludes our empirical studies. Single image super-resolution (SISR), which aims to recover a highresolution (HR) image from a single low-resolution (LR) image, has gained increasing research a ention for decades. is task nds direct applications in many areas such as face recognition <ref type="bibr" target="#b4">[5]</ref>, ne-grained crowdsourcing <ref type="bibr" target="#b23">[24]</ref> and HDTV <ref type="bibr" target="#b17">[18]</ref>. Over years, many SISR algorithms have been developed in the computer vision community. To tackle the SR problem, early techniques focused on interpolation methods such as bicubic interpolation and Lanczos resampling <ref type="bibr" target="#b2">[3]</ref>. Also, several studies utilized statistical image priors  <ref type="figure">Figure 9</ref>: Case study on a superregion near Peking Univ. See our github for further dynamic analysis on this area. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> to achieve be er performances. Advanced works aimed at learning the non-linear mapping between LR and HR images with neighbor embedding <ref type="bibr" target="#b0">[1]</ref> and sparse coding <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>. However, these approaches are still inadequate to reconstruct realistic and ne-grained textures of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study on Parameter Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on HappyValley</head><p>Recently, a series of models based on deep learning have achieved great success in terms of SISR since they do not require any humanengineered features and show the state-of-the-art performance. Since Dong et al. <ref type="bibr" target="#b1">[2]</ref> rst proposed an end-to-end mapping method represented as CNNs between the low-resolution (LR) and highresolution (HR) images, various CNN based architectures have been studied for SR. Among them, Shi et al. <ref type="bibr" target="#b18">[19]</ref> introduced an e cient sub-pixel convolutional layer which is capable of recovering HR images with very li le additional computational cost compared with the deconvolutional layer at training phase. Inspired by VGGnet for ImageNet classi cation <ref type="bibr" target="#b19">[20]</ref>, a very deep CNN was applied for SISR in <ref type="bibr" target="#b10">[11]</ref>. However, training a very deep network for SR is really hard due to the small convergence rate. Kim et al. <ref type="bibr" target="#b10">[11]</ref> showed residual learning speed up their training phase and veri ed that increasing the network depth could contribute to a signi cant improvement in SR accuracy.</p><p>Despite good performance on the RMSE accuracy, the generated image remains smooth and blurry. To address this problem, Ledig et al. <ref type="bibr" target="#b13">[14]</ref> rst proposed a perceptual loss function which consists of an adversarial loss to push their solution to the natural image manifold, and a content loss for the be er reconstruction of high-frequency details. Lim et al. <ref type="bibr" target="#b16">[17]</ref> developed an enhanced deep SR network that shows the state-of-the-art performance by removing unnecessary modules in <ref type="bibr" target="#b13">[14]</ref>. Apart from super-resolving classical images, there are limited studies that focus on utilizing super-resolution methods to solve real-world problems in the urban area. For example, Vandal et al. <ref type="bibr" target="#b25">[26]</ref>, presented a stacked SRCNN <ref type="bibr" target="#b1">[2]</ref> for statistical downscaling of climate and earth system simulations based on observational and topographical data.</p><p>However, these approaches are not suitable for the FUFI problem since the ow data present a very speci c structural constraint with regard to natural images, as such, the related arts cannot be simply applied to our application in terms of e ciency and e ectiveness. To the best of our knowledge, we are the rst to formulate and solve the problem for ne-grained urban ow inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Urban Flows Analysis</head><p>Due to the wide applications of tra c analysis and the increasing demand for real-time public safety monitoring, urban ow analysis has recently a racted the a ention of a large amount of researchers <ref type="bibr" target="#b30">[31]</ref>. Over the past years, Zheng et al. <ref type="bibr" target="#b30">[31]</ref> rst transformed public tra c trajectories into other data formats, such as graphs and tensors, to which more data mining and machine learning techniques can be applied. Based on our observation, there were several previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref> forecasting millions, or even billions of individual mobility traces rather than aggregated ows in a region.</p><p>Recently, researchers have started to focus on city-scale tra c ow prediction <ref type="bibr" target="#b8">[9]</ref>. Inspired by deep learning techniques that power many applications in modern society <ref type="bibr" target="#b12">[13]</ref>, a novel deep neural network was developed by Zhang et al. <ref type="bibr" target="#b29">[30]</ref> to simultaneously model spatial dependencies (both near and distant), and temporal dynamics of various scales (i.e., closeness, period and trend) for citywide crowd ow prediction. Following this work, Zhang et al. <ref type="bibr" target="#b28">[29]</ref> further proposed a deep spatio-temporal residual network to collectively predict in ow and out ow of crowds in every city grid.</p><p>To address the data scarcity issue in crowd ows, very recent study <ref type="bibr" target="#b26">[27]</ref> aims to transfer knowledge between di erent cities to help target city learn a be er prediction model from the source city. Apart from the above applications, we aim to solve a novel problem (FUFI) on urban ows in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have formalized the ne-grained urban ow inference problem and presented a deep neural network-based method (UrbanFM) to solve it. UrbanFM has addressed the two challenges that are speci c to this problem, i.e., the spatial correlation as well as the complexities of external factors, by leveraging the original distributional upsampling module and the external factor fusion subnet. Experiments have shown that our approach advances baselines by at least 4.5%, 17.0% and 54.1% on TaxiBJ dataset and 3.5%, 7.8% and 22% on HappyValley dataset in terms of the three metrics. Various empirical studies and visualizations have con rmed the advantages of UrbanFM on both e ciency and e ectiveness.</p><p>In the future, we will explore more on improving the model structure, and pay more a ention to reducing errors in hard regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Detailed Settings of Baselines</head><p>We detail the model con guration as well as hyperparameter searching spaces for each baseline in this section.</p><p>? Mean Partition (Mean): It is parameter-free and can be directly applied on the test set. ? Historical Average (HA): Firstly, we compute the mean distribution matrix on the training set (with no parameters). en, the matrix is applied to generate the ne-grained ow map over a coarse-grained observation. ? SRCNN: Since SRCNN under its default se ing achieves inferior performance and takes a long time to converge, we test di erent hyperparameters for it, so as to nd the best se ing. Suppose there are F 1 and F 2 lters in the two convolutional layers of such method. We conduct a grid search over F 1 = {64, 128, 256, 512, 768, 1024} and F 2 = {32, 64, 128, 256, 384, 512}. e se ing in which F 1 = 768 and F 2 = 384 outperforms the others in the validation set.</p><p>? ESPCN: Similar to SRCNN with three-staged architecture, we leverage F 1 = 768 and F 2 = 384 as the number of lters in di erent convolutional layers respectively. ? DeepSD: Experiments show that ESPCN is more e cient and e ective than SRCNN <ref type="bibr" target="#b18">[19]</ref>. In order to speed up this method based on stacked SRCNNs, we use ESPCN to replace the SRCNN in the original paper, (i.e., a stacked ESPCN). ? VDSR: e depth of convolutional blocks D and the number of lters F in convolutional layer are two main hyperparameters. We utilize the default se ing D = 20 and F = 64 as suggested by the authors <ref type="bibr" target="#b10">[11]</ref>. ? SRResNet <ref type="bibr" target="#b13">[14]</ref>: In our paper, we compare our method with SR-ResNet from multiple angles. ere are two main hyperparameters in SRResNet, including the depth of residual blocks M and number of lters in convolutional layer F . We test M = {16, 20} and F = {64, 128, 256} in di erent experiments, which is detailed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Detailed Settings of UrbanFM</head><p>We rst introduce how we implement N 2 -Normalization layer based on Pytorch, and further present the detailed se ings of two main components of our approach, i.e., inference network and external factor fusion subnet.</p><p>A.4.1 N 2 -Normalization Layer. <ref type="figure" target="#fig_18">Figure 11</ref> illustrates the Pytorch implementation of N 2 -Normalization layer, which plays a signicant role in our method.  <ref type="table" target="#tab_6">Table 6</ref> show the detailed con guration for the inference network which is depicted in <ref type="figure" target="#fig_3">Figure 3</ref> (from le to right). Note that the upscaling factor N = 4 in this example.  <ref type="figure" target="#fig_3">Figure 3</ref>, where settings k-s-n means the size of kernel, stride and number of lters in a certain convolutional layer. We omit the batch size in the format of output for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Se A.4.3 External Factor Fusion Subnet. Before inpu ing to the subnet, we the use embedding method to convert the categorical features (like day of week, weather condition 7 ) to learned representations respectively, i.e., real-valued vectors. As shown in <ref type="table" target="#tab_8">Table 7</ref>, we detail the embedding se ings for each external factor.  <ref type="table">Table 8</ref> shows the details of the external subnet. e se ings of dense layer denotes the number of hidden units, while that of dropout layer represents its randomly dropping rate. It is also illustrated from le to right of <ref type="figure" target="#fig_3">Figure 3</ref>. <ref type="table">Table 8</ref>: Details of External Factor Fusion in <ref type="figure" target="#fig_3">Figure 3</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Coarse-grained crowd flows (32x32) (b) Fine-grained crowd flows (64x64)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: e impact of external factors on the regional ow distributions. (a) We obtain Point of Interests (POIs) for different regions, and then categorize regions into di erent semantics according to the POI information. (b)-(d) depict the average ow distribution under various external conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>e UrbanFM framework for 4? upscaling (N = 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>with the scaling factor N to obtain X c up ? R N I ?N + and then generate the ne-grained inference byX f = X c up H f ? . 3 h ps://en.wikipedia.org/wiki/Nearest-neighbor interpolation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?</head><label></label><figDesc>HappyValley: We obtain this dataset by crawling from an open website 4 which provides hourly gridded crowd ow observations for a theme park named Beijing Happy Valley, with a total 1.25?10 5 m 2 area coverage, from 1/1/2018 to 10/31/2018. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>fs</head><label></label><figDesc>is s-th the inferred value and X f s is corresponding ground truth. Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) are de ned as: MAE = general, RMSE favors spiky distributions, while MAE and MAPE focus more on the smoothness of the outcome. Smaller metric scores indicate be er model performance. 4 heat.qq.com</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of crowd ows in HappyValley.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 2 )</head><label>2</label><figDesc>Image super-resolution. (3) Meteorological super-resolution. e rst two methods are designed by us based on intuition or empirical knowledge, while the next four methods are previously and currently state-of-the-art methods for single image super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>?</head><label></label><figDesc>Historical Average (HA): Similar to distributional upsampling, HA treats the value over each subregion a fraction of the value in the respective super region, where the faction is computed by averaging all training data. ? SRCNN [2]: SRCNN presents the rst successful introduction of convolutional neural networks (CNNs) into the SR problems. It consists of three layers: patch extraction, non-linear mapping and reconstruction. Filters of spatial sizes 9?9, 5?5, and 5?5 were used respectively. e number of lters in the two convolutional layers are 64 and 32 respectively. In SRCNN, the low-resolution input is upscaled to the high-resolution space using a single lter (commonly bicubic interpolation) before reconstruction. ? ESPCN [19]: Bicubic interpolation used in SRCNN is a special case of the deconvolutional layer. To overcome the low e ciency of such deconvolutional layer, E cient Sub-Pixel Convolutional Neural Network (ESPCN) employs a sub-pixel convolutional layer that aggregates the feature maps from LR space and builds the SR image in a single step. Conference'17, July 2017, Washington, DC, USA Y. Liang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparison over various structural constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>E ects of external factors on four di culties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>i d a t i o n R M S E I t e r a t i o n S R C N N E S P C N V D S R S R R e s N e t U r b a n F M (a) Model comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>i d a t i o n R M S E I t e r a t i o n U r b a n F M U r b a n F M -s l U r b a n F M -n e (b) Variant comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 : 1 )</head><label>71</label><figDesc>Convergence speed of various methods. Visualization Inference error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Conference' 17 ,Figure 8 :</head><label>178</label><figDesc>July 2017, Washington, DC, USA Y. Liang et al. Visualization for inference errors among di erent methods. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>(b) 10 :</head><label>10</label><figDesc>00 weekday (c) 21:00 weekday (d) 10:00 weekend (a) Studied Area Office Area Residence Restaurant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 10 :</head><label>10</label><figDesc>Distribution of urban ows in TaxiBJ dataset. 6 h ps://github.com/yoshall/UrbanFM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Implementation of N 2 -Normalization layer based on PyTorch 0.4.1 UrbanFM: Inferring Fine-Grained Urban Flows Conference'17, July 2017, Washington, DC, USA A.4.2 Inference Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset Description.</figDesc><table><row><cell>Dataset</cell><cell>TaxiBJ</cell><cell>HappyValley</cell></row><row><cell></cell><cell>P1: 7/1/2013-10/31/2013</cell><cell></cell></row><row><cell>Time span</cell><cell cols="2">P2: 2/1/2014-6/30/2014 1/1/2018-P3: 3/1/2015-6/30/2015 10/31/2018</cell></row><row><cell></cell><cell>P4: 11/1/2015-3/31/2016</cell><cell></cell></row><row><cell>Time interval</cell><cell>30 minutes</cell><cell>1 hour</cell></row><row><cell cols="2">Coarse-grained size 32?32</cell><cell>25?50</cell></row><row><cell>Fine-grained size</cell><cell>128?128</cell><cell>50?100</cell></row><row><cell cols="2">Upscaling factor (N ) 4</cell><cell>2</cell></row><row><cell cols="3">External factors (meteorology, time and event)</cell></row><row><cell cols="2">Weather (e.g., Sunny) 16 types</cell><cell>8 types</cell></row><row><cell>Temperature/?</cell><cell>[-24.6, 41.0]</cell><cell>[-15.0, 39.0]</cell></row><row><cell>Wind speed/mph</cell><cell>[0, 48.6]</cell><cell>[0.1, 15.5]</cell></row><row><cell># Holidays</cell><cell>41</cell><cell>33</cell></row><row><cell>Ticket prize/?</cell><cell>/</cell><cell>[29.9, 260]</cell></row></table><note>(a) Coarse-grained Crowd Flows in Happy Valley (25x50) (b) Fine-grained Crowd Flows in Happy Valley (50x100)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results comparisons on TaxiBJ over di erent time spans (P1-P4).</figDesc><table><row><cell>Methods</cell><cell>RMSE</cell><cell>P1 MAE</cell><cell cols="2">MAPE RMSE</cell><cell>P2 MAE</cell><cell cols="2">MAPE RMSE</cell><cell>P3 MAE</cell><cell cols="2">MAPE RMSE</cell><cell>P4 MAE</cell><cell>MAPE</cell></row><row><cell>MEAN</cell><cell cols="2">20.918 12.019</cell><cell>4.469</cell><cell cols="2">20.918 12.019</cell><cell>5.364</cell><cell cols="2">27.442 16.029</cell><cell>5.612</cell><cell cols="2">19.049 11.070</cell><cell>4.192</cell></row><row><cell>HA</cell><cell>4.741</cell><cell>2.251</cell><cell>0.336</cell><cell>5.381</cell><cell>2.551</cell><cell>0.334</cell><cell>5.594</cell><cell>2.674</cell><cell>0.328</cell><cell>4.125</cell><cell>2.023</cell><cell>0.323</cell></row><row><cell>SRCNN</cell><cell>4.297</cell><cell>2.491</cell><cell>0.714</cell><cell>4.612</cell><cell>2.681</cell><cell>0.689</cell><cell>4.815</cell><cell>2.829</cell><cell>0.727</cell><cell>3.838</cell><cell>2.289</cell><cell>0.665</cell></row><row><cell>ESPCN</cell><cell>4.206</cell><cell>2.497</cell><cell>0.732</cell><cell>4.569</cell><cell>2.727</cell><cell>0.732</cell><cell>4.744</cell><cell>2.862</cell><cell>0.773</cell><cell>3.728</cell><cell>2.228</cell><cell>0.711</cell></row><row><cell>DeepSD</cell><cell>4.156</cell><cell>2.368</cell><cell>0.614</cell><cell>4.554</cell><cell>2.612</cell><cell>0.621</cell><cell>4.692</cell><cell>2.739</cell><cell>0.682</cell><cell>3.877</cell><cell>2.297</cell><cell>0.652</cell></row><row><cell>VDSR</cell><cell>4.159</cell><cell>2.213</cell><cell>0.467</cell><cell>4.586</cell><cell>2.498</cell><cell>0.486</cell><cell>4.730</cell><cell>2.548</cell><cell>0.461</cell><cell>3.654</cell><cell>1.978</cell><cell>0.411</cell></row><row><cell>SRResNet</cell><cell>4.164</cell><cell>2.457</cell><cell>0.713</cell><cell>4.524</cell><cell>2.660</cell><cell>0.688</cell><cell>4.690</cell><cell>2.775</cell><cell>0.717</cell><cell>3.667</cell><cell>2.189</cell><cell>0.637</cell></row><row><cell>UrbanFM-ne</cell><cell>4.015</cell><cell>2.047</cell><cell>0.332</cell><cell>4.386</cell><cell>2.258</cell><cell>0.320</cell><cell>4.559</cell><cell>2.352</cell><cell>0.316</cell><cell>3.559</cell><cell>1.845</cell><cell>0.309</cell></row><row><cell>UrbanFM</cell><cell>3.950</cell><cell>2.011</cell><cell>0.327</cell><cell>4.329</cell><cell>2.224</cell><cell>0.313</cell><cell>4.496</cell><cell>2.318</cell><cell>0.315</cell><cell>3.501</cell><cell>1.815</cell><cell>0.308</cell></row><row><cell cols="6">? VDSR [11]: Since both SRCNN and ESPCN follow a three-stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">architecture, they have several drawbacks such as slow conver-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">gence speed and limited representation ability. Inspired by the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>VGG-net, Kim et al. presents a Super-Resolution method using Very Deep neural networks with depth up to 20.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results for di erent M-F settings. that adding more ResBlocks (larger M) or increasing the number of lters (larger F ) can improve the model performance. However, these also increase the training time and memory space. Considering the tradeo between training cost and performance, we set the default se ing of UrbanFM to be16-128.    </figDesc><table><row><cell cols="5">Methods Se ings #Params RMSE MAE MAPE</cell></row><row><cell>SRResNet</cell><cell>20-64</cell><cell>1.8M</cell><cell>4.317 2.586</cell><cell>0.725</cell></row><row><cell>UrbanFM</cell><cell>20-64</cell><cell>1.9M</cell><cell>4.094 2.101</cell><cell>0.321</cell></row><row><cell>SRResNet</cell><cell>16-64</cell><cell>1.5M</cell><cell>4.261 2.520</cell><cell>0.689</cell></row><row><cell>UrbanFM</cell><cell>16-64</cell><cell>1.7M</cell><cell>4.107 2.118</cell><cell>0.322</cell></row><row><cell cols="2">SRResNet 16-256</cell><cell>24.2M</cell><cell>4.178 2.418</cell><cell>0.614</cell></row><row><cell>UrbanFM</cell><cell>16-256</cell><cell>24.4M</cell><cell>4.068 2.087</cell><cell>0.316</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results comparison on Happy Valley.</figDesc><table><row><cell>Methods</cell><cell cols="3">Se ings #Params RMSE MAE MAPE</cell></row><row><cell>MEAN</cell><cell>x</cell><cell>x</cell><cell>9.206 2.269 0.799</cell></row><row><cell>HA</cell><cell>x</cell><cell>x</cell><cell>8.379 1.811 0.549</cell></row><row><cell>SRCNN</cell><cell>768</cell><cell>7.4M</cell><cell>8.291 2.175 0.816</cell></row><row><cell>ESPCN</cell><cell>768</cell><cell>7.5M</cell><cell>8.156 2.155 0.805</cell></row><row><cell>VDSR</cell><cell>20-64</cell><cell>0.6M</cell><cell>8.490 2.128 0.756</cell></row><row><cell>SRResNet</cell><cell>16-128</cell><cell>5.5M</cell><cell>8.318 1.941 0.679</cell></row><row><cell>UrbanFM-sl</cell><cell>16-128</cell><cell>5.5M</cell><cell>8.312 1.939 0.677</cell></row><row><cell cols="2">UrbanFM-ne 16-128</cell><cell>5.5M</cell><cell>8.138 1.816 0.537</cell></row><row><cell>UrbanFM</cell><cell>16-128</cell><cell>5.6M</cell><cell>8.030 1.790 0.531</cell></row><row><cell cols="2">5 RELATED WORK</cell><cell></cell><cell></cell></row><row><cell cols="3">5.1 Image Super-Resolution</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Details settings of Inference Network in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Embedding setting of external factors.</figDesc><table><row><cell>Data</cell><cell>Feature</cell><cell cols="2">#Categroies Embed Length</cell></row><row><cell></cell><cell>Temperature</cell><cell>x</cell><cell>1</cell></row><row><cell>Meteorology</cell><cell>Wind speed</cell><cell>x</cell><cell>1</cell></row><row><cell></cell><cell>Weather</cell><cell>16 (8)</cell><cell>3</cell></row><row><cell></cell><cell>Holiday</cell><cell>2</cell><cell>1</cell></row><row><cell>Time</cell><cell>Weekend Day of week</cell><cell>2 7</cell><cell>1 2</cell></row><row><cell></cell><cell>Hour of day</cell><cell>24</cell><cell>3</cell></row><row><cell>Event</cell><cell>Ticket price</cell><cell>x</cell><cell>1</cell></row></table><note>7 TaxiBJ witnesses 16 kinds of weather conditions: Sunny, Cloudy, Overcast, Rainy, Sprinkle, ModerateRain, HeavyRain, Rainstorm, understorm, FreezingRain, Snowy, LightSnow, ModerateSnow, HeavySnow, Foggy, Sandstorm, Dusty. For HappyValley, only 8 types of above weather conditions are included.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">e original data can be obtained through previously deployed sensors or from crowd-sourcing. 2 h ps://github.com/yoshall/UrbanFM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Distribution of coarse-grained flows (b) Distribution of fine-grained flows</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conference'17, July 2017, Washington, DC, USA Y. Liang et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX FOR REPRODUCIBILITY</head><p>To support the reproducibility of the results in this study, we have released our code and data <ref type="bibr" target="#b5">6</ref> . Our implement is based on Pytorch 0.4.1. Here, we present the details of the dataset, normalization method and experimental se ings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Statistics of Datasets</head><p>In section 4, we have illustrated how we split the training, validation and test set based on the two real-world datasets: TaxiBJ and HappyValley. Since there are some coarse-grained data with most zero entries (i.e., extremely noisy data), we directly remove them from the original dataset. Here, we display the details of available samples in each set in <ref type="figure">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Normalization Method</head><p>We employ data normalization before the training phase to speed up the convergence of our method. Recall that we obtain the inferred distribution H Since the original scale of coarse-and ne-grained ows are di erent, we plot each regional ow volumes from the ow maps of TaxiBJ in <ref type="figure">Figure 10</ref>, where a long tail can be observed in both se ings. An explanation is that some regions can sometimes witness a high ow volume, which can be a ributed to the rush hours or tra c jams <ref type="bibr" target="#b30">[31]</ref>. Due to the long tail, suppose we simply use the maximum of such ows as max-scaler, it will restrict most values to be much smaller than 1. Based on this observation, we set the two max-scaler 1500 and 100 in coarse-and ne-grained data respectively. Likewise, we use the same method to decide the proper scaler in HappyValley dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Se </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pa ern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lanczos ltering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CityMomentum: an online approach for crowd behavior prediction at a citywide level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Shibasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Adachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the ACM International Joint Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="559" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eigenface-domain super-resolution for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bahadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aziz</forename><forename type="middle">Umit</forename><surname>Gunturk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucel</forename><surname>Batur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Altunbasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Monson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><forename type="middle">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mersereau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="597" to="606" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Digital selection and analogue ampli cation coexist in a cortex-inspired silicon circuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Richard Hr Hahnloser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><forename type="middle">A</forename><surname>Sarpeshkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Sebastian</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page">947</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FCCF: forecasting citywide crowd ows based on big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sergey Io E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shi</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Lucas Eis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">International Case Studies of Smart Cities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang Keon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeah</forename><surname>Heeseo Rain Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongbok</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Anyang, Republic of Korea</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geo-MAN: Multi-level A ention Networks for Geo-sensory Time Series Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyu</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuwen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Arti cial Intelligence</title>
		<meeting>the International Joint Conference on Arti cial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">e IEEE Conference on Computer Vision and Pa ern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Super-resolution image reconstruction: a technical overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Kyu</forename><surname>Sung Cheol Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moon Gi</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="21" to="36" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an e cient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prediction of human emergency behavior and their mobility following largescale disaster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihide</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Sekimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shibasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient pro le prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super resolution using edge prior and single image detail synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2400" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sub-pixel mapping of rural land cover objects from ne spatial resolution satellite sensor imagery using super-resolution pixel-swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma W Ornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Peter M Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="491" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Radu Timo E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepsd: Generating high resolution climate change projections through single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Omas Vandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangram</forename><surname>Kodra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auroop R</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1663" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00386</idno>
		<title level="m">Crowd Flow Prediction by Deep Spatio-Temporal Transfer Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Conference on Arti cial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1655" to="1661" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DNNbased prediction model for spatio-temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuwen</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Urban computing: concepts, methodologies, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licia</forename><surname>Capra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouri</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
