<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Preventing Manifold Intrusion with Locality: Local Mixup</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-12">12 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Baena</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Lab-STICC</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6285</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<postCode>F-29238</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Drumetz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Lab-STICC</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6285</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<postCode>F-29238</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Lab-STICC</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6285</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<postCode>F-29238</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Preventing Manifold Intrusion with Locality: Local Mixup</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-12">12 Jan 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixup is a data-dependent regularization technique that consists in linearly interpolating input samples and associated outputs. It has been shown to improve accuracy when used to train on standard machine learning datasets. However, authors have pointed out that Mixup can produce out-of-distribution virtual samples and even contradictions in the augmented training set, potentially resulting in adversarial effects. In this paper, we introduce Local Mixup in which distant input samples are weighted down when computing the loss. In constrained settings we demonstrate that Local Mixup can create a trade-off between bias and variance, with the extreme cases reducing to vanilla training and classical Mixup. Using standardized computer vision benchmarks , we also show that Local Mixup can improve test accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Learning has become the golden standard for many tasks in the fields of machine learning and signal processing. Using a large number of tunable parameters, Deep Neural Networks (DNNs) are able to identify subtle dependencies in large training datasets to be later leveraged to perform accurate predictions on previously unseen data. Without constraints or enough samples, many models can fit the training data (high variance) and it is difficult to find the ones that would generalize correctly (low bias).</p><p>Regularization techniques have been deployed with the aim of improving generalization <ref type="bibr" target="#b8">(Goodfellow et al., 2016)</ref>. In <ref type="bibr" target="#b10">(Guo et al., 2019)</ref>, the authors categorize theses techniques into data-independent or data-Appearing in Proceedings of the 13 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: W&amp;CP 9. Copyright 2010 by the authors. dependent ones. For example some data-independent regularization techniques constrain the model by penalizing the norm of the parameters, for instance through weight decay <ref type="bibr" target="#b23">(Loshchilov and Hutter, 2017)</ref>. A popular data-dependent regularization technique consists of artificially increasing the size of the training set, which is referred to as data augmentation <ref type="bibr" target="#b27">(Simard et al., 2001)</ref>. In the field of computer vision, for example, it is very common to generate new samples using basic class-invariant transformations <ref type="bibr" target="#b12">He et al., 2016a)</ref>.</p><p>In <ref type="bibr" target="#b34">(Zhang et al., 2017)</ref>, the authors introduce Mixup, a data augmentation technique in which artificial training samples (x,?), called virtual samples, are generated through linear interpolations between two training samples (x i , y i ) and (x j , y j ). The associated output is computed as the corresponding linear interpolation on the respective outputs. Mixup improves generalization error of state-of-the-art models on ImageNet, CIFAR, speech, and tabular datasets <ref type="bibr" target="#b34">(Zhang et al., 2017)</ref>. This method is also used in the context of few shot learning <ref type="bibr" target="#b24">(Mangla et al., 2020;</ref><ref type="bibr" target="#b7">Dhillon et al., 2019)</ref>.</p><p>By using linear interpolation, virtual samples can in some cases contradict each other, or even generate out-of-distribution inputs. This phenomenon has been recently described in <ref type="bibr" target="#b10">(Guo et al., 2019)</ref> where the authors use the term manifold intrusion. As such, it is not clear if Mixup is always desirable. More generally, the question arises of whether Mixup could be constrained to reduce the risk of generating such spurious interpolations. In this paper we introduce Local Mixup, where virtual samples are weighted in the training loss. The weight of each possible virtual sample depends on the distance between the endpoints of the corresponding segment (x i , x j ). In particular, this method can be implemented to forbid interpolations between samples that are too distant from each other in the input domain, reducing the risk of generating spurious virtual samples.</p><p>Here are our main contributions:</p><p>? We introduce Local Mixup, a mixup method de-Figure 1: Illustration of the proposed Local Mixup method. On the left, only vanilla samples are used, without data augmentation. Ground truth is depicted in filled regions. On the middle we depict Local Mixup where we only interpolate samples which are close enough, leading to no contradiction with ground thuth. On the right we depict Mixup in which we interpolate all samples, leading to contradictory virtual samples.</p><p>pending on a single parameter whose extremes correspond to classical Mixup and Vanilla.</p><p>? In dimension one, we prove that Local Mixup allows to select a bias/variance trade-off. ? In higher dimensions, we show that Local Mixup can help achieve more accurate models than classical Mixup using standard vision datasets.</p><p>? Our work contributes more broadly to better understanding the impact of Mixup during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Introducing notations: In Machine or Deep Learning, a training dataset D train is used to learn the model's parameters, and a test one D test is used to evaluate the performance of the model on previously unseen inputs <ref type="bibr" target="#b1">(Bishop, 2006)</ref>. We also consider that both input and output data lie in metric spaces (X , d X ) and (Y, d Y ). Typically, X and Y are assumed to be Euclidean spaces with the usual metrics. We denote by f : X ? Y the parametric model to be trained and by F the hypothesis set, i.e. the set containing all candidate parametrizations of the model f ? F .</p><p>To train our model, we use an error function L that measures the discrepancy between the model outputs and expected ones. Training the model amounts to minimizing the training loss while generalization may be quantitavely evaluated by the test loss:</p><formula xml:id="formula_0">L vanilla = (x,y)?D L(f (x), y).</formula><p>Data augmentation and mixup: To improve generalization one can use regularization techniques <ref type="bibr" target="#b8">(Goodfellow et al., 2016)</ref>. Among them, data augmentation is a form of data-dependent regularization <ref type="bibr" target="#b10">(Guo et al., 2019)</ref>.</p><p>It artificially generates new samples, resulting in increasing D train <ref type="bibr" target="#b27">(Simard et al., 2001)</ref>, and can apply on the outputs y <ref type="bibr" target="#b28">(Sukhbaatar et al., 2014)</ref> or on the inputs x <ref type="bibr" target="#b35">(Zhang et al., 2016;</ref><ref type="bibr" target="#b6">DeVries and Taylor, 2017;</ref><ref type="bibr" target="#b33">Yun et al., 2019;</ref><ref type="bibr" target="#b5">Cubuk et al., 2018;</ref><ref type="bibr" target="#b12">He et al., 2016a)</ref>.</p><p>The use of data-dependent methods relying on some sort of mixing has recently emerged <ref type="bibr" target="#b34">(Zhang et al., 2017;</ref><ref type="bibr" target="#b29">Verma et al., 2019;</ref><ref type="bibr" target="#b33">Yun et al., 2019;</ref><ref type="bibr" target="#b6">DeVries and Taylor, 2017;</ref><ref type="bibr" target="#b14">Hendrycks et al., 2019;</ref><ref type="bibr" target="#b18">Kim et al., 2020;</ref><ref type="bibr" target="#b4">Chou et al., 2020;</ref><ref type="bibr" target="#b22">Liu et al., 2021;</ref><ref type="bibr" target="#b3">Chen et al., 2020;</ref><ref type="bibr" target="#b32">Yin et al.;</ref><ref type="bibr" target="#b26">Rame et al., 2021)</ref>. They usually mix two or more inputs and the corresponding labels.</p><p>The pioneering mixing method is Mixup <ref type="bibr" target="#b34">(Zhang et al., 2017)</ref>, whose mixed samples (x,?) are generated by linear interpolations between pairs of samples, i.e. x i,j,? = ?x i + (1 ? ?)x j and? i,j,? = ?y i + (1 ? ?)y j for some training samples (x i , y i ) and (x j , y j ) and some ? ? [0, 1] . The Mixup training criterion is defined as: </p><formula xml:id="formula_1">f * = argmin f ?F 1 n 2 E ? ? ? ? ? ? ? ? D 2 train L (? i,j,? , f (x i,j,? )) Lmixup ? ? ? ? ? ? ? .</formula><p>In other words, Mixup encourages the model f to associate linearly interpolated inputs with the corresponding linearly interpolated outputs <ref type="bibr" target="#b34">(Zhang et al., 2017)</ref>. The positive effect of this linear behavior in between samples questioned several authors who aimed at explaining theoretically and empirically Mixup. <ref type="bibr" target="#b2">Carratino et al. (2020)</ref> shows that Mixup can be interpreted as the combination of a data transformation and a data perturbation. A first transform shrinks both inputs and outputs towards their mean. The second transform applies a zero mean perturbation. The proof is given by reformulating the Mixup loss. <ref type="bibr" target="#b11">Gyawali et al. (2020)</ref> highlight that Mixup impacts the Lipschitz constant L of the gradient of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvements over mixup:</head><p>In other works, authors propose to improve Mixup using various approaches. For example in <ref type="bibr" target="#b4">(Chou et al., 2020)</ref>, the idea is to use different ? x , ? y to mix the input and the outputs, in <ref type="bibr" target="#b22">(Liu et al., 2021;</ref><ref type="bibr" target="#b26">Rame et al., 2021;</ref><ref type="bibr" target="#b33">Yun et al., 2019)</ref>, the authors explore using other (i.e. nonlinear) interpolation methods, in <ref type="bibr" target="#b32">(Yin et al.;</ref><ref type="bibr" target="#b9">Greenewald et al., 2021;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref> the authors extend the mixing to more than two elements.</p><p>Our proposed approach: In this paper, we aim at avoiding the phenomenon described as manifold intrusion, and introduced in <ref type="bibr" target="#b10">(Guo et al., 2019)</ref>. This phenomenon is depicted in <ref type="figure" target="#fig_0">Figure 1</ref> on the right, where we see that virtual samples created through mixup between distant red samples lie outside the manifold domain for the red class. As we do not have access to the underlying manifold domains when we train a model, the rationale of our contribution is to favor interpolations between samples that are close enough in the input domain. Where the method described in <ref type="bibr" target="#b10">(Guo et al., 2019)</ref> learns which interpolations should be kept through training, we advocate in this paper for a purely geometric approach where a decreasing weight is applied when computing the loss depending on the distance between interpolated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mixup in dimension 1</head><p>Let us consider the simple case where our model f is defined on R. Without loss of generality, let us consider that the training set</p><formula xml:id="formula_2">D train = {x i , y i } is ordered by increasing input, i.e, x i ? x i+1 .</formula><p>For a givenx, Mixup's loss implies that the output f * (x) of the model is determined by the set E(x) of all convex combinations that can be obtainx from two training inputs x i and x j :</p><formula xml:id="formula_3">E(x) = {i, j, ? i,j |x = ? ij x i + (1 ? ? ij ) x j }. It is clear that for anyx ? [x 0 , x n?1 ], E(x)</formula><p>is non empty and finite. In practice, the distribution of ? can be uniform <ref type="bibr" target="#b34">(Zhang et al., 2017;</ref><ref type="bibr" target="#b29">Verma et al., 2019)</ref> ? ? Beta(? = 1, ? = 1) = U(0, 1). In this case, we show that the output f * (x) of an input x ? [x 0 , x n ] is the barycenter of the target values corresponding to the points of E(x).</p><formula xml:id="formula_4">Lemma 3.1. ?x ? [x 0 , x n?1 ], f * (x) = 1 card(E(x)) (i,j,?i,j )?E(x) ? i,j y i + (1 ? ? i,j )y j .</formula><p>(1)</p><p>Proof. Letx ? [x 0 , x n?1 ] and 0 ? ? ? 1. For a given triplet (i, j, ?) ? E(x).</p><p>We have</p><formula xml:id="formula_5">E[L(y i , j, ? i,j , f * (x))|x, i, j, ? ij ] = L(y i , j, ? i,j , f * (x))</formula><p>as the value of y i , j, ? i,j andx are known. Then we minimize the error for all y i , j, ? i,j given by E(x). Then the value of f * (x) is only determined by the sum of the losses over E(x) since the elements of E(x) are equally probable (distributions of i, j, ? are uniform).</p><formula xml:id="formula_6">E[L(f * (x), y i,j,?i,j ) = E(x) E[L(f * (x), y i,j,?i,j )|x, i, j, ? ij ] = E(x) L(f * (x), y i,j,?i,j ) )<label>(2)</label></formula><p>We assume L to be either the cross entropy or the squared L2 loss. In either case, by nulling the derivative of Equation <ref type="formula" target="#formula_6">(2)</ref> w.r.t. the value f * (x), we get:</p><formula xml:id="formula_7">f * (x) = 1 card(E(x))) E(x) y i,j,?i,j</formula><p>A consequence of this lemma is the following theorem:</p><p>Theorem 3.2. The function f * that minimizes the loss on the training set is piecewise linear on</p><formula xml:id="formula_8">[x 0 , x n?1 ], linear on each segment [x i , x i+1 ] and de- fined by Equation (1). Whenx varies in [x i , x i+1 ]</formula><p>, the set of possible combinations (between training samples) leading tox does not change, only the corresponding coefficients ? vary linearly. Since the expression of Equation <ref type="formula">(1)</ref> is linear in each of those coefficients, f * is itself linear as a function ofx. The set of possible combinations will change wheneverx switches to another interval, e.g. [x i?1 , x i ]. In this case new combinations are possible and others may disappear, leading to another linear function. f * is still continuous everywhere because new or disappearing combinations are associated either to ? = 0 or ? = 1 forx = x j and j ? {1, ? ? ? , n}.</p><p>In practice inferring a function f * that minimizes that the Mixup Criterion is usually not desired in machine learning, and one looks for f with a sufficiently small loss to have a regularizing effect. Indeed f * is not likely to generalize well. Still, we note that it tends to an average of convex combinations and thus leads to a model with a low variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Local Mixup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Locality graphs</head><p>Consider a (training) dataset D made of pairs (x, y). We propose to build a graph from D as follows. We</p><formula xml:id="formula_9">define G D = V, W where V = {x | ?y, (x, y) ? D}. The symmetric real matrix W is based on D, where D is the pairwise distance matrix D[i, j] = d X (x i , x j ).</formula><p>In this work, we consider various ways to obtain W, but the rationale is always the same: to obtain a similarity matrix where large weights correspond to closest pairs of samples. Namely, we consider K-nearest neighbors graphs, where we set to 1 weights of target vertices corresponding to the K closest samples for a given source vertex and 0 otherwise; thresholded</p><formula xml:id="formula_10">graphs where W[i, j] = ?(D[i, j]) and ?(d) = 1 d?? ; smooth decreasing exponential graphs where W[i, j] = exp(??D[i, j]).</formula><p>The loss is then weighted using W:</p><formula xml:id="formula_11">L local mixup = D 2 train W[i, j]L (? i,j,? , f (x i,j,? )) . (3)</formula><p>For computational cost considerations, we compute a graph for each batch (random subset) of samples during stochastic gradient descent. As such, the weights associating two samples can vary depending on the chosen graph and random batch.</p><p>In the extreme case where some weights are 0, the corresponding virtual samples are discarded during gradient descent, resulting in only considering local interpolations of samples, hence the name Local Mixup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Low dimension</head><p>In this section, we are interested in proving that Local Mixup allows to tune a trade-off between bias and variance on trained models. For this purpose, we simplify the problem to dimension 1 and only consider K-nearest neighbor graphs.</p><p>In this case, note that varying K can create a range of settings where K = 0 boils down to vanilla training and K ? n where n is the number of training samples boils down to classical Mixup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Local Mixup and the bias/variance trade-off</head><p>Let us first recall the definitions of the bias and variance in the context of a machine learning problem. We define Bias and Variance as follow:</p><formula xml:id="formula_12">? Bias: Bias(f ) 2 = E train [(f (x) ? y) 2 ]. ? Variance: V ar(f ) = E train [(f ? E train [f ]) 2 ].</formula><p>We consider two settings. In the first one, the input domain Z/nZ is periodic and thus the number of samples is finite. In the second one, the input <ref type="figure">Figure 2</ref>: We depict here the terms of f * K (x i ) given by Eq (4) for different K. In blue the interpolations corresponding to 2Ky i and in red the terms of the sum S K . On the right, K = 2 and on the left K = 3.</p><formula xml:id="formula_13">x i?2 x i?1 x i x i+1 x i+2 x i?2 x i?1 x i x i+1 x i+2</formula><p>domain Z is infinite and outputs are independent and identically (i.i.d) generated using a random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Periodic setting</head><p>Let us consider that the training set D train is made of</p><formula xml:id="formula_14">pairs (x, y), where {x | ?y, (x, y) ? D train } = Z/nZ. We also consider d X (x, x ? ) = |x ? x ? | ? {0, ? ? ? , n ? 1}.</formula><p>In this case, we can write explicit formulations of f * K , the function that minimizes the Local Mixup criterion for K-nearest neighbors graphs. Following similar arguments to those used to obtain Equation <ref type="formula">(1)</ref>: for a given x i we know that the optimal value for f * K (x i ) would be an average of the the? that correspond to the possible interpolations. we obtain:</p><formula xml:id="formula_15">?x i ? Z/nZ, f * K (x i ) = 1 K(K + 3)/2 (2Ky i + S K (x i )),<label>(4)</label></formula><p>where S K (x i ) is defined recursively as follows:</p><formula xml:id="formula_16">S K+1 = 0 if K = 0 S K (x i ) + A K+1 (x i ) ?K ? 1 .<label>(5)</label></formula><p>and:</p><formula xml:id="formula_17">A K (x i ) = 1 K K?1 k=1 (K ? k) ? y i?k + k ? y i+K?k .</formula><p>On <ref type="figure">Figure 2</ref> we depicted for a given x i the different interpolations and? that contribute to f K (x i ). In blue the interpolation between x i and its direct neighbors, in red the interpolation between points other than x i that happen to intersect x i . As we increase K, the influence of S K (red points) increases.</p><p>We obtain the following Lemma, showing that the expected value of f * K is invariant with respect to K:</p><formula xml:id="formula_18">Lemma 4.1. [Expected value of f * k ] For any K, the expected value of f * K is E train [f * K ] = E train [y].<label>(6)</label></formula><p>Proof.</p><formula xml:id="formula_19">E train [f * K ] = 1 n n i=1 f * K (x i ) = 2 nK(K + 3) (2nKE train [y] + n i=1 K k=1 A k (x i )).</formula><p>and using the fact that y i+n = y i :</p><formula xml:id="formula_20">n i=1 K k=1 A k (x i ) = n i=1 K k=1 k?1 l=1 k ? l k y i?l + l k y i+k?l = n i=1 y i K k=1 k?1 l=1 1 = nE train [y] K(K ? 1) 2 . then E train [f * K ] = E train [y].</formula><p>We obtain the following theorem:</p><p>Theorem 4.2 (Convergence of f * K in the periodic setting). As K grows, it holds that:</p><formula xml:id="formula_21">?x i ? Z/nZ, f * K (x i ) ? E Dtrain [y],<label>(7)</label></formula><p>Bias</p><formula xml:id="formula_22">2 (f * K ) ? E train [(y i ? E train [y]) 2 ],<label>(8)</label></formula><formula xml:id="formula_23">V ar(f * K ) = E train [(f * K (x i ) ? E train [f * K (x i ]) 2 ] ? 0,<label>(9)</label></formula><p>V ar(f * K ) is eventually nonincreasing.</p><p>Proof. We can explicitly write the limit of S K . We first prove this lemma (the proof is omitted here but available as supplementary material):</p><p>Lemma 4.3. Let K = M n + r, M ? N * and 0 ? r &lt; n ? 1. We assume E train (y) ? 0, then:</p><formula xml:id="formula_24">(M + 1)n ? E train (y) + ? ? A K ? M n ? E train (y) ? ?,<label>(10)</label></formula><p>with ? = O(KE train (y)).</p><p>Then combined with Equation <ref type="formula" target="#formula_16">(5)</ref> we can demonstrate the convergence of the sum S K and find its limit:</p><formula xml:id="formula_25">Corollary 4.3.1. For K = N M ? ? S K ? 1 2 n i=1 y i M 2 n = 1 2 E train (y)K 2 .<label>(11)</label></formula><p>As a result, given Equation <ref type="formula" target="#formula_15">(4)</ref>, the limit of f * K is E train (y).</p><p>To prove the monotonicity of the variance we want to show: V ar(f * K+1 ) ? V ar(f * K ) for K large enough.</p><p>We use the K?nig-Huygens theorem and Lemma 4.1 to compute the difference between the two variances:</p><formula xml:id="formula_26">V ar(f K+1 ) ? V ar(f K ) = E Dtrain [(f K+1 (x)) 2 ] ? E Dtrain [(f K (x)) 2 ] = E Dtrain [(f K+1 (x)) 2 ? (f K (x)) 2 ].</formula><p>We then show that for any x ? [x 0 , x n?1 ] and K large enough, (f K+1 (x)) 2 ? (f K (x)) 2 . To do so we get an asymptotic equivalent:</p><formula xml:id="formula_27">(f K+1 (x)) 2 ? (f K (x)) 2 ? ? K C ? E 2 train [y], where C is a positive constant.</formula><p>This theorem states two main results: 1) in the case of Mixup the function that minimize the loss f * has zero variance and converges to E train [y]. 2). Eventually the variance of the function that minimizes the Local Mixup criterion is decreasing, showing that the proposed Local Mixup can indeed tune the trade-off between the bias and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i.i.d random output setting</head><p>Let us now consider that the training set is made of inputs {x | ?y, (x, y) ? D train } = Z and y i are i.i.d. according to a random variable R of variance ? 2 .</p><p>Theorem 4.4. For a signal with i.i.d outputs, the variance is eventually bounded by:</p><formula xml:id="formula_28">4 2 ? 2 K 2 ? V ar(f K (x i )) ? 8? 2 K .<label>(12)</label></formula><p>Proof. Let us choose x i and K &gt; 1. First observe that f * K (x i ) is a sum of random variables. We rewrite S K with the coefficients a K k = K l=k+1 l?k l :</p><formula xml:id="formula_29">S K = K?1 k=1 (y i?k + y i+k )a K k . We obtain: V ar(f * K (x i )) = V ar 2 ? (2Ky i + S K ) K(K + 3) .</formula><p>leading to:</p><p>V ar(f * K (x i )) = 4 2</p><formula xml:id="formula_30">K K(K + 3) 2 V ar(y i ) + K?1 k=1 2a (K) k K(K + 3) 2 (V ar(y i?k ) + V ar(y i+k )).</formula><p>We use the fact that 1 K ? a K k ? K. Then when K ? ?:</p><formula xml:id="formula_31">4 2 ? 2 K 2 ? V ar(f K (x i )) ? 8? 2 K .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Invariance of linear models</head><p>Interestingly, we can show that both Mixup and Local Mixup lead to the same optimal linear models, as stated in the following theorem: Proof. For mixup, we showed with Equation <ref type="formula">(1)</ref>  </p><formula xml:id="formula_32">d Y (f (x i ), f (x j )) ? qd X (x i , x j ).<label>(13)</label></formula><p>If f is q-Lipschitz continuous, we define the optimal Lipschitz constant Q sup as</p><formula xml:id="formula_33">Q sup = sup xi,xj?X,xi =xj d Y (f (x i ), f (x j )) d X (x i , x j ) .<label>(14)</label></formula><p>For simplicity, let us consider a classification problem where d Y is 0 if the two considered samples are of the same class and 1 otherwise.</p><p>Then the training set imposes a lower bound on the optimal Lipschitz constant:</p><formula xml:id="formula_34">Q sup ? min xi,xj?D,yi =yj d X (x i , x j ) ?1 Q(D)</formula><p>.</p><p>For Mixup and Local Mixup, the virtual samples increase the size of the training set, resulting in stronger constraints on the optimal Lipschitz constant.</p><p>In more details, consider the case of a thresholded graph with parameter ? when using Local Mixup. In this case, the increased training set for each class y can be written as S ? (y)</p><formula xml:id="formula_36">= {?x i + (1 ? ?)x j | 0 ? ? ? 1 y i = y j = y, d X (x i , x j ) ? ?},</formula><p>the set of all segments constructed from two samples that are close enough in the input domain and sharing the same label y. We then obtain the following theorem:</p><p>Theorem 4.6. The lower bound Q(D) is increasing with ?.</p><p>Proof. We directly use the inclusion S ? (y) ? S ? ? (y), ?? ? ? ? .</p><p>We shall show in the experiments that ? can indeed impact Q(D) on standard vision datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Low dimension</head><p>As stated in the introduction and <ref type="bibr" target="#b10">(Guo et al., 2019)</ref>, Mixup leads to interpolations that may be misleading for the model. To illustrate this effect, we consider a 2d toy dataset of two coiling spirals where such interpolations occur frequently. The two coiling spirals is a binary classification dataset: each spiral corresponds to a different class. We expect to retrieve better performance for Local Mixup compared to Mixup: local interpolations are likely to stay in the same spiral and therefore avoid manifold intrusion. For this experiment we use a thresholded graph with parameter ?.</p><p>To carry out this experiment, we generate 1000 samples for each class and add a Gaussian noise with standard deviation ? = 1.5 (controlling the spirals' thickness). A typical draw is depicted in <ref type="figure">Figure 3</ref>. We use a large value of ? to avoid trivial solutions to the problem. Once the dataset is generated we split it randomly into two parts: a training set containing 80% of the samples and a test set containing the remaining 20% (used to compute the error rates). We then use a fully connected neural network made of two hidden layers with 100 neurons and use the ReLU function as non linearity. We average the test errors over 1000 runs. For small values of ? many weights of the graph are zero and thus the corresponding interpolations are disregarded into the loss. This means that for a given batch only a small proportion of samples are regarded to compute the loss. Without any correction, different values of ? lead to different batch sizes. To avoid side effects, we vary the batch size so that in average the same number of samples are used to update the loss.</p><p>To select an appropriate value of ?, we first looked at the distribution of distances between pairs of inputs in the training set. This distribution is depicted in <ref type="figure">Figure 4</ref>. We observe that the distribution is relatively uniform between 0 and 4, and as such in our experiments we vary ? between 0 and 4 using steps of 0.5.</p><p>In <ref type="figure" target="#fig_1">Figure 5</ref>, we depict the evolution of the average error rate as a function of the parameter ?. Recall that the extremes for ? = 0 and ? = 4 correspond respectively to Vanilla and Mixup. One can note the significant benefit of Mixup and Local Mixup over Vanilla. As expected, Local Mixup presents a minimum error rate which is significantly smaller than Mixup's error rate. We can note that the minimum is reached with a value of ? smaller than the first quantile. This means that for this dataset Mixup interpolations given above this threshold are either useless or misleading for the network's training.</p><p>It is worth pointing out that this toy dataset is particularly suitable to generate contradictory virtual samples. We delve into more complex and real world datasets in the following subsection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">High dimension</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Lipschitz lower bound</head><p>To illustrate the impact of ? on the optimal Lipschitz constant, we use the dataset CIFAR-10 ( <ref type="bibr" target="#b19">Krizhevsky, 2012)</ref> which is made of small images of size 32x32 pixels and 3 colors. There are 50,000 images in the training set corresponding to 10 classes.</p><p>We are interested in showcasing the evolution of Q(D) when varying ?. The results are depicted in <ref type="figure" target="#fig_4">Figure 6</ref>. For classical Mixup we obtained Q(D) = 0.11 and for Vanilla Q(D) = 0.073. Note that these two extremes are reached with Local Mixup when ? = 0 and ? ? 50.</p><p>We observe that ? can be used to smoothly tune the lower bound Q(D). In practice, a lower Q(D) is preferable, but this only accounts for the optimal Lipschitz constant. Larger values of ? lead to larger training sets and thus potentially better generalization. 8.20 ? 0.13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Experiments on classification dataset</head><p>We now test our proposition on different classification datasets and architectures. We consider the datasets CIFAR10 <ref type="bibr" target="#b19">(Krizhevsky, 2012)</ref>, Fashion-MNIST <ref type="bibr" target="#b31">(Xiao et al., 2017)</ref> and SVHN <ref type="bibr" target="#b25">(Netzer et al., 2011)</ref>. Fashion-MNIST is composed of clothes images of size 28x28 pixels (grayscale) . There are 60,000 images in the training set corresponding to 10 classes. SVHN is a real-world image dataset made of small cropped digits of size 32x32 pixels and 3 colors. There are 73257 digits in the training set corresponding to 10 classes. For these tests, we use a smooth decreasing exponential graph parametrized by ?.</p><p>For CIFAR10, we implement a ResNet18 <ref type="bibr" target="#b13">(He et al., 2016b)</ref> as in <ref type="bibr" target="#b34">(Zhang et al., 2017)</ref>, and average the error rates over 100 runs. We report the mean and confidence interval at 95%. We observed that Local Mixup with a value of ? = 0.003 showed a smaller error rate than the Vanilla network and Mixup, with disjoint confidence intervals. For Fashion MNIST, we implement a Densenet <ref type="bibr" target="#b15">(Huang et al., 2017)</ref> and average the error rates over 10 runs. We also report the mean and confidence intervals at 95%. Again, Local Mixup with a value of ? = 1e ? 3 presents a smaller error rate than both the baseline and Mixup. Note that for this dataset and this network architecture Mixup impacts negatively the error rate, suggesting that on this dataset Mixup creates spurious interpolations as discussed in <ref type="bibr" target="#b10">Guo et al. (2019)</ref>. For SVHN we implement a <ref type="bibr">LeNet-5(LeCun et al., 1998)</ref> architecture (3 convolution layers). Again, Local Mixup performs better than both Vanilla and Mixup.</p><p>For these experiments, we also tried to use a K-nearest neighbor graph or a thresholded graph but without being able to achieve smaller error rates compared to Mixup or even Vanilla. This may indicate that some segments generated by Mixup are important to act as a regularizer during training even if some of them may generate manifold intrusions. By tuning ?, we weigh the importance of this regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Discussion</head><p>Experiments in both low and high dimensions demonstrated the capacity of Local Mixup to outperform Mixup thanks to the use of locality. Still, the choice of the added hyper-parameter (?, ? or K) is essential and data dependent. For now, we reported results selecting the parameter leading to the best test error rate among a small number of possibilities. In future work we would like to rely on quantitative information given on the topology such as the histogram of the distance or persistence diagrams <ref type="bibr" target="#b30">(Wasserman, 2018)</ref> to tune these hyper-parameters.</p><p>Note also that to embed the notion of locality we decided to use the Euclidean metric, although in general datasets lie in nonlinear manifolds. On CI-FAR10 for example, in <ref type="bibr" target="#b0">(Abouelnaga et al., 2016)</ref> the authors show that it is possible to achieve classification scores significantly better than the chance level using the Euclidean metric, but very far from stateof-the-art. There would be many possibilities to improve over using the Euclidean metric, including using pullback metrics <ref type="bibr" target="#b16">(Jost and Jost, 2008;</ref><ref type="bibr" target="#b17">Kalatzis et al., 2020)</ref> given by the euclidean distance between the samples once in the feature space corresponding to the penultimate layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduced a methodology called Local Mixup, in which pairs of samples are interpolated and weighted in the loss depending on the distance between them in the input domain. This methodology comes with a hyper-parameter that allows to provide a continuous range of solutions between Vanilla and classical Mixup. Using a simple framework, we showed that Local Mixup can control the bias/variance tradeoff of trained models. In more general settings, we showed that Local Mixup can tune a lower bound on the Lipschitz constant of the trained model. We used real world datasets to prove the ability of Local Mixup to achieve better generalization, as measured using the test error rate, than Vanilla and classical Mixup.</p><p>Overall, our methodology introduces a simple way to incorporate locality notions into Mixup. We believe that such a notion of locality is beneficial and could be leveraged to a greater level in future work, or could be incorporated to the various Mixup extensions that have been proposed in the community. In future work, we would like to investigate further the choice of the graph, the choice of the hyper-parameter that comes with it, and trainable versions of Local Mixup. Extending the theoretical results to more general contexts would definitely allow to gain further intuition on the effect of locality on Mixup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 4. 1 (</head><label>1</label><figDesc>Bias and Variance). Let us consider a training set D train and a function f from X to Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 4. 5 .</head><label>5</label><figDesc>For a linear model: f (x) = ax + b, a, b ? R, the function f * that minimizes the loss of Mixup and Local Mixup is the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Illustration of the two coiling spiral dataset with 1000 samples per class and ? = 1Histogram of Euclidean distances d X between pairs of inputs on the two coiling spirals dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Error rate as a function of ? for the two coiling spirals dataset. Values are averaged over 1000 runs. Extremes correspond respectively to Vanilla (? = 0) and Mixup (? &gt; 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Evolution of Q(D) on the dataset CIFAR10. Note that ? = 0 corresponds to Vanilla. ? = 50 corresponds to classical Mixup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the function f * is a piecewise linear function. The same equation applies for Local Mixup except that the set E x is smaller for Local Mixup as the number of endpoints is restricted. As a piecewise linear function, linear oneach segment [x i , x i+1 ]: f * can be written as f * = a i x+b i where each (a i , b i ) are defined on [x i , x i+1 ].Let us consider F to be restricted to linear functions, then the coefficients a, b are the averages of the (a i , b i ).The proofs given in low dimension have some limitations. Basically, the averaging effect happens since any point x within the interval [x 1 , x n ] can be written as at least one convex combination of pairs from the training set. Contradictions may occur as illustrated above when several combinations corresponds to x. In higher dimension such explicit contradictions are not necessarily expected. Still, we show that Local Mixup has an impact on the Lipschitz constant of the networks.</figDesc><table><row><cell>4.3 High Dimension and Lipschitz constraint</cell></row><row><cell>First recall the definition of a q-Lipschitz function:</cell></row><row><cell>Definition 4.2 (Lipschitz Continuous and Lipschitz</cell></row><row><cell>Constant). Given two metric spaces (X , d X ), (Y, d Y )</cell></row><row><cell>and a function f : X ? Y, f is Lipschitz continuous</cell></row><row><cell>if there exists a real constant q ? 0 s.t for all x i and</cell></row><row><cell>x</cell></row></table><note>j in X ,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Error rates (%) on CIFAR10, Fashion-MNIST and SVHN. Values are averaged on 100 runs for Ci-far10 and 10 runs for Fashion-MNIST and SVHN. Mean errors with their confidence interval are given.</figDesc><table><row><cell>MODEL</cell><cell>CIFAR-10</cell><cell>ERROR %</cell></row><row><cell></cell><cell>Baseline</cell><cell>4.98 ? 0.03</cell></row><row><cell>Resnet18</cell><cell>Mixup</cell><cell>4.13 ? 0.03</cell></row><row><cell></cell><cell>LM(? = 3e ? 3)</cell><cell>4.03 ? 0.03</cell></row><row><cell></cell><cell>FASHION-MNIST</cell><cell></cell></row><row><cell></cell><cell>Baseline</cell><cell>6.20 ? 0.2</cell></row><row><cell>DenseNet</cell><cell>Mixup</cell><cell>6.36 ? 0.16</cell></row><row><cell></cell><cell>LM (? = 1e ? 3)</cell><cell>5.97 ? 0.2</cell></row><row><cell></cell><cell>SVHN</cell><cell></cell></row><row><cell></cell><cell>Baseline</cell><cell>10.01 ? 0.15</cell></row><row><cell>LeNet</cell><cell>Mixup</cell><cell>8.31 ? 0.14</cell></row><row><cell></cell><cell>LM (? = 5e ? 2)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cifar-10: Knn-based ensemble of classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehya</forename><surname>Abouelnaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hager</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Rady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moustafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Computational Science and Computational Intelligence (CSCI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition. Machine learning</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Carratino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06049</idno>
		<title level="m">On mixup regularization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Kyrillidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12618</idno>
		<title level="m">Stackmix: A complementary mix algorithm</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remix: Rebalanced mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ping</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chieh</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02729</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristjan</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02933</idno>
		<title level="m">k-mixup regularization for deep learning via optimal transport</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3714" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhancing mixup-based semi-supervised learning with explicit lipschitz regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashnna</forename><surname>Kumar Gyawali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandesh</forename><surname>Ghimire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1046" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Riemannian geometry and geometric analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeurgen</forename><surname>Jost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">42005</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kalatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eklund</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05227</idno>
		<title level="m">Georgios Arvanitidis, and S?ren Hauberg. Variational autoencoders with riemannian brownian motion priors</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5275" to="5285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13027</idno>
		<title level="m">Automix: Unveiling the power of mixup</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mixmo: Mixing multiple inputs for multiple outputs via deep subnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remy</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06132</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognition -tangent distance and tangent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2001-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topological data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="501" to="532" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fashionmnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<title level="m">Batchmixup: Improving training by interpolating hidden states of the entire mini-batch</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
