<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascade R-CNN: Delving into High Quality Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-12-03">3 Dec 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
							<email>zwcai@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascade R-CNN: Delving into High Quality Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-12-03">3 Dec 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In object detection, an intersection over union (IoU)   threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code will be made available at https://github.com/zhaoweicai/cascade-rcnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is a complex problem, requiring the solution of two main tasks. First, the detector must solve the recognition problem, to distinguish foreground objects from background and assign them the proper object class labels. Second, the detector must solve the localization problem, to assign accurate bounding boxes to different objects. Both of these are particularly difficult because the detector faces many "close" false positives, corresponding to "close but not correct" bounding boxes. The detector must find the true positives while suppressing these close false positives. Many of the recently proposed object detectors are based on the two-stage R-CNN framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref>, where detection is framed as a multi-task learning problem that combines classification and bounding box regression. Unlike object recognition, an intersection over union (IoU) threshold is required to define positives/negatives. However, the commonly used threshold values u, typically u = 0.5, establish quite a loose requirement for positives. The resulting detectors frequently produce noisy bounding boxes, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (a). Hypotheses that most humans would consider close false positives frequently pass the IoU ? 0.5 test. While the examples assembled under the u = 0.5 criterion are rich and diversified, they make it difficult to train detectors that can effectively reject close false positives.</p><p>In this work, we define the quality of an hypothesis as its IoU with the ground truth, and the quality of the detector as the IoU threshold u used to train it. The goal is to investi-gate the, so far, poorly researched problem of learning high quality object detectors, whose outputs contain few close false positives, as shown in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>. The basic idea is that a single detector can only be optimal for a single quality level. This is known in the cost-sensitive learning literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>, where the optimization of different points of the receiver operating characteristic (ROC) requires different loss functions. The main difference is that we consider the optimization for a given IoU threshold, rather than false positive rate.</p><p>The idea is illustrated by <ref type="figure" target="#fig_0">Figure 1 (c) and (d)</ref>, which present the localization and detection performance, respectively, of three detectors trained with IoU thresholds of u = 0.5, 0.6, 0.7. The localization performance is evaluated as a function of the IoU of the input proposals, and the detection performance as a function of IoU threshold, as in COCO <ref type="bibr" target="#b19">[20]</ref>. Note that, in <ref type="figure" target="#fig_0">Figure 1</ref> (c), each bounding box regressor performs best for examples of IoU close to the threshold that the detector was trained. This also holds for detection performance, up to overfitting. <ref type="figure" target="#fig_0">Figure 1 (d)</ref> shows that, the detector of u = 0.5 outperforms the detector of u = 0.6 for low IoU examples, underperforming it at higher IoU levels. In general, a detector optimized at a single IoU level is not necessarily optimal at other levels. These observations suggest that higher quality detection requires a closer quality match between the detector and the hypotheses that it processes. In general, a detector can only have high quality if presented with high quality proposals.</p><p>However, to produce a high quality detector, it does not suffice to simply increase u during training. In fact, as seen for the detector of u = 0.7 of <ref type="figure" target="#fig_0">Figure 1</ref> (d), this can degrade detection performance. The problem is that the distribution of hypotheses out of a proposal detector is usually heavily imbalanced towards low quality. In general, forcing larger IoU thresholds leads to an exponentially smaller numbers of positive training samples. This is particularly problematic for neural networks, which are known to be very example intensive, and makes the "high u" training strategy quite prone to overfitting. Another difficulty is the mismatch between the quality of the detector and that of the testing hypotheses at inference. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, high quality detectors are only necessarily optimal for high quality hypotheses. The detection could be suboptimal when they are asked to work on the hypotheses of other quality levels.</p><p>In this paper, we propose a new detector architecture, Cascade R-CNN, that addresses these problems. It is a multi-stage extension of the R-CNN, where detector stages deeper into the cascade are sequentially more selective against close false positives. The cascade of R-CNN stages are trained sequentially, using the output of one stage to train the next. This is motivated by the observation that the output IoU of a regressor is almost invariably better than the input IoU. This observation can be made in <ref type="figure" target="#fig_0">Figure 1</ref> (c), where all plots are above the gray line. It suggests that the output of a detector trained with a certain IoU threshold is a good distribution to train the detector of the next higher IoU threshold. This is similar to boostrapping methods commonly used to assemble datasets in object detection literature <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref>. The main difference is that the resampling procedure of the Cascade R-CNN does not aim to mine hard negatives. Instead, by adjusting bounding boxes, each stage aims to find a good set of close false positives for training the next stage. When operating in this manner, a sequence of detectors adapted to increasingly higher IoUs can beat the overfitting problem, and thus be effectively trained. At inference, the same cascade procedure is applied. The progressively improved hypotheses are better matched to the increasing detector quality at each stage. This enables higher detection accuracies, as suggested by <ref type="figure" target="#fig_0">Figure 1</ref> (c) and (d).</p><p>The Cascade R-CNN is quite simple to implement and trained end-to-end. Our results show that a vanilla implementation, without any bells and whistles, surpasses all previous state-of-the-art single-model detectors by a large margin, on the challenging COCO detection task <ref type="bibr" target="#b19">[20]</ref>, especially under the higher quality evaluation metrics. In addition, the Cascade R-CNN can be built with any two-stage object detector based on the R-CNN framework. We have observed consistent gains (of 2?4 points), at a marginal increase in computation. This gain is independent of the strength of the baseline object detectors. We thus believe that this simple and effective detection architecture can be of interest for many object detection research efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Due to the success of the R-CNN <ref type="bibr" target="#b11">[12]</ref> architecture, the two-stage formulation of the detection problems, by combining a proposal detector and a region-wise classifier has become predominant in the recent past. To reduce redundant CNN computations in the R-CNN, the SPP-Net <ref type="bibr" target="#b14">[15]</ref> and Fast-RCNN <ref type="bibr" target="#b10">[11]</ref> introduced the idea of region-wise feature extraction, significantly speeding up the overall detector. Later, the Faster-RCNN <ref type="bibr" target="#b26">[27]</ref> achieved further speedsup by introducing a Region Proposal Network (RPN). This architecture has become a leading object detection framework. Some more recent works have extended it to address various problems of detail. For example, the R-FCN <ref type="bibr" target="#b3">[4]</ref> proposed efficient region-wise fully convolutions without accuracy loss, to avoid the heavy region-wise CNN computations of the Faster-RCNN; while the MS-CNN <ref type="bibr" target="#b0">[1]</ref> and FPN <ref type="bibr" target="#b20">[21]</ref> detect proposals at multiple output layers, so as to alleviate the scale mismatch between the RPN receptive fields and actual object size, for high-recall proposal detection.</p><p>Alternatively, one-stage object detection architectures have also become popular, mostly due to their computa-tional efficiency. These architectures are close to the classic sliding window strategy <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref>. YOLO <ref type="bibr" target="#b25">[26]</ref> outputs very sparse detection results by forwarding the input image once. When implemented with an efficient backbone network, it enables real time object detection with fair performance. SSD <ref type="bibr" target="#b22">[23]</ref> detects objects in a way similar to the RPN <ref type="bibr" target="#b26">[27]</ref>, but uses multiple feature maps at different resolutions to cover objects at various scales. The main limitation of these architectures is that their accuracies are typically below that of two-stage detectors. Recently, RetinaNet <ref type="bibr" target="#b21">[22]</ref> was proposed to address the extreme foreground-background class imbalance in dense object detection, achieving better results than state-of-the-art two-stage object detectors.</p><p>Some explorations in multi-stage object detection have also been proposed. The multi-region detector <ref type="bibr" target="#b8">[9]</ref> introduced iterative bounding box regression, where a R-CNN is applied several times, to produce better bounding boxes. CRAFT <ref type="bibr" target="#b32">[33]</ref> and AttractioNet <ref type="bibr" target="#b9">[10]</ref> used a multi-stage procedure to generate accurate proposals, and forwarded them to a Fast-RCNN. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref> embedded the classic cascade architecture of <ref type="bibr" target="#b30">[31]</ref> in object detection networks. <ref type="bibr" target="#b2">[3]</ref> iterated a detection and a segmentation task alternatively, for instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Object Detection</head><p>In this paper, we extend the two-stage architecture of the Faster-RCNN <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21]</ref>, shown in <ref type="figure" target="#fig_2">Figure 3</ref> (a). The first stage is a proposal sub-network ("H0"), applied to the entire image, to produce preliminary detection hypotheses, known as object proposals. In the second stage, these hypotheses are then processed by a region-of-interest detection subnetwork ("H1"), denoted as detection head. A final classification score ("C") and a bounding box ("B") are assigned to each hypothesis. We focus on modeling a multi-stage detection sub-network, and adopt, but are not limited to, the RPN <ref type="bibr" target="#b26">[27]</ref> for proposal detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bounding Box Regression</head><formula xml:id="formula_0">A bounding box b = (b x , b y , b w , b h )</formula><p>contains the four coordinates of an image patch x. The task of bounding box regression is to regress a candidate bounding box b into a target bounding box g, using a regressor f (x, b). This is learned from a training sample {g i , b i }, so as to minimize the bounding box risk</p><formula xml:id="formula_1">R loc [f ] = N i=1 L loc (f (x i , b i ), g i ),<label>(1)</label></formula><p>where L loc was a L 2 loss function in R-CNN <ref type="bibr" target="#b11">[12]</ref>, but updated to a smoothed L 1 loss function in Fast-RCNN <ref type="bibr" target="#b10">[11]</ref>.</p><p>To encourage a regression invariant to scale and location, L loc operates on the distance vector defined by</p><formula xml:id="formula_2">? = (? x , ? y , ? w , ? h ) ?0.5 0 0.5 ?0.5 0 0.5 ? x ? y 1st stage ? x = 0.0020 ? y = 0.0022 ? x = 0.1234 ? y = 0.1297 ?0.5 0 0.5 ?0.5 0 0.5 ? x ? y 2nd stage ? x = 0.0048 ? y = ?0.0012 ? x = 0.0606 ? y = 0.0613 ?0.5 0 0.5 ?0.5 0 0.5 ? x ? y 3rd stage ? x = 0.0032 ? y = ?0.0021 ? x = 0.0391 ? y = 0.0376 ?1 0 1 ?1 0 1 ? w ? h 1st stage ? w = 0.0161 ? h = 0.0498 ? w = 0.2272 ? h = 0.2255 ?1 0 1 ?1 0 1 ? w ? h 2nd stage ? w = ?0.0007 ? h = 0.0122 ? w = 0.1221 ? h = 0.1230 ?1 0 1 ?1 0 1 ? w ? h 3rd stage ? w = ?0.0017 ? h = 0.0004 ? w = 0.0798 ? h = 0.0773</formula><formula xml:id="formula_3">? x = (g x ? b x )/b w , ? y = (g y ? b y )/b h ? w = log(g w /b w ), ? h = log(g h /b h ).<label>(2)</label></formula><p>Since bounding box regression usually performs minor adjustments on b, the numerical values of (2) can be very small. Hence, the risk of (1) is usually much smaller than the classification risk. To improve the effectiveness of multi-task learning, ? is usually normalized by its mean and variance, i.e. ? x is replaced by ? ? x = (? x ? ? x )/? x . This is widely used in the literature <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Some works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref> have argued that a single regression step of f is insufficient for accurate localization. Instead, f is applied iteratively, as a post-processing step</p><formula xml:id="formula_4">f ? (x, b) = f ? f ? ? ? ? ? f (x, b),<label>(3)</label></formula><p>to refine a bounding box b. This is called iterative bounding box regression, denoted as iterative BBox. It can be implemented with the inference architecture of <ref type="figure" target="#fig_2">Figure 3</ref> (b) where all heads are the same. This idea, however, ignores two problems. First, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a regressor f trained at u = 0.5, is suboptimal for hypotheses of higher IoUs. It actually degrades bounding boxes of IoU larger than 0.85. Second, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the distribution of bounding boxes changes significantly after each iteration. While the regressor is optimal for the initial distribution it can be quite suboptimal after that. Due to these problems, iterative BBox requires a fair amount of human engineering, in the form of proposal accumulation, box voting, etc. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>, and has somewhat unreliable gains. Usually, there is no benefit beyond applying f twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Classification</head><p>The classifier is a function h(x) that assigns an image patch x to one of M + 1 classes, where class 0 contains </p><formula xml:id="formula_5">R cls [h] = N i=1 L cls (h(x i ), y i ),<label>(4)</label></formula><p>where L cls is the classic cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detection Quality</head><p>Since a bounding box usually includes an object and some amount of background, it is difficult to determine if a detection is positive or negative. This is usually addressed by the IoU metric. If the IoU is above a threshold u, the patch is considered an example of the class. Thus, the class label of a hypothesis x is a function of u,</p><formula xml:id="formula_6">y = g y , IoU (x, g) ? u 0, otherwise<label>(5)</label></formula><p>where g y is the class label of the ground truth object g. This IoU threshold u defines the quality of a detector. Object detection is challenging because, no matter threshold, the detection setting is highly adversarial. When u is high, the positives contain less background, but it is difficult to assemble enough positive training examples. When u is low, a richer and more diversified positive training set is available, but the trained detector has little incentive to reject close false positives. In general, it is very difficult to ask a single classifier to perform uniformly well over all IoU levels. At inference, since the majority of the hypotheses produced by a proposal detector, e.g. RPN <ref type="bibr" target="#b26">[27]</ref> or selective search <ref type="bibr" target="#b29">[30]</ref>, have low quality, the detector must be more discriminant for lower quality hypotheses. A standard compromise between these conflicting requirements is to settle on u = 0.5. This, however, is a relatively low threshold, leading to low quality detections that most humans consider close false positives, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>   that targets various quality levels,</p><formula xml:id="formula_7">L cls (h(x), y) = u?U L cls (h u (x), y u ),<label>(6)</label></formula><p>where U is a set of IoU thresholds. This is closely related to the integral loss of <ref type="bibr" target="#b33">[34]</ref>, in which U = {0.5, 0.55, ? ? ? , 0.75}, designed to fit the evaluation metric of the COCO challenge. By definition, the classifiers need to be ensembled at inference. This solution fails to address the problem that the different losses of (6) operate on different numbers of positives. As shown in the first figure of <ref type="figure" target="#fig_4">Figure 4</ref>, the set of positive samples decreases quickly with u. This is particularly problematic because the high quality classifiers are prone to overfitting. In addition, those high quality classifiers are required to process proposals of overwhelming low quality at inference, for which they are not optimized. Due to all this, the ensemble of (6) fails to achieve higher accuracy at most quality levels, and the architecture has very little gain over that of <ref type="figure" target="#fig_2">Figure 3</ref> (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Cascade R-CNN</head><p>In this section we introduce the proposed Cascade R-CNN object detection architecture of <ref type="figure" target="#fig_2">Figure 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cascaded Bounding Box Regression</head><p>As seen in <ref type="figure" target="#fig_0">Figure 1 (c)</ref>, it is very difficult to ask a single regressor to perform perfectly uniformly at all quality levels. The difficult regression task can be decomposed into a sequence of simpler steps, inspired by the works of cascade pose regression <ref type="bibr" target="#b5">[6]</ref> and face alignment <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>. In the Cascade R-CNN, it is framed as a cascaded regression problem, with the architecture of <ref type="figure" target="#fig_2">Figure 3 (d)</ref>. This relies on a cascade of specialized regressors</p><formula xml:id="formula_8">f (x, b) = f T ? f T ?1 ? ? ? ? ? f 1 (x, b),<label>(7)</label></formula><p>where T is the total number of cascade stages. Note that each regressor f t in the cascade is optimized w.r.t. the sample distribution {b t } arriving at the corresponding stage, instead of the initial distribution of {b 1 }. This cascade improves hypotheses progressively. It differs from the iterative BBox architecture of <ref type="figure" target="#fig_2">Figure  3</ref> (b) in several ways. First, while iterative BBox is a postprocessing procedure used to improve bounding boxes, cascaded regression is a resampling procedure that changes the distribution of hypotheses to be processed by the different stages. Second, because it is used at both training and inference, there is no discrepancy between training and inference distributions. Third, the multiple specialized regressors {f T , f T ?1 , ? ? ? , f 1 } are optimized for the resampled distributions of the different stages. This opposes to the single f of (3), which is only optimal for the initial distribution. These differences enable more precise localization than iterative BBox, with no further human engineering.</p><p>As discussed in Section 3.1, ? = (? x , ? y , ? w , ? h ) in (2) needs to be normalized by its mean and variance for effective multi-task learning. After each regression stage, these statistics will evolve sequentially, as displayed in <ref type="figure" target="#fig_1">Figure 2</ref>. At training, the corresponding statistics are used to normalize ? at each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cascaded Detection</head><p>As shown in the left of <ref type="figure" target="#fig_4">Figure 4</ref>, the distribution of the initial hypotheses, e.g. RPN proposals, is heavily tilted towards low quality. This inevitably induces ineffective learning of higher quality classifiers. The Cascade R-CNN addresses the problem by relying on cascade regression as a resampling mechanism. This is is motivated by the fact that in <ref type="figure" target="#fig_0">Figure 1</ref> (c) all curves are above the diagonal gray line, i.e. a bounding box regressor trained for a certain u tends to produce bounding boxes of higher IoU. Hence, starting from a set of examples (x i , b i ), cascade regression successively resamples an example distribution (x ? i , b ? i ) of higher IoU. In this manner, it is possible to keep the set of positive examples of the successive stages at a roughly constant size, even when the detector quality (IoU threshold) is increased. This is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>, where the distribution tilts more heavily towards high quality examples after each resampling step. Two consequences ensue. First, there is no overfitting, since examples are plentiful at all levels. Second, the detectors of the deeper stages are optimized for higher IoU thresholds. Note that, some outliers are sequentially removed by increasing IoU thresholds, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, enabling a better trained sequence of specialized detectors.</p><p>At each stage t, the R-CNN includes a classifier h t and a regressor f t optimized for IoU threshold u t , where u t &gt; u t?1 . This is guaranteed by minimizing the loss</p><formula xml:id="formula_9">L(x t , g) = L cls (h t (x t ), y t ) + ?[y t ? 1]L loc (f t (x t , b t ), g), (8) where b t = f t?1 (x t?1 , b t?1 )</formula><p>, g is the ground truth object for x t , ? = 1 the trade-off coefficient, [?] the indicator function, and y t is the label of x t given u t by <ref type="bibr" target="#b4">(5)</ref>. Unlike the integral loss of (6), this guarantees a sequence of effectively trained detectors of increasing quality. At inference, the quality of the hypotheses is sequentially improved, by applications of the same cascade procedure, and higher quality detectors are only required to operate on higher quality hypotheses. This enables high quality object detection, as suggested by <ref type="figure" target="#fig_0">Figure 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>The Cascade R-CNN was evaluated on MS-COCO 2017 <ref type="bibr" target="#b19">[20]</ref>, which contains ?118k images for training, 5k for validation (val) and ?20k for testing without provided annotations (test-dev). The COCO-style Average Precision (AP) averages AP across IoU thresholds from 0.5 to 0.95 with an interval of 0.05. These evaluation metrics measure the detection performance of various qualities. All models were trained on COCO training set, and evaluated on val set. Final results were also reported on test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>All regressors are class agnostic for simplicity. All cascade detection stages in Cascade R-CNN have the same architecture, which is the head of the baseline detection network. In total, Cascade R-CNN have four stages, one RPN and three for detection with U = {0.5, 0.6, 0.7}, unless otherwise noted. The sampling of the first detection stage follows <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>. In the following stages, resampling is implemented by simply using the regressed outputs from the previous stage, as in Section 4.2. No data augmentation was used except standard horizontal image flipping. Inference was performed on a single image scale, with no further bells and whistles. All baseline detectors were reimplemented with Caffe <ref type="bibr" target="#b17">[18]</ref>, on the same codebase for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Baseline Networks</head><p>To test the versatility of the Cascade R-CNN, experiments were performed with three popular baseline detectors: Faster-RCNN with backbone VGG-Net <ref type="bibr" target="#b28">[29]</ref>, R-FCN <ref type="bibr" target="#b3">[4]</ref> and FPN <ref type="bibr" target="#b20">[21]</ref> with ResNet backbone <ref type="bibr" target="#b15">[16]</ref>. These baselines have a wide range of detection performances. Unless noted, their default settings were used. End-to-end training was used instead of multi-step training. Faster-RCNN: The network head has two fully connected layers. To reduce parameters, we used <ref type="bibr" target="#b12">[13]</ref> to prune less important connections. 2048 units were retained per fully connected layer and dropout layers were removed. Training started with a learning rate of 0.002, reduced by a factor of 10 at 60k and 90k iterations, and stopped at 100k iterations, on 2 synchronized GPUs, each holding 4 images per iteration. 128 RoIs were used per image. R-FCN: R-FCN adds a convolutional, a bounding box regression, and a classification layer to the ResNet. All heads of the Cascade R-CNN have this structure. Online hard negative mining <ref type="bibr" target="#b27">[28]</ref> was not used. Training started with a learning rate of 0.003, which was decreased by a factor of 10 at 160k and 240k iterations, and stopped at 280k iterations, on 4 synchronized GPUs, each holding one image per iteration. 256 RoIs were used per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPN:</head><p>Since no source code is publicly available yet for FPN, our implementation details could be different. RoIAlign <ref type="bibr" target="#b13">[14]</ref> was used for a stronger baseline. This is denoted as FPN+ and was used in all ablation studies. As usual, ResNet-50 was used for ablation studies, and ResNet-101 for final detection. Training used a learning rate of 0.005 for 120k iterations and 0.0005 for the next 60k iterations, on 8 synchronized GPUs, each holding one image per iteration. 256 RoIs were used per image. {0.5, 0.6, 0.7}. The detector of u = 0.5 outperforms the detector of u = 0.6 at low IoU levels, but underperforms it at higher levels. However, the detector of u = 0.7 underperforms the other two. To understand why this happens, we changed the quality of the proposals at inference. <ref type="figure">Figure  5</ref> (b) shows the results obtained when ground truth bounding boxes were added to the set of proposals. While all detectors improve, the detector of u = 0.7 has the largest gains, achieving the best performance at almost all IoU levels. These results suggest two conclusions. First, u = 0.5 is not a good choice for precise detection, simply more robust to low quality proposals. Second, highly precise detection requires hypotheses that match the detector quality. Next, the original detector proposals were replaced by the Cascade R-CNN proposals of higher quality (u = 0.6 and u = 0.7 used the 2nd and 3rd stage proposals, respectively). <ref type="figure">Figure 5</ref> (a) also suggests that the performance of the two detectors is significantly improved when the testing proposals closer match the detector quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quality Mismatch</head><p>Testing all Cascade R-CNN detectors at all cascade stages produced similar observations. <ref type="figure">Figure 6</ref> shows that each detector was improved when used more precise hypotheses, while higher quality detector had larger gain. For example, the detector of u = 0.7 performed poorly for the low quality proposals of the 1st stage, but much better for the more precise hypotheses available at the deeper cascade stages. In addition, the jointly trained detectors of <ref type="figure">Figure  6</ref> outperformed the individually trained detectors of <ref type="figure">Figure  5 (a)</ref>, even when the same proposals were used. This indicates that the detectors are better trained within the Cascade R-CNN framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with Iterative BBox and Integral Loss</head><p>In this section, we compare the Cascade R-CNN to iterative BBox and the integral loss detector. Iterative BBox was implemented by applying the FPN+ baseline iteratively, three times. The integral loss detector has the same number of classification heads as the Cascade R-CNN, with U = {0.5, 0.6, 0.7}. Localization: The localization performances of cascade regression and iterative BBox are compared in <ref type="figure">Figure 7</ref> (a). The use of a single regressor degrades localization for hypotheses of high IoU. This effect accumulates when the regressor is applied iteratively, as in iterative BBox, and performance actually drops. Note the very poor performance of iterative BBox after 3 iterations. On the contrary, the cascade regressor has better performance at later stages, outperforming iterative BBox at almost all IoU levels.</p><p>Integral Loss: The detection performances of all classifiers in the integral loss detector, sharing a single regressor, are shown in <ref type="figure">Figure 7</ref> (b). The classifier of u = 0.6 is the best at all IoU levels, while the classifier of u = 0.7 is the worst. The ensemble of all classifiers shows no visible gain. <ref type="table">Table 1</ref> shows, both iterative BBox and integral loss detector improve the baseline detector marginally. The cascade R-CNN has the best performance for all evaluation metrics. The gains are mild for low IoU thresholds but significant for the higher ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Experiments</head><p>Ablation experiments were also performed. <ref type="table">Table 2</ref> summarizes stage performance. The 1st stage already outperforms the baseline detector, due to the benefits of multi-stage multi-task learning. The 2nd stage improves performance substantially, and the 3rd is equivalent to the 2nd. This differs from the integral loss detector, where the higher IOU classifier is relatively weak. While the former (later) stage is better at low (high) IoU metrics, the ensemble of all classifiers is the best overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-wise Comparison:</head><p>IoU Thresholds: A preliminary Cascade R-CNN was trained using the same IoU threshold u = 0.5 for all heads. In this case, the stages differ only in the hypotheses they   receive. Each stage is trained with the corresponding hypotheses, i.e. accounting for the distributions of <ref type="figure" target="#fig_1">Figure 2</ref>. The first row of <ref type="table" target="#tab_2">Table 3</ref> shows that the cascade improves on the baseline detector. This suggests the importance of optimizing stages for the corresponding sample distributions.</p><p>The second row shows that, by increasing the stage threshold u, the detector can be made more selective against close false positives and specialized for more precise hypotheses, leading to additional gains. This supports the conclusions of Section 4.2.</p><p>Regression Statistics: Exploiting the progressively updated regression statistics, of <ref type="figure" target="#fig_1">Figure 2</ref>, helps the effective multi-task learning of classification and regression. Its benefit is noted by comparing the models with/without it in <ref type="table" target="#tab_2">Table 3</ref>. The learning is not sensitive to these statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Stages:</head><p>The impact of the number of stages is summarized in <ref type="table" target="#tab_3">Table 4</ref>. Adding a second detection stage significantly improves the baseline detector. Three detection stages still produce non-trivial improvement, but the addition of a 4th stage (u = 0.75) led to a slight performance decrease. Note, however, that while the overall AP performance degrades, the four-stage cascade has the best performance for high IoU levels. The three-stage cascade achieves the best trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with the state-of-the-art</head><p>The Cascade R-CNN, based on FPN+ and ResNet-101 backbone, is compared to state-of-the-art single-model object detectors in <ref type="table">Table 5</ref>. The settings are as described in Section 5.1.1, but a total of 280k training iterations were run and the learning rate dropped at 160k and 240k iterations. The number of RoIs was also increased to 512. The first group of detectors on <ref type="table">Table 5</ref> are one-stage detectors, the second group two-stage, and the last group multi-stage <ref type="bibr">(3-</ref>  noted that our FPN+ implementation is better than the original FPN <ref type="bibr" target="#b20">[21]</ref>, providing a very strong baseline. In addition, the extension from FPN+ to Cascade R-CNN improved performance by ?4 points. The Cascade R-CNN also outperformed all single-model detectors by a large margin, under all evaluation metrics. This includes the single-model entries of the COCO challenge winners in 2015 and 2016 (Faster R-CNN+++ <ref type="bibr" target="#b15">[16]</ref>, and G-RMI <ref type="bibr" target="#b16">[17]</ref>), and the very recent Deformable R-FCN <ref type="bibr" target="#b4">[5]</ref>, RetinaNet <ref type="bibr" target="#b21">[22]</ref> and Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>. The best multi-stage detector on COCO, At-tractioNet <ref type="bibr" target="#b9">[10]</ref>, used iterative BBox for proposal generation. Although many enhancements were used in Attrac-tioNet, the vanilla Cascade R-CNN still outperforms it by 7.1 points. Note that, unlike Mask R-CNN, no segmentation information is exploited in the Cascade R-CNN. Finally, the vanilla single-model Cascade R-CNN also surpasses the heavily engineered ensemble detectors that won the COCO challenge in 2015 and 2016 (AP 37.4 and 41.6, respectively) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Generalization Capacity</head><p>Three-stage Cascade R-CNN of all three baseline detectors are compared in <ref type="table">Table 6</ref>. All settings are as above, with the changes of Section 5.5 for FPN+. 1 http://cocodataset.org/#detections-leaderboard Detection Performance: Again, our implementations are better than the original detectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref>. Still, the Cascade R-CNN improves on these baselines consistently by 2?4 points, independently of their strength. These gains are also consistent on val and test-dev. These results suggest that the Cascade R-CNN is widely applicable across detector architectures.</p><p>Parameter and Timing: The number of the Cascade R-CNN parameters increases with the number of cascade stages. The increase is linear in the parameter number of the baseline detector heads. In addition, because the computational cost of a detection head is usually small when compared to the RPN, the computational overhead of the Cascade R-CNN is small, at both training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a multi-stage object detection framework, the Cascade R-CNN, for the design of high quality object detectors. This architecture was shown to avoid the problems of overfitting at training and quality mismatch at inference. The solid and consistent detection improvements of the Cascade R-CNN on the challenging COCO dataset suggest the modeling and understanding of various concurring factors are required to advance object detection. The Cascade R-CNN was shown to be applicable to many object detection architectures. We believe that it can be useful to many future object detection research efforts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The detection outputs, localization and detection performance of object detectors of increasing IoU threshold u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Sequential ? distribution (without normalization) at different cascade stage. Red dots are outliers when using increasing IoU thresholds, and the statistics are obtained after outlier removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The architectures of different frameworks. "I" is input image, "conv" backbone convolutions, "pool" region-wise feature extraction, "H" network head, "B" bounding box, and "C" classification. "B0" is proposals in all architectures. background and the remaining the objects to detect. h(x) is a M + 1-dimensional estimate of the posterior distribution over classes, i.e. h k (x) = p(y = k|x), where y is the class label. Given a training set (x i , y i ), it is learned by minimizing a classification risk</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a). A na?ve solution is to develop an ensemble of classifiers, with the architecture of Figure 3 (c), optimized with a loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The IoU histogram of training samples. The distribution at 1st stage is the output of RPN. The red numbers are the positive percentage higher than the corresponding IoU threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>(a) is detection performance of individually trained detectors, with their own proposals (solid curves) or Cascade R-CNN stage proposals (dashed curves), and (b) is by adding ground truth to the proposal set. The detection performance of all Cascade R-CNN detectors at all cascade stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 (Figure 7 .</head><label>57</label><figDesc>a) shows the AP curves of three individually trained detectors of increasing IoU thresholds of U = (a) is the localization comparison, and (b) is the detection performance of individual classifiers in the integral loss detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>AP AP 50 AP 60 AP 70 AP 80 AP 90 The comparison with iterative BBox and integral loss. stage AP AP 50 AP 60 AP 70 AP 80 AP 90 The stage performance of Cascade R-CNN. 1 ? 3 indicates the ensemble of three classifiers on the 3rd stage proposals.</figDesc><table><row><cell>FPN+ baseline</cell><cell>34.9</cell><cell>57.0 51.9 43.6 29.7</cell><cell>7.1</cell></row><row><cell>Iterative BBox</cell><cell>35.4</cell><cell>57.2 52.1 44.2 30.4</cell><cell>8.1</cell></row><row><cell>Integral Loss</cell><cell>35.4</cell><cell>57.3 52.5 44.4 29.9</cell><cell>6.9</cell></row><row><cell cols="2">Cascade R-CNN 38.9</cell><cell cols="2">57.8 53.4 46.9 35.8 15.8</cell></row><row><cell>test 1</cell><cell>35.5</cell><cell>57.2 52.4 44.1 30.5</cell><cell>8.1</cell></row><row><cell>2</cell><cell>38.3</cell><cell cols="2">57.9 53.4 46.4 35.2 14.2</cell></row><row><cell>3</cell><cell>38.3</cell><cell cols="2">56.6 52.2 46.3 35.7 15.9</cell></row><row><cell>1 ? 2</cell><cell>38.5</cell><cell cols="2">58.2 53.8 46.7 35.0 14.0</cell></row><row><cell>1 ? 3</cell><cell>38.9</cell><cell cols="2">57.8 53.4 46.9 35.8 15.8</cell></row><row><cell cols="2">FPN+ baseline 34.9</cell><cell>57.0 51.9 43.6 29.7</cell><cell>7.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>IoU? statAP AP 50 AP 60 AP 70 AP 80 AP 90</figDesc><table><row><cell></cell><cell></cell><cell>36.8</cell><cell>57.8 52.9 45.4 32.0 10.7</cell></row><row><cell>?</cell><cell></cell><cell>38.5</cell><cell>58.4 54.1 47.1 35.0 13.1</cell></row><row><cell></cell><cell>?</cell><cell>37.5</cell><cell>57.8 53.1 45.5 33.3 13.1</cell></row><row><cell>?</cell><cell>?</cell><cell>38.9</cell><cell>57.8 53.4 46.9 35.8 15.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The ablation experiments. "IoU?" means increasing IoU thresholds, and "stat" exploiting sequential regression statistics.# stages test stage AP AP 50 AP 60 AP 70 AP 80 AP 90</figDesc><table><row><cell>1</cell><cell>1</cell><cell>34.9</cell><cell>57.0 51.9 43.6 29.7</cell><cell>7.1</cell></row><row><cell>2</cell><cell>1 ? 2</cell><cell>38.2</cell><cell cols="2">58.0 53.6 46.7 34.6 13.6</cell></row><row><cell>3</cell><cell>1 ? 3</cell><cell>38.9</cell><cell cols="2">57.8 53.4 46.9 35.8 15.8</cell></row><row><cell>4</cell><cell>1 ? 3</cell><cell>38.9</cell><cell cols="2">57.4 53.2 46.8 36.0 16.0</cell></row><row><cell>4</cell><cell>1 ? 4</cell><cell>38.6</cell><cell cols="2">57.2 52.8 46.2 35.5 16.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The impact of the number of stages in Cascade R-CNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>stages+RPN for the Cascade R-CNN). All the compared state-of-the-art detectors were trained with u = 0.5. It is</figDesc><table><row><cell></cell><cell>backbone</cell><cell>AP</cell><cell cols="2">AP 50 AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>YOLOv2 [26]</cell><cell>DarkNet-19</cell><cell>21.6</cell><cell>44.0</cell><cell>19.2</cell><cell>5.0</cell><cell>22.4</cell><cell>35.5</cell></row><row><cell>SSD513 [23]</cell><cell>ResNet-101</cell><cell>31.2</cell><cell>50.4</cell><cell>33.3</cell><cell>10.2</cell><cell>34.5</cell><cell>49.8</cell></row><row><cell>RetinaNet [22]</cell><cell>ResNet-101</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell></row><row><cell>Faster R-CNN+++ [16]*</cell><cell>ResNet-101</cell><cell>34.9</cell><cell>55.7</cell><cell>37.4</cell><cell>15.6</cell><cell>38.7</cell><cell>50.9</cell></row><row><cell>Faster R-CNN w FPN [21]</cell><cell>ResNet-101</cell><cell>36.2</cell><cell>59.1</cell><cell>39.0</cell><cell>18.2</cell><cell>39.0</cell><cell>48.2</cell></row><row><cell cols="2">Faster R-CNN w FPN+ (ours) ResNet-101</cell><cell>38.8</cell><cell>61.1</cell><cell>41.9</cell><cell>21.3</cell><cell>41.8</cell><cell>49.8</cell></row><row><cell cols="2">Faster R-CNN by G-RMI [17] Inception-ResNet-v2</cell><cell>34.7</cell><cell>55.5</cell><cell>36.7</cell><cell>13.5</cell><cell>38.1</cell><cell>52.0</cell></row><row><cell>Deformable R-FCN [5]*</cell><cell>Aligned-Inception-ResNet</cell><cell>37.5</cell><cell>58.0</cell><cell>40.8</cell><cell>19.4</cell><cell>40.1</cell><cell>52.5</cell></row><row><cell>Mask R-CNN [14]</cell><cell>ResNet-101</cell><cell>38.2</cell><cell>60.3</cell><cell>41.7</cell><cell>20.1</cell><cell>41.1</cell><cell>50.2</cell></row><row><cell>AttractioNet [10]*</cell><cell>VGG16+Wide ResNet</cell><cell>35.7</cell><cell>53.4</cell><cell>39.3</cell><cell>15.6</cell><cell>38.0</cell><cell>52.7</cell></row><row><cell>Cascade R-CNN</cell><cell>ResNet-101</cell><cell>42.8</cell><cell>62.1</cell><cell>46.3</cell><cell>23.7</cell><cell>45.5</cell><cell>55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Comparison with the state-of-the-art single-model detectors on COCO test-dev. The entries denoted by "*" used bells and whistles at inference. AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L .075s 206M 30.3 52.2 30.8 12.0 34.7 44.3 30.5 52.9 31.2 12.0 33.9 43.8 ? 0.29s 0.083s 256M 33.3 52.0 35.2 11.8 37.2 51.1 33.3 52.6 35.2 12.1 36.2 49.3 FPN+ ResNet-50 ? 0.30s 0.095s 165M 36.5 58.6 39.2 20.8 40.0 47.8 36.5 59.0 39.2 20.3 38.8 46.4 ? 0.33s 0.115s 272M 40.3 59.4 43.7 22.9 43.7 54.1 40.6 59.9 44.0 22.6 42.7 52.1 FPN+ ResNet-101 ? 0.38s 0.115s 238M 38.5 60.6 41.7 22.1 41.9 51.1 38.8 61.1 41.9 21.3 41.8 49.8 ? 0.41s 0.14s 345M 42.7 61.6 46.6 23.8 46.2 57.4 42.8 62.1 46.3 23.7 45.5 55.2 Detailed comparison on multiple popular baseline object detectors. All speeds are reported per image on a single Titan Xp GPU.</figDesc><table><row><cell cols="4">backbone cascade AP AP 50 Faster R-CNN train test param speed speed ? 0.12s 0.075s 278M 23.6 43.9 23.0 8.0 26.2 35.5 23.5 43.9 22.6 8.1 25.1 34.7 val (5k) test-dev (20k) VGG ? 0.14s 0.115s 704M 27.0 44.2 27.7 8.6 29.1 42.2 26.9 44.3 27.8 8.3 28.2 41.1</cell></row><row><cell>R-FCN</cell><cell>ResNet-50</cell><cell>? ?</cell><cell>0.19s 0.07s 133M 27.0 48.7 26.9 9.8 30.9 40.3 27.1 49.0 26.9 10.4 29.7 39.2 0.24s 0.075s 184M 31.1 49.8 32.8 10.4 34.4 48.5 30.9 49.9 32.6 10.5 33.1 46.9</cell></row><row><cell>R-FCN</cell><cell>ResNet-101</cell><cell>?</cell><cell>0.23s 0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Kaiming He for valuable discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2887" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJ-CAI</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="973" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attend refine repeat: Active box proposal generation via in-out localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1611.10012</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="294" to="309" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning chained deep features and classifiers for cascade in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1702.07054</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learn to combine multiple hypotheses for accurate face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CRAFT objects from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6043" to="6051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
