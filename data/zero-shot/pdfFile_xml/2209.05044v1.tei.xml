<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting the Next Action by Modeling the Abstract Goal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debaditya</forename><surname>Roy</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SCC</orgName>
								<orgName type="institution">IHPC</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SCC</orgName>
								<orgName type="institution">IHPC</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Centre for Frontier AI Research (CFAR)</orgName>
								<orgName type="institution">IHPC</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting the Next Action by Modeling the Abstract Goal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code available at https://github.com/debadityaroy/Abstract_Goal</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of anticipating human actions is an inherently uncertain one. However, we can reduce this uncertainty if we have a sense of the goal that the actor is trying to achieve. Here, we present an action anticipation model that leverages goal information for the purpose of reducing the uncertainty in future predictions. Since we do not possess goal information or the observed actions during inference, we resort to visual representation to encapsulate information about both actions and goals. Through this, we derive a novel concept called abstract goal which is conditioned on observed sequences of visual features for action anticipation. We design the abstract goal as a distribution whose parameters are estimated using a variational recurrent network. We sample multiple candidates for the next action and introduce a goal consistency measure to determine the best candidate that follows from the abstract goal. Our method obtains impressive results on the very challenging Epic-Kitchens55 (EK55), EK100, and EGTEA Gaze+ datasets. We obtain absolute improvements of +13.69, +11.24, and +5.19 for Top-1 verb, Top-1 noun, and Top-1 action anticipation accuracy respectively over prior state-of-the-art methods for seen kitchens (S1) of EK55. Similarly, we also obtain significant improvements in the unseen kitchens (S2) set for Top-1 verb (+10.75), noun (+5.84) and action (+2.87) anticipation. Similar trend is observed for EGTEA Gaze+ dataset, where absolute improvement of +9.9, +13.1 and +6.8 is obtained for noun, verb, and action anticipation. It is through the submission of this paper that our method is currently the new state-of-the-art for action anticipation in EK55 and EGTEA Gaze+. https://competitions.codalab.org/competitions/20071#results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action anticipation is an important problem having applications in human-robot collaboration, smart houses, assistive robotics and wearable virtual assistants. Action anticipation models aim to predict the most plausible future action that is going to happen in the immediate future. Humans are rational and actions taken by us take us closer to what we would like to attain at the end, i.e. our goal. After we identify the goal, humans formulate a plan to execute actions to achieve that goal as explained in the seven stages of action cycle <ref type="bibr" target="#b35">[35]</ref>. Therefore, to predict what a person is going to do next accurately, it is useful to infer their goals. Besides, it has been shown that it is possible to infer the goal of the person from observing their actions <ref type="bibr" target="#b3">[4]</ref>. However, action anticipation literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref> has not made use of this vital information that governs human actions or it seems that goal modeling is not a popular solution.</p><p>Goal inference from observed actions (or features) is an extremely challenging task. Two different goals can share the same partial action sequences while different persons may have different execution plans for the same goal. Therefore, goal modeling is a highly stochastic problem. In this paper, we make use of a stochastic method <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> for goal modeling to improve action anticipation that goes beyond the deterministic latent goal representation introduced in <ref type="bibr" target="#b30">[31]</ref>. Interestingly, if we know the goal of the person, then it reduces the uncertainty when predicting future actions that they might take to accomplish the goal as backed up by the cognitive science literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">35]</ref>. Our approach is also motivated by procedure planning <ref type="bibr" target="#b5">[6]</ref> where a sequence of intermediate actions is predicted based on goals provided as the final visual representation or the overall activity <ref type="bibr" target="#b26">[27]</ref>.</p><p>Explicit goal inference is not trivial in action anticipation as the goal of the activity is not available as ground truth. To this end, we outline our approach in <ref type="figure" target="#fig_0">Figure 1</ref>. We learn a latent distribution from the observed visual features using stochastic RNN <ref type="bibr" target="#b6">[7]</ref> which we call "feature-based abstract goal" distribution. Given the hidden state of RNN and a sampled feature-based abstract goal (z T in <ref type="figure" target="#fig_0">Figure 1</ref>), we obtain a representation for the observed action (a O ). Afterward, we model the next action-representation distribution that is conditioned on the sampled feature-based abstract goal and the observed action-representation and sample a candidate's next action-representation (a N ) from it. Given the observed and next action representations, we learn the distribution of the "action-based abstract goal" using the generative variational framework. Therefore, we infer two kinds of abstract goals -one using visual feature sequence and another using action-representations unlike <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> that use a single latent distribution to model past actions.</p><p>The setup of next action anticipation assumes that the next action in the sequence can be reliably inferred from the observed action(s). Hence, the feature-based abstract goal distribution derived from observed features and action-based abstract goal distribution derived from next action representation should be consistent. The action that is most likely to happen in the future ("next best action") is the one that maximizes consistency between the two abstract goal distributions. We sample multiple next action-representation candidates and introduce a new goal consistency criterion to measure the suitability of each candidate when predicting the next action-see <ref type="figure" target="#fig_0">Figure 1</ref>. During learning, we also use goal consistency as a loss function to obtain a better model. Such a mechanism is not present in previous stochastic approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> which only minimizes KL divergence between prior and posterior latent distribution to obtain the best samples. We show that goal consistency has the biggest impact on action anticipation.</p><p>Our approach yields large improvements when predicting the next action in unscripted activities from the Epic-Kitchens55 and EGTEA Gaze+ datasets. We also obtain significantly better results for unseen kitchens of the Epic-Kitchens100. Our contributions are: (1) a novel stochastic abstract goal concept for action anticipation. (2) a novel stochastic model to learn abstract goal distributions and then use it for effective action anticipation in unscripted activities. (3) a novel goal consistency term that measures how well a plausible future action (next action) aligns with abstract goal distributions. The code will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Goals in action anticipation</head><p>The activity label of the entire action sequence is used to anticipate the next action in <ref type="bibr" target="#b32">[33]</ref>. In <ref type="bibr" target="#b30">[31]</ref>, observed features are used to obtain a fixed latent goal from visual features. However, humans often pursue multiple goals simultaneously <ref type="bibr" target="#b8">[9]</ref>. Hence, we propose an abstract goal distribution that serves as a representation for one or more underlying intention(s). The final visual representation is considered as the goal in <ref type="bibr" target="#b5">[6]</ref>. Our abstract goal allows us to anticipate actions without any knowledge of the overall activity label <ref type="bibr" target="#b32">[33]</ref> or final visual representation <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features for action anticipation</head><p>Multiple representations of such as spatial, motion, and bag of objects are used to predict future actions in <ref type="bibr" target="#b12">[13]</ref>. The authors showed that unrolling an LSTM for multiple time steps in the future is beneficial for the next action prediction. In <ref type="bibr" target="#b29">[30]</ref>, human-object interactions are encoded as features and fed to a transformer encoder-decoder to predict the features of future frames and the corresponding future actions. In <ref type="bibr" target="#b22">[23]</ref>, spatial attention maps of future human-object interactions are estimated to predict the next action.</p><p>In <ref type="bibr" target="#b36">[36]</ref>, multiple future visual representations are generated from an input frame using a CNN and the future action is predicted. Similarly in <ref type="bibr" target="#b39">[39]</ref>, an RNN is used to generate the intermediate frames between the observed frames and the anticipated action. In <ref type="bibr" target="#b19">[20]</ref>, temporal features are computed using time-conditioned skip connections to anticipate the next action. In <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>, RNNs are used to predict future actions conditioned on observed action labels. Similarly, in <ref type="bibr" target="#b16">[17]</ref>, a transformer is used to encode past actions and duration while another transformer decoder is used to predict both future actions and their duration. In <ref type="bibr" target="#b15">[16]</ref>, every frame is divided into patches and combined using the Visual Transformer (ViT) <ref type="bibr" target="#b9">[10]</ref> to obtain a frame representation. These frame representations are combined using a temporal transformer to predict future features and action labels. In <ref type="bibr" target="#b38">[38]</ref>, long-range sequences are summarized by processing smaller temporal sequences using multi-scale ViTs and caching them in memory as context. Each context is attended hierarchically in time using multiple temporal transformers for action anticipation. We use a Gated Recurrent Unit (GRU) to summarize frame features and learn feature-based abstract goal distribution parameters to generate observed and next action representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Past-Future Correlation for anticipation</head><p>An action anticipation model that correlates past observed features with the future using Jaccard vector similarity is presented in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b14">[15]</ref> proposes a neural memory network to compare an input (spatial representation or labels) with the existing memory content to predict future action labels. Similarly, <ref type="bibr" target="#b28">[29]</ref> proposes an action anticipation framework with a self-regulated learning process. They correlate similarities and differences between past and current frames. In <ref type="bibr" target="#b25">[26]</ref>, a predictive model directly anticipates the future action and a low-rank linear transitional model predicts the current action and correlates with the predicted future actions. Similarly, counterfactual reasoning is used to improve action anticipation in <ref type="bibr" target="#b41">[41]</ref>. Our approach correlates the past and future by enforcing goal consistency between the two abstract goal distributions computed using observed features and the next action. Although our method can be also used for long term action forecasting <ref type="bibr" target="#b27">[28]</ref> and early action recognition <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b31">32]</ref>, in this work we primarily focus on action anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Action anticipation with abstract goals</head><p>In this section we explain our model design outlined in <ref type="figure" target="#fig_0">Figure 1</ref> and then describe the related work in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature-based abstract goal</head><p>In this section we describe how to generate feature-based abstract goal representation using variational RNN (VRNN) framework <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. Let us denote the observed feature sequence by x 1 , x 2 , ? ? ? , x T where x t ? R d f . Following standard VRNN, a Gaussian distribution q t = N (? t,prior , ? t,prior ) is used to model the prior distribution of the abstract goal (z t ) given the observed feature sequence t ? {1, ? ? ? , T } where ? t,prior , ? t,prior ? R dz . The parameters of abstract goal distribution q t are estimated using an MLP denoted by ? prior : R d h ? R dz ) using the hidden state of a RNN (h t?1 ? R d h ) learned from the previous t ? 1 features as follows:</p><formula xml:id="formula_0">q(z t |x 1:t?1 ) ? N (? t,prior , ? t,prior ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">(? t,prior , ? t,prior ) = ? prior (h t?1 ).<label>(2)</label></formula><p>Note that there are two separate MLPs, one to obtain ? t,prior and another to obtain ? t,prior . We apply sof tplus activation to the ? prior network that estimates the standard deviation (? t,prior ). Unless otherwise specified, all ? &lt;&gt; (?) used in our model are two layered neural networks with ReLU activation.</p><p>The posterior distribution of the abstract goal (r) computes the effect of observing the incoming new feature x t as follows:</p><formula xml:id="formula_2">r(z t |x 1:t ) ? N (? t,pos , ? t,pos ).<label>(3)</label></formula><p>As before, the parameters of r are computed by a two layered MLP ? pos : R 2?dz ? R dz using both the last hidden state of RNN (h t?1 ) and the incoming feature (x t ) as follows:</p><formula xml:id="formula_3">? t,pos , ? t,pos = ? pos ([? x (x t ), ? h (h t?1 )]),<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">? x : R d f ? R dz , ? h : R d h ? R dz</formula><p>are linear layers and [?, ?] represents vector concatenation. We use the reparameterization trick <ref type="bibr" target="#b20">[21]</ref> to sample an abstract goal (z t ? R dz ) from the prior distribution q(z t |x 1:t?1 ) as follows:</p><formula xml:id="formula_5">z t = ? t,prior + ? t,prior ,<label>(5)</label></formula><p>where ? N (0, 1) ? R dz is a standard Gaussian distribution. Then sampled z t is used to obtain the next hidden state of the RNN 1 as follows:</p><formula xml:id="formula_6">h t = RN N (h t?1 , [? x (x t ), ? z (z t )]), ?t ? 1, ? ? ? , T<label>(6)</label></formula><p>where ? z : R dz ? R dz acts as a feature extractor over z t . The sampled abstract goal (z t ) can be used to reconstruct (or generate) the feature sequence as done in VRNN framework <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. However, we use it to represent feature-based abstract goal. Our intuition comes from the fact that humans derive action plans from goals, and videos are a realization of this action plan. Therefore, by construction, goal determines the video (feature evolution in our case). Interestingly, as the abstract goal latent variable encapsulates the video feature generation process, by analogical similarity, we make the proposition that latent variable (z t ) represents the notion of feature-based abstract goal. Therefore, we denote the "feature-based abstract goal distribution" as follows:</p><formula xml:id="formula_7">p(z T ) = q(z T |x 1:T ?1 ).<label>(7)</label></formula><p>The abstract goal distribution represents all abstract goals with respect to a particular observed features sequence. Any observed action may lead to more than one goal. Our abstract goal representation captures these variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Action representations</head><p>Human actions are causal in nature and the next action in a sequence depends on the earlier actions. For example, washing vegetables is succeeded by cutting vegetables when the goal is "making a salad". We capture the causality between observed and next actions using "the observed action representation" and the "next action representation". We obtain the observed action representation (a O ) using feature-based abstract goal and the hidden state of RNN as follows:</p><formula xml:id="formula_8">a O = ? O ([? z (z T ), ? h (h T )]).<label>(8)</label></formula><p>Here</p><formula xml:id="formula_9">? O : R 2?dz ? R d h and z T is sampled from the abstract goal distribution p(z T ) using Equation 5.</formula><p>Then we obtain the distribution of next action representation (a N ) conditioned on the hidden state of the RNN and the observed action representation denoted by p(a N |h T , a O ). The reason for modeling next action representation as a distribution conditioned on hidden state and the observed action representation is twofold. First, a particular observed action may lead to different next actions depending on the context and goal. Note that in our model, both observed action representation a O and the RNN hidden state h T depend on the feature-based abstract goal representation. Second, there can be variations in human behavior when executing the same task. The next action representations are generated using a Gaussian distribution N (? a N , ? 2 a N ) where ? a N , ? 2 a N ? R dz whose parameters are estimated as follows:</p><formula xml:id="formula_10">p(a N |h T , a O ) ? N (? a N , ? 2 a N ) where (? a N , (? a N )) = ? N ([? h (h T ), ? a (a O )]),<label>(9)</label></formula><p>where ? a : R d h ? R dz and ? N : R 2?dz ? R dz are two separate MLPs. Now we sample multiple next action representations from the next action representation distribution using the reparameterization trick as in <ref type="bibr">Equation 10</ref>,</p><formula xml:id="formula_11">a N = ? a N + ? a N ,<label>(10)</label></formula><p>where ? N (0, 1) ? R dz is a standard Gaussian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Action-based abstract goal representation</head><p>Now, we obtain action-based abstract goal from observed and next action representations using generative variational framework <ref type="bibr" target="#b20">[21]</ref>. The distribution for action-based abstract goal is modeled with a Gaussian distribution conditioned on the next action representation denoted by q(z N |a N ) whose parameters are computed as:</p><formula xml:id="formula_12">q(z N |a N ) ? N (? N q , ? N q ), where (? N q , ? N q ) = ? N q (? a (a N )),<label>(11)</label></formula><p>where</p><formula xml:id="formula_13">? N q , ? N q ? R dz and ? N q : R d h ? R dz is implemented with two MLPs.</formula><p>On the other hand, parameters of the action-based abstract goal distribution conditioned on both observed and next action representation are given as follows:</p><formula xml:id="formula_14">r(z N |a N , a O ) ? N (? N r , ? N r ) where (? N r , ? N r ) = ? N r ([? a (a N ), ? a (a O )])<label>(12)</label></formula><p>where ? N r , ? N r ? R dz and ? N r : R d h ? R dz is a dual headed MLP. Finally, the action-based abstract goal distribution for the next action p(z N ) is given by the distribution</p><formula xml:id="formula_15">p(z N ) = q(z N |a N ).<label>(13)</label></formula><p>We use both feature-based and action-based abstract goal representation to find the best candidate for next action as explained in next section. It should be noted that while the q(z N |a N ) only depends on a N , the r(z N |a N , a O ) depends on both a N and a O .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Next action anticipation with goal consistency</head><p>Given a sampled feature-based abstract goal z T , we select the best next action representation a * N using the divergence between p(z T ) distribution (eq. 7) and p(z N ) distribution (eq. 13). We call this divergence as the goal consistency criterion. Given z T , observed action a O and the next sampled action a N , the goal consistency criterion is derived from the symmetric KL-divergence between p(z T ) and p(z N ) as follows:</p><formula xml:id="formula_16">D(a N ) = D KL (p(z T )||p(z N )) + D KL (p(z N )||p(z T )) 2 .<label>(14)</label></formula><p>We choose the best next action candidate (i.e. the anticipated action candidate representation) a * N that minimizes the goal consistency criterion. The rationale is that the best anticipated action should have an action-based abstract goal representation p(z N ) that aligns with the feature-based abstract goal distribution p(z T ). We use the following algorithm to find the best next action candidate a * N .</p><p>1: Sample feat-based abstract goal zT from eq. 7 ? zT ? q(zT |x1:T ?1) 2: Get observed action representation aO (eq. 8) 3: Get next action representation distribution p(aN |hT , aO) (eq. 9) 4: Sample K next action representations N = {a 1 N , ? ? ? a K N } ? p(aN |hT , aO) 5: Best next action a * N = argmin a k N ?N D(a k N )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Best next action selection</head><p>Finally, we predict the anticipated action from the selected next action representation as follows:</p><formula xml:id="formula_17">y = ? c (a * N )<label>(15)</label></formula><p>where ? c : R dz ? R dc is the MLP classifier and? is the class score vector. It should be noted that in Algorithm 1, we sample only one feature-based abstraction goal in line 1 of the algorithm. However, during training we sample Q number of feature-based abstraction goals and for each of them we sample K number of next action representations. In this case, we select the best candidate from all K ? Q next action representation candidates using Equation <ref type="bibr" target="#b13">14</ref>. So, the next best action is consistent and does not rely too much on sampling as long as we sample sufficient candidate next actions. Even if the feature-based abstract goal P (z T ) is obtained from VRNN framework <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>, the formulation of action representations a O and a N , action-based abstract goal P (z N ) and goal consistency criterion is drastically different from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>. In <ref type="bibr" target="#b30">[31]</ref>, goal consistency is defined between latent goals before and after the action using a hard threshold. Instead, our goal consistency is a symmetric KL divergence between p(z T ) and p(z N ) distributions which aims to align the two abstract goal distributions. This also results in a massive improvement in next action anticipation performance as shown in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss functions and training of our model</head><p>Our anticipation network is trained using a number of losses. In contrast to prior stochastic methods <ref type="bibr" target="#b24">[25]</ref> [1], we introduce three KL divergence losses, based on a) feature-based abstract goal, b). action-based abstract goal (L N G ), 3. goal-consistency loss (L GC ). The first loss function is used to learn the parameters of the feature-based abstract goal distribution. We compute the KL-divergence between the conditional prior and posterior distributions for every feature in the observed feature sequence and minimize the sum given as follows:</p><formula xml:id="formula_18">L OG = T t=1 D KL (r(z t |x 1:t )||q(z t |x 1:t?1 )).<label>(16)</label></formula><p>This is based on the intuition that the abstract goal should not change due to a new observed feature. Our second loss arises when we learn the action-based abstract goal distribution. As in equation <ref type="bibr" target="#b15">16</ref> above, we compute the KL-divergence between r(z N |a * N , a O ) and q(z N |a * N ) distributions of action-based abstract goal distributions as follows:</p><formula xml:id="formula_19">L N G = D KL (r(z N |a * N , a O )||q(z N |a * N )).<label>(17)</label></formula><p>We denote the corresponding best action-based abstract goal distribution by p(z * N ) = q(z N |a * N ). The intuition is same as before, the goal should not change because of the next best action a * N . Furthermore, the feature-based and action-based abstract goal distributions should be aligned with respect to the selected next best action a * N . Therefore, we minimize the symmetric KL-Divergence between the feature-based and best-action-based abstract goal distribution as follows:</p><formula xml:id="formula_20">L GC = D KL (p(z T )||p(z * N ) + D KL (p(z * N )||p(z T ) 2 .<label>(18)</label></formula><p>We coin this loss as goal consistency loss. This loss is based on D(a N ) in <ref type="figure" target="#fig_0">Equation 14</ref> with the only difference being that p(z * N ) = q(z N |a * N ) is computed with respect to the selected best next action representation a * N . Finally, we have the cross-entropy loss for comparing the model's prediction y with the ground truth one-hot label y as follows</p><formula xml:id="formula_21">L N A = ? y log(?).<label>(19)</label></formula><p>The loss function to train the model is a combination of all losses given as follows:</p><formula xml:id="formula_22">L total = L OG + L N G + L GC + L N A .<label>(20)</label></formula><p>We experimented with adding different weights to each loss but there is no significant difference in learning. Therefore, we weight them equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets, features, and training details</head><p>We use three well known action anticipation datasets, EPIC-KITCHENS55 [8] (EK55), EPIC-KITCHENS100 <ref type="bibr" target="#b8">[9]</ref> (EK100) and EGTEA Gaze+ <ref type="bibr" target="#b21">[22]</ref> to evaluate. We validate our models using the TSN features obtained from RGB and optical flow videos, and bag of object features provided by <ref type="bibr" target="#b12">[13]</ref> for a fair comparison with existing approaches. Our base model has the following parameters: observed duration -2 seconds, frame rate -3 fps, RNN (GRU) hidden dimension d h = 256, abstract goal dimension d z = 128, number of sampled feature-based abstract goals (Q = 3), number of next-action-representation candidates (K = 10), and fixed anticipation time -1s (following EK55 and EK100 evaluation server criteria), unless specified otherwise. We use a batch size of 128 videos and train for 15 epochs with a learning rate of 0.001 using Adam optimizer with weight decay (AdamW) in Pytorch. All our MLPs have 256 hidden dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with state-of-the-art</head><p>We compare the performance of Abstract Goal (our method) with current state-of-the-art approaches on both the seen and unseen test sets of EK55 datasets in <ref type="table" target="#tab_1">Table 1</ref> using a late fusion of TSN-RGB, TSN-Flow, and Object features like most of the prior work. We train separate models for verb and noun anticipation and combine their predictions to obtain action anticipation accuracy. Our method outperforms all other prior state-of-the-art methods by a significant margin for both seen kitchens (S1) and unseen kitchens (S2). Notably, we outperform Transformer-based AVT <ref type="bibr" target="#b15">[16]</ref> and Temporal-Aggregation <ref type="bibr" target="#b32">[33]</ref> in all measures in both seen and unseen kitchens except for Top-5 accuracy on unseen kitchens. A similar trend can be seen for EGTEA Gaze+ dataset in <ref type="table" target="#tab_2">Table 2</ref> where our method outperforms all compared methods. We believe this improvement is due to two factors, (i) stochastic modeling is massively important for action anticipation, and (ii) the effective use of goal information is paramount for better action anticipation.     <ref type="table">Table 4</ref>: The impact of goal consistency criterion and loss. @1 and @5 denotes Top-1 and Top-5 accuracy and V stands for verb and N stands for noun.</p><p>Despite, these excellent results in both EK55 and EGTEA Gaze+ datasets, our overall results on EK100 are not state-of-the-art-see <ref type="table" target="#tab_3">Table 3</ref>. The overall performance is affected by tail classes where our method performs not as well as recent transformer methods (see <ref type="table" target="#tab_1">Supplementary Table  1</ref>) that are heavily trained on external data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">38]</ref>. EK100 dataset is dominated by long-tailed distribution where 228 noun classes out of 300 are in the tail classes. Similarly, 86 verb classes out of 97 are in the tail classes. In our model, the next-action-representation is modeled with a Gaussian distribution (Equation 11), and therefore, it is not able to cater to exceptionally long tail class distributions as in EK100. This is a limitation of our method. We do not witness the tail-class issue in EK55 as the performance measure used is accuracy compared to mean-recall in EK100. Accuracy is influenced heavily by frequent classes but mean-recall treats all classes equally. However, our model shows excellent generalization results on unseen kitchens of EK100 dataset outperforming the best Transformer model <ref type="bibr" target="#b38">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of goal consistency criterion and loss</head><p>In this section, we evaluate the impact of Goal Consistency (GC) criterion and loss (L GC ) using the validation set of EK55 and EK100 datasets. We train separate models for verb and noun anticipation using TSN-RGB (RGB) and Object (OBJ) features, respectively. As Mean and Median sampling are used in prior variational prediction models <ref type="bibr" target="#b0">[1]</ref>, here we use mean and median sampling as two baselines to show the effect of GC. After sampling Q ? K number of next-action representations (a N ), instead of selecting the best next-action candidate using GC (Algorithm 1), we obtain the mean/median vector of all sampled candidates and then make the prediction using the classifier (e.g. mean vector= a N Q?K ). We also experimented with a majority/median class prediction baseline. In this case, we take all Q ? K predictions from the classifier (from the next action-representation candidates) and pick the majority/median class as the final prediction. Everything else stays the same for all these mean/majority/median baseline models, except we do not use GC loss (L GC ) or the GC criterion <ref type="bibr">(Equation 14</ref>). Results are reported in <ref type="table">Table 4</ref>.</p><p>As can be seen from the results, there is a significant impact of GC. Especially, there is an improvement of 3.39 and 2.37 for top-1 verb and noun accuracy respectively using our GC model in the EK55 dataset for Q = 1, K = 10 over Mean sampling baseline. Similar trend can be seen for EK100 and Q = 3, K = 10 as well. Our model also outperforms majority and median class sampling baselines for both [Q = 1, K = 10] and [Q = 3, K = 10] configurations indicating the effectiveness of goal consistency. Overall, our method with GC loss and criterion performs better than all other variants. Perhaps this is because the GC criterion allows the model to regularize the candidate selection while GC loss allows the model to enforce this during the training. This clearly shows the  <ref type="table">Table 5</ref>: Ablation on the sensitivity of number of sampled feature-based-abstract-goals (Q) and next-action representation candidate K on EK55 and EK100 validation set.</p><p>impact of goal consistency formulation of our model for action anticipation. To understand goal consistency qualitatively, we measured KLD between abstract goals of different videos. We found that videos with similar goals have lower KLD (6.41 ? 10 ?5 ) than dissimilar goals (2 ? 10 ?4 ). Details in <ref type="table" target="#tab_2">Supplementary Table 2</ref>.</p><p>We perform a more controlled experiment to further evaluate the impact of GC loss where we set Q = 1 and K = 1 and train our model with and without GC loss (L GC ). It should be noted that when Q = 1 and K = 1, there is no impact of GC criterion. To obtain a statistically meaningful result, we repeat this experiment 10 times and report the mean performance. As it can be seen from the results in <ref type="table">Table 4</ref> (last two rows), clearly the GC loss has a positive impact even when we just sample a single action candidate from our stochastic model. Adding GC loss improves the top1-verb prediction by 2.57 and top1-noun prediction by 2.35 on EK55 dataset. On EK100 dataset, the improvement is 2.98 and 4.55 for top-1 verb and noun prediction. This clearly proves that not only does the GC criterion help, but also the GC loss improves the performance of the model. We see that compared to our model variant [Q = 1, K = 1 with L GC ] the [Q = 1, K = 10 with L GC ] model performs significantly better (last row vs row 5 of <ref type="table">Table 4</ref>). This indicates the impact of next-action-representation sampling (Equation 11) even for a single sampled feature-based abstract goal (Q = 1). We conclude that the goal consistency loss, the goal consistency criterion, and next-action-representation distribution modeling (all novel concepts introduced in this paper) are effective for action anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sensitivity of model on Q and K</head><p>Next, we evaluate how sensitive our model is to the number of sampled feature-based abstract goals Q and sampled next action-representation candidates K. Results are shown in <ref type="table">Table 5</ref>. Our model is not that sensitive to the sampled number of feature-based abstract goals Q, especially when K &gt; 1. However, having Q = 3 is better than Q = 1 for the majority of the performance measures except for top5-noun accuracy. As the model is not that sensitive to Q, we select Q = 3 as the default. The performance of the model increases with increasing K. We select K = 10 as our default based on results in <ref type="table">Table 5</ref>. Our model uses both GC loss and criterion during training. GC criterion helps the model to pick the most suitable next action candidate while GC loss encourages the model to generate good candidates. Therefore, our model is regularized to predict the most likely candidates accurately and may not need to rely too much on the sampling process. We also notice that the model is not sensitive to different starting seeds in the sampling process-see <ref type="table">Supplementary Table 4</ref>.  <ref type="table">Table 6</ref>: Loss ablation on EK55 validation set. i.e.L N A -Next action cross-entropy loss, L OG -Featurebased abstract goal loss, L N G -Action-based abstract goal loss, L GC -Goal consistency loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Losses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluating the impact of all loss functions</head><p>We also study the impact of each loss function described in Section 3.5 and report the results in <ref type="table">Table 6</ref>. If we use only the supervised cross-entropy loss (i.e., L N A ), then the performance is the worst, especially for verbs. Both L OG and L N G help in regularizing the abstract goal representations (z t and a N ), and therefore results improve significantly. Especially, the L N A + L N G is the best loss combination for a pair of losses. When we combine all four losses, we get the best results. While L N A + L N G regularizes the learning of abstract goal representations, L GC which minimizes the divergence between feature-based and action-based goal distributions improves the choice of next verb or noun among the plausible candidates. We conclude that all four losses are important for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation on model architecture</head><p>We also compare our model with standard Variational RNN classification. We obtain the latent variable z T and the observed action representation a O from Equation 8, we classify a O using a classifier to obtain the future action. In this case, we train the VRNN model with cross-entropy loss and KL-divergence (Equation <ref type="bibr" target="#b15">16</ref>). We call this baseline as VRNN, where we sample 30 candidates for a O to match our baseline architecture with 30 next action candidates (Q = 3, K = 10). As shown in <ref type="table" target="#tab_7">Table 7</ref>, our method performs much better than the VRNN method (either considering mean or median prediction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Ablation on anticipation horizon</head><p>In <ref type="table">Table 8</ref>, we see that our model produces consistent anticipation performance for various anticipation time horizons on EK55 dataset. One model is trained for each anticipation duration. On all anticipation times, abstract goal outperforms existing approaches substantially (5-12%). Even when the anticipation time is increased to 2 seconds, abstract goal performance does not drop significantly compared to RU-LSTM <ref type="bibr" target="#b13">[14]</ref> or Temp. Agg. <ref type="bibr" target="#b32">[33]</ref>. We conclude that our model can produce better predictions over longer anticipation times, due to stochastic modeling of action anticipation with goal consistency. Additional ablation studies in <ref type="table" target="#tab_3">Supplementary Tables 3-9</ref>.  <ref type="table">Table 8</ref>: Comparing Action Anticipation performance for different anticipation times on EK55 validation set. Our model consistently outperforms other approaches by a large margin for all anticipation times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-5 Action Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations, Discussion, and Conclusions</head><p>We present a novel approach for action anticipation where abstract goals are learned with a stochastic recurrent model. We significantly outperform existing approaches on EPIC-KITCHENS55 and EGTEA Gaze+ datasets. Our model generalizes to unseen kitchens in both EPIC-KITCHENS55 and EPIC-KITCHENS100 datasets, even outperforming Transformer-based models by a large margin. We also show the importance of goal consistency criterion, goal consistency loss, next-action representation modeling, and architecture. One limitation of the current work is the inability to directly interpret the abstract goal representation learned by our model. Second, our method is not able to tackle long-tail-class distribution issues. In the future, we aim to address these limitations of our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the model design for abstract goal-based action anticipation. Yellow ellipses represent distributions and pink boxes represent various variables of the model. Best viewed on a screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>VERB NOUN ACT. VERB NOUN ACT. VERB NOUN ACT. VERB NOUN ACT.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Seen Kitchens (S1)</cell><cell></cell><cell></cell><cell cols="4">Unseen Kitchens (S2)</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Top-1 accuracy</cell><cell cols="3">Top-5 accuracy</cell><cell cols="3">Top-1 accuracy</cell><cell cols="3">Top-5 accuracy</cell></row><row><cell>RU-LSTM [13]</cell><cell>33.04</cell><cell>22.78</cell><cell>14.39</cell><cell>79.55</cell><cell>50.95</cell><cell>33.73</cell><cell>27.01</cell><cell>15.19</cell><cell>08.16</cell><cell>69.55</cell><cell>34.38</cell><cell>21.10</cell></row><row><cell>Lat. Goal [31]</cell><cell>27.96</cell><cell>27.40</cell><cell>8.10</cell><cell>78.09</cell><cell>55.98</cell><cell>26.46</cell><cell>22.40</cell><cell>19.12</cell><cell>04.78</cell><cell>72.07</cell><cell>42.68</cell><cell>16.97</cell></row><row><cell>SRL [29]</cell><cell>34.89</cell><cell>22.84</cell><cell>14.24</cell><cell>79.59</cell><cell>52.03</cell><cell>34.61</cell><cell>27.42</cell><cell>15.47</cell><cell>08.88</cell><cell>71.90</cell><cell>36.80</cell><cell>22.06</cell></row><row><cell cols="2">ImagineRNN [39] 35.44</cell><cell>22.79</cell><cell>14.66</cell><cell>79.72</cell><cell>52.09</cell><cell>34.98</cell><cell>29.33</cell><cell>15.50</cell><cell>09.25</cell><cell>70.67</cell><cell>35.78</cell><cell>22.19</cell></row><row><cell>Temp. Agg. [33]</cell><cell>37.87</cell><cell>24.10</cell><cell>16.64</cell><cell>79.74</cell><cell>53.98</cell><cell>36.06</cell><cell>29.50</cell><cell>16.52</cell><cell>10.04</cell><cell>70.13</cell><cell>37.83</cell><cell>23.42</cell></row><row><cell>MM-Trans [30]</cell><cell>28.59</cell><cell>27.18</cell><cell>10.85</cell><cell>78.64</cell><cell>57.66</cell><cell>30.83</cell><cell>26.80</cell><cell>18.40</cell><cell>06.76</cell><cell>70.40</cell><cell>44.18</cell><cell>20.04</cell></row><row><cell>MM-TCN [40]</cell><cell>37.16</cell><cell>23.75</cell><cell>15.45</cell><cell>79.48</cell><cell>51.86</cell><cell>34.37</cell><cell>30.66</cell><cell>14.92</cell><cell>08.91</cell><cell>72.00</cell><cell>36.67</cell><cell>21.68</cell></row><row><cell>AVT [16]</cell><cell>34.36</cell><cell>20.16</cell><cell>16.84</cell><cell>80.03</cell><cell>51.57</cell><cell>36.52</cell><cell>30.66</cell><cell>15.64</cell><cell>10.41</cell><cell>72.17</cell><cell>40.76</cell><cell>24.27</cell></row><row><cell>Abstract Goal</cell><cell cols="2">51.56 35.34</cell><cell cols="3">22.03 82.56 58.01</cell><cell cols="3">38.29 41.41 22.36</cell><cell cols="3">13.28 73.10 41.62</cell><cell>24.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of anticipation accuracy with state-of-the-art on EK55 evaluation server. ACT. is action.</figDesc><table><row><cell>Method</cell><cell cols="6">Top-1 accuracy VERB NOUN ACT. VERB NOUN ACT. Mean Class accuracy</cell></row><row><cell>I3D-Res50 [5]</cell><cell>48.0</cell><cell>42.1</cell><cell>34.8</cell><cell>31.3</cell><cell>30.0</cell><cell>23.2</cell></row><row><cell>FHOI [23]</cell><cell>49.0</cell><cell>45.5</cell><cell>36.6</cell><cell>32.5</cell><cell>32.7</cell><cell>25.3</cell></row><row><cell>RU-LSTM [13]</cell><cell>50.3</cell><cell>48.1</cell><cell>38.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AVT [16]</cell><cell>54.9</cell><cell>52.2</cell><cell>43.0</cell><cell>49.9</cell><cell>48.3</cell><cell>35.2</cell></row><row><cell>Abstract Goal</cell><cell>64.8</cell><cell>65.3</cell><cell>49.8</cell><cell>63.4</cell><cell>55.6</cell><cell>37.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison on anticipation performance on EGTEA Gaze+. All methods are evaluated on fixed anticipation time of 0.5s following<ref type="bibr" target="#b15">[16]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="6">Overall VERB NOUN ACT. VERB NOUN ACT. Unseen Kitchens</cell></row><row><cell>RU-LSTM [9]</cell><cell>25.25</cell><cell>26.69</cell><cell>11.19</cell><cell>19.36</cell><cell>26.87</cell><cell>09.65</cell></row><row><cell>Temp. Agg. [33]</cell><cell>21.76</cell><cell>30.59</cell><cell>12.55</cell><cell>17.86</cell><cell>27.04</cell><cell>10.46</cell></row><row><cell>AVT [16]</cell><cell>26.69</cell><cell>32.33</cell><cell>16.74</cell><cell>21.03</cell><cell>27.64</cell><cell>12.89</cell></row><row><cell cols="2">TransAction [18] 36.15</cell><cell>32.20</cell><cell>13.39</cell><cell>27.60</cell><cell>24.24</cell><cell>10.05</cell></row><row><cell>MeMViT [38]</cell><cell>32.20</cell><cell cols="3">37.00 17.70 28.60</cell><cell>27.40</cell><cell>15.20</cell></row><row><cell>Abstract goal</cell><cell>31.40</cell><cell>30.10</cell><cell cols="2">14.29 31.36</cell><cell cols="2">35.56 17.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Goal candidate (Q) &amp;</cell><cell></cell><cell cols="2">EK55</cell><cell></cell><cell></cell><cell cols="2">EK100</cell><cell></cell></row><row><cell cols="2">Action candidate (K)</cell><cell>V@1</cell><cell>V@5</cell><cell>N@1</cell><cell>N@5</cell><cell>V@1</cell><cell>V@5</cell><cell>N@1</cell><cell>N@5</cell></row><row><cell>Mean</cell><cell></cell><cell>41.79</cell><cell>72.23</cell><cell>25.79</cell><cell>49.50</cell><cell>44.51</cell><cell>76.89</cell><cell>22.72</cell><cell>50.78</cell></row><row><cell>Median</cell><cell>Q=1,</cell><cell>41.16</cell><cell>71.32</cell><cell>24.30</cell><cell>48.31</cell><cell>45.44</cell><cell>77.91</cell><cell>22.15</cell><cell>51.23</cell></row><row><cell>Majority class</cell><cell>K=10</cell><cell>41.98</cell><cell>72.89</cell><cell>25.98</cell><cell>50.01</cell><cell>42.98</cell><cell>74.56</cell><cell>24.13</cell><cell>53.45</cell></row><row><cell>Median class</cell><cell></cell><cell>41.02</cell><cell>72.11</cell><cell>22.88</cell><cell>49.87</cell><cell>44.19</cell><cell>77.00</cell><cell>22.97</cell><cell>51.98</cell></row><row><cell>Our model</cell><cell></cell><cell cols="8">45.18 77.30 28.16 51.08 48.84 80.52 27.50 55.83</cell></row><row><cell>Mean</cell><cell></cell><cell>39.40</cell><cell>72.23</cell><cell>24.22</cell><cell>48.96</cell><cell>45.90</cell><cell>77.88</cell><cell>22.41</cell><cell>50.87</cell></row><row><cell>Median</cell><cell>Q=3,</cell><cell>41.32</cell><cell>71.32</cell><cell>26.60</cell><cell>51.70</cell><cell>45.63</cell><cell>77.02</cell><cell>24.33</cell><cell>52.87</cell></row><row><cell>Majority class</cell><cell>K=10</cell><cell>38.39</cell><cell>69.42</cell><cell>24.70</cell><cell>48.22</cell><cell>45.72</cell><cell>78.61</cell><cell>22.61</cell><cell>50.89</cell></row><row><cell>Median class</cell><cell></cell><cell>40.43</cell><cell>71.43</cell><cell>26.52</cell><cell>52.33</cell><cell>45.84</cell><cell>78.09</cell><cell>23.78</cell><cell>52.33</cell></row><row><cell>Our model</cell><cell></cell><cell cols="8">44.68 77.14 28.29 53.78 49.02 80.86 28.52 54.91</cell></row><row><cell cols="2">Without L GC Q=1,</cell><cell>38.31</cell><cell>70.77</cell><cell>19.74</cell><cell>43.11</cell><cell>43.82</cell><cell>77.45</cell><cell>21.25</cell><cell>51.99</cell></row><row><cell>With L GC</cell><cell>K=1</cell><cell cols="8">40.88 71.43 22.09 46.29 46.80 78.41 26.80 53.32</cell></row></table><note>Comparison on EK100 dataset on evaluation server using test set. Accuracy measured by class-mean recall@5 (%) following the standard protocol.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>+ L OG + L N G + L GC 46.37 77.97 29.86 52.74</figDesc><table><row><cell></cell><cell>V@1</cell><cell>V@5</cell><cell>N@1</cell><cell>N@5</cell></row><row><cell>L N A</cell><cell>21.36</cell><cell>69.69</cell><cell>27.76</cell><cell>51.89</cell></row><row><cell>L N A + L OG</cell><cell>44.42</cell><cell>77.79</cell><cell>28.41</cell><cell>51.31</cell></row><row><cell>L N A + L N G</cell><cell>46.01</cell><cell>77.94</cell><cell>29.05</cell><cell>52.32</cell></row><row><cell>L N A + L GC</cell><cell>43.83</cell><cell>77.43</cell><cell>28.06</cell><cell>51.87</cell></row><row><cell>L N A + L OG + L N G</cell><cell>44.47</cell><cell>77.12</cell><cell>28.51</cell><cell>51.34</cell></row><row><cell>L N A + L OG + L GC</cell><cell>45.47</cell><cell>77.42</cell><cell>28.61</cell><cell>52.34</cell></row><row><cell>L N A</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation on architecture. We compare our model with standard VRNN classification on EK55 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our RNN is a standard GRU cell.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research/project is supported in part by the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG2-RP-2020-016) and the National Research Foundation Singapore under its AI Singapore Program (Award Number: AISG-RP-2019-010).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertainty-aware anticipation of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When will you do what?-anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yazan Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5343" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Action understanding as inverse planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="349" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Goal inference as inverse planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">R</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Procedure planning in instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="334" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The epic-kitchens dataset: Collection, challenges and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Anticipating human actions by correlating past with the future with jaccard similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samitha</forename><surname>Herath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13224" to="13233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S?ren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2207" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rolling-unrolling lstms for action anticipation from first-person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6252" to="6261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Forecasting future action sequences with neural memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshala</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th British Machine Vision Conference 2019, BMVC 2019</title>
		<meeting><address><addrLine>Cardiff, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">298</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anticipative Video Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Future transformer for long-term action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayoung</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Jong</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3052" to="3061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transaction: ICL-SJTU submission to epic-kitchens action anticipation challenge 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benny</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Zhong</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2107.13259</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for driver activity anticipation via sensory-fusion architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3118" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Time-conditioned action anticipation in one shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9925" to="9934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In the eye of the beholder: Gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Forecasting human-object interaction: joint prediction of motor attention and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="704" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long-term action forecasting using multi-headed attention-based variational recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debaditya</forename><surname>Siyuan Brandon Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="2419" to="2427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A variational auto-encoder model for stochastic point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Abdu Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leveraging the present to anticipate the future in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Forecasting future action sequences with attention: a new approach to weakly supervised action forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Bin Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-regulated learning for egocentric video activity anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaobo</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action anticipation using pairwise human-object interactions and transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debaditya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action anticipation using latent goal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debaditya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2022-01" />
			<biblScope unit="page" from="2745" to="2753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Encouraging lstms to anticipate actions very early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh Sadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="280" to="289" />
		</imprint>
	</monogr>
	<note>Lars Petersson, and Lars Andersson</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal aggregate representations for long-range video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipika</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action anticipation with rbf kernelized feature mapping rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="301" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The design of everyday life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">40</biblScope>
			<pubPlace>Berg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interactive prototype learning for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8168" to="8177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08383</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to anticipate egocentric actions by imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1143" to="1152" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-modal temporal convolutional network for anticipating actions in egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Zatsarynna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2249" to="2258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">What if we could not see? counterfactual analysis for egocentric action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
