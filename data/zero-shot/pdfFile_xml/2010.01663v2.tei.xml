<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Jeya</forename><forename type="middle">Maria</forename><surname>Jose Valanarasu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE, Ilker</roleName><forename type="first">Vishwanath</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Vishal</forename><forename type="middle">M Patel</forename><surname>Hacihaliloglu</surname></persName>
						</author>
						<title level="a" type="main">KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most methods for medical image segmentation use U-Net or its variants as they have been successful in most of the applications. After a detailed analysis of these "traditional" encoder-decoder based approaches, we observed that they perform poorly in detecting smaller structures and are unable to segment boundary regions precisely. This issue can be attributed to the increase in receptive field size as we go deeper into the encoder. The extra focus on learning high level features causes U-Net based approaches to learn less information about low-level features which are crucial for detecting small structures. To overcome this issue, we propose using an overcomplete convolutional architecture where we project the input image into a higher dimension such that we constrain the receptive field from increasing in the deep layers of the network. We design a new architecture for image segmentation-KiU-Net which has two branches: (1) an overcomplete convolutional network Kite-Net which learns to capture fine details and accurate edges of the input, and (2) U-Net which learns high level features. Furthermore, we also propose KiU-Net 3D which is a 3D convolutional architecture for volumetric segmentation. We perform a detailed study of KiU-Net by performing experiments on five different datasets covering various image modalities. We achieve a good performance with an additional benefit of fewer parameters and faster convergence. We also demonstrate that the extensions of KiU-Net based on residual blocks and dense blocks result in further performance improvements. Code: https://github.com/jeya-maria-jose/KiU-Net-pytorch</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M EDICAL image segmentation plays a pivotal role in computer-aided diagnosis systems which are helpful in making clinical decisions. Segmenting a region of interest like an organ or lesion from a medical scan is critical as it contains details like the volume, shape and location of the region of interest. Automatic methods proposed for medical image segmentation help in aiding radiologists for making fast and labor-less annotations. Early medical segmentation methods Jeya Maria Jose Valanarasu, Vishwanath A. Sindagi and Vishal M. Patel are with the Whiting School of Engineering, Johns Hopkins University, 3400 North Charles Street, Baltimore, MD 21218-2608, email: (jvalana1,vishwanathsindagi,vpatel36)@jhu.edu Ilker Hacihaliloglu is with Rutgers, The State University of New Jersey, NJ, USA, e-mail: ilker.hac@rutgers.edu were based on traditional pattern recognition techniques like statistical modeling and edge detection filters. Later, machine learning approaches using hand-crafted features based on the modality and type of segmentation task were developed. Recently, the state of the art methods for medical image segmentation for most modalities like magnetic resonance imaging (MRI), computed tomography (CT) and ultrasound (US) are based on deep learning. As convolutional neural networks (CNNs) extract data-specific features which are rich in quality and effective in representing the image and the region of interest, deep learning reduces the hassle of extracting manual features from the image.</p><p>Most of the architectures developed for semantic segmentation in both computer vision and medical image analysis are encoder-decoder type convolutional networks. Seg-Net <ref type="bibr" target="#b0">[1]</ref> was the first such type of network that was widely recognized. In the encoder block of Seg-Net, every convolutional layer is followed by a max-pooling layer which causes the input image to be projected onto a lower dimension similar to an undercomplete auto-encoder. The receptive field size of the filters increases with the depth of the network thereby enabling it to extract high-level features in the deeper layers. The initial layers of the encoder extract low-level information like edges and small anatomical structures while the deeper layers extract high-level information like objects (in the case of vision datasets) and organs/lesions (in the case of medical imaging datasets). A major breakthrough in medical image segmentation was brought by U-Net <ref type="bibr" target="#b1">[2]</ref> where skip connections were introduced between the encoder and decoder to improve the training and quality of the features used in predicting the segmentation. U-Net has became the backbone of almost all the leading methods for medical image segmentation in recent years. Subsequently, many more networks were proposed which built on top of the U-Net architecture. U-Net++ <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> proposed using nested and dense skip connection for further reducing the semantic gap between the feature maps of the encoder and decoder. UNet3+ <ref type="bibr" target="#b4">[5]</ref> proposed using full-scale skip connections where skip connections are made between different scales. 3D U-Net <ref type="bibr" target="#b5">[6]</ref> and V-Net <ref type="bibr" target="#b6">[7]</ref> were proposed as extensions of U-Net for volumetric segmentation in 3D medical scans. In other extensions of U-Net like Res-UNet <ref type="bibr" target="#b7">[8]</ref> and Dense-UNet <ref type="bibr" target="#b8">[9]</ref>, the convolutional blocks in encoder and decoder consisted of residual connections <ref type="bibr" target="#b9">[10]</ref> and dense (a) (b) (c) (d) (e) (f) (g) blocks <ref type="bibr" target="#b10">[11]</ref> respectively. It can be noted that all the above extensions of U-Net used the same encoder-decoder architecture and their contributions were either in skip connections, using better convolutional layer connections or in applications. The main problem with the above family of networks is that they lack focus in extracting features for segmentation of small structures. As the networks are built to be deeper, more highlevel features get extracted. Even though the skip connections facilitate transmission of local features to the decoder, from our experiments we observed that they still fail at segmenting small anatomical landmarks with blurred boundaries. Although U-Net and its variants are good at segmenting large structures, they fail when the segmentation masks are small or have noisy boundaries which can be seen in <ref type="figure" target="#fig_0">Fig 1.</ref> Similarly, in <ref type="figure" target="#fig_1">Fig 2 it</ref> can be observed that U-Net 3D [6] fails to extract low-level information like edges and so fails to give a sharp prediction for volumetric segmentation. U-net and its variants belong to undercomplete convolutional architectures which is what causes the network to focus on high-level features.</p><p>To this end, we proposed using overcomplete convolutional architectures for segmentation in <ref type="bibr" target="#b11">[12]</ref>. We call our overcomplete architecture Kite-Net (Ki-Net) which transforms the input to higher dimensions (in the spatial sense). Note that Kite-Net does not follow the traditional encoder-decoder style of architecture, where the inputs are mapped to lower dimensional embeddings (in the spatial sense). Compared to the use of max-pooling layers in the traditional encoder and upsampling layers in the traditional decoder, Kite-Net has upsampling layers in the encoder and max-pooling layers in the decoder. This ensures that the receptive field size of filters in the deep layers of the network does not increase like in U-Net thus facilitating Kite-Net to extract fine details of boundaries as well as small structures even in the deeper layers. Although Kite-Net extracts high quality low-level features, the lack of filters extracting high-level features makes Kite-Net not perform on par with U-Net when the dataset consists of both small and large structure annotations. Hence, we propose a multi-branch network, KiU-Net, where one branch is overcomplete (Ki-Net) and another is undercomplete (U-Net). Furthermore, we propose to effectively combine the features across the two branches using a novel cross-residual fusion strategy which results in efficient learning of KiU-Net. In this journal extension, we make the following contributions:</p><formula xml:id="formula_0">(a) (b) (c)</formula><p>? We explore the problem of segmenting sharp edges in volumetric segmentation and understand why it is not solved using U-Net 3D and its family of architectures. ? We propose KiU-Net 3D which is an extension of KiU-Net for volumetric segmentation. Here, we use a 3D convolution-based overcomplete network for efficient extraction of low-level information. <ref type="bibr">?</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review the deep learning works proposed for medical image segmentation with a focus on capturing fine details. We mainly focus on methods that deal with datasets which we conduct our experiments on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preserving Local Details:</head><p>There have been a few existing works which focus on improving the performance on local details. In <ref type="bibr" target="#b12">[13]</ref>, Qu et al. proposes a full resolution convolutional neural network (Full-Net) that maintains full resolution feature maps to improve the localization accuracy. In FullNet, there are no max-pooling or downsampling layers which makes the network not increase the receptive field size thus focusing more on boundaries. In <ref type="bibr" target="#b13">[14]</ref>, Wang et al. proposes an interactive geodesic framework where the resolution-preserving network is proposed by combining user interactions and geodesic distance transforms. It can be noted that although these methods try to solve the problem of segmenting local features better, they do not offer a complete solution. For example, DeepIGeoS requires user interactions and FullNet just tries to have no downsampling layers to reduce the enlargement of receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overcomplete Representations:</head><p>In signal processing, overcomplete representations <ref type="bibr" target="#b14">[15]</ref> were first explored for making dictionaries such that the number of basis functions can be more than the number of input signal samples. This enables a higher flexibility for capturing structure in the data. In <ref type="bibr" target="#b14">[15]</ref>, the authors show that overcomplete bases work as a better approximators of any underlying statistical distribution of a data. It has also been widely used for reconstruction of signals under the presence of noise and for source separation in a mixture of signals. It is mainly popular for these tasks because of its greater robustness in the presence of noise when compared to undercomplete representations. For denoising autoencoders <ref type="bibr" target="#b15">[16]</ref>, models with overcomplete hidden layer expression were observed to perform better as they are more useful feature detectors. The authors note that the proposed idea of denoising autoencoders in fact improved the feature detecting ability of an overcomplete fully connected network and it performs better than the standard bottleneck architectures for that task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Datasets and previous works:</head><p>For brain anatomy segmentation from US scans, methods based on U-Net <ref type="bibr" target="#b16">[17]</ref>, PSP-Net <ref type="bibr" target="#b17">[18]</ref> have been employed. Also, methods based on confidence based segmentation <ref type="bibr" target="#b18">[19]</ref> have also been proposed to solve the small anatomy segmentation problem. For gland segmentation, Chen et al. <ref type="bibr" target="#b19">[20]</ref> proposed a deep contour-aware network for accurate gland segmentation. Also, Bentaieb et al. <ref type="bibr" target="#b20">[21]</ref> proposed a topology aware FCN for histology gland segmentation by formulating a new loss to encode geometric priors. There have also been a lot of works that deal with retinal vessel segmentation based on fully convolutional networks <ref type="bibr" target="#b21">[22]</ref>, patch-based methods <ref type="bibr" target="#b22">[23]</ref> and other multiscale and multilevel deep network configurations <ref type="bibr" target="#b23">[24]</ref>. For brain tumor segmentation for MRI scans, a lot of methods have been proposed based on 2D U-Net and its variations <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b27">[28]</ref>. Many other methods based on Res-Net <ref type="bibr" target="#b9">[10]</ref>, pixel-net <ref type="bibr" target="#b28">[29]</ref> and PSP-net <ref type="bibr" target="#b29">[30]</ref> have been proposed for brain tumor segmentation in <ref type="bibr" target="#b30">[31]</ref>. 3D convolution based methods have been proved to be better for segmentation of brain tumor when compared to training 2D convolution networks on individual 2D slices of MRI scans and then combining them back together to get 3D segmentation. So, 3D U-Net based methods have been proposed in many recent works for brain tumor segmentation. Some of the most notable works for BraTS dataset are as follows: Myronenko et al. <ref type="bibr" target="#b31">[32]</ref> where a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers method accurate for brain tumor segmentation. Chen et al. <ref type="bibr" target="#b32">[33]</ref> proposes a separable 3D U-Net that use separable 3D convolution thereby reducing computational complexity that the 3D convolutions bring in. Isensee et al. <ref type="bibr" target="#b33">[34]</ref> showed that how measures like region based training, additional training data and postprocessing steps can achieve significant performance boost without any change in architecture. Some of the notable works on LiTS dataset are as follows: Li et al. <ref type="bibr" target="#b8">[9]</ref> proposed a densely connected U-Net for liver and tumor segmentation. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> combined using 2D convolutions and 3D convolutions by using 2D convolutions used at the bottom of the encoder to decrease the complexity while using 3D convolutions in other layers extract the spatial and temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we first discuss the issues with the U-Net/U-Net 3D an its family of architectures and motivate why we propose using overcomplete representations. Later, we describe the proposed architectures in detail.</p><p>1) Issues with traditional encoder-decoder networks: In the dataset that we collected for Brain Anatomy Segmentation from US images, the segmentation masks are heterogeneous in terms of the size of the structures. U-Net and its variants yield relatively good performance for this dataset as seen in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, in our experiments we observed that these methods fail to detect tiny structures in most of the cases. This does not cause much decrement in terms of the overall dice accuracy for the prediction since the datasets predominantly contain images with large structures. However, it is crucial to detect tiny structures with a high precision since it plays an important role in diagnosis. Furthermore, even for the large structures, U-Net based methods result in erroneous boundaries especially when the boundaries are blurry as seen in <ref type="figure" target="#fig_0">Fig 1 (b)</ref>,(c).</p><p>In order to clearly illustrate these observations, we evaluated U-Net on the Brain Anatomy Segmentation from US images dataset and the results are shown in <ref type="figure" target="#fig_0">Fig 1.</ref> It can be observed from the bottom row of this figure <ref type="figure" target="#fig_0">(Fig 1 (b)</ref>,(c)) that U-Net fails to detect the tiny structures. Further, the first row demonstrates that in the case of large structures, although U-Net produces an overall good prediction, it is unable to accurately segment out the boundaries. Additionally, we also made similar observations when U-Net based 3D architecture was used for volumetric segmentation of lesion. Specifically, the predictions from U-Net are blurred as it fails to segment the surface perfectly especially when the surface of the tumor is not smooth and has a high curvature (see <ref type="figure" target="#fig_1">Fig 2(a)</ref>).</p><p>To gain a further understanding of why U-Net based models are unable to segment small structures and boundaries accurately, we analyze the network architecture in detail. In each convolutional block of the encoder, the input set of features to that block get downsampled due to max-pooling layer. This makes sure that the encoder projects the input image to a lower dimension in spatial sense. This combination of convolution layer and max-pooling layer in the encoder causes the receptive field of the filters in deep layers of encoder to increase. With an increased receptive field, the deeper layers focus on high level features and thus are unable to extract features for segmenting small masks or fine edges. The only convolution filters that capture low-level information are the first few layers. Thus, it can be noted that in a traditional "encoder-decoder" architecture, the network is designed such that the number of filters increases as we go deeper in the network <ref type="bibr" target="#b11">[12]</ref>.</p><p>2) Overcomplete Networks: The idea of overcomplete networks (in the spatial sense) in the convnet-deep learning era has been unexplored. To this end, we propose Kite-Net which is an overcomplete version of U-Net. In Kite-Net, the encoder projects the input image into a spatially higher dimension. This is achieved by incorporating bilinear upsampling layers in the encoder. This form of the encoder constrains the receptive field from increasing like in U-Net as we carefully select the kernel size of the filters and upsampling coefficient such that the deep layers learn to extract fine details and features to segment small masks effectively. Furthermore, in the decoder, each conv block has a conv layer followed by a max-pooling layer.</p><p>To analyze this in detail, let I be the input image, F 1 and F 2 be the feature maps extracted from first and second conv blocks respectively. Let the initial receptive field of the conv filter be k ? k on the image. In an undercomplete network, the receptive field size change due to max-pooling layer is dependent on two variables: pooling coefficient and stride of the pooling filter. For convenience, the pooling coefficient and stride is both set as 2 in our network. Considering this configuration, the receptive field of conv block 2 (to which</p><formula xml:id="formula_1">F 1 is forwarded) on the input image would be 2 ? k ? 2 ? k.</formula><p>Similarly, the receptive field of conv block 3 (to which F 2 is forwarded) would be 4 ? k ? 4 ? k. This increase in receptive field can be generalized for an i th layer in an undercomplete network as follows:</p><formula xml:id="formula_2">RF (w.r.t I) = 2 2 * (i?1) ? k ? k.</formula><p>In comparison, the proposed overcomplete network has an upsampling layer with a coefficient 2 in the conv blocks replacing the max-pooling layer. As the upsampling layer actually works opposite to that of max-pooling layer, the receptive field of conv bock 2 on the input image now would be 1</p><formula xml:id="formula_3">2 ? k ? 1 2 ? k.</formula><p>Similarly, the receptive field of conv block 3 now would be 1</p><formula xml:id="formula_4">4 ? k ? 1 4 ? k.</formula><p>This increase in receptive field can be generalized for i th layer in the overcomplete network as follows:</p><formula xml:id="formula_5">RF (w.r.t I) = 1 2 2 * (i?1) ? k ? k.</formula><p>Note that the above calculations are based on a couple of assumptions. We assume that the pooling coefficient and stride are both set as 2 in both overcomplete and undercomplete network. Also, we consider that the receptive field change caused by the conv layer in both undercomplete and overcomplete networks would be the same and do not consider in our calculations. This can be justified as we have maintained the conv kernel size to 3 ? 3 with stride 1 and padding 1 throughout our network and this setting does not actually affect the receptive as much as max-pooling or upsampling layer does. Similar properties holds true for 3D convolution too. Previous methods like Full-Net try to maintain the resolution of the image similar at each stage. However, upsampling at the encoder constraints the receptive field more as it can be seen from the above calculations.</p><p>3) Architecture Details of KiU-Net 3D: With the success of KiU-Net for 2D segmentation <ref type="bibr" target="#b35">[36]</ref>, we propose KiU-Net 3D for volumetric segmentation from 3D medical scans. In the encoder of Kite-Net 3D branch, every conv block has a conv 3D layer followed by a trilinear upsampling layer with coeffecient of two and ReLU activation. In decoder, every conv block has a conv 3D layer followed by a 3D max-pooling layer with coeffecient of two and ReLU activation. Similarly, in the encoder of U-Net 3D branch, every conv block has a conv 3D layer followed by a 3D max-pooling layer with coefficient of two and ReLU activation. In decoder, every conv block has a conv 3D layer followed by a trilinear upsampling layer with coefficient of two and ReLU activation. We have CRFB block across each layer in KiU-Net 3D similar to KiU-Net. The difference in the CRFB block architecture of KiU-Net 3D is that we have conv 3D layers and trilinear upsampling instead of conv 2D layers and bilinear upsampling like in KiU-Net. The output of both the branches are then added and forwarded to 1 ? 1 ? 1 conv 3D layer to get the prediction voxel. All the conv layers in our network (except for the last layer) have 3 ? 3 ? 3 kernel sizes with stride 1 and padding 1. KiU-Net 3D is illustrated in <ref type="figure" target="#fig_2">Fig 3 (a)</ref>. <ref type="bibr" target="#b3">4</ref>) Cross residual feature block (CRFB): In order to further exploit the capacity of the two networks, we propose to combine the features of the two networks at multiple scales through a novel cross residual feature block (CRFB). That is, at each level in the encoder and decoder of KiU-Net 3D, we combine the respective features using a CRFB. As we know that the features learned by U-Net 3D and Kite-Net 3D are different from each other, this characteristic can be used to further improve the training of the individual networks. So, we try to learn the complementary features from both the networks which will further improve the quality of features learned by the individual networks.</p><p>The CRFB block is illustrated in <ref type="figure" target="#fig_2">Fig 3.(b</ref>  tion of conv 3D layer and max-pooling layer. The conv block that F i U i is forwarded through has a combination of conv 3D layer and upsampling layer. These cross-residual features are then added to the original features F i U and F i Ki to obtain the complementary featuresF i</p><formula xml:id="formula_6">U andF i Ki ,F i U = F i U + R i Ki and F i Ki = F i Ki + R i U .</formula><p>With this kind of complementary feature extraction from the two networks, we observe a considerable improvement in the segmentation performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we describe the experimental settings and the datasets that we use for 2D medical image segmentation and 3D medical volumetric segmentation to evaluate and compare the proposed KiU-Net and KiU-Net 3D networks respectively.</p><p>A. KiU-Net 1) Datasets: Brain Anatomy Segmentation (US): Intraventricular hemorrhage (IVH) which results in the enlargement of brain ventricles is one of the main causes of preterm brain injury. The main imaging modality used for diagnosis of brain disorders in preterm neonates is cranial US because of its safety and cost-effectiveness. Also, absence of septum pellucidum is an important biomarker for septo-optic dysplasia diagnosis. Automatic segmentation of brain ventricles and septum pellucidum from these US scans is essential for accurate diagnosis and prognosis of these ailments. After obtaining institutional review board (IRB) approval, US scans were collected from 20 different premature neonates (age &lt; 1 year). The total number of images collected were 1629 with annotations out of which 1300 were allocated for training and 329 for testing. Before processing, each image was resized to 128 ? 128 resolution.</p><p>Gland Segmentation (Microscopic): Accurate segmentation of glands is important to obtain reliable morphological statistics. Histology images in the form of Haematoxylin and Eosin (H&amp;E) stained slides are generally used for gland segmentation <ref type="bibr" target="#b38">[39]</ref>. Gland Segmentation (GLAS) dataset contains a total of 165 images out of which 85 are taken for training and 80 for testing. We pre-process the images by resizing them to 128 ? 128 resolution.</p><p>Retinal Nerve Segmentation (Fundus): Extraction of arteries and veins on retinal fundus images are essential for delineation of morphological attributes of retinal blood vessels, such as length, width, patterns and angles. These attributes are then utilized for the diagnosis and treatment of various ophthalmologic diseases such as diabetes, hypertension, arteriosclerosis and chorodial neovascularization <ref type="bibr" target="#b39">[40]</ref>. We use Retinal Images vessel Tree Extraction (RITE) dataset <ref type="bibr" target="#b40">[41]</ref> which is a subset of the DRIVE dataset. RITE dataset contains 40 images split into 20 for training and 20 for testing. We pre-process the images by resizing them to 128 ? 128 resolution.</p><p>2) Training and Implementation: As the purpose of these experiments are to demonstrate the effectiveness of proposed architecture, we do not use any application specific loss function or metric loss functions in our experiments. We use a binary cross-entropy loss between the prediction and ground truth to train the network. The cross-entropy loss is defined as  <ref type="bibr" target="#b0">[1]</ref> 0.8279 6.8794e-09 0.7861 2.8945e-08 0.5223 1.2856e-11 U-Net <ref type="bibr" target="#b1">[2]</ref> 0.8537 3.8945e-10 0.7976 5.5374e-07 0.5524 6.2381e-10 U-Net++ <ref type="bibr" target="#b2">[3]</ref> 0.8659 9.0365e-09 0.8005 4.0036e-07 0.5410 7.0523e-10 Full-net <ref type="bibr" target="#b12">[13]</ref> 0.8602 6.0234e-08 0.7995 1.7805e-07 0.7054 2.6589e-10 DeepIGeos <ref type="bibr" target="#b13">[14]</ref> 0.8354 5.004e-09 0.7899 4.0245e-08 0.5345 6.5807e-09 sSE-UNet <ref type="bibr" target="#b36">[37]</ref> 0.8795 8.8852e-08 0.8214 9.5025e-06 0.6822 1.3665e-11 CPFNet <ref type="bibr" target="#b37">[38]</ref> 0.8812 6.803e-08 0.8237 6.3305e-08 0.6724 5.0255e-09 KiU-Net 0.8943 -0.8325 -0.7517 follows:</p><formula xml:id="formula_7">L CE(p,p) = ?( 1 wh w?1 x=0 h?1 y=0</formula><p>(p(x, y) log(p(x, y)))+</p><p>(1 ? p(x, y))log(1 ?p(x, y))), where w and h are the dimensions of image, p(x, y) corresponds to the image andp(x, y) denotes the output prediction at a specific pixel location (x, y). We set the batch-size as 1 and learning rate as 0.001. We use Adam optimizer for training. We use these hyperparameters uniformly across all the experiments. We train our networks for 300 epochs or until convergence depending on the dataset. The experiments were conducted using the Pytorch framework in Python. The experiments were performed using two NVIDIA -RTX 2080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. KiU-Net 3D</head><p>For volumetric segmentation, we use two widely used public datasets: BraTS challenge dataset and LiTS challenge dataset for our experiments. 1) Datasets: Brain Tumor Segmentation (MRI): Brain Tumor Segmentation (BraTS) challenge has a curated collection of MRI scans with expert annotations of brain tumor. It contains multimodal MRI scans of confirmed cases of glioblastoma (GBM/HGG) and low grade glioma (LGG). The modalities present in these scans are native (T1), post-contrast T1 (T1ce), T2-weighted (T2) and T2 attenuated inversion recovery (T2-FLAIR) <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>. The annotations are provided for four classes -enhancing tumor, peritumoral edema and the necrotic and non-enhancing tumor core. We used the 2019 version of the BraTS challenge training data for training our the baselines and our proposed method. It has a total of 335 MRI scans. For quantitative comparisons, we use the validation dataset provided by BraTS 2019 challenge which has 125 scans. For qualitative comparisons, we use the new 33 scans added to the training set of BraTS 2020 challenge dataset as the validation datasets do not come with ground truth annotations. Each MRI scan contains 155 slices each of dimensions 255 ? 255.</p><p>Liver Segmentation (CT): Liver Tumor Segmentation Challenge (LiTS) dataset <ref type="bibr" target="#b44">[45]</ref> contains contrast-enhanced abdominal CT scans along with annotations of liver and liver lesions. It can be noted that the evaluation for LiTS dataset, we train on the 109 CT scans from the LiTS training dataset and test it on 21 CT scans between 27 and 48 and report the dice accuracy.</p><p>2) Training and Implementation: For training the KiU-Net 3D, we use the cross entropy loss between the prediction and the input scan. Since, the scan can be viewed as a 3D voxel, the cross entropy loss can be formulated as follows:</p><formula xml:id="formula_8">L CE(p,p) = ? C?1 c=0 1 whl l?1 z=0 w?1 x=0 h?1 y=0 p(z, x, y) log(p(z, x, y)),</formula><p>where w, h are the dimensions of each slice while l is the number of slices in the scan, p(z, x, y) corresponds to the input 3D scan andp(z, x, y) denotes the output prediction at a specific pixel location (z, x, y) and C corresponds to the number of classes found in the dataset. The above loss formulation suits the multi-class segmentation framework of BraTS dataset where C = 4. For both the datasets, we use a learning rate of 0.0001 with batch size 1 and use Adam optimizer. We do voxel-wise training (similar to patch-wise training on 2D) for the BraTS dataset with voxel shapes 128? 128 ? 50. The experiments were conducted using the Pytorch framework in Python. The experiments were performed using two NVIDIA -RTX 2080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In this section, we discuss the results and outcomes of the experiments that we conducted using KiU-Net and KiU-Net 3D. First, we discuss the quantitative evaluations where the proposed method is compared with other recent approaches using metrics that are widely used for medical image segmentation. Next, we provide qualitative results where we visualize sample predictions to analyze why the proposed method's performance is superior as compared to the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Results</head><p>1) KiU-Net: Following existing approaches like <ref type="bibr" target="#b3">[4]</ref>, we use Dice Index (F1-score) for evaluating and comparing the proposed method (KiU-Net) on the medical image segmentation datasets:</p><formula xml:id="formula_9">Dice = 2T P 2T P + F P + F N ,</formula><p>where TP, FP and FN correspond to the number of pixels that are true positives, false positives and false negatives respectively of the prediction when compared with the ground truth. <ref type="table" target="#tab_2">Table I</ref> shows the results for the experiments on all the 3 datasets for image segmentation. As it can be observed, we compare KiU-Net with some of the widely used backbone Note that all the other networks were trained from scratch for comparison using the same pipeline as we trained KiU-Net for fair comparison. We also conduct a paired t-test and report the p-value to show the statistical significance of our results. The t-test is conducted between the dice accuracy of our proposed method and all the other baseline methods. Please note that the p-values are calculated using the dice accuracy for Brain US, GlaS, RITE datasets. We note that all the values are way below 0.05 proving the statistical significance of our results.</p><p>2) KiU-Net 3D: For 3D volumetric segmentation, we adopt the performance metrics used in the BraTS challenge and LiTS challenge. For brain tumor segmentation from MRI scans, we report the Dice accuracy of enhancing tumor (ET), whole tumor (WT) and tumor core (TC). We also report the Hausdorff distance for all these three classes. More details about these metrics for brain tumor segmentation can be found in <ref type="bibr" target="#b43">[44]</ref>. Similarly for liver segmentation in LiTS dataset, we report metrics such as Dice score, Jaccard index, volume overlap error (VOE), false negative rate (FNR), false positive rate (FPR), average symmetric surface distance (ASSD) and maximum symmetric surface distance (MSD). The details of these metrics can be found in <ref type="bibr" target="#b44">[45]</ref>. For both these datasets we compare KiU-Net 3D with U-Net 3D, Seg-Net 3D, V-Net, Res U-Net 3D and MS U-Net 3D. We also compare with some application specific methods like HDense-UNet. Additionally, we also conduct experiments with slice-wise training of the scans using 2D versions of all these networks. <ref type="table" target="#tab_2">Tables II   Input Image</ref> Seg-Net U-Net U-Net++ KiU-Net GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retina Nerve Segmentation RITE Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gland Segmentation GLAS Dataset Brain Anatomy Segmentation</head><p>Brain US Dataset <ref type="figure">Fig. 4</ref>. Comparison of qualitative results between SegNet, UNet , UNet++ and KiU-Net for Brain anatomy segmentation using the brain US dataset, gland segmentation using the GLAS dataset and retina nerve segmentation using the RITE dataset.</p><p>and III report the quantitative metrics comparison for BraTS and LiTS dataset respectively. We observe that KiU-Net 3D outperforms other methods. The methods under comparison are trained from scratch. Note that we have used the same pipeline for all these experiments for a fair comparison. Since the primary goal of these experiments is to show that KiU-Net can act as a better backbone network architecture, we do not perform any pre-processing or post-processing which can improve the performance further irrespective of the network architecture. We also conduct a paired t-test and report the p-value to show the statistical significance of our results. The t-test is conducted between the dice accuracy of our proposed method and all the other baseline methods. Please note that the p-values are calculated using the dice accuracy of tumor segmentation in LiTS dataset and dice of whole tumor accuracy for the BraTS dataset. We note that all the values are way below 0.05 proving the statistical significance of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results</head><p>1) KiU-Net: <ref type="figure">Fig 4</ref> illustrates the predictions of KiU-Net along with Seg-Net, U-Net and U-Net++ for all 3 medical image segmentation datasets we used for evaluation. In the first row, we can observe that KiU-Net is able to segment the small ventricles accurately and the other "traditional" networks fail to do so. Similarly from second, third and fourth rows, we can observe that the predictions of KiU-net are able to segment out the edges significantly better when compared to the other networks. In the predictions of RITE dataset which has very low number of images to train, our network performs reasonably well as compared to others learns, thus demonstrating that low level features for nerve segmentation are used effectively in our network.</p><p>2) KiU-Net 3D: <ref type="figure" target="#fig_3">Fig 5 illustrates</ref> the predictions of KiU-Net 3D for volumetric segmentation experiments. The first two rows correspond to the results for BraTS dataset and the bottom two rows correspond to the results for LiTS dataset. Note that the first row and third rows correspond to segmentation prediction of a single slice of the scan where as the second and fourth row correspond to the 3D segmentation prediction of the scan. The 3D segmentation results are visualized using ITKSnap <ref type="bibr" target="#b49">[49]</ref> where each scan prediction consists of 155 2D images in the case of BraTS dataset and 48 2D images in the case of LiTS dataset. From visualizations of the brain tumor segmentation task, it can be observed that KiU-Net is able to segment the surface and edges of the tumor significantly better than any other network and is closer to the ground truth. For BraTS dataset, the red regions correspond to tumor core, yellow regions correspond to non-enhancing tumor and the green regions correspond to edema. From the second row for BraTS dataset, it can be observed that the tumor surface of the ground truth has sharp edges. While all the other methods smooth out these edges, KiU-Net predicts the sharp edges of the surface of the tumor more precisely. While these results are focused on demonstrating the superiority of KiU-Net in segmenting small lesions, the results on the LiTS dataset show that the proposed method is equally effective in segmenting larger regions. From the bottom two rows of <ref type="figure" target="#fig_3">Fig 5,</ref> we can observe that KiU-Net performs better than other networks in segmenting large masks as well. Based on this, we would like to point out that even though KiU-Net focuses more on low-level features when compared to UNet or UNet++, its performance is on-par/better as compared to them while segmenting large masks as well. We also observe that performing 2D segmentation on individual 2D images and then combining them to form a 3D scan does not work well. This can be observed from the first 3 images from the last row where the surfaces are not smooth while the ground truth looks smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSIONS</head><p>As we propose a generic architecture as the solution to image and volumetric segmentation, we believe it is important to study if the network can be further improved using some notable techniques in deep-learning literature. Also, we study other key properties like number of parameters, converge rate and memory requirements of our proposed method in detailed. As it is clear from the above discussions that KiU-Net is a good backbone architecture for both image and volumetric segmentation, we experiment with other variants of KiU-Net which result in further improvements. In this section, we describe these variants and present the detailed results corresponding to each of these variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Further Improvements</head><p>1) Res-KiUNet: In Res-KiUNet, we employ residual connections in both the branches of KiU-Net. We use residual learning in every conv block at each level in both the encoder and decoder part of both branches. If x and y are the input and output of each conv block (F ()) of our network, the residual connection can be formulated as follows: y = F (x) + x. We illustrate the architecture details of Res-KiUNet in <ref type="figure" target="#fig_4">Fig 6</ref> where the residual connections are denoted using red arrows.  Residual connections are helpful in efficient learning of the network since we can propagate gradients to initial layers faster and thus solving the problem of vanishing gradients.</p><formula xml:id="formula_10">(a) (b) (c) (d) (e)</formula><p>2) Dense-KiUNet: In Dense-KiUNet, we employ dense blocks after every conv layer in both the branches. We use a dense block of 4 conv layers where the input consists of k feature maps. Each conv layer outputs k/4 feature maps which is concatenated with the input to all the next conv layers. The output of all these conv layers are then concatenated to obtain k output feature maps. This is added with the input and sent to the next layer in the network. The output of the dense block is forwarded to a max-pooling layer in the encoder of undercomplete branch and in the decoder of overcomplete branch. In the encoder of overcomplete branch and in the decoder of undercomplete branch, the output of the dense block is forwarded to an upsampling layer. To evaluate both Res-KiUNet and Dense KiUNet, we conduct experiments on the GlaS dataset and report the dice and Jaccard metrics in <ref type="table" target="#tab_2">Table IV</ref>. Further, we visualize the results of these experiments in <ref type="figure" target="#fig_5">Fig 7.</ref> It can be observed that Dense-KiUNet and Res-KiUNet provide further improvements in performance as compared to KiU-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Number of Parameters</head><p>Seg-Net, U-Net and U-Net++ have a 5 layer deep encoder and decoder. The number of filters in each block of these networks increase gradually as we go deeper in the network. For example, U-Net uses this sequence of filters for its 5 layers -64, 128, 256, 512 and 1024. Although KiU-Net is a multi-branch network, we limit the complexity of our network by using fewer layers and filters. Specifically, we use a 3 layer deep network with 32, 64 and 128 respectively as the number of filters. Due to this, the number of parameters in our network is significantly fewer as compared to other methods. In <ref type="table" target="#tab_5">Table V</ref>, we tabulate the number of parameters for KiU-Net and other recent networks and it can be observed that KiU-Net has ?10? lesser parameters as compared to U-Net and ?40? fewer parameters as compared to SegNet. Further, it is important to note that the other approaches have have resulted in higher complexity in an attempt to improve the performance, however, our approach is able to obtain better performance while have significantly fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convergence</head><p>The convergence of loss function is an important characteristic associated with a network. A faster convergence means  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We conduct an ablation study to analyze the effectiveness of different blocks in the proposed method (KiU-Net). For these experiments, we use the brain anatomy segmentation US dataset. We start with the basic undercomplete ("traditional") encoder-decoder convolutional architecture (UC) and overcomplete convolutional architecture (OC). Note that these networks do not contain any skip connections (SK). Next, we add skip connections to the UC and OC baselines. These networks are basically U-Net (UC+skip connections) and Kite-Net (OC+skip connections). We then fuse both these networks by adding the feature map output of both the networks at the end. This is in fact KiU-Net without the CRFB block. Finally, we show the performance of our proposed architecture -KiU-Net. <ref type="table" target="#tab_2">Table VI</ref> shows the results of all these ablation experiments. It can be observed that the performance improves with addition of each block to the network. Please note that the performances of OC and Kite-Net are lower because these predictions contain only the edges of the masks and do not contain any high-level information. <ref type="figure" target="#fig_0">Fig 10 illustrates</ref> the qualitative improvements after adding each major block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Memory Required</head><p>We note that the memory required is dependent on the depth and the number of filters used in the network. So, with a reduced depth and reduced number of filters, the network's  memory requirements are less while maintaining a decent performance. An ablation study was performed for the same using GlaS dataset and the results can be observed in Tables VII and VIII. Note that while performing the experiments for reduced depth, the number of filters were kept the same. While performing the experiments for different number of filters, the depth was kept as 3 like in the main paper. Note that a generic U-Net which is 5 layers deep with filters 64,128,256,512 and 1024 gives a F1 score of 0.7976 on GlaS dataset as reported in the manuscript. It can be seen that even with less computation cost and time, our proposed architecture gives better results than U-Net.</p><formula xml:id="formula_11">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Dependence on batch-size</head><p>We conducted experiments where we tried to fit U-Net/U-Net++ to the GPU with the maximum memory capacity <ref type="bibr">(11 GB)</ref>. The batch size we used for U-Net/U-Net++ was 4 while for KiU-Net we used only a batch size of 1. The comparison in terms of performance on GlaS dataset can be seen in <ref type="table" target="#tab_2">Table  IX</ref>. It can be observed that with a higher batch size, the change in performance for U-Net/U-Net++ is negligible while KiU-Net achieves a significant performance boost when compared to the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Dependence on resolution of image</head><p>In our main experiments, the images were resized to 128 ? 128 for Brain US, GlaS and RITE datasets. We can also fix the resolution to some higher value like 512 ? 512 but in that case we would be interpolating some images to a higher resolution as the width and height of most images (in Brain US and GLAS datasets) fall below that. However, to show that we could still utilize KiU-Net for images of higher resolution we resize all images in the RITE dataset to 512 ? 512 and conduct experiments for the same. In <ref type="table" target="#tab_9">Table X</ref>, we tabulate the results and compare the difference in performance while conducting the experiments for U-Net and KiU-Net on two different resolutions of the same dataset (128?128, 512?512) for GlaS dataset. It can be observed that there is a slight change in performance for both U-Net and KiU-Net on a higher resolution of the image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we proposed KiU-Net and KiU-Net 3D for image and volumetric segmentation, respectively. These are two-branch networks consisting of an undercomplete and an overcomplete autoencoder. Our novelty lies in proposing overcomplete convolutional architecture (Kite-Net) for learning small masks and finer details of surfaces and edges more precisely. The two branches are effectively fused using a novel cross-residual feature fusion method that results effective training. Further, we experiment with different variants of the proposed method like Res-KiUNet and Dense-KiUNet. We conduct extensive experiments for image and volumetric segmentation on 5 datasets spanning over 5 different modalities. We demonstrate that the proposed method performs significantly better when compared to the recent segmentation methods. Furthermore, we also show that our network comes with additional benefits such as lower model complexity and faster convergence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Input B-Mode Ultrasound Image. Predictions from (b) U-Net, (d) KiU-Net (ours), (f) Ground Truth. (c),(e) and (g) are the zoomed in patches from (b),(d) and (f) respectively. The boxes in the original images correspond to the zoomed in portion for the zoomed images. Our method segments small anatomy and edges better than U-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A sample brain tumor segmentation prediction using (a) U-Net 3D, (b) KiU-Net 3D, and (c) Ground-Truth for BraTS dataset. KiU-Net 3D results in better segmentation of fine details when compared to U-Net 3D as it focuses more on low-level information effectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Architecture details of KiU-Net 3D for 3D volumetric segmentation. (b) Details of Cross Residual Fusion Block (CRFB) for KiU-Net 3D. In KiU-Net 3D, the input 3D voxel is forwarded to the two branches of KiU-Net 3D: Kite-Net 3D and U-Net 3D which have 3D CRFB blocks connecting them at each level. The feature maps from the last layer of both the branches are added and passed through 1 ? 1 3D conv to get the prediction. In CRFB, the residual features of Kite-Net 3D are learned and added to the features of U-Net 3D to forward the complementary features to U-Net and vice-versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of qualitative results between SegNet, UNet , KiU-Net, SegNet 3D, UNet 3D and KiU-Net 3D for the brain tumor segmentation using the BraTS dataset and the Liver segmentation using the LiTS dataset. The first and third row correspond to the prediction from a 2D slice in the MRI brain scan and abdominal CT scan respectively. The second and third rows visualize the 3D volume segmentation predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Details of Res-KiU-Net architecture. The input image is forwarded to the two branches of Res-KiUNet where each branch has residual connections at each level. The feature maps are added at the last layer and passed through a 1?1 conv 2D layer to get the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>(a) Input Image, Prediction using (b) KiU-Net (c) Res-KiUNet (d) Dense KiU-Net (e) Ground Truth for GLAS dataset. It can be observed that the predictions of Res-KiUNet and Dense-KiUNet are better in terms of quality when compared to KiU-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig 8 illustrates the architecture details of Dense-KiUNet and the dense block we have used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Details of Dense-KiU-Net architecture. The input image is forwarded to the two branches of Dense-KiUNet where each branch has dense blocks at each level. The feature maps are added at the last layer and forwarded through a 1 ? 1 conv 2D layer to get the prediction . In the right side of the figure, dense block architecture has been visualized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative results of ablation study on test images. (a) B-Mode input US image. (b) Ground Truth annotation. Prediction of segmentation masks by (c) UC -Under-complete architecture (d) OC -Over-complete architecture (e) UC + SK (under-complete architecture with skip connections) (f) UC + OC with SK (combined architecture with skip connections) (g) KiU-Net (ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We propose Res-KiUNet and Dense-KiUNet architectures where we use residual connections and dense blocks respectively for improving the learning of the network.</figDesc><table><row><cell>We evaluate the performance of the proposed architecture</cell></row><row><cell>for volumetric and image segmentation across 5 datasets:</cell></row><row><cell>Brain Tumor Segmentation (BraTS), Liver Tumor Seg-</cell></row><row><cell>mentation (LiTS), Gland Segmentation (GlaS), Retinal</cell></row><row><cell>Images vessel Tree Extraction (RITE) and Brain Anatomy</cell></row><row><cell>segmentation. These datasets individually correspond to 5</cell></row><row><cell>different modalities: ultrasound (US), magnetic resonance</cell></row><row><cell>imaging (MRI), computed tomography (CT), microscopic</cell></row></table><note>?and fundus images. With these additional experiments on multiple datasets, we demonstrate that the proposed method generalizes well to different modalities.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Denoting the features maps from U-Net 3D as F i U and F i Ki as the feature maps from Kite-Net 3D after i th block in KiU-Net, the crossresidual features R i</figDesc><table><row><cell></cell><cell cols="2">Kite-Net 3D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CRFB</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input 3D Voxel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prediction</cell><cell cols="2">Features Maps From Kite-Net</cell><cell cols="2">To Kite-Net</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell></row><row><cell>CRFB</cell><cell>CRFB</cell><cell>CRFB</cell><cell>CRFB</cell><cell>CRFB</cell><cell>+</cell><cell></cell><cell></cell><cell>R U</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R Ki</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell><cell cols="2">To U-Net</cell><cell cols="2">Feature Maps</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">From U-Net</cell></row><row><cell></cell><cell cols="2">U-Net 3D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv 3D+</cell><cell>Conv 3D+</cell><cell></cell><cell cols="2">Cross Residual</cell><cell></cell><cell></cell><cell>Residual</cell><cell></cell><cell>Residual</cell></row><row><cell>Upsampling+ ReLU</cell><cell>Max-Pooling+ ReLU</cell><cell>CRFB</cell><cell>Fusion Block</cell><cell></cell><cell>1x1 Conv3D</cell><cell>R Ki</cell><cell>from Kite-Net</cell><cell>R U</cell><cell>from U-Net</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note>U and R i Ki are first extracted using a conv block. The conv block that F i Ki is forwarded to has a combina-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON FOR 2D IMAGE SEGMENTATION WITH RESPECT TO EXISTING METHODS.</figDesc><table><row><cell>Network</cell><cell>Brain US</cell><cell></cell><cell>GlaS</cell><cell></cell><cell>RITE</cell></row><row><cell>Dice</cell><cell>p-value</cell><cell>Dice</cell><cell>p-value</cell><cell>Dice</cell><cell>p-value</cell></row><row><cell>Seg-Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON FOR BRAIN TUMOR VOLUME SEGMENTATION IN THE BRATS DATASET WITH RESPECT TO EXISTING METHODS.TABLE III PERFORMANCE COMPARISON FOR LIVER AND LIVER LESION SEGMENTATION IN THE LITS DATASET WITH RESPECT TO EXISTING METHODS.</figDesc><table><row><cell>Type</cell><cell>Network</cell><cell>Dice-ET</cell><cell cols="2">Dice-WT</cell><cell>Dice-TC</cell><cell cols="2">H95-ET</cell><cell>H95-WT</cell><cell>H95-TC</cell><cell>p-value</cell></row><row><cell></cell><cell>Seg-Net [1]</cell><cell>0.4994</cell><cell></cell><cell>0.7611</cell><cell>0.6887</cell><cell cols="2">65.6867</cell><cell>20.3247</cell><cell>20.0050</cell><cell>3.784e-08</cell></row><row><cell>Image</cell><cell>U-Net [2]</cell><cell>0.5264</cell><cell></cell><cell>0.8083</cell><cell>0.7032</cell><cell cols="2">17.5458</cell><cell>13.9467</cell><cell>19.2653</cell><cell>8.568e-10</cell></row><row><cell></cell><cell>KiU-Net</cell><cell>0.6637</cell><cell></cell><cell>0.8612</cell><cell>0.7061</cell><cell></cell><cell>9.4176</cell><cell>12.7896</cell><cell>13.0401</cell><cell>-</cell></row><row><cell></cell><cell>Seg-Net 3D [1]</cell><cell>0.5599</cell><cell></cell><cell>0.8062</cell><cell>0.7073</cell><cell cols="2">10.0037</cell><cell>10.4584</cell><cell>10.7513</cell><cell>1.002e-07</cell></row><row><cell></cell><cell>V-Net [7]</cell><cell>0.6132</cell><cell></cell><cell>0.8318</cell><cell>0.7011</cell><cell cols="2">42.2162</cell><cell>20.3525</cell><cell>13.0526</cell><cell>9.665e-08</cell></row><row><cell></cell><cell>Deeper V-Net 3D [7]</cell><cell>0.6251</cell><cell></cell><cell>0.8388</cell><cell>0.7084</cell><cell cols="2">39.2542</cell><cell>19.4505</cell><cell>13.6517</cell><cell>6.998e-08</cell></row><row><cell></cell><cell>U-Net 3D [6]</cell><cell>0.6711</cell><cell></cell><cell>0.8448</cell><cell>0.7059</cell><cell cols="2">10.2162</cell><cell>13.0005</cell><cell>15.0856</cell><cell>4.225e-06</cell></row><row><cell>Voxel</cell><cell>Deeper U-Net 3D [6]</cell><cell>0.6891</cell><cell></cell><cell>0.8512</cell><cell>0.7294</cell><cell></cell><cell>9.8882</cell><cell>12.5415</cell><cell>12.0452</cell><cell>7.568e-10</cell></row><row><cell></cell><cell>Res U-Net 3D [46]</cell><cell>0.6667</cell><cell></cell><cell>0.8526</cell><cell>0.7091</cell><cell></cell><cell>7.270</cell><cell>8.5454</cell><cell>9.5708</cell><cell>9.551e-07</cell></row><row><cell></cell><cell>MS U-Net 3D [6], [47]</cell><cell>0.7125</cell><cell></cell><cell>0.8652</cell><cell>0.7114</cell><cell></cell><cell>8.2465</cell><cell>9.4205</cell><cell>12.6541</cell><cell>3.802e-11</cell></row><row><cell></cell><cell>KiU-Net 3D</cell><cell>0.7321</cell><cell></cell><cell>0.8760</cell><cell>0.7392</cell><cell></cell><cell>6.3228</cell><cell>8.9424</cell><cell>9.8929</cell><cell>-</cell></row><row><cell></cell><cell>Type</cell><cell>Network</cell><cell></cell><cell cols="3">Tumor Segmentation</cell><cell cols="2">Liver Segmentation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dice</cell><cell>p-value</cell><cell></cell><cell>Dice</cell><cell>p-value</cell></row><row><cell></cell><cell></cell><cell>Seg-Net [1]</cell><cell></cell><cell>0.6452</cell><cell>2.306e-25</cell><cell></cell><cell cols="2">0.7656 2.998e-22</cell></row><row><cell></cell><cell>Image</cell><cell>U-Net [2]</cell><cell></cell><cell>0.6950</cell><cell>6.519e-21</cell><cell></cell><cell cols="2">0.7723 5.665e-16</cell></row><row><cell></cell><cell cols="3">DeepLab v3+ [48]</cell><cell>0.6860</cell><cell>5.552e-22</cell><cell></cell><cell cols="2">0.8570 3.254e-10</cell></row><row><cell></cell><cell></cell><cell>KiU-Net</cell><cell></cell><cell>0.7105</cell><cell>-</cell><cell></cell><cell>0.8035</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Seg-Net 3D [1]</cell><cell></cell><cell>0.6822</cell><cell>3.003e-23</cell><cell></cell><cell cols="2">0.8789 6.028e-25</cell></row><row><cell></cell><cell></cell><cell>U-Net 3D [6]</cell><cell></cell><cell>0.6912</cell><cell>9.022e-15</cell><cell></cell><cell cols="2">0.9346 5.005e-10</cell></row><row><cell></cell><cell>Voxel</cell><cell>Dense-UNet</cell><cell></cell><cell>0.5944</cell><cell>6.556-08</cell><cell></cell><cell cols="2">0.9336 1.985e-11</cell></row><row><cell></cell><cell></cell><cell>Li et al. [9]</cell><cell></cell><cell>0.7350</cell><cell>1.887e-09</cell><cell></cell><cell cols="2">0.9380 1.057e-08</cell></row><row><cell></cell><cell></cell><cell>KiU-Net 3D</cell><cell></cell><cell>0.7750</cell><cell>-</cell><cell></cell><cell>0.9423</cell><cell>-</cell></row></table><note>architectures for medical image segmentation like Seg-Net, U-Net, U-Net++, sSE-UNet and CPFNet. We also compare KiU-Net with methods proposed previously for focusing on local details like FullNet and DeepGIoS. The methods under comparison are trained from scratch. Compared to all these, KiU-Net achieves a significant boost in terms of performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF PERFORMANCE METRICS FOR RES-KIUNET AND DENSE KIUNET USING THE GLAS DATASET.</figDesc><table><row><cell cols="4">Performance Metrics KiU-Net Res-KiUNet Dense-KiUNet</cell></row><row><cell>Dice</cell><cell>0.8325</cell><cell>0.8385</cell><cell>0.8431</cell></row><row><cell>Jaccard</cell><cell>0.7278</cell><cell>0.7303</cell><cell>0.7422</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF NUMBER OF PARAMETERS</figDesc><table><row><cell>Network</cell><cell cols="4">Seg-Net [1] U-Net [2] U-Net++ [3] KiU-Net</cell></row><row><cell>No. of Parameters</cell><cell>12.5M</cell><cell>3.1M</cell><cell>9.0M</cell><cell>0.29M</cell></row><row><cell cols="5">Fig. 9. Comparison of convergence of loss function between KiU-Net,</cell></row><row><cell cols="5">UNet++ [3], UNet [2] and SegNet [1]. The convergence of KiU-Net is</cell></row><row><cell cols="3">faster when compared to all other methods.</cell><cell></cell><cell></cell></row></table><note>is always beneficial as it results in significantly lower training complexity. Fig 9 compares the convergence trends for Seg- Net, U-Net, U-Net++ and KiU-Net when trained on GLAS dataset. It can be observed that KiU-Net converges faster when compared to other networks. Similar trends were observed while training the network on other datasets as well.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDY USING THE BRAIN US DATASET.</figDesc><table><row><cell>Metrics</cell><cell>UC</cell><cell>OC</cell><cell>UC+SK</cell><cell cols="3">OC+SK UC+OC+SK KiU-Net</cell></row><row><cell>Dice</cell><cell>0.82</cell><cell>0.56</cell><cell>0.85</cell><cell>0.60</cell><cell>0.86</cell><cell>0.89</cell></row><row><cell>Jaccard</cell><cell>0.75</cell><cell>0.43</cell><cell>0.79</cell><cell>0.47</cell><cell>0.78</cell><cell>0.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>OF PERFORMANCE WHEN TRAINING KIU-NET WITH DIFFERENT DEPTHS USING THE GLAS DATASET.TABLE VIII COMPARISON OF PERFORMANCE WHEN TRAINING KIU-NET WITH DIFFERENT NUMBER OF FILTERS USING THE GLAS DATASET.</figDesc><table><row><cell>Depth</cell><cell cols="2">No. of parameters</cell><cell cols="2">Memory</cell><cell cols="2">Training Time (in s)</cell><cell>Inference Time (in s)</cell><cell>Dice</cell></row><row><cell>2</cell><cell></cell><cell>0.21M</cell><cell cols="2">1.25 GB</cell><cell></cell><cell>3.03</cell><cell>0.03</cell><cell>0.8111</cell></row><row><cell>3</cell><cell></cell><cell>0.29M</cell><cell cols="2">5.55 GB</cell><cell></cell><cell>30.26</cell><cell>0.35</cell><cell>0.8325</cell></row><row><cell cols="2">Number of filters</cell><cell cols="2">No. of parameters</cell><cell cols="2">Memory</cell><cell>Training Time (in s)</cell><cell>Inference Time (in s)</cell><cell>Dice</cell></row><row><cell>8,16,32</cell><cell></cell><cell>0.22</cell><cell></cell><cell cols="2">2.03 GB</cell><cell>7.02</cell><cell>0.07</cell><cell>0.8195</cell></row><row><cell>16,32,64</cell><cell></cell><cell>0.29 M</cell><cell></cell><cell cols="2">5.55 GB</cell><cell>30.26</cell><cell>0.35</cell><cell>0.8325</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>OF PERFORMANCE WITH DIFFERENT BATCH SIZES FOR U-NET AND U-NET++ USING THE GLAS DATASET.</figDesc><table><row><cell>Method</cell><cell>Batch size</cell><cell>Dice</cell><cell>Jaccard</cell></row><row><cell>U-Net</cell><cell>1</cell><cell>0.7976</cell><cell>0.6763</cell></row><row><cell>U-Net</cell><cell>4</cell><cell>0.7998</cell><cell>0.6782</cell></row><row><cell>U-Net++</cell><cell>1</cell><cell>0.8005</cell><cell>0.6893</cell></row><row><cell>U-Net++</cell><cell>4</cell><cell>0.8033</cell><cell>0.6910</cell></row><row><cell>KiU-Net</cell><cell>1</cell><cell>0.8325</cell><cell>0.7278</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X COMPARISON</head><label>X</label><figDesc>OF PERFORMANCE WITH DIFFERENT RESOLUTIONS OF IMAGES USING THE GLAS DATASET.</figDesc><table><row><cell>Method</cell><cell>Resolution</cell><cell>Dice</cell><cell>Jaccard</cell></row><row><cell>U-Net</cell><cell>128 ? 128</cell><cell>79.76</cell><cell>67.63</cell></row><row><cell>U-Net</cell><cell>512 ? 512</cell><cell>80.23</cell><cell>68.03</cell></row><row><cell>KiU-Net</cell><cell>128 ? 128</cell><cell>83.25</cell><cell>72.78</cell></row><row><cell>KiU-Net</cell><cell>512 ? 512</cell><cell>83.77</cell><cell>73.06</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unet 3+: A full-scale connected unet for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1055" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d u-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>I?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weighted res-unet for high-quality retina vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 9th International Conference on Information Technology in Medicine and Education (ITME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Kiu-net: Towards accurate segmentation of biomedical images using over-complete representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04878</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving nuclei/gland instance segmentation in histopathology images by full resolution neural network and spatial constrained loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Riedlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="378" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepigeos: a deep interactive geodesic framework for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1559" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning overcomplete representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic segmentation of the cerebral ventricle in neonates using deep learning with 3d reconstructed freehand ultrasound imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sciolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sdika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Quetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Delachartre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Ultrasonics Symposium (IUS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic real-time cnn-based neonatal brain ventricles segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Cuccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="716" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to segment brain anatomy from 2d ultrasound with less data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dcan: Deep contour-aware networks for object instance segmentation from histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Topology aware fully convolutional networks for histology gland segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bentaieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic retinal blood vessel segmentation based on fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1112</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Patch-based fully convolutional neural network with skip connections for retinal blood vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1742" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilevel and multiscale deep neural network for retinal blood vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Veeramalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">946</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic segmentation of brain tumor from 3d mr images using segnet, u-net, and psp-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Three pathways u-net for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pre-conference proceedings of the 7th medical image computing and computer-assisted interventions (MICCAI) BraTS Challenge</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Brain tumor detection and segmentation using deep learning u-net on multi modal mri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fridman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pre-Conference Proceedings of the 7th MICCAI BraTS Challenge</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks using u-net for automatic brain tumor segmentation in multimodal mri volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kermi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Khadir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pixelnet: Representation of the pixels, by the pixels, and for the pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06506</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glioma prognosis: segmentation of the tumor and survival prediction using shape, geometric and clinical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="142" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d mri brain tumor segmentation using autoencoder regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">S3d-unet: separable 3d u-net for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="358" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">No new-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kickingereder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Light-weight hybrid convolutional network for liver tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="4271" to="4277" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kiu-net: Towards accurate segmentation of biomedical images using over-complete representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="363" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recalibrating fully convolutional networks with spatial and channel &quot;squeeze and excitation&quot; blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="540" to="549" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cpfnet: Context pyramid fusion network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3008" to="3018" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: The glas challenge contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abr?moff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automated separation of binary overlapping trees in low-contrast color retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abr?moff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Garvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="436" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">170117</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chlebus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hesser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04056</idno>
		<title level="m">The liver tumor segmentation benchmark (lits)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation based on 3d residual u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thakur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="218" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using a 3d fcn with multi-scale loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jesson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="392" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Cody</forename><surname>Hazlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
