<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hardware Architecture of Embedded Inference Accelerator and Analysis of Algorithms for Depthwise and Large-Kernel Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-29">29 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tse-Wei</forename><surname>Chen</surname></persName>
							<email>twchen@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Inc</orgName>
								<address>
									<addrLine>30-2, Shimomaruko 3-chome, Ohta-ku</addrLine>
									<postCode>146-8501</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Canon Information Technology (Beijing) Co., Ltd. (CIB)</orgName>
								<address>
									<addrLine>12A Floor, Yingu Building, No.9 Beisihuanxi Road, Haidian</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Canon Information Technology (Beijing) Co., Ltd. (CIB)</orgName>
								<address>
									<addrLine>12A Floor, Yingu Building, No.9 Beisihuanxi Road, Haidian</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchao</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Canon Information Technology (Beijing) Co., Ltd. (CIB)</orgName>
								<address>
									<addrLine>12A Floor, Yingu Building, No.9 Beisihuanxi Road, Haidian</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kinya</forename><surname>Osa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Canon Inc</orgName>
								<address>
									<addrLine>30-2, Shimomaruko 3-chome, Ohta-ku</addrLine>
									<postCode>146-8501</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masami</forename><surname>Kato</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Canon Inc</orgName>
								<address>
									<addrLine>30-2, Shimomaruko 3-chome, Ohta-ku</addrLine>
									<postCode>146-8501</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hardware Architecture of Embedded Inference Accelerator and Analysis of Algorithms for Depthwise and Large-Kernel Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-29">29 Apr 2021</date>
						</imprint>
					</monogr>
					<note>arXiv:2104.14125v1 [cs.CV]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks (CNNs)</term>
					<term>embedded vision</term>
					<term>depthwise convolution</term>
					<term>hardware utilization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In order to handle modern convolutional neural networks (CNNs) efficiently, a hardware architecture of CNN inference accelerator is proposed to handle depthwise convolutions and regular convolutions, which are both essential building blocks for embedded-computer-vision algorithms. Different from related works, the proposed architecture can support filter kernels with different sizes with high flexibility since it does not require extra costs for intra-kernel parallelism, and it can generate convolution results faster than the architecture of the related works. The experimental results show the importance of supporting depthwise convolutions and dilated convolutions with the proposed hardware architecture. In addition to depthwise convolutions with large-kernels, a new structure called DDC layer, which includes the combination of depthwise convolutions and dilated convolutions, is also analyzed in this paper. For face detection, the computational costs decrease by 30%, and the model size decreases by 20% when the DDC layers are applied to the network. For image classification, the accuracy is increased by 1% by simply replacing 3 ? 3 filters with 5 ? 5 filters in depthwise convolutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has been widely applied to image processing and computer vision applications. In the field of embedded vision and robotics, it is important to implement convolutional neural networks (CNNs) with low computational costs <ref type="bibr" target="#b0">[1]</ref>. Many researchers propose efficient algorithms to accelerate the deep-learningbased algorithms while keeping the recognition accuracy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Chollet proposes a network architecture called Xception, which includes depthwise convolution layers and point-wise convolution layers, to improve the performance <ref type="bibr" target="#b1">[2]</ref> of image classification. Howard et al. propose a network architecture called MobileNet, which also includes depthwise convolution layers and pointwise convolution layers <ref type="bibr" target="#b4">[5]</ref>, to reduce the computational costs for embedded computing. Unlike the regular convolution layers in CNNs, the numbers of input feature maps and output feature maps in the depthwise convolution layers are the same, and the computational cost of convolution operations is proportional to the number of input feature maps or output feature maps. <ref type="bibr">Sandler et al.</ref> propose MobileNetV2, in which the depthwise convolution layers are still one of the basic structures <ref type="bibr" target="#b9">[10]</ref>. In addition to these works, there are various kinds of architectures utilizing the concept of depthwise convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Since depthwise convolutions have become common building blocks for compact networks for embedded vision, the hardware engineers also propose different kinds of accelerators and inference engines to implement them efficiently. <ref type="bibr">Liu et al.</ref> propose an FPGA-based CNN accelerator to handle depthwise convolutions and regular convolutions with the same computation cores <ref type="bibr" target="#b5">[6]</ref>. The same architecture is used for the depthwise convolutions and the regular convolutions, but it is difficult to increase the hardware utilization of the depthwise convolutions because the inputs of some processing elements are always set to zeros. <ref type="bibr">Su et al.</ref> propose an acceleration scheme for MobileNet, utilizing modules for depthwise convolutions and regular convolutions <ref type="bibr" target="#b10">[11]</ref>. Similar to Liu's work <ref type="bibr" target="#b5">[6]</ref>, since two separated modules are used for depthwise convolutions and regular convolutions, the hardware resource cannot be fully utilized. Yu et al. propose a hardware system, Light-OPU, which accelerates regular convolutions, depthwise convolutions, and other lightweight operations with one single uniform computation engine <ref type="bibr" target="#b17">[18]</ref>. It efficiently utilizes the hardware by exploring intra-kernel parallelism. However, since the architecture is optimized for 3 ? 3 filter kernels, the computational time becomes 4 times when the size of filter kernels is increased from 3 ? 3 to 5 ? 5. The hardware cost increases because extra line buffers are required to store the data. In modern CNN architectures, 3 ? 3 filter kernels are commonly used, but sometimes it is necessary to increase the receptive field by using large-kernel filters <ref type="bibr" target="#b6">[7]</ref> or dilated convolutions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> for certain kinds of applications, such as face detection, image classification, and image segmentation. Therefore, a hardware system that can efficiently handle regular convolutions, depthwise convolutions, and large-kernel filters is desired.</p><p>The contribution of this paper is twofold. First, a new hardware architecture for embedded inference accelerators is proposed to handle both depthwise convolutions and regular convolutions. The proposed architecture can efficiently handle convolutions with kernels larger than 3 ? 3, and it can achieve shorter processing time than the related works. The experimental results show that the size of the proposed hardware is 1.97M gates, while the number of parallel processing units for MAC (Multiply-Accumulate) operations is 512. Second, the features of the supported network architectures are analyzed. By replacing regular convolutions with large-kernel depthwise convolutions, we can reduce the computational costs while keeping the accuracy.</p><p>The paper is organized as follows. First, in Sec. 2, the proposed hardware architecture and supported network architectures are introduced. Then, the ex-perimental results are discussed in Sec. 3. Finally, the conclusions are given in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Hardware Architecture and Supported Network Architectures</head><p>The proposed hardware architecture can efficiently handle regular convolutions and depthwise convolutions with large-kernel filters. The hardware architecture and the supported network architecture are introduced in this section. The supported network architecture, the solution based on the proposed hardware, the time chart, and the algorithm are introduced in the following subsections.  The upper part of <ref type="figure" target="#fig_0">Figure 1</ref> shows the two modes for regular convolutions and depthwise convolutions supported by the proposed hardware architecture. The main feature of the architectures is the function of hardware mode switching. On the left side, all of the input feature maps for parallel processing are stored in the memory to generate the same numbers of output feature maps. On the right side, only 1 input feature map for parallel processing is stored in the memory to generate multiple output feature maps. When the regular convolution mode is enabled, the intermediate convolution results of multiple input feature maps are accumulated to generate the final convolution results for a single output feature map. When the depthwise convolution mode is enabled, the intermediate convolution results of multiple input feature maps become the final convolution results of multiple output feature maps. In both of the two modes, the hardware architecture can generate the same numbers of convolution results, and the multipliers in the convolution cores are fully utilized. The overhead of the hardware architecture is the buffer to store the data of multiple input feature maps in the depthwise convolution mode. However, large-kernel convolutions can be efficiently handled since the proposed hardware architecture does not require extra costs for intra-kernel parallelism <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Architecture with Two Types of Convolution Layers</head><p>The lower part of <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of the network arhictecture that is focused in this paper. There are 3 layers in this example, the depthwise convolution layer, the activation layer, and the pointwise convolution layer. The depthwise convolution layer may include the filters with kernels larger than or equal to 3?3. Different from the related works, such as MobileNet <ref type="bibr" target="#b4">[5]</ref>, the depthwise convolution layers also include dilated convolution kernels, which increase the size of the receptive field and increase the accuracy of the inference result for some applications. The combination of dilated convolutions and depthwise convolutions is abbreviated as DDC. The activation layers include functions such as Rectified Linear Unit (ReLU) and quantization functions. The pointwise convolution layer may include the filters with kernel sizes equal to 1 ? 1 but not limited to 1 ? 1. When the kernels size is not 1 ? 1, the pointwise convolutions can also be regarded as regular convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Solution to Depthwise Convolutions and Regular Convolutions</head><p>The proposed hardware architecture is shown in <ref type="figure">Figure 2</ref>, where the black blocks represent the memory sets, and the white blocks represent the logics. The memory sets include the feature map memory and the filter weight memory. The feature map memory and the weight memory are used to store the input/output feature maps and the filter weights, respectively. The logics include the address generator, the convolution layer processing unit (CLPU), and the activation and pooling layer processing unit (APLPU). The address generator receives the information of network architectures and generates the addresses for the feature map memory and the weight memory. The blocks in the spatial domain of the feature maps are processed sequentially.</p><p>The CLPU includes P E num convolution cores to compute the results of convolutions of the input feature maps and the input filter weights in parallel. Each convolution core contains M AC P E sets of multipliers and adders to execute MAC operations. Note that IC represents the number of input feature maps, and P E num denotes the number of convolution cores. The same operations can be applied to regular convolutions and depth-wise convolutions with difference kernel sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Time Chart</head><p>The upper part of <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of regular convolutions. There are IC input feature maps and OC output feature maps in this layer. From cycle 0 to cycle T , the 1st input feature map and the corresponding filter weights are transferred. From cycle T to cycle 2T , the 2nd input feature map and the corresponding filter weights are transferred, and the results of multiplications in the convolution operations based on the 1st input feature maps are computed. From cycle 2T to cycle 3T , the 3rd input feature map and the corresponding filter weights are transferred. The results of multiplications in the convolution operations based on the 2nd input feature maps are computed, and the results of accumulations in the convolution operations based on the 1st input feature maps are computed. From cycle 3T to cycle (IC + 2)T , the results of convolution operations based on the remaining input feature maps are computed. From cycle (IC + 2)T to cycle (IC + 3)T , the results of ReLU based on the convolutions of the 1st to the IC-th input feature maps are generated.</p><p>The lower part of <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of depthwise convolutions. There are IC input feature maps and OC output feature maps in this layer, and the values of OC and IC are the same. From cycle 0 to cycle T , the 1st to the IC/2-th input feature maps and the corresponding filter weights are transferred. From cycle T to cycle 2T , the (IC/2 + 1)-th to the IC-th input feature maps and the corresponding filter weights are transferred, and the results of multiplications in the convolution operations based on the 1st to the IC/2-th input feature maps are computed. From cycle 2T to cycle 3T , the results of multiplications in the convolution operations based on the (IC/2 + 1)-th to the IC-th input feature maps are computed, and the results of accumulations in the convolution operations based on the 1st to the IC/2-th input feature maps are computed. From cycle 3T to cycle 4T , the results of accumulations in the convolution operations based on the (IC/2 + 1)-th to the IC-th input feature maps are computed, and the results of ReLU based on the convolutions of the 1st to the IC/2-th input feature maps are generated. From cycle 4T to cycle 5T , the results of ReLU based on the convolutions of the (IC/2 + 1)-th to the IC-th input feature maps are generated.   <ref type="figure" target="#fig_5">Figure 4</ref> shows the proposed algorithms. There are 4 levels in the nested loop structure. In the 1st level of loops, each convolution layer of the networks is processed sequentially, and the parameters of the corresponding layer are set. The mode for depthwise convolutions and regular convolutions is selected in this level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Algorithm</head><p>When the mode for regular convolutions is selected, the operations in the 2nd level to the 4th level of loops are executed. First, in the 2nd level of loops, each output channel of the layer is processed sequentially. Then, in the 3rd level of loops, each block of the output channel is processed sequentially. Finally, in the 4th level of loops, each input channel is processed sequentially. The filter weights of the i-th convolution layer, the j-th output channel, the n-th input channel are set, and the convolution results of P E num output blocks are computed in parallel.</p><p>When the mode for depthwise convolutions is selected, the operations in the 2nd level to the 3rd level of loops are executed. The number of input channels is equal to the number of output channels, and each channel is processed sequentially. The filter weights of the i-th convolution layer, the j-th output channel, the j-th input channel are set, and the convolution results of P E num output blocks are computed in parallel.  Since the number of loops is reduced when the mode for depthwise convolutions is selected, redundant operations, in which some of filter weights are set to zeros, are not required. The common architecture for the two modes makes the hardware achieve higher utilization than the related works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results and Analysis</head><p>The section contains 3 parts. The first part is the comparison of accuracy of face detection and image classification. The second part is the analysis of the computational cost of the proposed hardware architecture. The third part is the comparison of specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison of Accuracy</head><p>One of the strengths of the proposed hardware architecture is the function to handle the combination of large-kernel convolutions and depthwise convolutions. To analyze the effectiveness of large-kernel convolutions and depthwise convolutions, the experiments for two kinds of applications are performed. The first application is face detection, and the second application is image classification. Two experiments are performed for both of the applications.</p><p>In the first experiment, the accuracy of face detection on the WIDER FACE dataset <ref type="bibr" target="#b16">[17]</ref> is analyzed. The network of RetinaFace <ref type="bibr" target="#b2">[3]</ref> is tested with depthwise convolutions and large-kernel convolutions, and the backbone network is MobileNetV1-0.25 <ref type="bibr" target="#b4">[5]</ref>.</p><p>The WIDER FACE dataset <ref type="bibr" target="#b16">[17]</ref> consists of 32,203 images and 393,703 face bounding boxes with a high degree of variability in scale, pose, expression, occlusion and illumination. In order to compare the proposed network architecture with RetinaFace, the WIDER FACE dataset is split into training (40%), validation (10%) and testing (50%) subsets, and three levels of difficulty (i.e. Easy, Medium, and Hard) are defined by incrementally incorporating hard samples. We use the WIDER FACE training subset as the training dataset, and the WIDER FACE validation subset as the testing dataset. The operations in the context module, which is an important structure used to increase the receptive field, include a series of 3 ? 3 convolutions. Since the feature maps are processed with different numbers of 3 ? 3 filters consecutively, the effects are similar to applying large-kernel convolutions (e.g. 5 ? 5, 7 ? 7) to the feature maps. To show the advantage of the functions of the proposed hardware architecture, some of the operations are replaced with depthwise convolutions and dilated filters. To be specific, two cascaded 3 ? 3 convolutions are replaced with a single 3 ? 3 convolution where the dilation rate is 1, and three cascaded 3 ? 3 convolutions are replaced with a single 3 ? 3 convolution where the dilation rate is 2. The accuracy of RetinaFace <ref type="bibr" target="#b2">[3]</ref> and its variations are shown in <ref type="table" target="#tab_1">Table 1</ref>, which includes the proposed network architecture, the depthwise and dilated convolution (DDC) layers. Since the accuracy of the network without the context module is not available in the original paper <ref type="bibr" target="#b2">[3]</ref>, we add an ablation study to verify the effectiveness of the context module. The results show that the context module can increase the accuracy of face detection by about 1% for the Easy category and the Medium category, and by 4% for the Hard category. The proposed hardware can support the quantized RetinaFace and its variations. In the quantized networks, the feature maps and the filter weights are both quantized into 8-bit data. It is shown that the quantized versions have similar accuracy with the floating-point versions.</p><p>When the filters in the context module are replaced with the DDC layers and pointwise convolutions, the accuracy decreases by less than 1% for all the categories. The parameter settings are shown in <ref type="table" target="#tab_2">Table 2</ref>. We trained the RetinaFace using the SGD optimizer (momentum at 0.9, weight decay at 0.0005, and batch size of 32) on the NVIDIA Titan Xp GPUs with 12GB memory. The learning rate starts from 10 ?3 , and is divided by 10 at the 190th and at the 220th epoch. The training process terminates at the 250th epoch. The computational costs and the model sizes of RetinaFace <ref type="bibr" target="#b2">[3]</ref> and the proposed work are shown in <ref type="table">Table 3</ref>. When the operations in the context module are replaced with depthwise convolutions and dilated filters, the model size of the context module decreases from 138 KB to 23 KB, and the computational cost of the context module decreases from 708 MACs per input pixel to 119 MACs per input pixel. By applying depthwise convolutions and dilated filters to the network, the total computational costs decrease by about 30%, and the total model size decreases by about 20%.</p><p>In the second experiment, to show the relation between the size of filter kernels and the inference result, the accuracy of image classification on Ima-geNet <ref type="bibr" target="#b8">[9]</ref> is analyzed. Similar to the first experiment, MobileNetV1-0.25 is used for testing. Since the architecture of MobileNetV1 <ref type="bibr" target="#b4">[5]</ref> is composed of depthwise convolution layers, it can be used to evaluate the effectiveness of large-kernel convolutions by simply increasing the kernel sizes of the filters. The accuracy of MobileNetV1 and its variations is shown in with dilated convolutions, where the concept of hybrid dilated convolutions is adopted <ref type="bibr" target="#b13">[14]</ref>. When some 3 ? 3 filters in MobileNetV1 are replaced by 5 ? 5 filters, the accuracy is increased by more than 1%. When some of 3 ? 3 filters are replaced by dilated 3 ? 3 filters, the network can also achieve higher accuracy than the original architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Top-1 Accuracy Top-5 Accuracy MobileNetV1-0.25 <ref type="bibr" target="#b4">[5]</ref> 68.39% 88.35% MobileNetV1-0.25 with 5 ? 5 filters 69.44% 89.90% MobileNetV1-0.25 with dilated filters (A) <ref type="bibr" target="#b0">1</ref> 68.52% 88.61% MobileNetV1-0.25 with dilated filters (B) <ref type="bibr" target="#b1">2</ref> 68.98% 88.85% <ref type="table">Table 4</ref>. Accuracy of MobileNetV1-0.25 and the proposed network on ImageNet <ref type="bibr" target="#b8">[9]</ref>.</p><p>The first experiment shows that the combination of depthwise convolutions and dilated convolutions keeps the accuracy while reducing the computational costs, and the second experiment shows that the combination of depthwise convolutions and large-kernel convolutions can increase the accuracy.</p><p>In brief, these results show the importance of supporting depthwise convolutions and large-kernel convolutions, including dilated convolutions, with the proposed hardware architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computational Time</head><p>To compare the proposed hardware architecture with the related work, the processing time for regular convolutions and depthwise convolutions is expressed as equations. The processing time (number of cycles) for the regular convolutions is shown as follows.</p><formula xml:id="formula_0">T M = max IC ? IN ? IM BW F M ? OC P E num , IC ? OC ? X ? Y BW W ,<label>(1)</label></formula><formula xml:id="formula_1">T C = X ? Y ? IC ? ON ? OM M AC P E ? OC P E num ,<label>(2)</label></formula><p>where T M and T C are the memory access time and the computational time, respectively. The processing time can be expressed as max(T M , T C ), and the definition of parameters in the equations are shown as follows. </p><formula xml:id="formula_2">T M = max OC ? IN ? IM BW F M , OC ? X ? Y BW W ,<label>(3)</label></formula><formula xml:id="formula_3">T C = X ? Y ? ON ? OM M AC P E ? OC P E num<label>(4)</label></formula><p>All of the processing elements are used for both regular convolutions and depthwise convolutions.</p><p>In the related work <ref type="bibr" target="#b17">[18]</ref>, the computational costs (the number of cycles) for the convolution operations are also expressed as equations. To compare the efficiency, we set P E num = 64, and M AC P E = 8 for the related work. For simplicity, IC and OC are set to the same value for both regular convolutions and depthwise convolutions. The results are shown in <ref type="figure" target="#fig_7">Fig. 5</ref>, where (a) and (b) refer to the cases when the size of filter kernels is 3 ? 3, and (c) and (d) refer to the cases when the size of filter kernels is 5 ? 5. For regular convolutions, the computational time of the proposed work and the related work is almost the same when the size of filter kernels is 3 ? 3 or 5 ? 5. For depthwise convolutions, the proposed work has shorter computational time than the related work when the size of filter kernels is 3 ? 3 or 5 ? 5. In the proposed architecture, since the filter weights and the feature maps can be transferred in parallel, the memory access time can be shortened so that the operations of dilated convolutions achieve faster speed than the related work. Also, since the proposed architecture can process the filter weights sequentially, the computational time is proportional to the size of filter kernels, and the computational efficiency does not decrease when the size of filter kernels becomes large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Specifications</head><p>The specifications of the proposed embedded inference accelerator, which is designed to process modern CNNs, are shown in <ref type="table">Table 5</ref>. The main feature of the proposed architecture is that it can compute depthwise convolutions with large kernels, including the DDC layers mentioned in the previous section. We use the 28-nm CMOS technology library for the experiments. The resolution of input images is VGA, and the performance is 512 MAC operations per clock cycle for 8-bit feature maps and 8-bit filter weights. As shown in <ref type="figure">Figure 2</ref>, since regular convolutions and depthwise convolutions can be selected according to the type of layers, the hardware architecture can also process Reti-naFace (with MobileNetV1-0.25) efficiently. The gate counts of the CLPU and the APLPU are 1.50M and 0.47M, respectively. The processing speed for Reti-naFace, which includes the post-processing algorithm, is 150fps. Compared with RetinaFace, it takes only 83% of processing time to compute the inference result of the proposed network architecture, DDC layers. Since the proposed hardware can support depthwise convolutions with large filter kernels, the target networks can be switched according to the requirement of processing time. The bit widths of filter weights and the feature maps are quantized to 8 bits. The post-processing time with an external CPU is included. 4 N ch,i represents the number of output channels in the i-th convolution layer in the network. <ref type="table">Table 5</ref>. Specifications of the proposed embedded inference accelerator. <ref type="table" target="#tab_6">Table 6</ref> shows the comparison of hardware specifications with three related works. Since it is difficult to compare the performance of FPGA and ASIC, we focus on the hardware utilization of MAC units for regular convolutions and depthwise convolutions. In the first related work <ref type="bibr" target="#b5">[6]</ref>, when dealing with depthwise convolutions, the filter kernels are filled with zeros, and the advantage is that both regular convolutions and depthwise convolutions can be computed by using the same hardware architecture. Suppose that the hardware can handle regular convolutions with T n input channels and T m output channels in parallel, the hardware utilization becomes 100</p><p>Tm % when dealing with depthwise convolutions with T n input channels and T m output channels. In the second related work <ref type="bibr" target="#b10">[11]</ref>, regular convolutions and depthwise convolutions are handled with different hardware architectures. The advantage is that both regular convolutions and depthwise convolutions can be handled efficiently, but some of the processing elements do not function when either depthwise convolutions or regular convolutions are handled. In the third related work <ref type="bibr" target="#b17">[18]</ref>, the processing elements can effectively handle regular convolutions and depthwise convolutions, but as discussed in Sec. 3.2, the performance decreases when the kernel size of filters is larger than or equal to 5 ? 5.</p><p>The proposed architecture can handle regular convolutions and depthwise convolutions efficiently with large-kernel filters. The overhead to support the function is the buffer for input feature maps, which is mentioned in Sec. 2.1.</p><p>Liu et al. <ref type="bibr">[</ref> 1 Utilization represents the hardware utilization of MAC processing elements for regular convolutions and depthwise convolutions in an ideal case. <ref type="bibr" target="#b1">2</ref> We suppose that Tm = Tn = 8 <ref type="bibr" target="#b5">[6]</ref>, and the utilization becomes 100% ? 1 Tm = 13%. <ref type="bibr" target="#b2">3</ref> We suppose that M ? = N ? <ref type="bibr" target="#b10">[11]</ref>. <ref type="bibr" target="#b3">4</ref> Since ? = 4 <ref type="bibr" target="#b17">[18]</ref>, the utilization for 5 ? 5 filter kernels is 100% ? 5 2 3 2 ?? = 69%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this paper, a new solution which combines the advantages of algorithms and hardware architectures, is proposed to handle modern CNNs for embedded computer vision. The contribution of this paper is twofold. First, a new hardware architecture is proposed to handle both depthwise convolutions and regular convolutions. The proposed architecture can support filters with kernels larger than 3 ? 3 with high efficiency, and the processing time is shorter than the related works. The experimental results show that the gate count of the proposed hardware is 1.97M gates, and the number of parallel processing units for MAC operations is 512. Second, the features of the supported network architecture, DDC, are analyzed with two applications. By replacing regular convolutions with largekernel depthwise convolutions, it is possible to reduce the computational costs while keeping the accuracy.</p><p>For future work, we plan to extend the functions of the proposed hardware architecture to handle other kinds of complicated network architectures. In addition, we will apply the combination of the dilated convolutions and the depthwise convolutions to other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Functions of the proposed hardware and an example of the supported network architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 Fig. 2 .</head><label>32</label><figDesc>shows a time chart of the hardware architecture, where the operations are executed in pipeline. In this example, we set IC = 2P E num for simplicity. Proposed hardware architecture for regular convolutions and depthwise convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Filter</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Timechart for depthwise convolutions and regular convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>extract the m-th block of the n-th channel set the filter weights for ( i, n, j ) calculate the convolution result of the output block partially calculate the activation result of the output block otherwise th block of the n-th channel set the filter weights for ( i, n, n ) calculate the convolution result of the output block partially calculate the activation result of the output block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Algorithm of the proposed hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>IC: the number of input channels. OC: the number of output channels. IN ? IM : the size of input blocks. ON ? OM : the size of output blocks. X ? Y : the size of filters. BW F M : the bandwidth to transfer feature maps. BW W : the bandwidth to transfer filter weights. P E num = 8: the number of processing elements. M AC P E = 64: the number of parallel MAC operations implemented in 1 processing element. The memory access time and the computational time for the depthwise convolutions are shown as follows. Similarly, the processing time can be expressed as max(T M , T C ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of the computational time and the number of input channels with the related work [18] when the size of filter kernels is (a)(b) 3 ? 3 and (c)(d) 5 ? 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Feature Maps Feature Maps Feature Maps Feature Maps Activation Layer Pointwise Convolution Layer Depthwise Convolution Layer Input Output Input Output Hardware Mode Switching ReLU, etc. Dilated Filter, etc. 1x1 Filter, etc.</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>with CM 1 and DDC 2 90.32% 87.68% 73.53% RetinaFace B [3] (with CM 1 ) 89.98% 87.11% 72.01% RetinaFace B w/o CM 1 88.68% 84.98% 68.56% RetinaFace B with CM 1 and DDC 2 89.60% 86.13% 71.93% Quantized RetinaFace B [3] (with CM 1 ) 89.70% 86.91% 71.89% Quantized RetinaFace B with CM 1 and DDC 2 89.56% 86.02% 71.75% 1 CM stands for "context module." 2 DDC stands for "depthwise and dilated convolutions." A The networks are trained from a pre-trained model. The networks are trained from scratch. Accuracy of RetinaFace [3] and the proposed network on the WIDER FACE [17] validation subset.</figDesc><table><row><cell>Network</cell><cell>Easy Medium Hard</cell></row><row><cell>RetinaFace A [3] (with CM 1 )</cell><cell>90.70% 87.88% 73.50%</cell></row><row><cell>RetinaFace A w/o CM 1</cell><cell>89.55% 86.21% 68.83%</cell></row><row><cell>RetinaFace A with CM 1 and DDC 2</cell><cell>90.28% 87.13% 73.24%</cell></row><row><cell>Quantized RetinaFace A [3] (with CM 1 )</cell><cell>90.72% 87.45% 73.56%</cell></row><row><cell>Quantized RetinaFace A</cell><cell></cell></row></table><note>B</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Parameter settings of the experiments for RetinaFace<ref type="bibr" target="#b2">[3]</ref>.</figDesc><table><row><cell>Backbone</cell><cell>MobileNetV1-0.25</cell></row><row><cell>Prediction Levels</cell><cell>8?, 16?, 32? down-sampling</cell></row><row><cell>Anchor Settings</cell><cell>16 ? 16, 32 ? 32, 8? prediction layers</cell></row><row><cell></cell><cell>64 ? 64, 128 ? 128, 16? prediction layers</cell></row><row><cell></cell><cell>256 ? 256, 512 ? 512, 32? prediction layers</cell></row><row><cell>Batch Size</cell><cell>32</cell></row><row><cell>No. of Epochs</cell><cell>250</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 3 .</head><label>43</label><figDesc>There are two networks Computational costs and the model sizes of RetinaFace<ref type="bibr" target="#b2">[3]</ref> and the proposed network.</figDesc><table><row><cell>Network</cell><cell cols="2">Model Size Computational Cost</cell></row><row><cell></cell><cell cols="2">(Bytes) (MACs/Input pixel)</cell></row><row><cell></cell><cell>CM 1 Total CM 1</cell><cell>Total</cell></row><row><cell>RetinaFace 1 [3]</cell><cell>138K 1.12M 708</cell><cell>1,888</cell></row><row><cell cols="2">RetinaFace with CM and DDC 2 23K 0.90M 119</cell><cell>1,298</cell></row><row><cell>1 CM stands for "context module."</cell><cell></cell><cell></cell></row><row><cell cols="2">2 DDC stands for "depthwise and dilated convolutions."</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison with the related works.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Dilated convolutions are applied to Layer 14 to Layer 18. The kernel size of filters is 3 ? 3, and the dilation rates are set to 2, 2, 2, 2, 2 for the 5 layers. 2 Dilated convolutions are applied to Layer 14 to Layer 18. The kernel size of filters is 3 ? 3, and the dilation rate are set to 2, 3, 2, 3, 2 for the 5 layers.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Condensation-Net: Memory-efficient network architecture with cross-channel pooling layers and virtual feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Osa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno>coRR, abs/1610.02357</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno>coRR, abs/1905.00641</idno>
		<title level="m">RetinaFace: Singlestage dense face localisation in the wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno>coRR, abs/1911.11907</idno>
		<title level="m">GhostNet: More features from cheap operations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>coRR, abs/1704.04861</idno>
		<title level="m">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An FPGA-based CNN accelerator integrating depthwise separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">281</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large kernel matters -improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>coRR, abs/1703.02719</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno>coRR, abs/2004.03333</idno>
		<title level="m">Binary neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<idno>coRR, abs/1801.04381</idno>
		<title level="m">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Faraone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H W</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y K</forename><surname>Cheung</surname></persName>
		</author>
		<title level="m">Redundancy-reduced MobileNet acceleration on reconfigurable logic for ImageNet classification</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
	<note>Proceedings of International Symposium on Applied Reconfigurable Computing</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>coRR, abs/1905.11946</idno>
		<title level="m">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>coRR, abs/1907.09595</idno>
		<title level="m">MixConv: Mixed depthwise convolutional kernels</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<idno>coRR, abs/1702.08502</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">FastFCN: Rethinking dilated convolution in the backbone for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno>coRR, abs/1903.11816</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>coRR, abs/1511.06523</idno>
		<title level="m">WIDER FACE: A face detection benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Light-OPU: An FPGA-based overlay processor for lightweight convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2020-02" />
			<biblScope unit="page">122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
