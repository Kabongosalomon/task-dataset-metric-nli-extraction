<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mohamed Bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mohamed Bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a weakly-supervised temporal action localization framework, called D2-Net, which strives to temporally localize actions using video-level supervision. Our main contribution is the introduction of a novel loss formulation, which jointly enhances the discriminability of latent embeddings and robustness of the output temporal class activations with respect to foreground-background noise caused by weak supervision. The proposed formulation comprises a discriminative and a denoising loss term for enhancing temporal action localization. The discriminative term incorporates a classification loss and utilizes a top-down attention mechanism to enhance the separability of latent foreground-background embeddings. The denoising loss term explicitly addresses the foregroundbackground noise in class activations by simultaneously maximizing intra-video and inter-video mutual information using a bottom-up attention mechanism. As a result, activations in the foreground regions are emphasized whereas those in the background regions are suppressed, thereby leading to more robust predictions. Comprehensive experiments are performed on multiple benchmarks, including THUMOS14 and ActivityNet1.2. Our D2-Net performs favorably in comparison to the existing methods on all datasets, achieving gains as high as 2.3% in terms of mAP at IoU=0.5 on THUMOS14. Source code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action localization is a challenging problem, which aims to jointly classify and localize the temporal boundaries of actions in videos. Most existing approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b34">35]</ref> are based on strong supervision, requiring manually annotated temporal boundaries of actions during training. In contrast to these strong framelevel supervision based methods, weakly-supervised action localization learns to localize actions in videos, leveraging only video-level supervision. Weakly-supervised action localization is therefore of greater importance since the manual annotation of temporal boundaries in videos is laborious, expensive and prone to large variations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Existing methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref> for weaklysupervised action localization typically use video-level annotations in the form of action classes and learn a sequence of class-specific scores, called temporal class activation maps (TCAMs). In general, a classification loss is used to obtain the discriminative foreground regions in TCAMs. Some approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> learn TCAMs using action labels and obtain temporal boundaries via a post-processing step, while others <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b15">16]</ref> use a TCAM-generating video classification branch along with an explicit localization branch to directly regress action boundaries. Nevertheless, the localization performance is heavily dependent on the quality of the TCAMs. The quality of TCAMs is likely to improve in fully-supervised settings where frame-level annotations are available. Such frame-level information (true foreground and background regions) are unavailable in the weakly-supervised paradigm. In such a paradigm, the predicted foreground regions often overlap with the groundtruth background regions, while predicted background regions are likely to overlap with the ground-truth foreground regions. This leads to noisy activations, i.e., false positives and false negatives, in the learned TCAMs. Most existing weakly-supervised action localization methods that learn TCAMs typically rely on separating foreground and background regions (foreground-background separation) and do not explicitly handle its noisy outputs.</p><p>In this work, we address the problem of foregroundbackground separation along with explicit tackling of noise in TCAMs for weakly-supervised action localization. We propose a unified loss formulation that is jointly optimized to classify and temporally localize action snippets (group of frames) in videos. Our loss formulation comprises a discriminative and a denoising loss term. The discriminative loss seeks to maximally separate backgrounds from actions  <ref type="figure">Figure 1</ref>. Impact of our proposed loss formulation on the quality of the output TCAMs. Compared to the baseline (without our discriminative and denoising loss terms), the introduction of the discriminative loss term improves the separation between foreground and background activations (e.g., third and fourth ground-truth action instance from the left). Furthermore, our final D2-Net comprising both the discriminative and the denoising loss terms reduces the noise in the TCAMs, leading to more robust TCAMs.</p><p>(foregrounds) via interlinked classification and localization learning objectives (Sec. 3.1). The denoising loss (Sec. 3.2) complements the discriminative term by explicitly addressing the foreground-background noise in activations, thereby producing robust TCAMs (see <ref type="figure">Fig. 1</ref>).</p><p>In our loss formulation, we learn distinct latent embeddings such that their foreground-background separation is maximized based upon the corresponding top-down attention generated from the output TCAMs. Furthermore, the embeddings are employed to generate pseudo-labels based on their foreground scores (bottom-up attention). These pseudo-labels are utilized to explicitly handle the noise by emphasizing the corresponding output activations in pseudoforeground regions, while suppressing the activations in pseudo-background regions. This pseudo-background suppression and pseudo-foreground enhancement is achieved by maximizing the mutual information (MI) between activations and generated pseudo-labels within an action video (intra-video). Maximizing MI between predicted activations and labels decreases the uncertainty of predictions, leading to more robust predictions. In addition to capturing intra-video MI, our formulation also strives to maximize MI between the action class predictions and video-level ground-truth labels, across videos in a mini-batch (inter-video). Contributions: We introduce a weakly-supervised action localization framework, D2-Net, which incorporates a novel loss formulation that jointly enhances the foregroundbackground separability and explicitly tackles the noise to robustify the output TCAMs. Our main contributions are:</p><p>? We introduce a discriminative loss term, which simultaneously aims at video categorization and enhanced foreground-background separation. ? We introduce a denoising loss term to improve the robustness of TCAMs. Our denoising loss explicitly addresses noise in TCAMs by maximizing the MI between activations and labels within a video (intra-video) and across videos (inter-video). To the best of our knowledge, we are the first to introduce a loss term that simultaneously captures MI across multiple snippets within a video and across all videos in a batch for weakly-supervised action localization. ? Experiments are performed on multiple benchmarks, including THUMOS14 <ref type="bibr" target="#b6">[7]</ref> and ActivityNet1.2 <ref type="bibr" target="#b2">[3]</ref>. Our D2-Net performs favorably against existing weaklysupervised methods on all datasets, achieving gains as high as 2.3% mAP at IoU=0.5 on THUMOS14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several weak supervision strategies have been explored in the context of action localization, including category labels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref>, sparse temporal points <ref type="bibr" target="#b19">[20]</ref>, order of actions <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>, instance count <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44]</ref> and singleframe annotations <ref type="bibr" target="#b17">[18]</ref>. Most existing weakly-supervised action localization methods employ category labels as weak supervision and typically utilize features extracted from backbone networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b3">4]</ref> trained on the action recognition task. The work of <ref type="bibr" target="#b38">[39]</ref> proposes a selection module for detecting the relevant temporal segments and employs a classification loss for training. The Autoloc method <ref type="bibr" target="#b33">[34]</ref> extends <ref type="bibr" target="#b38">[39]</ref> by adding an explicit localization branch and utilizes an outerinner contrastive loss for its training. In contrast, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9]</ref> match similar segments of actions in paired videos by employing classification and similarity-based losses that require multiple videos of same actions in a mini-batch. Different from these works, our approach explicitly addresses the issue of large number of easy negatives overwhelming a smaller number of hard positives via sample re-weighting and performs foreground-background separation by inter-linking classification and localization objectives. Snippet-level loss: While the work of <ref type="bibr" target="#b24">[25]</ref> employs a background-aware loss along with a self-guided loss for modeling the background, <ref type="bibr" target="#b21">[22]</ref> additionally utilizes an iterative multi-pass erasing step for discovering different action segments in TCAMs. Differently, the training in <ref type="bibr" target="#b16">[17]</ref> alternates between updating a key-instance assignment branch and a classification branch via Expectation Maximization. In contrast, the recent work of <ref type="bibr" target="#b12">[13]</ref> classifies the foreground/background snippets as in/out-of-distribution based on the feature magnitude and entropy over foreground classes. However, all these approaches aggregate per-snippet losses for training and do not explicitly capture the mutual information (MI) between the activations and labels, which is likely to be more beneficial due to the absence of snippetlevel labels in a weakly-supervised setting. Different from existing methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>, our approach addresses the problem of foreground-background noise by exploiting both inter-and intra-video MI between class acti-  vations and corresponding labels, resulting in robust TCAMs. To the best of our knowledge, we are the first to propose a weakly-supervised action localization approach that simultaneously captures MI across multiple snippets within a video and across videos in a mini-batch (see also <ref type="figure">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Our D2-Net strives to improve the separation of foreground-background feature representations in videos, while jointly enhancing the robustness of output TCAMs w.r.t. foreground-background noise. This leads to better differentiation between foreground actions and surrounding background regions, resulting in enhanced action localization in the challenging weakly-supervised setting. Here, we first present our overall architecture, followed by a detailed description of our proposed losses for training D2-Net. Overall architecture of D2-Net is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Given a video v, we divide it into non-overlapping snippets of L = 16 frames each. Features are then extracted to encode appearance (RGB) and motion (optical flow) information. Similar to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b22">23]</ref>, we use the Inflated 3D (I3D) <ref type="bibr" target="#b3">[4]</ref> to obtain d = 2048 dimensional features for each 16-frame snippet. Let F ? R s?d denote features for a video, where s is the number of snippets. The extracted features become the inputs to our D2-Net, which comprises two parallel streams for RGB and optical flow. Each stream consists of three temporal convolutional (TC) layers. The first two layers learn latent discriminative embeddings x(t) ? R d/2 (with time t ? [1, s]), from the input features F. The output of the final TC layer is passed through a sigmoid activation. Subsequently, the outputs from both streams are averaged to obtain TCAMs T ? R s?C representing a sequence of class-specific scores over time for C action classes. The main contribution of our work is the introduction of a novel loss formulation to train the proposed D2-Net. Our training objective combines a discriminative (L Dis ) and a denoising term (L D ), with a balancing weight ?,</p><formula xml:id="formula_0">L = L Dis + ?L D .<label>(1)</label></formula><p>These two loss terms utilize foreground-background attention sequences computed in opposite directions: (i) the discriminative loss L Dis utilizes a top-down attention, which is computed from the output TCAMs (the top-most layer) and (ii) the denoising loss L D utilizes a bottom-up attention, which is derived from the foreground scores of the latent embeddings (intermediate layer features). We describe these losses in detail in Sec. 3.1 and 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.Foreground-Background Discriminability:L Dis</head><p>In this work, we introduce a discriminative loss (L Dis ) to learn separable class-agnostic foreground and action-free background feature representations, in terms of latent embeddings, using a top-down attention from the TCAMs. The embedding of a video with s snippets is defined by a weighted temporal pooling based on the class activations T ? R s?C . Let the top-down foreground attention ?(t) = max c T[t, c] denote the maximum foreground activation across all action classes c ? {1, . . . , C}, where t ? [1, s] and C is the number of classes. Then, the class-agnostic foreground and background embeddings are:</p><formula xml:id="formula_1">x f g = ?(t)&gt;? ?(t)x(t), x bg = ? b (t)&gt;? ? b (t)x(t), (2) where ? =0.5 and ? b (t)=1??(t)</formula><p>is the background attention. Maximizing the distance between foreground and background embeddings enhances the separability of the corresponding output activations, leading to improved localization. In addition, different sets of action classes are likely to share certain characteristics among them e.g., Hammer Throw and Discus Throw have similar spatial context and motion. Hence, clustering foreground embeddings amongst themselves at a coarse level is likely to aid "coarse-to-fine" snippet-level classification. Similarly, clustering background embeddings helps in learning an approximate universal background embedding, which is likely to aid in generalization at test time to new backgrounds. Hence, three weight terms, w f b , w f g and w bg , are introduced in our L Dis , targeting foreground-background separation, foreground grouping and background grouping, respectively. They are defined as:</p><formula xml:id="formula_2">w f b = max(0, cos(x f g ,x bg )), w f g = ?(1 ? cos(x f g ,x f g )), w bg = ?(1 ? cos(x bg ,x bg )),<label>(3)</label></formula><p>where x andx denote embeddings from different videos in a mini-batch. Here, ? denotes the intra-class compactness weight used for grouping same class (foreground vs. background) embeddings. Alongside robust localization, our other objective is the multi-label classification of action categories. A major challenge is introduced by the class-imbalance problem, where easy background snippets overwhelmingly outnumber the hard foregrounds. To address this, inspired by the focal loss for object detection <ref type="bibr" target="#b13">[14]</ref>, we propose to include penalty terms based on the weights (Eq. 3), in our L Dis . To this end, a video-level prediction p ? R C is obtained by performing a temporal top-k pooling on T. Our L Dis term, which jointly addresses the classimbalance and enhances foreground-background separation, is defined by</p><formula xml:id="formula_3">L Dis = ? c:y[c]=1 (1 ? p[c] + w f g + w f b ) ? log(p[c]) ? c:y[c]=0 (p[c] + w bg + w f b ) ? log(1 ? p[c]),<label>(4)</label></formula><p>where y ? {0, 1} C denotes the video-level label and ? is the focusing parameter. The first term in Eq. 4 denotes the loss for a positive action class, while the second term incorporates the loss for a negative class. The weight term</p><formula xml:id="formula_4">w f b (see Eq.</formula><p>3) is added for both positive action classes and background classes since it represents the foregroundbackground separation. The terms w f g and w bg enhance intra-class compactness for the positive and background classes, respectively. The first term in Eq. 4 indicates that the loss due to a positive action class c is low only when (i) its predicted probability p[c] is high, and (ii) the foreground grouping w f g and foreground-background separation w f b for the corresponding video are both simultaneously low. A similar observation holds in the second term for the negative class. Thus, L Dis enhances the discriminability of embeddings x(t) by encouraging foreground-background separation while simultaneously achieving classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Robust Temporal Class Activation Maps: L D</head><p>Our discriminative loss L Dis improves action localization by enhancing the distinctiveness of latent embeddings. However, the temporal locations of true foreground regions are unknown under weak supervision, resulting in noisy output temporal class activations (and noisy top-down attention) learned from video-level labels. Consequently, the foreground and background embeddings (x f g and x bg ), learned from the top-down attention ?(t), are likely to be noisy. Our goal is to explicitly reduce this foreground-background noise caused by the absence of snippet-level labels and improve the robustness of the output class activations. To this end, we introduce a denoising loss L D comprising a novel pseudo-Determinant based Mutual Information (pDMI) loss. Our L D exploits both intra-and inter-video mutual information (MI) between the class activations and corresponding labels.</p><p>Our pseudo-Determinant based Mutual Information (pDMI) loss is inspired by the Determinant based Mutual Information (DMI) <ref type="bibr" target="#b42">[43]</ref>. The original DMI, proposed for multi-class classification, is computed as the determinant of a joint distribution matrix, i.e., DMI(P, Y)=| det(U)|.</p><p>Here, U = 1 /nPY is the joint distribution over the predicted posterior probabilities P and the ground-truth (noisy) labels Y. The matrices P and Y are of sizes C ? n and n ? C, where n denotes the mini-batch size and C the number of classes. The DMI loss L dmi is defined as</p><formula xml:id="formula_5">L dmi = ?E[log(| det(U)|)],<label>(5)</label></formula><p>where E denotes Expectation. Note that L dmi depends on the determinant of U. To ensure a non-zero det(U), the label matrix Y must be full-rank, i.e., a mini-batch must contain instances from all classes. This is prohibitive for a large number of classes. Such a mini-batch sampling for action localization also leads to memory issues in GPUs due to the long duration of untrimmed videos in the dataset, especially when capturing inter-video MI. Our pDMI loss overcomes these limitations and ensures a non-degenerate value of DMI by avoiding an explicit computation of the determinant. To this end, we observe that for the DMI loss to tend to zero, the determinant of the joint distribution | det(U)| must tend to one. Formally,</p><formula xml:id="formula_6">L dmi ? ? 0 =? | det(U)| ? ? 1 =? U ? ? I. (6)</formula><p>As a result, DMI is maximum when | det(U)|=1, with the identity matrix I as an optima for U of size C ? C (since elements of U?[0, 1]). Furthermore, the condition number ? for the optimal solution I is minimum, i.e., ?=1. Hence, instead of maximizing | det(U)|, we can alternatively minimize its ?. In effect, U becomes better-conditioned and this improves the robustness of the activations towards label noise. The proposed pDMI loss L pdmi is then given by</p><formula xml:id="formula_7">L pdmi = E[log(pDMI(P, Y))] = E[log(? U )],<label>(7)</label></formula><p>where ? U denotes the condition number of U. Since the rank of U is r ? C, ? U is computed as ? 1 /? r , where {? 1 , . . . , ? r } are non-zero singular values of U. Thus, our pDMI loss avoids an explicit computation of the determinant and overcomes the limitations of the standard DMI. <ref type="figure" target="#fig_2">Fig. 3</ref> shows plots of ? U vs. |det(U)| for joint distribution matrices U that are randomly sampled (left) and encountered during intra-video MI training (right, described in Sec. 3.2.1). It can be observed that minimizing ? U indeed maximizes |det(U)|, i.e., DMI, in turn maximizing MI. Consequently, our pDMI serves as a promising alternative to the original DMI when optimizing with noisy temporal action labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Snippet-level and Video-level Noise Removal</head><p>To robustify the TCAMs, we employ our L pdmi at two levels: (i) snippet-level to exploit intra-video MI, and (ii) video-level to exploit inter-video MI. Snippet-level denoising incorporates a bottom-up attention to emphasize the foreground activations, while suppressing the background ones by capturing the MI between the temporal activations and corresponding foreground labels within a video. On the other hand, the video-level denoising step exploits MI between the video representations and corresponding labels, across videos, to achieve the same objective. <ref type="figure">Fig. 4</ref> shows a conceptual illustration of loss computation with and without capturing MI. Snippet-level joint distribution: It captures the MI between the foreground-background activations and the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-snippet / per-video loss Mutual Information (MI) based loss</head><p>At video-level, C = #Action classes.</p><p>n: #snippets in a video (snippet-level) OR #videos in a batch (video-level) p 1 p 2 p n <ref type="figure">Figure 4</ref>. A conceptual illustration of loss computation with (on the right) and without (on the left) capturing mutual information (MI). Typically, existing methods compute the loss without MI (e.g., cross-entropy loss) by aggregating individual losses (Li) between prediction pi and labels yi either at a per-video or per-snippet level. Instead, we compute a collective loss across (i) all snippets within a video (snippet-level) and (ii) all videos in a batch (video-level), by capturing the MI between predictions (P) and labels (Y).</p><p>snippet-level pseudo-labels within a video. For this, we utilize a bottom-up attention mechanism, which encodes the foreground scores ? (t) of latent embeddings x(t) for the corresponding snippets. The scores ? (t) are computed w.r.t. a reference background embedding x ref and are given by</p><formula xml:id="formula_8">? (t) = 0.5(1 ? cos(x(t), x ref )), t ? [1, s], (8) where x [m] ref = 0.9x [m?1] ref +0.1x ?,[m] bg</formula><p>is progressively computed as a running mean of x bg over m iterations. Here, x ?,[m] bg denotes the mean of the background embeddings in a mini-batch at iteration m. Let t f ={t:? (t)&gt;0.5} and t b ={t:? (t)&lt;0.5} denote the time instants for selecting the foreground and background activations w.r.t. ? (t). Using the pseudo-foreground temporal locations t f , a row matrix ? f of width n f =|t f | is constructed using top-down attention ?(t), t?t f . Similarly, ? b of width n b =|t b | is constructed for the pseudo-background snippets. Then, the prediction matrix P 1 and pseudo-label matrix Y 1 are given by</p><formula xml:id="formula_9">P 1 = ? f ? b 1 ? ? f 1 ? ? b , Y 1 = 1 /z 1 n f 0 n f 0 n b 1 n b ,<label>(9)</label></formula><p>where z=n f +n b , P 1 ?R 2?z , Y 1 ?R z?2 , 1 k and 0 k are k dimensional column vectors of ones and zeros. The snippetlevel joint distribution is then defined as</p><formula xml:id="formula_10">U 1 = P 1 Y 1 .</formula><p>Video-level joint distribution: Here, the noise stems from the video-level prediction p ? R C and is predominantly caused by the temporal top-k pooling. Under the weaklysupervised setting, all the top-k locations predicted for an action class need not necessarily belong to that class. Moreover, actions in untrimmed videos may not span k= s/8 snippets. Hence, denoising the video-level prediction p eventually robustifies the output class activations at the snippetlevel. Let the prediction P 2 and label Y 2 be P 2 = p 1 , . . . , p n and Y 2 = 1 /n y 1 , . . . , y n ,</p><p>where p i ? R C and y i ? {0, 1} C denote the video-level prediction and associated label of i-th video in a mini-batch. Then, the video-level joint distribution that captures the MI between class activations and action classes across videos is</p><formula xml:id="formula_12">U 2 =P 2 Y 2 .</formula><p>We finally define our denoising loss as</p><formula xml:id="formula_13">L D = L DS + L DV (11) = E[log(pDMI(P 1 , Y 1 ))] + E[log(pDMI(P 2 , Y 2 ))],</formula><p>where the pDMI loss is given by Eq. 7. Here, L DS and L DV denote the snippet-level and video-level losses. Thus, our denoising loss improves the TCAMs, at the snippet-level and video-level, by making them robust to the foregroundbackground noise under the weakly-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference: Action Localization from TCAMs</head><p>At inference, given a video, D2-Net outputs a bottom-up attention sequence ? (Eq. 8) of length s and a class activation map T of size s ? C. We perform top-k pooling to obtain the predicted class probabilities p ? R C , which are then used to find the relevant action classes above a threshold p th = 0.5 max(p). For every relevant class c, its corresponding class activations T c ? R s are multiplied element-wise with ? ? R s to obtain a refined sequence r c = ? T c . The snippets with activations above a threshold are retained and a 1-D connected component is used to obtain segment proposals. Multiple thresholds are used to obtain a larger pool of proposals. Each proposal is then scored using the contrast between the mean activation of the proposal itself and its surrounding areas <ref type="bibr" target="#b33">[34]</ref>, S = S i ? S o , where S i and S o respectively denote the mean activation of the proposal and its neighboring background. The neighboring background is obtained by inflating the proposal on either side by 25% of its width, as in <ref type="bibr" target="#b33">[34]</ref>. Proposals with high overlap are removed using class-wise NMS. Only high-scoring proposals (i.e., S &gt; S th ) are retained as final detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets: We evaluate D2-Net on multiple challenging temporal action localization benchmarks. The THU-MOS14 <ref type="bibr" target="#b6">[7]</ref> dataset contains temporal annotations for 200 validation and 212 test videos from 20 action categories. The dataset is challenging since each video contains 15 action instances on an average. As in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1]</ref>, the validation and test set are used for training and evaluating, respectively. The ActivityNet1.2 <ref type="bibr" target="#b2">[3]</ref> dataset has annotations of 100 categories in 4819 training and 2383 validation videos, with 1.5 activity instances per video on an average. As in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26]</ref>, we use the training and validation sets to respectively train and evaluate. Implementation details: For each snippet, 2048-d features are extracted from RGB and Flow I3D models pre-trained on Kinetics <ref type="bibr" target="#b3">[4]</ref>. The kernel size and dilation rate of the temporal convolutional layers are: (3, 1) for THUMOS14 and <ref type="table">Table 1</ref>. State-of-the-art comparison on the THUMOS14 dataset. Methods with superscript '+' require strong frame-level supervision for training. Our D2-Net performs favorably in comparison to existing weakly-supervised methods and achieves consistent improvements, in terms of mean average precision (mAP). (5, 2) for ActivityNet1.2. The first two convolutions in each stream are followed by a leaky ReLU with 0.2 negative slope. Our D2-Net is trained with a mini-batch size of 10 for 20K iterations, using the Adam <ref type="bibr" target="#b9">[10]</ref> optimizer with a 10 ?4 learning rate and 0.005 weight decay. The k for top-k is set to s/8 , as in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b22">23]</ref>. All the hyperparameters are chosen via cross-validation. The balancing parameter ? is set to 0.2 and 10 ?3 for THUMOS14 and ActivityNet1.2. The intraclass compactness weight ? and focusing parameter ? are set to 0.01 and 2 for both datasets. Multiple thresholds from 0.025 to 0.5 with increments of 0.025 are used for proposal generation. The NMS threshold is set to 0.5 while the score threshold S th for retaining detections in a video is set to 10% of the maximum proposal score in that video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">State-of-the-art Comparison</head><p>Tab. 1 and 2 compare D2-Net with state-of-the-art methods on THUMOS14 and ActivityNet1.2, respectively. Methods with '+' require strong supervision for training. THUMOS14: Similar to ours, all weakly-supervised methods in Tab. 1 use an I3D backbone, except Autoloc <ref type="bibr" target="#b33">[34]</ref>, which uses TSN <ref type="bibr" target="#b39">[40]</ref>. While BM <ref type="bibr" target="#b24">[25]</ref> considers an additional background class, DGAM <ref type="bibr" target="#b31">[32]</ref> extends BM using a VAE <ref type="bibr" target="#b10">[11]</ref>. Although DML <ref type="bibr" target="#b8">[9]</ref> and EM-MIL [17] achieve a promising mAP of 29.6 and 30.5 at IoU=0.5, they do not generalize well to ActivityNet1.2 (see Tab. 2). As discussed earlier, the recent work of UM <ref type="bibr" target="#b12">[13]</ref> employs out-of-distribution detection of background snippets. We also empirically <ref type="table">Table 2</ref>. State-of-the-art comparison on the ActivityNet1.2 dataset. Our D2-Net performs favorably compared to existing weakly-supervised approaches. Furthermore, our D2-Net performs comparably to SSN <ref type="bibr" target="#b47">[48]</ref>, which is trained with strong supervision (denoted with superscript '+'). AVG denotes the mean of the mAP values for IoU in [0.5, 0.95] with steps of 0.05. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>As discussed earlier, our D2-Net comprises a discriminative L Dis and a denoising loss L D . Here, we perform comparisons by replacing the two proposed loss terms (L Dis and L D ) in our framework with either the standard crossentropy loss L CE or the focal loss L F . In addition, we also show the performance of our D2-Net with only L Dis . Tab. 3 presents these performance comparisons, in terms of mAP and F1, on THUMOS14. Employing a standard cross-entropy loss (L CE in Tab. 3) in our framework results in an mAP score of 23.0 at IoU=0.5. We observe that <ref type="table">Table 3</ref>. Performance comparison by replacing our two loss terms (LDis and LD) in the proposed D2-Net with either the standard cross-entropy loss (LCE) or the focal loss (LF ). In addition, we also show the performance of our D2-Net with only LDis. Results are shown in terms of mAP and F1 score at IoU=0.5, on THUMOS14. Replacing the proposed loss terms in our framework with LCE and LF results in mAP scores at IoU=0.5 of 23.0 and 26.7, respectively. Our D2-Net with the discriminative loss term LDis achieves consistent improvement in performance over LF with an absolute gain of 5.5% in terms of mAP at IoU=0.5. Furthermore, our final D2-Net comprising both loss terms (LDis and LD) achieves the best performance with absolute gains of 12.9% and 9.2% in terms of mAP at IoU=0.5 over LCE and LF , respectively. training with the standard focal loss (obtained by zeroing the weights w in Eq. 4) helps alleviate the issue of a large number of easy samples overwhelming hard samples. This setting, L F in Tab. 3, gains 3.7% mAP at IoU=0.5 over L CE , thereby highlighting the need to tackle imbalance between easy backgrounds and hard foregrounds. To the best of our knowledge, we are the first to evaluate the standard focal loss, L F , in weakly-supervised action localization setting. Our D2-Net with the discriminative loss term L Dis , which jointly addresses class-imbalance and enhances backgroundforeground separation, provides consistent improvements over L F and achieves 32.2% mAP at IoU=0.5. An absolute gain of 5.5% in terms of mAP at IoU=0.5 is obtained by the introduction of our proposed L Dis in place of L F . Furthermore, our D2-Net comprising both L Dis and L D obtains the best results with an mAP score of 36.0% at IoU=0.5. Our D2-Net achieves absolute gains of 12.9% and 9.2% in terms of mAP at IoU=0.5, over L CE and L F , respectively. It is noteworthy that our final D2-Net, containing both L Dis and L D , obtains a significant gain of 5.9% in terms of F1 score over L Dis alone. This improvement over L Dis alone is obtained due to explicitly addressing the noise in TCAMs by our L D , leading to a substantial reduction (28%) in the number of false positives without affecting the recall. Impact of MI-based denoising: We also perform an experiment by replacing the proposed pDMI loss in our L D with the standard L1 and BCE losses for denoising the snippetlevel activations. The L1 and BCE losses, which do not explicitly capture MI, achieve mAP scores of 32.9% and 33.5% at IoU=0.5, respectively, on THUMOS14 (see Tab. 4). Our D2-Net, which employs MI-based pDMI loss in L D , achieves improved results with an mAP score at IoU=0.5 of 36.0%. These results suggest that our MI-based denoising is able to robustify the TCAMs in a weakly-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Qualitative results: <ref type="figure" target="#fig_4">Fig. 5</ref> shows a qualitative comparison between the baseline (red) and D2-Net (blue), along with the ground-truth (GT) action segments (green). The baseline employs only L F and is the same as the one used in <ref type="figure">Fig. 1</ref>. Example test videos with Diving and Throw Discus actions from THUMOS14 are shown in the first two rows. The baseline incorrectly merges multiple GT instances (e.g., 1 to 5 GT in Diving) and produces false positives in background regions (e.g., towards the beginning of Diving video). Our D2-Net correctly detects these multiple action instances and suppresses most false positives in the background regions. The third row shows an example test video with Mowing Lawn activity from ActivityNet1.2. The baseline incorrectly detects the presence of the activity over the entire video length. In contrast, our D2-Net improves the detection of multiple activity instances, leading to promising localization performance. Additional results and discussions are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a weakly-supervised action localization approach, called D2-Net, that comprises a discriminative and a denoising loss. The discriminative loss term strives for improved foreground-background separability through interlinked classification and localization objectives. The denoising loss term complements the discriminative term by tackling the foreground-background noise in the activations. This is achieved by maximizing the mutual information between activations and labels within a video (intra-video) and across videos (inter-video). Comprehensive experiments performed on multiple benchmarks show that our D2-Net performs favorably against existing methods on all datasets. <ref type="table">Table 5</ref>. Performance comparison by ablating the penalty term in LDis, on the THUMOS14 dataset. The penalty term in our LDis includes the standard focal loss penalty along with the proposed grouping and separating terms (w f g , w bg and w f b ). In comparison to the standard focal loss LF , our LDis without the focal loss penalty term achieves promising performance. This is further improved by our final LDis, indicating the efficacy of integrating w f g , w bg and w f b into the penalty term. Here, we present additional qualitative and quantitative analysis of the weakly-supervised action localization performance of our proposed D2-Net. The quantitative analysis w.r.t. robustness and impact of design choices are presented in Sec. A, followed by the qualitative results in Sec. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Quantitative Analysis</head><p>In this section, we present additional quantitative results w.r.t. model sensitivity, ablations and state-of-the-art comparison on the Charades <ref type="bibr" target="#b36">[37]</ref> dataset. Ablations for penalty term in L Dis : Here, we present an ablation to analyse the impact of the weights in the penalty term of our proposed discriminative loss term (Eq. 4 in main paper). Tab. 5 shows the performance comparison on the THUMOS14 dataset for ablating the penalty term. The penalty term in standard focal loss (L F in Tab. 5) comprises only the prediction dependent term (e.g., (1 ? p[c]) for a positive class). In contrast, our L Dis without focal penalty comprises only the grouping and clustering weights (e.g., (w f g + w f b ) for a positive class). Furthermore, our final L Dis includes both the standard focal penalty along with the grouping and clustering weights. Tab. 5 shows that replacing the standard penalty term with our grouping and clustering weights based penalty term (denoted as L Dis w/o focal penalty) achieves promising performance over L F . The performance is further improved in our final L Dis , which combines the standard penalty along with our grouping and clustering weights in the penalty term. This shows the efficacy of integrating our grouping and clustering weights (w f g , w bg and w f b ) into the penalty term, for improving the localization. Impact of snippet-level and video-level denoising: Tab. 6 shows the impact of individually integrating the mutual information (MI) based snippet-level (L DS ) and video-level (L DV ) denoising terms with L Dis . Integrating both these terms individually improves the localization performance over L Dis alone. While integrating L DS achieves 34.3% mAP at IoU=0.5, integrating L DV suppresses more false positives and results in an mAP of 33.2%. Furthermore, <ref type="table">Table 6</ref>. Impact of snippet-level and video-level denoising on the THUMOS14 dataset. Integrating snippet-level (LDS) and videolevel (LDV ) denoising terms individually with LDis improves the localization performance over LDis alone. Moreover, integrating both denoising terms with the discriminative loss term (i.e., LDis + LD) in our D2-Net achieves improved localization performance, indicating the importance of both snippet-level and video-level denoising for temporal localization.  <ref type="table">Table 7</ref>. Impact of varying ? on the THUMOS14 dataset. Suboptimal localization performances are observed when there is no/very high intra-class grouping, i.e., ? is 0 or 1. Promising localization performance is achieved when the intra-class embeddings are coarsely grouped, i.e., ? ? [0.01, 0.1]. our D2-Net, which integrates both snippet-level and videolevel denoising terms with the discriminative loss term (i.e., L Dis + L D ) achieves improved localization performance, indicating the importance of both snippet-level and video-level denoising for temporal localization. Impact of varying ?: Tab. 7 shows the impact of varying the degree of intra-glass grouping on the THUMOS14 dataset. We observe that when there is no/very high intra-class grouping amongst the foreground embeddings (or background embeddings), the temporal localization of actions is hampered. Furthermore, promising localization performance is achieved when the intra-class grouping is performed at a coarse level, i.e., ? ? [0.01, 0.1]. This shows that grouping the intra-class embeddings coarsely amongst themselves helps in learning discriminative embeddings, leading to improved localization performance.  <ref type="figure">Figure 6</ref>. Action localization performance w.r.t. balancing parameter ? in (a) and focusing parameter ? in (b) on the THUMOS14 dataset. The performance is shown for both validation and test sets. These experiments show that our D2-Net is reasonably robust to such variations of the balancing and focusing parameters and achieves promising localization performance. comparison of our approach with existing weakly-supervised methods on the Charades dataset. Note that a stronglysupervised approach of TGM <ref type="bibr" target="#b26">[27]</ref> achieves an mAP of 22.3. Among the weakly-supervised approaches, the graph convolution networks based ActGraph <ref type="bibr" target="#b27">[28]</ref>  Robustness Analysis: Here, we analyse the robustness of our D2-Net w.r.t. variations in the balancing parameter ? and focusing parameter ?. The performance variations of our approach on both validation and test sets of the THU-MOS14 dataset are shown in <ref type="figure">Fig. 6</ref>. The validation accuracy is obtained through cross-validation. The two parameters ? and ? are varied independently, while keeping the other constant at its respective optimal setting. Varying the balancing weight ? results in a performance variation as shown in <ref type="figure">Fig. 6a</ref>. We observe that the performance is optimal when ? is around 0.2 and decreases slowly on either side. As ? is increased, the denoising loss term (L D in Eq. 1 of main paper) overpowers the discriminative loss (L Dis ), resulting in a decreased localization performance. In contrast, as ? is decreased, the noise in the temporal class activations remains, resulting in reduced localization performance. Hence, we set ? = 0.2 in our experiments. Similarly, an optimal localization performance of 36.0 mAP is achieved when the focusing parameter ? is set to 2 and decreases on either side of it (see <ref type="figure">Fig. 6b</ref>). Note that a similar variation in performance is also observed when using the standard focal loss <ref type="bibr" target="#b13">[14]</ref> for generic object detection. Hence, as in <ref type="bibr" target="#b13">[14]</ref>, we set ? as 2 throughout our experiments. These experiments show that our D2-Net is reasonably robust to such variations of the balancing and focusing parameters and achieves promising localization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Qualitative Results</head><p>Here, we present qualitative temporal action localization results of our D2-Net framework on example videos from the THUMOS14 <ref type="bibr" target="#b6">[7]</ref> and ActivityNet1.2 <ref type="bibr" target="#b2">[3]</ref> datasets. In each figure <ref type="figure" target="#fig_4">(Fig. 7 to 15</ref>), sample frames from a video are shown in the top row followed by the ground-truth segments (green) and predicted detections (blue). The height of a detection is indicative of its score.</p><p>THUMOS14: <ref type="figure">Fig. 7 to 8</ref> and <ref type="figure">Fig. 10</ref> to 11 illustrate the localization results of our D2-Net on example videos, with Pole Vault, Javelin Throw, Volleyball Spiking and High Jump actions from the THUMOS14 dataset. Examples show different scenarios: temporally adjacent instances (Javelin Throw, High Jump), well separated instances (Pole Vault) and action pause (Volleyball Spiking). Our D2-Net detects many of these actions, reasonably well. Generally, well separated actions are detected correctly, as in Pole Vault <ref type="figure">(Fig. 7)</ref>. Further, an action instance and its slow motion replay are annotated incorrectly as a single action for the fourth instance in Javelin Throw <ref type="figure">(Fig. 8)</ref>, which is correctly detected as two instances by our approach. Accurately detecting the action instances containing video pauses in between, similar to the first and second instances in Volleyball Spiking <ref type="figure">(Fig. 10)</ref>, is challenging due to the absence of motion information in the corresponding snippets. The temporally adjacent instances of High Jump <ref type="figure">(Fig. 11</ref>) are correctly delineated. These results show that our approach achieves promising localization performance on these variety of actions.  <ref type="figure">Figure 8</ref>. Fourth instance of Javelin Throw is incorrectly annotated as a single instance though it has two instances: action and its slow motion replay. Our D2-Net correctly detects the two as separate instances. ent scenarios: well separated instances (Cricket), temporally adjacent activities (Washing Hands), long and short activity instances (Playing Harmonica), and long activity (Windsurfing). Well separated activity instances, similar to the instances of Cricket <ref type="figure" target="#fig_1">(Fig. 12</ref>) are generally detected correctly. The two instances of Washing Hands <ref type="figure" target="#fig_2">(Fig. 13</ref>) are detected as a single instance, since the background that is separating the two instances is indiscriminable from the foreground activity. While the long and short activity instances are both detected correctly for Playing Harmonica activity ( <ref type="figure">Fig. 14)</ref>, an additional false detection is observed due to the visual presence of the performer on stage (but not playing) in the corresponding image frames. Though the annotation for the end of Windsurfing activity is inaccurate and includes background regions also as foreground activity, our D2-Net correctly detects the end of the temporally long activity <ref type="figure" target="#fig_4">(Fig. 15</ref>). These qualitative results show that our proposed approach achieves promising action localization performance on a variety of activities. Foreground-Background Separation: <ref type="figure">Fig. 9</ref> shows the foreground-background separability comparison, utilizing t-SNE scatter plots, between the baseline and our D2-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Javelin Throw</head><p>Here, foreground and background embeddings per video are obtained by average pooling (temporally) the latent embeddings at their respective ground-truth snippet locations. <ref type="figure">Fig. 9</ref> shows that the foreground and background embeddings in the baseline overlap with each other. In contrast, our D2-Net better separates the foreground and background, compared to the baseline, leading to improved localization of foreground actions in the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>Proposed (D2-Net)</p><p>Background embedding Foreground embedding <ref type="figure">Figure 9</ref>. Illustration of foreground-background separability obtained in the latent embedding space of (a) the baseline using the standard focal loss and (b) our D2-Net via t-SNE scatter plots on the THUMOS14 test set. In both cases, foreground and background embeddings per video are obtained as the mean of latent embeddings at their respective ground-truth locations. Our D2-Net better separates the foreground and background, compared to the baseline.</p><p>Volleyball Spiking <ref type="figure">Figure 10</ref>. The first two instances of Volleyball Spiking have a considerable pause in the video, resulting in the absence of motion for the corresponding frames. E.g., an inset of sample frames in the second instance shows the pause in the video containing zero motion. This absence of discriminative motion information leads to four incorrect detections for these two GT instances. <ref type="figure">Figure 11</ref>. Temporally adjacent action instances of High Jump (sixth and seventh instances) are correctly detected as distinct instances by our D2-Net. Washing Hands <ref type="figure" target="#fig_2">Figure 13</ref>. The two adjacent ground-truth Washing Hands instances are jointly detected as a single instance by our D2-Net, since the separating background is indiscriminable from the foreground activity. Sample background frames, shown inset, contain hands along with soap lather and flowing water and are visually similar to the foreground activity. <ref type="figure">Figure 14</ref>. Both the long and short duration instances of Playing Harmonica are detected correctly by D2-Net. However, a false detection arises due to the presence of the performer on stage (but not playing) in the corresponding image frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High Jump</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Playing Harmonica</head><p>Windsurfing <ref type="figure" target="#fig_4">Figure 15</ref>. The ground-truth annotation for the end of Windsurfing activity is inaccurate since background regions are also included as foreground activity, as shown by the inset frames. Our D2-Net accurately detects the temporally long activity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Discriminative and Denoising loss terms D2-Net: Discriminative + Denoising loss terms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of our D2-Net. The focus of our design is the introduction of a novel loss formulation that jointly enhances the discriminability of latent embeddings and explicitly addresses the foreground-background noise in the output class activations. The network comprises two identical parallel streams (RGB and flow) consisting of three temporal convolutional TC layers. The second TC layer activations from both streams are averaged to obtain latent embeddings x. The final outputs of both streams are then averaged to obtain the temporal class activation maps (TCAMs) T of untrimmed input videos. A discriminative loss LDis (Sec. 3.1) is introduced to enhance the foreground-background separability ( ) of embeddings x by utlizing a top-down attention mechanism, in addition to achieving video classification. Furthermore, a denoising loss LD (Sec. 3.2) is introduced to explicitly address the foreground-background noise ( ) in the class activations of T, by utilizing a bottom-up attention. The network is trained jointly using both loss terms LDis and LD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Condition number (? U ) vs. Determinant (|det(U)|) for joint distribution matrices U. On the left: 25k randomly sampled U. On the right: U obtained during our snippet-level training. In both cases, minimizing ? U leads to maximizing |det(U)| (DMI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>:</head><label></label><figDesc>Prediction vector of size C y i : Label vector of size C At snippet-level, C = 2 (fg vs bg).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative temporal action localization results of our proposed D2-Net on example test videos, with Diving, Throw Discus actions from THUMOS14, and Mowing Lawn activity from ActivityNet1.2. For each video, example frames (top row), ground-truth GT segments (green), baseline detections (red) and D2-Net detections (blue) are shown. The height of a detection is indicative of its score. The Baseline incorrectly merges multiple GT instances, has false positives in background regions and falsely detects the presence of the activity over the entire video length. Our D2-Net correctly detects multiple instances (e.g., 1 to 5 GT in Diving, 3 to 5 in Throw Discus) and suppresses most false positives in the background regions, achieving promising localization performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ActivityNet1. 2 :Figure 7 .</head><label>27</label><figDesc>Fig. 12to 15 illustrate the localization results of our D2-Net on example videos, with Cricket, Washing Hands, Playing Harmonica and Windsurfing actions from the ActivityNet1.2 dataset. Examples show differ-Pole Vault Well separated action instances of Pole Vault are generally accurately detected by our D2-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 .</head><label>12</label><figDesc>Well separated instances of Cricket activity are detected accurately by our D2-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Similar to our D2-Net, all weaklysupervised methods in Tab. 2 use I3D backbone. Following standard evaluation protocol<ref type="bibr" target="#b2">[3]</ref>, we report the mean of the mAP scores (denoted as AVG) at different IoU thresholds ([0.5, 0.95] in steps of 0.05). The generative modeling based approach DGAM<ref type="bibr" target="#b31">[32]</ref> and background suppression based BaS-Net<ref type="bibr" target="#b11">[12]</ref> perform comparably, achieving mean mAP scores of 24.4 and 24.3, respectively. In comparison, the recent approaches such as UM<ref type="bibr" target="#b12">[13]</ref> and ASL<ref type="bibr" target="#b18">[19]</ref> achieve localization performances of 25.9 and 25.8, respectively, in terms of mean mAP. Our proposed D2-Net performs comparably against these existing approaches and achieves a promising localization performance of 26.0 mean mAP. Additional results are provided in the appendix.</figDesc><table><row><cell></cell><cell>0.5</cell><cell>mAP @ IoU 0.75</cell><cell>0.95</cell><cell>AVG</cell></row><row><cell>SSN [48] +</cell><cell>41.3</cell><cell>27.0</cell><cell>6.1</cell><cell>26.6</cell></row><row><cell>DML [9]</cell><cell>35.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EM-MIL [17]</cell><cell>37.4</cell><cell>-</cell><cell>-</cell><cell>20.3</cell></row><row><cell>CMCS [15]</cell><cell>36.8</cell><cell>22.0</cell><cell>5.6</cell><cell>22.4</cell></row><row><cell>3C-Net [23]</cell><cell>37.2</cell><cell>-</cell><cell>-</cell><cell>21.7</cell></row><row><cell>BaS-Net [12]</cell><cell>38.5</cell><cell>24.2</cell><cell>5.6</cell><cell>24.3</cell></row><row><cell>DGAM [32]</cell><cell>41.0</cell><cell>23.5</cell><cell>5.3</cell><cell>24.4</cell></row><row><cell>UM [13]</cell><cell>41.2</cell><cell>25.6</cell><cell>6.0</cell><cell>25.9</cell></row><row><cell>ASL [19]</cell><cell>40.2</cell><cell>-</cell><cell>-</cell><cell>25.8</cell></row><row><cell>Ours: D2-Net</cell><cell>42.3</cell><cell>25.5</cell><cell>5.8</cell><cell>26.0</cell></row></table><note>validate the complementarity of our approach with UM by intergrating the loss terms and observe an average gain of 1% mAP across different IoUs. Our D2-Net performs well against existing weakly-supervised approaches, including the recent CoLA [47] and ASL [19]. Our approach achieves an absolute gain of 2.3% at IoU=0.5 over the best existing method (UM). Moreover, promising localization performance is obtained at other IoU thresholds. ActivityNet1.2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Dis + L D 65.7 60.2 52.3 43.4 36.0 36.7 Impact of MI-based denoising on THUMOS14. Our D2-Net, employing MI-based pDMI loss in LD performs favorably compared to utilizing standard losses (L1 and BCE) in LD.</figDesc><table><row><cell>term</cell><cell>0.1</cell><cell cols="2">mAP @ IoU 0.2 0.3 0.4</cell><cell>0.5</cell><cell>F1</cell></row><row><cell>L CE</cell><cell cols="5">55.0 47.6 38.7 30.7 23.0 23.5</cell></row><row><cell>L F</cell><cell cols="5">58.8 52.4 44.3 35.7 26.7 27.2</cell></row><row><cell>L Dis</cell><cell cols="5">65.4 59.7 50.1 40.4 32.2 30.7</cell></row><row><cell cols="2">D2-Net: L L1</cell><cell>BCE</cell><cell cols="3">Ours: D2-Net</cell></row><row><cell>mAP at IoU=0.5</cell><cell>32.9</cell><cell>33.5</cell><cell></cell><cell>36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>State-of-the-art comparison on the Charades dataset. Our D2-Net performs favorably compared to existing weaklysupervised approaches.</figDesc><table><row><cell>Gamma (?)</cell><cell>0.1</cell><cell>0.2</cell><cell>mAP @ IoU 0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>0.0</cell><cell>64.8</cell><cell>59.3</cell><cell>51.8</cell><cell>42.5</cell><cell>34.2</cell></row><row><cell>0.01</cell><cell>65.8</cell><cell>60.1</cell><cell>52.3</cell><cell>43.4</cell><cell>36.0</cell></row><row><cell>0.1</cell><cell>65.5</cell><cell>60.0</cell><cell>52.0</cell><cell>43.1</cell><cell>35.7</cell></row><row><cell>1.0</cell><cell>65.2</cell><cell>59.9</cell><cell>51.3</cell><cell>41.9</cell><cell>33.7</cell></row><row><cell cols="6">ActGraph [28] WSGN [6] Ours: D2-Net</cell></row><row><cell>mAP</cell><cell>15.8</cell><cell></cell><cell>18.3</cell><cell>19.2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>State-of-the-art Comparison: The Charades [37] dataset comprises 9848 indoor videos with 157 everyday activity classes. On an average, there are 6.8 activity instances per video, with complex activities co-occurring. As in<ref type="bibr" target="#b35">[36]</ref>, we use the standard training and validation split and follow the same localization evaluation. Tab. 8 shows the performance</figDesc><table><row><cell></cell><cell></cell><cell>Test</cell><cell>Validation</cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell>Validation</cell><cell></cell><cell></cell></row><row><cell></cell><cell>37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP (%)</cell><cell>34 35 36</cell><cell></cell><cell></cell><cell></cell><cell>mAP (%)</cell><cell>34 35 36</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>3</cell><cell>3.5</cell></row><row><cell></cell><cell></cell><cell cols="3">Balancing weight</cell><cell></cell><cell></cell><cell cols="3">Focussing weight</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>achieves 15.8% mAP, while Gaussian networks-based WSGN [6] obtains 18.3. Our D2-Net performs favorably against existing weaklysupervised methods, achieving a promising performance of 19.2 mAP.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by ARC DECRA Fellowship DE200101100, NSF CAREER Grant #1149783 and VR starting grant 2016-05543.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Refineloc: Iterative refinement for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00227</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised gaussian networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheston</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The thumos challenge on action recognition for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A hybrid attention mechanism for weaklysupervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00545</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization using deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Weakly-supervised temporal action localization by uncertainty modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07006</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Weaklysupervised action localization with expectationmaximization multi-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00163</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sf-net: Singleframe supervision for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxin</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised action selection learning in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Satya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Gorti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adversarial backgroundaware loss for weakly-supervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06643</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action completeness modeling with background aware networks for weaklysupervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Moniruzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruwen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">C</forename><surname>Leu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Phuc Xuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action graphs: Weakly-supervised action localization with graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maheen</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fineto-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling the temporal extent of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Autoloc: weaklysupervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multistage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hide-andseek: Forcing a network to be meticulous for weaklysupervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dmi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Segregated temporal assembly recurrent networks for weakly supervised multiple action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Two-stream consensus network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11594</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cola: Weakly-supervised temporal action localization with snippet contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
