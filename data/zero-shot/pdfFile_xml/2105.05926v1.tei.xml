<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Diversity Learning for Zero-Shot Multi-label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Ben-Cohen</surname></persName>
							<email>avi.bencohen@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zamir</forename><surname>Nadav</surname></persName>
							<email>nadav.zamir@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Emanuel</surname></persName>
							<email>emanuel.benbaruch@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Baruch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Friedman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelnik-Manor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Diversity Learning for Zero-Shot Multi-label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training a neural network model for recognizing multiple labels associated with an image, including identifying unseen labels, is challenging, especially for images that portray numerous semantically diverse labels. As challenging as this task is, it is an essential task to tackle since it represents many real-world cases, such as image retrieval of natural images. We argue that using a single embedding vector to represent an image, as commonly practiced, is not sufficient to rank both relevant seen and unseen labels accurately. This study introduces an end-to-end model training for multi-label zero-shot learning that supports semantic diversity of the images and labels. We propose to use an embedding matrix having principal embedding vectors trained using a tailored loss function. In addition, during training, we suggest up-weighting in the loss function image samples presenting higher semantic diversity to encourage the diversity of the embedding matrix. Extensive experiments show that our proposed method improves the zero-shot model's quality in tag-based image retrieval achieving SoTA results on several common datasets (NUS-Wide, COCO, Open Images).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Identifying all the relevant labels that describe the objects or scene in an image is an essential task in computer vision real-world applications. With the ongoing increase of photos stored online comes a growing need for better image tagging and tag-based retrieval for various use cases such as search, organization, or data collection. Recent datasets in this field enabled progress in this direction by introducing a large number of classes with annotations of their presence in each image. Yet, annotating a large number of classes for many images, each with high semantic diversity, can be very time-consuming and practically infeasible for realworld applications. Although current conventional multilabel classification methods can deal with a large number of classes, they are still limited by the annotated (seen) set of <ref type="figure">Figure 1</ref>: Our model extracts a set of principal embedding vectors used as a transformation matrix A i where each row sets a ranking principal direction for labels in the word vector space based on their relevancy. By using multiple directions, it can deal with multiple diverse image semantic concepts. In addition, we propose that images with larger semantic diversity (top images) should be up-weighted during training compared to ones with lower semantic diversity (bottom images). labels provided with the datasets.</p><p>On the other hand, Zero-shot (ZS) learning for multilabel classification adds the ability to recognize labels from additional categories that do not exist during training (unseen). This is usually done by transferring knowledge between the seen and unseen labels. In most cases, a text model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> is used to transfer this knowledge using word vectors. Then, a visual model is trained, learning a transformation between the visual space and the word vector, label space.</p><p>Most studies on ZS classification focus on the single label problem, i.e., recognizing a single unseen label in each image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54]</ref>. However, a single label per image does not provide a full description of it and usu-ally is not practical for real-world applications. Other studies tried to tackle the ZS multi-label classification problem. <ref type="bibr" target="#b34">[35]</ref> trained a multi-label classifier on the seen classes and linearly combined word vectors of these classes using the prediction scores to represent an image. Based on that representative vector, the ranking of unseen labels was done by computing similarities to their word vectors. <ref type="bibr" target="#b54">[55]</ref> trained a network to output per-image a single principal direction that ranks relevant labels higher than non-relevant ones. However, this method faces difficulties with images that include multiple labels with high semantic diversity. In these cases, the extracted principal direction needs to be robust to high variability in the relevant labels' word vectors. For example, the classes "dog" and "rice" are quite different, but might still be present in the same image. As we grow with the number of annotated classes in images, the probable semantic diversity in each image grows. This high semantic diversity problem requires special treatment, which is hard to achieve using a single principal direction.</p><p>Several works had approached the problem of high semantic diversity of the labels in an image using a pre-trained object detector and learning to select bounding boxes of seen or unseen labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39</ref>]. Yet, these approaches require annotated bounding boxes as ground truth, making it not scalable for a large number of labels. Alternatively, <ref type="bibr" target="#b20">[21]</ref> used attention techniques to estimate the relevant regions based on a pre-trained model's features. However, this usually requires a large pre-trained model to get rich regional features (VGG-19) and a complex loss function to be tuned.</p><p>In this paper, we propose a method that aims to properly cope and leverage the semantic diversity of the labels in each image, by allowing multiple principal directions, constructed as a transformation matrix in the loss function. Also, sample images with larger semantic diversity are upweighted in the loss function as these images are considered hard examples. As a result, our model learns to extract a per-image transformation designed to handle the image label diversity challenges ( <ref type="figure">Figure 1</ref>). We believe that by doing so, we learn a model that is better suited for understanding and recognizing multiple seen and unseen labels in an image.</p><p>We further show how we achieve results that are on par or better than SoTA while keeping a relatively simple endto-end training scheme using our suggested loss function.</p><p>The main contributions presented in this study include:</p><p>? A loss function tailored to the problem of ZS multilabel classification.</p><p>? We show that up-weighting samples with higher semantic diversity further improves the model generalization.</p><p>? An efficient end-to-end training scheme for ZS models is proposed, reaching SoTA results in tag-based image retrieval while keeping high-performance for image tagging on several datasets (NUS-Wide, Open-Images, and MS-COCO) with a smaller number of model parameters compared to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent studies on multi-label classification reported notable success by exploiting dependencies among labels via graph neural networks to represent label relationships or word embeddings based on prior knowledge <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46]</ref>. Other approaches try to model the image parts using attentional regions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>. Although these approaches show promising results, they usually include a complex architecture, and other approaches reported similar and even better results using a more conventional training flow with advanced loss modifications <ref type="bibr" target="#b3">[4]</ref>. While most of these approaches are effective for images, including the seen classes they were trained for, they don't generalize well to unseen classes.</p><p>The main objective of zero-shot learning is to overcome this challenge and extract both seen and unseen labels for each image. This is usually done using semantic label information like attributes <ref type="bibr" target="#b21">[22]</ref> or word vector representation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>. The central concept is to combine the visual features with the semantic word vectors representing each label using a similarity metric. Based on the similarity, unseen labels could be classified <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>. Most of the methods for zero-shot learning concentrate on finding the most dominant label in an image <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43</ref>]. Despite their great success, these solutions do not generalize well to the problem of zero-shot multi-label classification and do not tackle the multi-label diversity challenges included in it.</p><p>In contrast to the zero-shot single-label classification task, multiple seen/unseen labels are assigned to an image in the zero-shot multi-label classification task. There is a limited number of studies addressing this problem. An interesting concept was suggested by <ref type="bibr" target="#b34">[35]</ref> where predictions of a classifier trained on seen tags were linearly combined in the word embedding space to form a semantic embedding for that image to tackle the zero-shot single-label classification problem. This semantic embedding was later used to rank unseen labels based on their word vector's similarity to that embedding vector. <ref type="bibr" target="#b27">[28]</ref> extended their work by proposing a hierarchical semantic embedding to make the label embedding more representative for the multi-label task. <ref type="bibr" target="#b14">[15]</ref> proposed a transductive learning strategy to promote the regression model learned from seen classes to generalize well to unseen classes. In the Fast0Tag approach <ref type="bibr" target="#b54">[55]</ref>, the authors proposed a fast zero-shot tagging method by estimating a principal direction for an image. They show that word vectors of relevant tags in a given image rank ahead of the irrelevant tags along this principal direction in the word vector space. Another approach suggested by <ref type="bibr" target="#b26">[27]</ref>, is using structured knowledge graphs to describe the relationships between multiple labels from the semantic label space and show how it can be applied to multi-label and zeroshot multi-label classification tasks. Due to the difficulty in distinguishing between multiple instances in an image using only global features, some studies try to identify important sub-regions in the image that includes the relevant labels by utilizing region proposal methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b36">37]</ref>. In recent work, <ref type="bibr" target="#b20">[21]</ref> proposed a shared multi-attention model for multi-label zero-shot learning that can focus on the relevant regions, obviating the need for object detection or region proposal networks. VGG-19 backbone is used to extract rich regional features, and a 4-term loss function is formed to tackle multiple challenges encountered during training. The derived model is then used to extract multiple attentions projected into the joint visual-label semantic embedding space to determine their labels. While this method tackles the diversity challenge by using multiple attention features for comparison to seen and unseen tags, it includes a complicated loss function consisting of a ranking loss and 3 regularization terms that require careful parameter tuning during training, while the image label diversity isn't used implicitly in any of these loss functions.</p><p>Finally, in our proposed method, we use the image semantic label diversity directly during training to improve and generalize our model better to diverse images. As far as we know, this is the first work to analyze the zero-shot semantic diversity problem and offer a method to exploit this information in a novel loss function. In addition, our end-to-end training flow does not require a large backbone model or object proposals for training while still achieving state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semantic Diversity Learning</head><p>In this section, we present our proposed method for training multi-label zero-shot models. The problem and network architecture will be presented first, following a detailed description of our semantic diversity-based loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setting</head><p>Let us denote by S and U the seen and unseen sets of tags, respectively, where seen tags stand for tags that have been seen during training and 'unseen' means tags that were not included in the training annotations. The entire set of tags is defined by C = S ? U.</p><p>Let {(I n , Y n ); n = 1, 2, ..., N } denote the training data where I n is the n-th image, and Y n is the corresponding set of seen tags. We assume that each tag will be represented by a semantic word vector {v c } c?C . Based on these notations, we define the task of multi-label zero-shot learning as assigning the relevant unseen tags y i ? U for a given image I i , and generalized multi-label zero-shot learning as assigning the relevant seen or unseen tags y i ? C for a given image I i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>The proposed network architecture is illustrated in <ref type="figure">Figure</ref> 2. We used TResNet-M <ref type="bibr" target="#b40">[41]</ref> convolutional neural network (CNN) as a backbone for our visual model. TResNet-M is a GPU-optimized model that reports high accuracy and efficiency on several competitive computer vision datasets. Using an efficient model design allows us to train easily in an end-to-end manner. By modifying its last fully connected layer, the vision model is trained to output a M ? d w linear transformation matrix where d w is the length of the word vectors, and M is a parameter of our architecture representing the number of principal direction vectors. This matrix enables an image-dependent word ranking by projecting word vectors in different directions and using a ranking criteria over it. A similar concept was presented in <ref type="bibr" target="#b51">[52]</ref> for multi-label classification, where the transformation was learned per image and optimized to reach a linear combination of word vectors that allows it to distinguish the relevant from the non-relevant tags. However, in our experiments, simply using the method suggested in <ref type="bibr" target="#b51">[52]</ref> for the zero-shot learning task, the resulting model failed to generalize well for the unseen tags. Hence, we propose a loss function as well as ranking criteria more suitable for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss</head><p>The per-image linear transformation A extracted from the vision model should provide a high rank for all relevant tags even when there exists a large difference in their word embedding representation. Hence, we design our loss function to deal with the semantic diversity problem in zeroshot multi-label learning. Namely, we would like to provide a higher rank for a positive word vector p j and a lower rank for a negative word vector n k , hence minimize the following:</p><formula xml:id="formula_0">u jk = max (An k ) ? max (Ap j )<label>(1)</label></formula><p>The usage of a max function is crucial for this purpose as it allows each row in the matrix (principal direction) to be optimized in a different direction than other rows. In this formulation, one matrix row is sufficient to rank a label correctly, thus letting other rows focus on the additional relevant labels and output high scores for all of them. This is ideal if there is a high semantic diversity in the image that makes it difficult for a single row in the transformation matrix to deal with the multiple and diverse set of tags. Similar intuition is presented in multi-class support vector machines (SVM) <ref type="bibr" target="#b8">[9]</ref>, where the confidence value for the correct label is optimized to be larger by a certain margin than the confidences assigned to the rest of the labels. Otherwise, we get a loss that is linearly proportional to the difference between the confidence of the correct label and the maximum among the other labels' confidences. Also, by using the max operation when comparing a specific pair of positive and negative labels, we allow gradients propagation only through the most dominant row in the matrix for each tag, hence, not modifying the entire matrix for each pair which allows each row to focus on different semantic concepts.</p><p>Having this in mind, we define the ranking loss, inspired by <ref type="bibr" target="#b54">[55]</ref>, as follows:</p><formula xml:id="formula_1">L rank = ? d 1 ? n j k log (1 + e u jk )<label>(2)</label></formula><p>Where ? n = |P ||P |, while |P | is the size of the set of ground-truth tags for a single image and |P | is the size of the set of the non relevant tags. The division by ? n is used as a normalization. Images with a large number of tags will be treated the same as these with a low number of tags. We consider images with high label diversity more difficult as they require our model to learn how to rank several different semantic concepts higher than others. Hence, we would like to put additional focus on these examples during training. We denote by ? d the per-image semantic diversity weight (SDW). The SDW up-weights more diverse images (hard samples), thus increasing focus on them in our loss function. We define ? d as the sum of variances across the relevant set of tags in an image:</p><formula xml:id="formula_2">? d = 1 + dw i=1 var(P i )<label>(3)</label></formula><p>We note that our ranking loss function provides high flexibility when learning the transformation matrix which is good for learning diverse tags per image. However, this flexibility should be controlled when training on large and noisy datasets. The matrix rows could become too diverse and, by such, making it sensitive to outliers and drive it to not generalize well when training on large and noisy datasets. Hence, we add a regularization term to deal with these cases defined by</p><formula xml:id="formula_3">L reg = dw m=1 var(A m ) 1<label>(4)</label></formula><p>This regularization term puts a constraint on the matrix rows from being too diverse. In practice, reducing the variance between rows encourages learning correlative information between tags known to be useful in multi-label setting <ref type="bibr" target="#b6">[7]</ref>. This regularization term is similar in a way to multi-class SVM regularization <ref type="bibr" target="#b8">[9]</ref>. However, by looking at the variance, we ignore the mean of each column in the matrix, making it invariant to translations. This is different from the standard l 2 norm regularization used in multi-class SVM. The main reasoning is that our loss function is optimized over a given word vector space that its features are not standardized. Hence, using the variance as a regularization instead of the standard l 2 norm is more suitable for our use case. Our final loss function is defined by</p><formula xml:id="formula_4">L f inal = 1 N N i=1 (1 ??)L rank (A i , y i ) +?L reg (A i )<label>(5)</label></formula><p>Where? sets the regularization weight. In practice we use ? =?|P | as the regularization parameter which is invariant to the number of negative tags per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Several experiments were conducted to analyze and evaluate our method for zero-shot multi-label classification. In section 4.2 we compare our approach to other state-of-theart works in the field. To better understand each component's contribution in our framework, we conduct an ablation study as discussed in section 4.3, and the regularization parameter is further analyzed in section 4.4. Next, to visualize what the transformation matrix learns using our  method, a set of qualitative results are presented and discussed in section 4.5. As our method aims towards dealing with the semantic diversity challenge, we wish to analyze our results on the more diverse set of images as discussed in section 4.6. Finally, in section 4.7 we discuss and analyze our results using a different number of principal directions in the transformation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets: Three datasets were used to evaluate our proposed methodology. The NUS-WIDE <ref type="bibr" target="#b7">[8]</ref> dataset includes 270K images with 81 human-annotated categories used as unseen classes in addition to a set of 925 labels obtained from Flickr users tags automatically that are used as seen classes. The MS COCO <ref type="bibr" target="#b29">[30]</ref> dataset is divided into training and validation sets with 82,783 and 40,504 images, re-spectively. This dataset is commonly used for multi-label zero-shot object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref> and was also used in recent works of multi-label zero-shot classification <ref type="bibr" target="#b36">[37]</ref>. We follow <ref type="bibr" target="#b2">[3]</ref> with our split to seen and unseen tags, resulting in 48 seen and 17 unseen classes based on their cluster embedding in the semantic space and WordNet hierarchy <ref type="bibr" target="#b32">[33]</ref>. We use the provided list of images, including 73,774 images with only seen objects for training, and 6,608 images containing both seen and unseen objects for testing. The Open Images (v4) <ref type="bibr" target="#b25">[26]</ref> dataset consists of 9 million training images, 41,620 validation images, and 125,456 test images. This dataset introduces several challenges: this large-scale dataset is larger by orders of magnitude when compared to NUS-WIDE or MS COCO, and its images are only partially annotated where not all labels were verified as true-positives or negatives in each image. Similar to <ref type="bibr" target="#b20">[21]</ref>, we use 7,186 labels, having at least 100 images in training set for each seen class. The most frequent 400 test labels not present in the training data are selected as unseen classes. Evaluation Metrics: We follow <ref type="bibr" target="#b20">[21]</ref> and use the mean Average Precision (mAP) and F1 score at top-K predictions in each image. The mAP evaluates the accuracy for tag-based retrieval, i.e., it answers the question of how good our model is at ranking images for each given label, while the top-K F1 score captures its accuracy for image tagging, measuring how good it is at ranking relevant labels for each image. Implementation Details: Unless stated otherwise, all experiments were conducted with the following training configuration. We use as a backbone TResNet-M, pre-trained on the ImageNet dataset <ref type="bibr" target="#b9">[10]</ref>. See appendix A for a comparison to other backbones. The model was fine-tuned using Adam optimizer <ref type="bibr" target="#b23">[24]</ref> and 1-cycle cosine annealing policy <ref type="bibr" target="#b44">[45]</ref> with maximal learning rate of 1e-4. We use cutout <ref type="bibr" target="#b11">[12]</ref> Fast0Tag <ref type="bibr">[</ref>  with probability of 0.5, True-weight-decay <ref type="bibr" target="#b30">[31]</ref> of 3e-4 and standard ImageNet augmentations. The regularization parameter ? was set to 0.3. We train the network for 7/7/20 epochs and a batch-size of 192/96/32 for NUS-Wide/Open Images/MS-COCO, respectively.</p><p>For our tag embedding representations we use a FastText pre-trained model <ref type="bibr" target="#b16">[17]</ref> with a vector size of d w = 300. The word vectors are 2 normalized. At inference, our trained model takes an image I i as input and provides a corresponding transformation matrix A i as output. Let T = {t j } denote the set of word vectors representing each tag in the label set. For image tagging, we compute r ij = max(A i t j ) for each seen/unseen tag and rank them such that higher values represent the more relevant tags. For tag-based image retrieval, for a query tag, we compute similarly r ij for all given images and rank them from most relevant to least. Baseline: We use as a baseline to our method implementation of Fast0Tag <ref type="bibr" target="#b54">[55]</ref> loss function integrated within our training framework. In addition, we also compare our method to a baseline with multiple principal directions (M = 7), substituting our max function in equation <ref type="formula" target="#formula_0">(1)</ref> with l 2 norm similar to <ref type="bibr" target="#b51">[52]</ref>, and removing our regularization term and SDW. <ref type="table" target="#tab_0">Table 1</ref> shows a comparison of our proposed method to other state-of-the-art methods on NUS-WIDE and Open Images. Our method outperforms all other methods in terms of mAP for both datasets. We also present the top-K Precision (P) and Recall (R) in addition to the F1 score. Note that we used K ? {3, 5} for NUS-WIDE and K ? {10, 20} for Open Images due to a large number of available labels in it. Compared to the recently introduced shared multi-attention-based approach (LESA) <ref type="bibr" target="#b20">[21]</ref> we achieve better performance on open images. We improved results in both zero-shot/generalized zero-shot learning tasks by 9.3%/20.4%, 7.3%/18.6%, and 21.2%/29.9% in F 1(K = 10), F 1(K = 20), and mAP respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to State-of-the-art</head><p>As for NUS-WIDE, although the LESA approach shows a moderate improvement in performance in terms of F1 for zero-shot learning, our proposed method shows a much higher gain in terms of mAP with an improvement of 6.5% for both zero-shot and generalized zero-shot. In addition, our method achieves improved results in terms of F1 for generalized zero-shot of 4.1% and 4.2% for F 1(K = 3), F 1(K = 5) respectively.</p><p>During this paper's writing, recent work in the field was published as a pre-print showing a Generative approach for zero-shot learning <ref type="bibr" target="#b17">[18]</ref>. In this approach, the authors propose to train two separate classifiers, one focused on the zero-shot learning task and another on the generalized zeroshot learning task. This differs from our problem formulation, as we wish to have a single model that is trained for both zero-shot and generalized zero-shot tasks. The formulation used in our study seems to be more suitable for realworld applications, i.e., evaluating one single model for the two tasks under the same working point.</p><p>MS-COCO dataset is quite different from NUS-Wide and Open Images as it holds a relatively small number of seen and unseen labels. In <ref type="table" target="#tab_1">Table 2</ref> we compare to the method presented in <ref type="bibr" target="#b36">[37]</ref> which is based on an object detection model. In comparison, our model achieves slightly lower results for zero-shot learning with significant improvement in the generalized zero-shot metrics. As presented in <ref type="figure" target="#fig_0">Figure 2</ref>, our framework does not include any additional modules such as object detectors, region proposals, or attention layers and can still achieve high-quality results. Note that for COCO, we used M = 2. We discuss the motivation for this in section 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To better understand each component's contribution in our solution, we perform an ablation study as shown in <ref type="table" target="#tab_3">Table 3</ref>. We compare to Fast0Tag <ref type="bibr" target="#b54">[55]</ref> method that uses a single principal direction per image as a starting point. We also implement Fast0Tag in our framework for a fair <ref type="figure">Figure 3</ref>: Qualitative results, showing the most relevant tags for each row in the transformation matrix using our proposed method and the baseline. Using our proposed method, we can see that different semantic concepts are being learned by different rows in the matrix. comparison, which already shows much better results compared to the original paper. However, the baseline with additional principal directions shows lower performance in terms of mAP, indicating that a naive addition of principal directions is insufficient to improve the model performance. The addition of SDW has been shown to improve the results when used together with the loss proposed in <ref type="bibr" target="#b54">[55]</ref>. It also improves the results of our proposed method (column f to ours), indicating that SDW can be beneficial for different methods and loss functions that can support per-sample weighting. In addition, the regularization term has shown to improve the results when using M = 2 while for M = 7 higher regularization showed higher performance in terms of ZS mAP while keeping on par results in terms of GZS mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Regularization Parameter</head><p>The regularization term presented in equation (4) provides control over the matrix transformation rows' diversity. <ref type="figure" target="#fig_1">Figure 4</ref> presents the results in terms of mAP and F1 (K = 3) on NUS-Wide test set using different ? values for M = {3, 7}. The contribution of the regularization term is noticeable for a different number of rows. However, for a larger number of rows (M = 7) stronger regularization provides better performance. A possible reason for that is that a large number of rows in the matrix can lead to a decrease in utilization of all rows in practice. Using our proposed regularization, we better utilize the different rows in the matrix and thus better generalize on the test set. Especially for zero-shot learning, the generalizability of the model is crucial for retrieving images with unseen tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Assessment</head><p>Our proposed method focuses on semantic diversity learning using a matrix transformation. Each row in this matrix can be described as a principal direction responsible for a set of relevant labels. In <ref type="figure">Figure 3</ref> we compare our method results to the baseline model. For each image, we show the most relevant results in the top ? 10 retrieved labels. The numbers in the figure indicate the most dominant row that provided the highest score for the corresponding set of tags. In several of these sample images, we can see that the tags learned using our approach were separated based on their main concept, e.g., in the top left image, hand and hands belong to the same row, while for the baseline, they are separated. Moreover, using our method, we can see that more relevant tags were discovered in some cases, e.g., in the bottom right image, additional tags such as "clothing" and "dress" were discovered by the same row that learned to understand this concept in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Performance on Diverse Images</head><p>As our method aims towards learning diverse concepts in an image, we wish to evaluate its performance on the more-diverse samples in the dataset. Since diverse images usually include more labels, we perform an experiment to evaluate our image tagging method on image samples that include more than 6 labels from both zero-shot and generalized zero-shot sets. <ref type="table" target="#tab_4">Table 4</ref> presents the results using the baseline and our proposed method (M = 7). Since SDW up-weights more diverse images in the loss function, we show results both with and without it compared to the baseline. Our method outperforms the baseline without SDW and achieves even higher results when adding it during training, demonstrating its effectiveness with managing diverse samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Matrix Principal directions</head><p>The size of the per-image transformation matrix A is set by the parameter M that defines the number of principal directions it has. In <ref type="figure" target="#fig_2">Figure 5</ref> the mAP improvement in terms of zero-shot and generalized zero-shot is presented for a different number of rows in the matrix using a fixed set of parameters (e.g., regularization parameter = 0.1) on the NUS-Wide test set.</p><p>Noticeably, an increasing number of rows improves the generalized zero-shot results. While the set of principal di- rections used increases, it is easier for the model to learn the seen tags in the image and improve the generalized zeroshot performance. As for zero-shot learning, while there is an improvement when increasing M up to 7 ? 8 rows, a further increase in the number of rows causes a decrease in mAP. This indicates that the model does not generalize well to unseen tags in our method when using too many principal directions. A possible solution for this would be to increase the regularization parameter. In our experiments, we have found that M = 7 covers the semantic diversity in the image compared to other choices for NUS-Wide and Open Images. While for MS-COCO, which is much smaller in the number of labels, M = 2 was found experimentally to show superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>The zero-shot multi-label classification task introduces the challenge of recognizing multiple and diverse labels in an image, including categories not seen during the training process. This task is even more challenging for images with high semantic diversity.</p><p>In this study, we propose an end-to-end training scheme including a novel loss function tailored to semantic diversity in zero-shot learning. Our loss function consists of a semantic diversity weight for more diverse images, utilizing multiple principal directions to enable specialization of principal vectors in different semantic concepts and a matrix variance regularization term to improve model generalizability to unseen tags. Extensive experiments show that our proposed loss function improves the zero-shot model's quality in terms of tag-based image retrieval showing SoTA results while keeping high-performance for image tagging on several standard datasets (NUS-Wide, COCO, Open Images).</p><p>In our experiments we use TResNet-M <ref type="bibr" target="#b40">[41]</ref> as a backbone for our visual model, due to its efficiency and reported high accuracy on several competitive computer vision datasets. To further extend our analysis and comparison with prior works we also explore two popular backbone architectures, VGG19 <ref type="bibr" target="#b43">[44]</ref> and ResNet50 <ref type="bibr" target="#b19">[20]</ref> in <ref type="table">Table 5</ref>. We report results using our approach as well as adding a comparison to Fast0Tag <ref type="bibr" target="#b54">[55]</ref> loss function with our E2E training scheme as a baseline. As can be seen, using our approach with VGG19 as a backbone, the results in terms of mAP for both zero-shot and generalized zero-shot are superior compared to prior works but lower than our current backbone, while using ResNet50 as a backbone improves over VGG19 in all metrics. Best results are achieved using TResNet-M backbone. In addition it can also be seen that the results in terms of mAP for tag-based image retrieval using different backbone variations are higher than current prior works, suggesting that our training scheme extends and may improve the quality of various model architectures. <ref type="table">Table 5</ref>: Results using alternative backbones on NUS-WIDE test set. We report the results in terms of F1(K = 3), F1(K = 5), and mAP for ZSL and GZSL tasks. Best results are in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reproduciblity</head><p>To support future research in the field, we currently work to publish our trained models and share a fully reproducible training code on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Qualitative Results</head><p>We present in <ref type="figure">figure 6</ref> additional qualitative results using our proposed method for several sample images from NUS-WIDE test set. It can be seen that in several cases the unseen tags (marked by asterisks) are ranked in the top-10. In addition, while some of the unseen tags are incorrect based on the ground truth annotation, in most cases there exists a noticeable semantic relation between these tags to the image.  <ref type="figure">Figure 6</ref>: Qualitative results showing the top-10 tags retrieved using our proposed method. Bold text represents the correct tags according to the provided ground truth in NUS-WIDE test set. Asterisks mark unseen tags.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overview of our end-to-end training scheme for multi-label zero-shot learning. A CNN model is used to extract a per-image matrix transformation A i that includes several principal directions. L rank loss pushes A i towards ranking positive labels higher than negative ones, and L reg regularizes its principal directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Zero-shot multi label classification quality as a function of ? and M = {3, 7} in the transformation matrix. (a) The mAP for the multi-label ZS task on NUS-wide test set; (b) Similar to (a) showing F 1(K = 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>mAP improvement with different number of rows (M ) in the transformation matrix compared to M = 1 for zero-shot and generalized zero-shot tasks on NUS-Wide test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>State-of-the-art comparison for ZSL and GZSL tasks on the NUS-WIDE and Open Images datasets. We report the results in terms of mAP, as well as precision (P), recall (R), and F1 score at K?{3, 5} for NUS-WIDE and K?{10, 20} for Open Images. Best results are in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="6">NUS-WIDE ( #seen / #unseen = 925/81)</cell><cell cols="7">Open-Images ( #seen / #unseen = 7186/400)</cell></row><row><cell>Method</cell><cell>Task</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell>K = 5 R</cell><cell>F1</cell><cell>mAP</cell><cell>P</cell><cell>K = 10 R</cell><cell>F1</cell><cell>P</cell><cell>K = 20 R</cell><cell>F1</cell><cell>mAP</cell></row><row><cell>CONSE [35]</cell><cell cols="7">ZSL GZSL 11.5 5.1 17.5 28.0 21.6 13.9 37.0 20.2 7.0 9.6 7.1 8.1</cell><cell>9.4 2.1</cell><cell>0.2 2.4</cell><cell>7.3 2.8</cell><cell>0.4 2.6</cell><cell cols="3">0.2 11.3 0.3 1.7 3.9 2.4</cell><cell>40.4 43.5</cell></row><row><cell>LabelEM [2]</cell><cell cols="7">ZSL GZSL 15.5 6.8 15.6 25.0 19.2 13.4 35.7 19.5 9.5 13.4 9.8 11.3</cell><cell>7.1 2.2</cell><cell>0.2 4.8</cell><cell>8.7 5.6</cell><cell>0.5 5.2</cell><cell cols="3">0.2 15.8 0.4 3.7 8.5 5.1</cell><cell>40.5 45.2</cell></row><row><cell>Fast0Tag [55]</cell><cell cols="7">ZSL GZSL 18.8 8.3 11.5 15.9 11.7 13.5 22.6 36.2 27.8 18.2 48.4 26.4</cell><cell>15.1 3.7</cell><cell cols="6">0.3 12.6 0.7 14.8 17.3 16.0 9.3 21.5 12.9 0.3 21.3 0.6</cell><cell>41.2 45.2</cell></row><row><cell>One Attention per Label [23]</cell><cell cols="7">ZS GZSL 17.9 7.9 10.9 15.6 11.5 13.2 20.9 33.5 25.8 16.2 43.2 23.6</cell><cell>10.4 3.7</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>LESA (M=10) [21]</cell><cell cols="7">ZSL GZSL 23.6 10.4 14.4 19.8 14.6 16.8 25.7 41.1 31.6 19.7 52.5 28.7</cell><cell>19.4 5.6</cell><cell cols="6">0.7 25.6 1.4 16.2 18.9 17.4 10.2 23.9 14.3 0.5 37.4 1.0</cell><cell>41.7 45.4</cell></row><row><cell>Ours (M=7)</cell><cell cols="7">ZSL GZSL 27.7 13.9 18.5 23.0 19.3 21.0 24.2 41.3 30.5 18.8 53.4 27.8</cell><cell>25.9 12.1</cell><cell cols="6">6.1 47.0 10.7 4.4 68.1 8.3 35.3 40.8 37.8 23.6 54.5 32.9</cell><cell>62.9 75.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Task</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>CONSE [35]</cell><cell>ZSL GZSL</cell><cell>11.4 23.8</cell><cell>28.3 28.8</cell><cell>16.2 26.1</cell></row><row><cell>Fast0tag [55]</cell><cell>ZSL GZSL</cell><cell>24.7 38.5</cell><cell>61.4 46.5</cell><cell>25.3 42.1</cell></row><row><cell>Deep0tag [37]</cell><cell>ZSL GZSL</cell><cell>26.5 43.2</cell><cell>65.9 52.2</cell><cell>37.8 47.3</cell></row><row><cell>Ours (M=2)</cell><cell>ZSL GZSL</cell><cell>26.3 59.0</cell><cell>65.3 60.8</cell><cell>37.5 59.9</cell></row></table><note>State-of-the-art comparison on the MS COCO dataset split into 48 seen and 17 unseen classes. We re- port the results in terms of precision (P), recall (R), and F1 score at K=3 for ZSL and GZSL tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study showing the contribution of the different components in our training scheme compared to the loss presented in Fast0Tag showing the original implementation results, our implementation results using our training framework, and the baseline on NUS-Wide test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>ZS multi-label classification results with M = 7 rows in the transformation matrix, for samples with more than 6 unseen labels, in terms of precision (P), recall (R), and F1 for top 10 retrieved labels on NUS-Wide test set.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Baseline</cell><cell>31.7</cell><cell>44.8</cell><cell>37.1</cell></row><row><cell>Our w/o SDW</cell><cell>36.2</cell><cell>51.2</cell><cell>42.4</cell></row><row><cell>Our</cell><cell>36.6</cell><cell>51.9</cell><cell>42.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Backbone Variations</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-cue zero-shot learning with strong supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14119</idno>
		<title level="m">Asymmetric loss for multi-label classification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with joint class-aware map disentangling and label correlation embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Journal of machine learning research</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zero shot learning via multi-scale manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Owechko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7112" to="7119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.07790</idno>
		<title level="m">Tim Hospedales, Tao Xiang, and Shaogang Gong. Transductive multi-label zero-shot learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01755</idno>
		<title level="m">Multi-label image recognition with multi-class attentional regions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshita</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11606</idno>
		<title level="m">Generative multi-label zero-shot learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A shared multi-attention framework for multi-label zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3464" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3174" to="3183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Kuan</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot image tagging by hierarchical semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SI-GIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SI-GIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-shot recognition using dual visualsemantic mapping paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuetan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3279" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantically consistent regularization for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6060" to="6069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep0tag: Deep multiple instance learning for zero-shot image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="242" to="255" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transductive learning for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6082" to="6091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="547" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple instance visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8247" to="8255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-label classification with label graph superimposing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12265" to="12272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quynh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention-driven dynamic graph convolutional network for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multilabel deep visualsemantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Nan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12709" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fast zeroshot image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
