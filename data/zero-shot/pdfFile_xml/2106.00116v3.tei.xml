<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effect of Pre-Training Scale on Intra-and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
							<email>m.cherti@fz-juelich.de</email>
							<affiliation key="aff0">
								<orgName type="department">Juelich Supercomputing Center</orgName>
								<orgName type="institution">Research Center Juelich Helmholtz AI Juelich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
							<email>j.jitsev@fz-juelich.de</email>
							<affiliation key="aff0">
								<orgName type="department">Juelich Supercomputing Center</orgName>
								<orgName type="institution">Research Center Juelich Helmholtz AI Juelich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effect of Pre-Training Scale on Intra-and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning aims to exploit pre-trained models for more efficient follow-up training on wide range of downstream tasks and datasets, enabling successful training also on small data. Recently, strong improvement was shown for transfer learning and model generalization when increasing model, data and compute budget scale in the pre-training. To compare effect of scale both in intra-and inter-domain full and few-shot transfer, in this study we combine for the first time large openly available medical X-Ray chest imaging datasets to reach a dataset scale comparable to ImageNet-1k. We then conduct pre-training and transfer to different natural or medical targets while varying network size and source data scale and domain, being either large natural (ImageNet-1k/21k) or large medical chest X-Ray datasets 1 . We observe strong improvement due to larger pre-training scale for intra-domain natural-natural and medical-medical transfer. For inter-domain natural-medical transfer, we find improvements due to larger pre-training scale on larger X-Ray targets in full shot regime, while for smaller targets and for few-shot regime the improvement is not visible. Remarkably, large networks pre-trained on very large natural ImageNet-21k are as good or better than networks pre-trained on largest available medical X-Ray data when performing transfer to large X-Ray targets. We conclude that high quality models for inter-domain transfer can be also obtained by substantially increasing scale of model and generic natural source data, removing necessity for large domain-specific medical source data in the pre-training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Re-using models obtained by pre-training on available source datasets to improve learning performance on upcoming target datasets is core idea behind transfer learning. It has a long history in machine learning field <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and was also employed already at the very early rise of deep neural networks in the vision domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Different architectures like AlexNet <ref type="bibr" target="#b4">[5]</ref>, OverFeat <ref type="bibr" target="#b5">[6]</ref>, and VGG <ref type="bibr" target="#b6">[7]</ref> were pre-trained on supervised tasks using ImageNet-1k, a publicly available natural image dataset that contains about 1.4 Million images and 1000 classes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. After pre-training, the resulting models were taken as off-the-shelf generic features reservoirs and re-used by re-training, or fine-tuning, on various downstream target datasets and tasks, including classification, object detection, and segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Importantly, the transfer approach allowed to improve performance on target datasets when compared to training from scratch with randomly initialized weights <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Further, it enabled to train models of good quality also on comparatively small amounts of data, in contrast to large amounts usually required for learning high-quality models when training a deep neural network from scratch.</p><p>Recent line of work on scaling laws in language modeling <ref type="bibr" target="#b11">[12]</ref> and vision demonstrated strong improvement for model's ability to generalize on unseen test data when increasing model, data, and compute budget scale during the training. In language modeling, very large Transformer networks pre-trained on very large text data have also shown very strong transfer performance on a broad range of novel tasks, compared to pre-trained models of smaller scale <ref type="bibr" target="#b12">[13]</ref>. In the same line, experimental studies on large-scale pre-training and transfer in image domain found evidence that increasing network model and data size during pre-training results in transfer performance benefits <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>.</p><p>The majority of the studies looking at the effect of pre-training scale on transfer deal with the intradomain scenario scenario, where source and target data are close to each other, often originating from the same domain, being for instance natural images. This raises the question whether the observed positive effect of larger scale will also uphold in the inter-domain transfer scenario when using different types of source and target data that are not so closely related.</p><p>To address this, we conduct a series of large-scale pre-training and transfer experiments where we vary not only ResNet model <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b14">14]</ref> and dataset size during pre-training, but also the domain of the source and the target datasets, being either natural or medical X-Ray chest images, which allows us to study effect of scale on both intra-and inter-domain transfer. To vary source data scale in natural domain, we take either large ImageNet-1k or much larger ImageNet-21k <ref type="bibr" target="#b7">[8]</ref>. To vary pre-training data scale for medical X-Ray domain, we combine here, for the first time, large openly available medical X-Ray chest imaging datasets (CheXpert <ref type="bibr" target="#b17">[17]</ref>, MIMIC-CXR <ref type="bibr" target="#b18">[18]</ref>, PadChest <ref type="bibr" target="#b19">[19]</ref>, NIH Chest X-ray14 <ref type="bibr" target="#b20">[20]</ref>) into supersets, with the largest scale comparable to ImageNet-1k. We then transfer to either natural or X-Ray image datasets as target. For transfer, we also vary the operation in either full or few-shot regime, where only few examples per class are shown to the pre-trained models during fine-tuning on a target dataset. As large-scale pre-training requires heavy computational resources, we make use of a state-of-the art supercomputer (JUWELS Booster <ref type="bibr" target="#b21">[21]</ref>) tailored for distributed training to conduct our experiments.</p><p>The results we obtain show that both intra-and inter-domain transfer benefit from larger pre-training scale. They also reveal a differentiated picture suggesting that effect of larger scale on transfer is expressed differently in intra-and inter-domain scenario and for full shot and few-shot regime. Remarkably, and also of high relevance for practice, we observe that large networks pre-trained on very large generic natural ImageNet-21k are as good or better than networks pre-trained on largest available medical domain-specific X-Ray superset data when performing full shot transfer to large X-Ray targets. This indicates that high quality models for domain specific medical X-Ray targets can be obtained by increasing scale of the network and of the generic natural image data in the pre-training, without relying on large amount of domain-specific data that is often not available in practice. In contrast to previous studies that dealt with smaller scales <ref type="bibr" target="#b22">[22]</ref>, we conclude that inter-domain transfer from natural to medical images benefits from substantially larger pre-training scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and related work</head><p>Scaling laws for generalization and transfer. Strong evidence that increasing model and data size for the training may result in steady improvement of generalization comes from language modeling studies systematically looking on the dependency of test error on model, data size, and compute budget used for training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">23]</ref>. The experiments conducted there set up scaling laws with a power law shape and show consistent further decrease of test error when further increasing model, data size, and compute budget over many orders in magnitude hand in hand. For images, a similar line of work by Henighan et al. shows a decrease of test classification top-1 error when fine-tuning on ImageNet pre-trained generative image models of increasing size <ref type="bibr" target="#b24">[24]</ref>. Those works use selfsupervised training of autoregressive models, in language modeling performed on text and in image domain on image patches, employing transformer networks as a backbone. Additional backup for this line of work comes from studies that revise the dependency of generalization performance on model, data size, and epoch number during training and report double or multi descent curves for the test error <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>. There, keeping on increasing model, data size or training time substantially also shows continuous drop in the test error pointing to generalization improvement, for instance when crossing the interpolation threshold and transiting into the over-parameterized regime by scaling up the model size.</p><p>Improving transfer by scaling up pre-training. Also improvement in transfer on downstream datasets and tasks is strongly evident from large-scale language modeling experiments. In the study by Brown et al. <ref type="bibr" target="#b12">[13]</ref>, large transformer networks in the order of hundred billions of parameters (GPT-3) pre-trained on large text datasets in the order of billions of sentences were shown to have much stronger transfer performance than smaller GPT network models, measured by the test error on different downstream tasks. The difference in transfer performance between different sized models was especially pronounced in the very low data regime when doing zero-shot or few-shot transfer with only few examples available during fine-tuning. Further systematic study on transfer improvements induced by increasing scale was done by Hernandez et al. <ref type="bibr" target="#b23">[23]</ref>, who examined scaling laws for transfer on language modeling tasks in the low-data regime, being defined as transfer using less than 10% of the available target data. The authors have shown that increasing model size in the pre-training decreases test error on the target data, emphasizing that test error improvement cannot be observed when increasing model size and training directly from scratch on the target without pre-training. It was also pointed out that the degree of proximity between the source and the target dataset plays a role when predicting effect of the scale on the transfer performance, leading to a revised version of the scaling law that took this additional dependency into account.</p><p>In the image domain, the performed studies on transfer improvement due to scale have still far less systematic character. Models and datasets used for training on images are 3-4 orders of magnitude behind those studied in language modeling <ref type="bibr" target="#b12">[13]</ref>. Recently, number of works were starting to employ datasets like ImageNet-21k <ref type="bibr" target="#b7">[8]</ref>, YFCC-100M <ref type="bibr" target="#b28">[28]</ref>, JFT-300M <ref type="bibr" target="#b29">[29]</ref> or JFT-3B <ref type="bibr" target="#b30">[30]</ref> that are much larger than standard ImageNet-1k to pre-train large network models on large data and observe the effect of scaling up on transfer. The work on Big Transfer by Kolesnikov et al. <ref type="bibr" target="#b14">[14]</ref> performed supervised classification based pre-training on ImageNet-1k, ImageNet-21k, and JFT-300M using different sized deep residual networks (ResNets <ref type="bibr" target="#b16">[16]</ref>) to study the performance of pre-trained models on transfer across different target datasets. They found consistent improvement in transfer performance when using larger models and larger data during pre-training. In the same direction, works by <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b30">30]</ref> pre-trained different sized network models on ImageNet-1k, ImageNet-21k or JFT-3B <ref type="bibr" target="#b30">[30]</ref> also observing consistent transfer improvement when scaling-up model and data size during pre-training.</p><p>Studies mentioned above deal with closely related source and target datasets containing mostly natural images data. In general, various works related to testing transfer performance across different target datasets often employ targets that are rather close to pre-training source data, like studies introducing transfer benchmarking datasets that use targets resembling mostly natural image domain <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">31]</ref> or domain specific transfer studies that stay within their given domain, for instance in medical imaging <ref type="bibr" target="#b33">[32]</ref>.</p><p>Only few studies so far attempt to measure transfer performance between datasets that are further apart, for instance natural and medical images, while systematically varying model and data size during pre-training. Work done by Raghu et al. <ref type="bibr" target="#b22">[22]</ref> has found no significant difference between models pre-trained on ImageNet-1k and models trained from scratch on target datasets containing medical images. However, it was not using datasets larger than standard ImageNet-1k or networks larger than standard ResNet-50 for pre-training. Another work examines transfer on CheXpert while varying network model size during pre-training <ref type="bibr" target="#b34">[33]</ref>. It does find slight benefit for transfer when pre-training with larger models, however it does not vary source data size in the pre-training, using only standard ImageNet-1k as a source. A recent study by Mustafa et al. builds up on Big Transfer work <ref type="bibr" target="#b14">[14]</ref> and compares transfer performance of different sized ResNet network models pre-trained on ImageNet-1k, ImageNet-21k, and JFT-300M on different medical imaging target datasets <ref type="bibr" target="#b35">[34]</ref>. Slight evidence for transfer improvement was observed when using larger model and larger dataset sizes during pre-training, with inconsistencies across conditions and datasets, where in some cases no significant benefit from larger pre-training scale was seen. The work does not compare models pre-trained on natural images data to models pre-trained on medical imaging data of different scale when measuring transfer performance on medical imaging targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments &amp; Results</head><p>In order to test the impact of model and data pre-training scale on transfer performance in full and few-shot regime under different source and target data type constellations in intra-and interdomain scenarios, we conducted experiments on pre-training different sized ResNet models on supervised classification task using either large natural image datasets ImageNet-1k or ImageNet-21k, or compositions of chest X-Ray medical imaging datasets CheXpert, MIMIC-CXR, PadChest, and NIH Chest X-ray14 (see Suppl. Tab. 3 for comprehensive list and further details of datasets). The pre-trained models were then fine-tuned on different target datasets that contain either natural or medical images. In the following, we describe the experimental procedures and outcomes in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large-scale pre-training</head><p>For pre-training, we largely followed the training procedure and used the network architecture of <ref type="bibr" target="#b14">[14]</ref>. More concretely, we pre-trained both ResNet-50x1 and ResNet-152x4 (in following R50x1 and R152x4) from <ref type="bibr" target="#b14">[14]</ref> on different natural image and medical datasets. Smaller R50x1 has 26M weight parameters, while larger R152x4 has 928M parameters. This substantial difference in size allows us to compare the effect of model scaling in the pre-training on subsequent transfer. The following describes the training procedure and hyper-parameters used for natural image and medical image domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Natural image domain</head><p>For natural images, we pre-trained the two models (R50x1 and R152x4) on ImageNet-1k (? 1.4 Millions images) and the much larger full ImageNet-21k (? 14 Millions images). For ImageNet-1k and Imagenet-21k models, we used a standard supervised classification setup with softmax as an output activation and cross entropy as a loss.</p><p>We followed the training hyper-parameters of <ref type="bibr" target="#b14">[14]</ref>, with the difference that we used stochastic gradient descent (SGD) with adaptive gradient clipping (AGC) from <ref type="bibr" target="#b36">[35]</ref>, as we found that it helps both pre-training and transfer. With AGC, we found that the default base learning rate used in <ref type="bibr" target="#b14">[14]</ref> made training unstable for the ImageNet-21k experiments, so we reduced it from 0.03 to 0.01, but otherwise we used a base learning rate of 0.03. The rest of the hyper-parameters were similar, namely we used a momentum of 0.9, 90 epochs, ? 5000 warmup iterations, a batch size of 4096, the linear learning rate rescaling rule of <ref type="bibr" target="#b37">[36]</ref>, and the standard step-wise learning rate schedule for ImageNet <ref type="bibr" target="#b14">[14]</ref>. For data augmentation, we used the standard random resized crop data augmentation as in <ref type="bibr" target="#b14">[14]</ref>. In ImageNet-1k experiments, we additionally used RandAugment <ref type="bibr" target="#b38">[37]</ref> and changed the learning rate schedule from step-wise to cosine annealing <ref type="bibr" target="#b39">[38]</ref>, as it improved the pre-training results. In order to speedup training, we used data parallel training with Horovod <ref type="bibr" target="#b40">[39]</ref>, using 256 A100 GPUs for R152x4 models and 128 A100 GPUs for R50x1 models. A pre-training on ImageNet-21k with large R152x4 takes about 81 hours using 256 GPUs, while with small R50x1 it needs about 13.5 hours to finish using 128 GPUs on JUWELS Booster supercomputer <ref type="bibr" target="#b21">[21]</ref> (see Suppl. Sec. A and Suppl. <ref type="figure" target="#fig_3">Fig. 3</ref> for more details on distributed training)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Medical image domain</head><p>For medical data, we pre-trained the two models (R50x1 and R152x4) on combinations of several medical datasets, which as supersets may contain any of the available datasets: CheXpert, MIMIC-CXR, NIH X-ray14, PadChest. We refer to those combinations as X-Ray supersets in following. The largest source X-Ray superset contains about 873K X-Ray chest radiographs (see also Suppl. Tab. 3). The medical datasets are multi-label, as each image can be associated to several diseases. The datasets are combined by finding intersecting labels (diseases) and using the intersected labels as a target. In order to substantially vary data scale for medical domain, we start with single available X-Ray datasets and progressively add other datasets into X-Ray supersets of successively growing size, which provides us with X-Ray source datasets spanning scales from small (? 200k samples) to large (? 870k samples) to perform pre-training on. For processing the datasets and extracting the labels from raw data, we used TorchXRayVision <ref type="bibr" target="#b41">[40]</ref> from the work of <ref type="bibr" target="#b33">[32]</ref>.</p><p>We followed the literature on medical datasets <ref type="bibr" target="#b33">[32]</ref> and pre-trained using a multi-label setup where we have independent binary tasks, one for each label (disease), and we used sigmoid as an output activation function and binary cross entropy as a loss for each label.</p><p>We used the same hyper-parameters as in the natural image domain, except that the base learning rate was set to 0.01 instead of 0.03, as we found that a learning rate of 0.03 led to more overfitting. We followed <ref type="bibr" target="#b33">[32]</ref> and used a center crop based on the smallest side, then resized the image to 224 ? 224. In order to combat overfitting, we used data augmentation from <ref type="bibr" target="#b33">[32]</ref>, which included random translation, random rotation, and random scaling. In addition to the data augmentation used in <ref type="bibr" target="#b33">[32]</ref>, we also do random horizontal flipping. In order to speedup training, we used data parallel distributed training with Horovod <ref type="bibr" target="#b40">[39]</ref>, using 64 A100 GPUs in all pre-training setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning and transfer evaluation</head><p>For fine-tuning, we used the BiT-HyperRule <ref type="bibr" target="#b14">[14]</ref>, which is a heuristic that selects fine-tuning hyperparameters (learning rate schedule, resolution, usage of MixUp, and total number of steps) based on training set size and image resolution. We used a batch size of 128, and an initial learning rate of 0.001 on all experiments. Like in <ref type="bibr" target="#b14">[14]</ref>, we do not use weight decay. Like in pre-training, we used stochastic gradient descent (SGD) with adaptive gradient clipping (AGC), as we found it to improve few-shot results. We used a momentum of 0.9. In each experiment, the classification head of the pre-trained model was replaced with a new classification head for the fine-tuning task. We fine-tuned all the layers of the network. For each experiment, we performed 5 independent runs with different seeds to have an estimate of the variance of the performance. We ran each fine-tuning experiment on a single A100 GPU.</p><p>As in <ref type="bibr" target="#b14">[14]</ref>, we consider two kinds of setups, few-shot setups (we used 1 or 5 or 10 or 100 or 500 examples per class) and fine-tuning on the full training set. We used CIFAR-10, CIFAR-100 <ref type="bibr" target="#b42">[41]</ref>, Flowers-102 <ref type="bibr" target="#b43">[42]</ref>, and Oxford-IIIT Pet <ref type="bibr" target="#b44">[43]</ref> for natural image fine-tuning. For medical image finetuning, we used single-label Tuberculosis <ref type="bibr" target="#b45">[44]</ref> and COVIDx <ref type="bibr" target="#b46">[45]</ref> as small X-Ray targets (? 800 and 16k samples each), and multi-label CheXpert, MIMIC-CXR, NIH or PadChest as larger X-Ray targets (magnitude order of 100k-300k samples, see also Suppl. Tab. 3). In addition, to perform few-shot experiments similar to natural domain, we employ PadChest-cl, single-label dataset derived from PadChest, where we keep only images with exactly one label (one disease). For Flowers-102 and COVIDx, since the datasets are strongly imbalanced, we used oversampling. We measure either final accuracy or mean AUC on the test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results.</head><p>The experiments allow us to look on both intra-and inter-domain transfer performance following pre-training when varying model size, source data size and source and target dataset domain. In following, we report the obtained results.</p><p>Effect of scale on intra-domain transfer. Results we obtain either for natural-natural or medicalmedical full shot transfer (Tab. 1) deliver a clear picture showing transfer improvement across target datasets when increasing pre-training model and data scale. Most consistent is the improvement due to increase of network size, while for data scale there are only few single cases where the increase does not result in improvement (e.g, when using large ResNet-152x4 on Pets for natural-natural and for PadChest for medical-medical scenario; see Supplementary for more detailed results for each transfer experiment scenario).</p><p>For few-shot transfer, we observe a differentiated picture. In line with previous work, for naturalnatural transfer we obtain strong improvement due to larger scale in the very low data regime of 1or 5-shot transfer, reaching in some cases 20% ? 30% absolute difference in test accuracy in favor of larger scale (as seen for CIFAR-100, <ref type="figure" target="#fig_0">Fig. 1a</ref>). In contrast, for medical-medical scenario, there is no evidence for few-shot transfer improvement due to larger scale (Figs. 1b, 2b, 2d; see also Supplementary for further details). Increasing number of shots and approaching full shot regime, the improvement due to scale becomes more and more visible. The observed variance is larger for few-shot transfer experiments, which may suggest less stable fine-tuning in those cases where model has to adapt to target data based on only very limited number of examples.  For small X-Ray targets (Tuberculosis and COVIDx), we do not observe such consistent improvement due to larger scale. For instance, while we see improvement due to larger data scale for small ResNet-50x1 on both small targets, the improvement is not there when increasing network size. There is also no evidence for positive effect of larger scale on few-shot transfer, neither for large nor for small X-Ray targets <ref type="figure" target="#fig_0">(Figs. 1b, 2a, 2c</ref>) Again, variance observed in few-shot regime is large, and is getting smaller and smaller when increasing number of shots and moving towards full shot transfer.</p><formula xml:id="formula_0">Target ResNet-50x1 ResNet-152x4 (natural, medical) S-MED L-MED 1K-NAT 21K-NAT S-MED L-MED 1K-NAT 21K-NAT CIFAR-10 (1) - -</formula><p>Remarkably, when further comparing intra-and inter-domain transfer performance, we observe that large ResNet-152x4 pre-trained on very large generic natural ImageNet-21k are as good or better than networks pre-trained on largest available medical domain specific X-Ray superset data when performing full shot transfer to large X-Ray targets (Tab. 1, <ref type="figure" target="#fig_2">Figs. 2a, 2b</ref>). This fits into overall picture of larger model and data pre-training scale improving transfer on larger targets observed here, as ImageNet-21k has order of magnitude larger scale than the largest X-Ray superset constructed for this study.     Larger pre-training scale improves intra-domain transfer We obtain evidence that both naturalnatural domain transfer and medical-medical domain transfer are improved when increasing model and data size during pre-training (Tab. 1; see also Suppl. material). For natural-natural transfer scenario, the improvement is evident for both full and few-shot regime. Increasing data size by using ImageNet-21k instead of ImageNet-1k or increasing network model size by using ResNet-152x4 instead of smaller ResNet-50x1 creates strong, consistent boost in transfer performance across all natural target datasets, which is in line with previous observations <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>. Improvement is especially pronounced in few-shot regime (e.g, <ref type="figure" target="#fig_0">Fig. 1a</ref>, see also Suppl. material), also adding evidence for more data efficient transfer due to larger scale. The picture is more differentiated for medical-medical transfer scenario. For full transfer regime, the improvement due to larger pre-training scale is clear and consistent across different targets (Tab. 1). In few-shot transfer however, in contrast to natural-natural scenario <ref type="figure" target="#fig_0">(Fig. 1a</ref>), there are no benefits due to larger scale <ref type="figure" target="#fig_2">(Figs. 2b, 2d</ref>). Here we have to keep in mind that both absolute data size and increase in data scale we obtain by going from ImageNet-1k to ImageNet-21k (14M samples) in natural pre-training is much stronger than what we achieve in medical pre-training, going from one of X-Ray datasets to the largest combined X-Ray superset that still has much smaller data volume (? 870k samples) than ImageNet-21k. This difference in data scale may also explain the observed differences in few-shot regime, while we also cannot rule out that domain type (natural or medical) could as well play an important role in determining how transfer is affected by pre-training scale.</p><p>Larger pre-training scale improves inter-domain transfer for larger targets. In contrast to intradomain transfer, where natural-natural or medical-medical source and target datasets are closely related, in natural-medical inter-domain transfer the source and target are much further apart. It is therefore not trivial that effect of pre-training scale brings similar or any improvement in this case as well. Strong discrepancy between source and target may render transfer ineffective, as it was indeed observed in previous studies on natural-medical transfer done on smaller scales <ref type="bibr" target="#b22">[22]</ref>. In contrast to these studies, we do find here significant positive effect of larger pre-training scale on inter-domain natural-medical full shot transfer for larger medical targets (Tab. 1; see also Suppl. material). The transfer improvement is clearly expressed when increasing both model and natural image data pre-training scale across all large X-Ray targets.</p><p>Another remarkable finding arises when further comparing performance of intra-domain medicalmedical and inter-domain natural-medical transfer. The largest ResNet-152x4 pre-trained on the largest generic natural ImageNet-21k turns out to be as good or in many cases better than any network pre-trained on the largest available medical domain specific X-Ray superset data when performing full shot transfer to large X-Ray targets (Tab. 1, <ref type="figure" target="#fig_2">Figs. 2a, 2b)</ref>. The finding indicates that by substantially increasing model and generic source natural image data scale during pre-training, we can obtain models for transfer to medical domain-specific X-Ray images that match or even outperform models pre-trained with large amounts of domain-specific X-Ray data, which may be often not available in practice. The observation fits into overall outcome that strongly increasing model and data pre-training scale improve full shot transfer on larger targets -ImageNet-21k scale (14M samples) is more than order of magnitude larger than scale of the largest X-Ray superset we have constructed for this study (? 0.87M samples).</p><p>In contrast to full shot transfer on large X-Ray targets, for smaller targets, COVIDx and Tuberculosis, neither full transfer nor few-shot transfer show improvement when increasing model and natural source data scale during pre-training ( <ref type="figure" target="#fig_2">Fig. 2d</ref>, see also Suppl. material). We also see no evidence for positive effect of larger scale on few-shot transfer for large PadChest-cl target <ref type="figure" target="#fig_0">(Figs. 1b, 2a</ref>).</p><p>Thus, we again obtain differential picture of scaling benefits on transfer. Further scaling up of model and data during pre-training may homogenize this picture and make scaling benefit look more consistent through different conditions, showing improvement for smaller targets and for few-shot transfer. For instance, we could not use substantially larger datasets like JFT-300M or JFT-3B <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b30">30]</ref> which are proprietary and not available publicly. Additionally, computational budget was here not enough to experiment with networks larger than ResNet-152x4. However, there may be also fundamental limitations prohibiting transfer improvement no matter how large pre-training scale may become that are due to strong incompatibility between source and target domains. There is some evidence from language modeling studies that hints on such fundamental limitations for transfer improvement on target datasets far from source when doing straightforward scaling without further changes in model architecture. For instance, the work by Hendrycks et al. <ref type="bibr" target="#b47">[46]</ref> finds no improvement of transfer when increasing size of large Transformer networks pre-trained on very large conventional text datasets and fine-tuning those on a specific target dataset far apart from the source, containing mathematical text tasks of advanced difficulty.</p><p>Limitations of the current study. There are several limitations of the current study that impede more general conclusions about effect of pre-training scale on intra-and inter-domain transfer from the observations made in this study. In the conducted transfer experiments, we made use of a heuristic hyper-parameter selection rule -BiT-HyperRule, as introduced in <ref type="bibr" target="#b14">[14]</ref> -that determines pre-training hyper-parameters directly from target datasets on which transfer is to be performed. This rule may be heavily biased towards transfer on natural image datasets, as those were the targets used in the original study. If modifying the rule to take also target domain -natural or medical -into account, the derived hyper-parameters may serve a much better basis for fine-tuning during transfer. In general, performing hyper-parameter tuning for training procedure can strongly boost performance <ref type="bibr" target="#b48">[47]</ref>, and this is no different for transfer procedure. Therefore, it cannot be excluded that performing hyper-parameter tuning for each transfer task would alter the effect of larger pre-training scale on transfer. Hyper-parameter tuning would however also impose further cost on transfer that is avoided by employing the hyper-rule.</p><p>We also have not explored other backbone network architectures except the standard ResNet. Although ResNet has proven itself a versatile network architecture for dealing with various vision tasks, it cannot be ruled out that while for instance the inductive bias inherent to its convolutional design is well suited for working on natural image statistics with strong local spatial correlations, it may be less suited for providing good basis for generalization when dealing with other types of image signals.</p><p>Scaling up ResNet architecture may thus be a viable strategy to improve generalization capability on natural image data, while other, more generic architectures, may be required to benefit from scaling in the same way across more diverse data types. We also did not experiment with larger datasets than ImageNet-21k -as those are still mostly proprietary and were not publicly available, as it is the case for JFT-300M <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b14">14]</ref>. For medical imaging domain, we could not experiment with increasing data scale substantially, as the amount of openly available X-Ray chest data is currently still limited. Finally, we studied the dependence of transfer improvement on pre-training scale exclusively in supervised classification problem setting. Promising work is also done on pre-training in unsupervised fashion with unlabeled data <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b51">50]</ref>, where benefits of scaling up pre-training for transfer may turn out to be as well substantial.</p><p>Conclusion and outlook. To summarize, we presented here evidence that substantially increasing model and data scale in the pre-training provides benefits for both intra-and inter-domain transfer across various target datasets from natural and medical X-Ray image domain. The effect of pretraining scale on transfer performance depends on transfer scenario. Transfer improvement due to larger pre-training scale was found to be substantial in natural-natural or medical-medical, intradomain transfer scenarios where source and target datasets were closely related, being especially strongly pronounced in the few-shot transfer regime for natural-natural case and concentrated in fullshot scenario in medical-medical case. For natural-medical inter-domain transfer, clear positive effect of larger pre-training scale was found for full shot transfer on large X-Ray targets. On small X-Ray targets and for few-shot transfer regime, no clear inter-domain transfer improvements were observed. Remarkably, the largest ResNet-152x4 network pre-trained on very large generic natural ImageNet-21k matched or even outperformed networks pre-trained on largest medical domain-specific X-Ray superset data combined for this study when performing full shot transfer to large X-Ray targets. This is relevant for the practice, as large amount of medical domain-specific data is often not available for pre-training. Here we show that high quality models for large X-Ray targets can be also obtained by substantially increasing pre-training model and generic natural image source data scale instead, obliterating need for large domain-specific data.</p><p>The study offers different follow-up directions. One of these are experiments with larger scale both for network and data size, for instance going beyond ImageNet-21k, or combining in the pre-training different source datasets that may contain both natural and medical images. This may also include experiments with scaling up other architectures than ResNet. Another direction to study effect of scale is to employ various unsupervised learning strategies for pre-training <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b51">50]</ref> instead of supervised learning. Yet another fruitful path is to provide a measure of source and target domain similarity and to experiment with more than two distinct domains, varying systematically relatedness between different source and target data. First steps in this direction for language modeling was already undertaken in <ref type="bibr" target="#b23">[23]</ref>. Following these directions would pave the path towards scaling laws for transfer in the image domain, taking into account different pre-training regimes and affinity between source and target domains, to enable systematic prediction of transfer performance and improvement due to increase of pre-training scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader and Social Impact</head><p>Our work aims on advancing transfer learning, which can make learning algorithms perform better and more efficient by re-using models already pre-trained on various tasks and therefore requiring less compute and data to learn solutions for other relevant tasks. The approach to improve transfer learning by increasing scale of the pre-training is generic and has impact far beyond vision domain, for instance in language modeling, and is not bound to any specific application. As any generic method, it can be therefore applied to enhance technologies for sensitive applications, for instance in health domain or in public surveillance, that may have both strong positive and negative social impact, depending on policies introduced on their usage. Special care should be taken about applications in clinical domain where further development of diagnostic tools based on data driven machine learning should be accompanied by a broad panel of experts from corresponding domains. The method depends on computationally heavy large-scale pre-training that is energy demanding on the one hand. On the other hand, it contains a promise to pay off the energy budget put into training by obtaining generic models that can be very efficiently adapted to a large range of problems via transfer, saving computational and energy costs that would otherwise incur for their solution from scratch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Scaling and training time</head><p>Here, we report scaling behavior during large-scale pre-training for ResNet networks we used in the experiments.</p><p>We performed scaling experiments to assess the scalability of data parallel training distributed across many GPUs on multiple nodes using Horovod. The efficiency in <ref type="figure" target="#fig_3">Figure 3b</ref> (upper part of the figure with percentages) is computed using the following formula: E(N ) = 100 ? T (N ) N ?T <ref type="bibr" target="#b0">(1)</ref> . T (N ) is the total measured throughput in Im/s for N GPUs. The best achievable efficiency, when scaling is perfect, is 100%.</p><p>We also provide the raw throughput (Im/s) numbers in <ref type="figure" target="#fig_3">Figure 3a</ref> and Tab. 2. On 1024 GPUs, we achieve an efficiency of ? 93.7% with single precision (FP32). To make sure distributed training is stable, we check the end accuracy of full training for each number of GPUs to reassure we reach target accuracy acceptable for standard ImageNet-1k Top-1 and Top-5 results.</p><p>Achieved scaling on JUWELS Booster allows to perform full pre-training on ImageNet-21k with large R152x4 in about 81 hours using 256 GPUs. For small R50x1, full training needs about 13.5 hours to finish using 128 GPUs.    Dataset Size Source pre-training Natural Images ImageNet-1k <ref type="bibr" target="#b7">[8]</ref> 1.4M images, 1000 classes ImageNet-21k <ref type="bibr" target="#b7">[8]</ref> 14M images, 21842 classes X-Ray Chest Imaging CheXpert <ref type="bibr" target="#b17">[17]</ref> 224K radiographs of 65K patients, 14 classes NIH Chest X-ray14 <ref type="bibr" target="#b20">[20]</ref> 112K radiographs of 32K patients, 14 classes PadChest <ref type="bibr" target="#b19">[19]</ref> 160K radiographs of 67K patients, 19 classes MIMIC-CXR <ref type="bibr" target="#b18">[18]</ref> 377K radiographs of 65K patients, 14 classes Total X-Ray images 873K chest radiographs, 229K patients Target transfer Natural Images CIFAR-10, 100 <ref type="bibr" target="#b42">[41]</ref> 60K images, 10,100 classes Oxford Flowers-102 <ref type="bibr" target="#b43">[42]</ref> 8K images, 102 classes Oxford-IIIT Pet <ref type="bibr" target="#b44">[43]</ref> 7.3K images, 37 classes X-Ray Chest Imaging PadChest <ref type="bibr" target="#b19">[19]</ref> 160K radiographs of 67K patients, 19 / 27 classes COVIDx <ref type="bibr" target="#b46">[45]</ref> 16K radiographs, 15K patients, 2 / 3 classes Tuberculosis <ref type="bibr" target="#b45">[44]</ref> 800 radiographs, 800 patients, 2 classes All datasets employed in our experiments are publicly available and can be obtained following links in the Tab. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Further transfer results</head><p>Here, we present more detailed results of transfer experiments described in the main document.</p><p>For medical X-Ray targets, we provide tables reporting transfer performance (Tabs. <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9)</ref> listing each source X-Ray dataset and supersets used for pre-training, as outlined in the experiments description in the main document. Test Accuracy    <ref type="table">Table 6</ref>: Intra-domain transfer using different sized medical X-Ray source data for pre-training with different sized ResNets, target MIMIC-CXR Mean AUC metric. "+" indicates addition into a successively larger source superset. Clear transfer improvement is evident by scaling the model size. Using a superset containing CheXpert and PadChest improves the results, but adding NIH does not or does very little, this could be explained by the fact that NIH is the smallest dataset among the medical pre-training datasets, and a larger increase in the superset would be needed to substantially improve the transfer results, as it has been observed in transfer results that were obtained using models pre-trained on much larger natural data.  <ref type="table">Table 7</ref>: Intra-domain transfer using different sized medical X-Ray source data for pre-training with different sized ResNets, target CheXpert Mean AUC metric. "+" indicates addition into a successively larger source superset. Clear transfer improvement is evident by scaling the model size. Using a superset containing PadChest and MIMIC CXR improves the results, adding NIH does not lead to further improvement. This could be explained by the fact that NIH is the smallest dataset among the medical pre-training datasets, and a larger increase in the superset would be needed to substantially improve the transfer results, as it has been observed in transfer results that were obtained using models pre-trained on much larger natural data.  <ref type="table">Table 8</ref>: Intra-domain transfer using different sized medical X-Ray source data for pre-training with different sized ResNets, target PadChest Mean AUC metric. "+" indicates addition into a successively larger source superset. Clear transfer improvement is evident by scaling the model size. Improvement by increasing data size is not evident and only happens using the small R50x1 model and a superset containing CheXpert and MIMIC, adding NIH (which is smaller compared to CheXpert and MIMIC) the superset does not help further. This indicates that larger increase in the superset may be necessary to further improve the transfer results, as it has been observed when using models pre-trained on much larger natural data.  <ref type="table">Table 9</ref>: Intra-domain transfer using different sized medical X-Ray source data for pre-training with different sized ResNets, target NIH (Mean AUC metric). "+" indicates addition into a successively larger source superset. Clear transfer improvement is evident by scaling the model size. We also observe transfer improvement by scaling data size, however the improvement seems to flatten. Since the transfer results using pre-trained models on larger natural data show a better performance, this indicates that a larger superset scale may be necessary to further improve transfer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Code and Data availability</head><p>Repository containing code used for running experiments and producing figures in this study can be found at https://github.com/SLAMPAI/large-scale-pretraining-transfer. All datasets used in the study are openly available and are listed together with references to the original work in the <ref type="table" target="#tab_7">Table 3</ref>. Further details on the usage of the datasets in the conducted experiments are also provided in the linked repository.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Few-and full shot transfer performance on a natural and a medical X-Ray target when varying model and data scale in pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Few-shot and full shot transfer performance on medical X-Ray targets of different size for intra-and inter-domain scenarios, medical-medical or natural-medical. Each color represents a combination of model and data scale during pre-training.4 Discussion &amp; ConclusionOur observations of the transfer performance dependency on the pre-training model and data scale and on source and target domain alignment suggest that both intra-and inter-domain transfer benefit from larger pre-training scale. The effect of pre-training scale depends however on transfer conditions, revealing a differentiated picture of when larger scale may lead to transfer improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Distributed training for R152x4, scaling behavior on JUWELS Booster using A100 GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Few-shot and full shot transfer performance on COVIDx dataset when varying model, source data size and domain in pre-training. In the full shot transfer, improvement due to model and data scale is evident when pre-training on X-Ray chest imaging source data. In few-shot regime, no transfer improvement due to larger model or data size is observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>94.26 ? 0.05 95.78 ? 0.09 --96.93 ? 0.05 97.82 ? 0.07 CIFAR-100 (1) --75.90 ? 0.05 82.47 ? 0.21 --83.90 ? 0.09 88.54 ? 0.14 Flowers-102 (1) --74.94 ? 0.99 98.21 ? 0.22 --89.41 ? 0.25 99.49 ? 0.08 Pets (1) --85.21 ? 0.58 87.23 ? 0.18 --93.32 ? 0.30 93.21 ? 0.14 COVIDx (1) 68.50 ? 0.18 76.05 ? 0.21 76.30 ? 1.30 78.35 ? 1.63 78.65 ? 0.84 83.00 ? 1.16 78.10 ? 0.95 78.90 ? 0.49 Tuberculosis (1) 79.83 ? 0.45 81.65 ? 0.91 79.83 ? 1.50 83.47 ? 0.83 79.01 ? 0.45 90.91 ? 0.83 81.49 ? 2.23 80.83 ? 2.51 MIMIC CXR<ref type="bibr" target="#b1">(2)</ref> 84.17 ? 0.03 86.38 ? 0.03 85.41 ? 0.10 86.82 ? 0.10 87.63 ? 0.04 88.00 ? 0.03 86.85 ? 0.06 87.79 ? 0.13 CheXpert<ref type="bibr" target="#b1">(2)</ref> 82.10 ? 0.07 86.66 ? 0.05 84.83 ? 0.14 86.60 ? 0.14 84.92 ? 0.07 87.82 ? 0.03 86.82 ? 0.06 87.77 ? 0.07 PadChest<ref type="bibr" target="#b1">(2)</ref> 68.06 ? 0.24 68.14 ? 0.21 76.72 ? 0.27 80.99 ? 0.22 75.91 ? 0.12 75.23 ? 0.17 79.59 ? 0.17 83.94 ? 0.19 PadChest-Cl<ref type="bibr" target="#b1">(2)</ref> 73.01 ? 0.13 78.<ref type="bibr" target="#b34">33</ref> ? 0.08 80.17 ? 0.17 82.03 ? 0.17 81.79 ? 0.07 82.68 ? 0.05 82.55 ? 0.05 84.02 ? 0.24 NIH (2) 70.11 ? 0.15 74.21 ? 0.57 75.53 ? 0.47 81.02 ? 0.57 77.95 ? 0.13 78.95 ? 0.13 79.82 ? 0.38 82.80 ? 0.41</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Varying model and data pre-training scale for intra-and inter-domain transfer. Pre-training is performed with either natural or medical source data (ordered by increasing scale) being one of large XBold indicates best transfer performance for a fixed network size and pinpoints the effect of data scale on transfer. Italics indicates transfer performance with no significant difference between data scale. Red indicates best overall performance for a given target.</figDesc><table><row><cell>-Ray</cell></row></table><note>Effect of scale on inter-domain transfer. Here we transfer on either small or large medical X-Ray chest imaging targets after pre-training on natural sources of different size, ImageNet-1k (1.4M samples) or much larger ImageNet-21k (14M samples). For all large X-Ray targets (NIH, CheXpert, PadChest, PadChest-cl or MIMIC-CXR) we observe clear full-shot transfer improvement due to larger pre-training scale (Tab. 1). The effect is consistent for both model and data scale across large X-Ray targets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Supplementary: Effect of Pre-Training Scale on Intra-and Inter-Domain Full and Few-Shot Transfer Learning for Naturaland Medical X-Ray Chest Images</figDesc><table><row><cell>A Distributed Training</cell></row><row><cell>A.1 JUWELS Booster Supercomputer</cell></row><row><cell>Installed in November 2020, JUWELS Booster [21] features 936 compute nodes that host four</cell></row><row><cell>NVIDIA A100 GPUs each, providing 3744 GPUs in total. The installed A100 Tensor Core</cell></row><row><cell>GPUs (40 GB) provide 19.5 TFLOP/s of FP64 TC computing performance each. The GPUs are</cell></row><row><cell>hosted by AMD EPYC 7402 CPUs with 2 ? 24 cores (SMT-2) per node, clocked with 2.8 GHz. Each</cell></row><row><cell>node is diskless and is equipped with 512 GB of RAM. The network of JUWELS Booster is based</cell></row><row><cell>on Mellanox HDR200 InfiniBand, with four Mellanox ConnectX 6 devices per node, each providing</cell></row><row><cell>200 Gbit/s bandwidth per direction.</cell></row><row><cell>The NVIDIA A100 GPUs installed into JUWELS Booster reach peak efficiency of</cell></row><row><cell>48.75 GFLOP/(s W) when utilizing the FP64 Tensor Cores. This makes JUWELS Booster rank</cell></row><row><cell>highest in the Green500 list of November 2020 as the most energy efficient supercomputer among</cell></row><row><cell>the first 100 machines of the Top500 list with 25 GFLOP/(s W).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Scaling behavior in Im/s of ImageNet-1k training using ResNet-152x4 architecture from<ref type="bibr" target="#b14">[14]</ref> with batch size 128. For each GPU, one MPI process is assigned. Computations were done on up to 256 nodes on JUWELS Booster. Throughput performance during training is reported for single precision mode (FP32). The corresponding speedup is provided relative to reference training with 1 GPU. Note that the measured Im/s throughput includes I/O.</figDesc><table><row><cell>#GPUs</cell><cell>Im/s</cell><cell>speedup</cell></row><row><cell>1</cell><cell>129.14</cell><cell>1.00</cell></row><row><cell>4</cell><cell>508.00</cell><cell>3.93</cell></row><row><cell>8</cell><cell>1009.30</cell><cell>7.82</cell></row><row><cell>16</cell><cell>2023.78</cell><cell>15.67</cell></row><row><cell>32</cell><cell>4029.69</cell><cell>31.21</cell></row><row><cell>64</cell><cell>8022.31</cell><cell>62.12</cell></row><row><cell>128</cell><cell>15959.86</cell><cell>123.59</cell></row><row><cell>256</cell><cell>31758.59</cell><cell>245.93</cell></row><row><cell>512</cell><cell>62496.35</cell><cell>483.96</cell></row><row><cell>1024</cell><cell>124003.59</cell><cell>960.26</cell></row><row><cell cols="2">B Additional details on experimental results</cell><cell></cell></row></table><note>B.1 Datasets employed in experiments.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Datasets used as source for pre-training and target for transfer. The url of each dataset we will use is provided below.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Few-shot and full shot transfer performance on target datasets when varying model size and dataset size in pre-training. Transfer improvement due to model and source data size is evident, especially strongly pronounced in few-shot regime.</figDesc><table><row><cell></cell><cell>90 100</cell><cell>R50x1 on ImageNet-1k R50x1 on ImageNet-21k R152x4 on ImageNet-1k R152x4 on ImageNet-21k</cell><cell></cell><cell></cell><cell></cell><cell>90 100</cell><cell cols="2">R50x1 on ImageNet-1k R50x1 on ImageNet-21k R152x4 on ImageNet-1k R152x4 on ImageNet-21k</cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy</cell><cell>60 70</cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy</cell><cell>60 70</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell>1 Shot</cell><cell cols="2">5 Shot Shots during transfer 10 Shot</cell><cell>Full Shot</cell><cell>30</cell><cell>1 Shot</cell><cell>5 Shot</cell><cell cols="2">10 Shot Shots during transfer 100 Shot</cell><cell>500 Shot</cell><cell>Full Shot</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) Flowers-102</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) CIFAR-10</cell></row><row><cell cols="4">30 40 50 60 70 80 Figure 4: 1 Shot 90</cell><cell>5 Shot</cell><cell cols="3">10 Shot Shots during transfer 100 Shot</cell><cell cols="2">500 Shot</cell><cell>Full Shot</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Few-shot and full shot transfer performance on Tuberculosis dataset when varying model, source data size and domain in pre-training. In the full shot transfer, improvement due to model and data scale is evident when pre-training on X-Ray chest imaging source data. In few-shot regime, no transfer improvement due to larger model or data size is observed. Few-shot and full shot transfer performance on Tuberculosis dataset when pre-training with different model sizes on different sources (natural or medical datasets) of various sizes. In natural-medical scenario (a), no transfer improvement due to model or data scale is evident. In medical-medical scenario (b), larger model and data size lead to transfer improvement in full shot regime, without benefits in few-shot mode. Full shot transfer performance on target datasets when varying model and source data size, taking the smallest and largest pre-training datasets available for each domain.</figDesc><table><row><cell>86 88</cell><cell>Pre-training dataset ImageNet-1k ImageNet-21k</cell><cell>80 82 84</cell><cell cols="2">Pre-training dataset CheXpert-MIMIC-NIH-PadChest CheXpert ImageNet-21k ImageNet-1k</cell><cell>82 84</cell></row><row><cell cols="4">1 Shot 10 Shot Shots during transfer R50x1 on ImageNet-21k 30 40 50 60 70 80 90 Test Accuracy Figure 6: 1 Shot 5 Shot 50 60 70 80 90 Test Accuracy Model R50x1 on ImageNet-1k R152x4 on ImageNet-1k R152x4 on ImageNet-21k imagenet21k_bit152x4 (a) Tuberculosis, natural sources 5 Shot 100 Shot R152x4 Pre-training Model architecture 76 78 82 84 Test Accuracy 74 76 78 Test Accuracy 80 (a) CIFAR-100 R50x1 70 72 68 Figure 7: R50x1 Figure 8:</cell><cell cols="2">10 Shot Shots during transfer Full Shot 1 Shot 50 60 70 80 90 Test Accuracy Model R50x1 on CheXpert 100 Shot 5 Shot R50x1 on CheXpert-MIMIC-NIH-PadChest Full Shot 10 Shot 100 Shot Shots during transfer R152x4 on CheXpert R152x4 on CheXpert-MIMIC-NIH-PadChest (b) Tuberculosis, medical sources 78 80 Test Mean AUC R152x4 Pre-training Model architecture 76 74 (b) COVIDx R50x1 Pre-training Model architecture (c) PadChest-Cl</cell><cell>Full Shot Pre-training dataset ImageNet-1k ImageNet-21k CheXpert CheXpert-MIMIC-NIH R152x4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Intra-and inter-domain transfer using natural ImageNet-1k and ImageNet-21k for pre-training with different sized ResNets (1) -Top-1 Acc [%] metric; (2) -mean AUC metric. Bold indicates best transfer performance for a fixed network size and pinpoints the effect of data scale on transfer. Italics indicates transfer performance with no significant difference between data scale. Red indicates best overall performance for a given target. Clear transfer improvement emerges for natural-natural scenario due to both model and data scale. For natural-medical scenario the positive effect of larger scale is consistently given for larger targets, but not for smaller ones. For instance, for very small Tuberculosis target, larger data scale improves transfer for small ResNet-50x1, while larger model scale does not lead to any transfer improvement.</figDesc><table><row><cell>Target Domain</cell><cell>Dataset</cell><cell>1K</cell><cell>ResNet-50x1 21K</cell><cell>ResNet-152x4 1K 21K</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at: https://github.com/SLAMPAI/large-scale-pretraining-transfer Preprint. Under review.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We would like to express gratitude to all the people who are working on making code, models and data publicly available, advancing community based research and making research more reproducible. Special thanks go to creators and maintainers of open available X-Ray medical imaging datasets that also enabled our research, some of those gathered under difficult circumstances of the COVID-19 pandemics. The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputers JUWELS, JUWELS Booster at J?lich Supercomputing Centre (JSC). We also acknowledge computing resources from the Helmholtz Data Federation and further computing time provided on supercomputer JUSUF in frame of offer for epidemiology research on COVID-19 by JSC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Direct transfer of learned information among neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lorien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candace</forename><forename type="middle">A</forename><surname>Mostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ace</forename><forename type="middle">A</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="584" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Factors of transferability for a generic convnet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Yoshua Bengio and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Factors of influence for transfer learning across diverse appearance domains and task types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13318</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<imprint>
			<publisher>Mateusz Litwin</publisher>
			<pubPlace>Scott</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<title level="m">Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviana</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">R</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ying</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Padchest: A large chest x-ray image dataset with multi-label annotated reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelia</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose-Maria</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>De La Iglesia-Vay?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101797</biblScope>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mohammadhadi Bagheri, and Ronald Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juelich Supercomputing</forename><surname>Center</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Juwels Booster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Supercomputer</surname></persName>
		</author>
		<ptr target="https://apps.fz-juelich.de/jsc/hps/juwels/configuration.html#hardware-configuration-of-the-system-name-booster-module" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3347" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scaling laws for transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01293</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hallacy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14701</idno>
		<title level="m">John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling</title>
		<meeting><address><addrLine>Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconciling modern machinelearning practice and the classical bias-variance trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15849" to="15854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep double descent: Where bigger models and more data hurt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Kaplun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamini</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Triple descent and the two kinds of overfitting: where &amp; why do they appear</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biroli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3058" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02968</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<imprint>
			<pubPlace>Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta-Dataset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the limits of cross-domain generalization in automated x-ray prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Paul Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Hashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupert</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Bertrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="136" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Chextransfer: Performance and parameter efficiency of imagenet models for chest x-ray interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oishi</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Health, Inference, and Learning, CHIL &apos;21</title>
		<meeting>the Conference on Health, Inference, and Learning, CHIL &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Supervised transfer learning at scale for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05913</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">High-performance largescale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Del</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">TorchXRayVision: A library of chest X-ray datasets and models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Paul Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Viviano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupert</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Hashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Bertrand</surname></persName>
		</author>
		<ptr target="https://github.com/mlmed/torchxrayvision,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sema</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Y?-Xi?ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu-Xuan</forename><surname>W?ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thoma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="475" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Covid-net: a tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zhong Qiu Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19549</biblScope>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m">Measuring mathematical problem solving with the math dataset</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Measuring the effects of data parallelism on neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 33, 2020. R50x1 on CheXpert R50x1 on CheXpert-MIMIC R50x1 on CheXpert-MIMIC-NIH R50x1 on CheXpert-MIMIC-NIH-PadChest R152x4 on CheXpert R152x4 on CheXpert-MIMIC R152x4 on CheXpert-MIMIC-NIH R152x4 on CheXpert-MIMIC-NIH-PadChest R50x1 on ImageNet-1k R50x1 on ImageNet-21k R152x4 on ImageNet-1k R152x4 on ImageNet-21k</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<title level="m">R50x1 on CheXpert R50x1 on CheXpert-MIMIC R50x1 on CheXpert-MIMIC-NIH R50x1 on CheXpert-MIMIC-NIH-PadChest R152x4 on CheXpert R152x4 on CheXpert-MIMIC R152x4 on CheXpert-MIMIC-NIH R152x4 on CheXpert-MIMIC-NIH-PadChest R50x1 on ImageNet-1k R50x1 on ImageNet-21k R152x4 on ImageNet-1k R152x4 on ImageNet-21k</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Natural CIFAR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Intra-domain transfer using different sized medical X-Ray source data for pre-training with different sized ResNets (1) -Top-1 Acc [%] metric; (2) -mean AUC metric</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Target ResNet-50x1 ResNet-152x4</note>
	<note>+&quot; indicates addition into a successively larger source superset. Clear transfer improvement is evident due to larger model and data scale across different targets</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
