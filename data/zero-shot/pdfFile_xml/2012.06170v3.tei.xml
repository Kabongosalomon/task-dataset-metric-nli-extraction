<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Yarlagadda</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyank</forename><surname>Jyoti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamgopal</forename><surname>Karthik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Gandhi</surname></persName>
						</author>
						<title level="a" type="main">ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the ViNet architecture for audiovisual saliency prediction. ViNet is a fully convolutional encoder-decoder architecture. The encoder uses visual features from a network trained for action recognition, and the decoder infers a saliency map via trilinear interpolation and 3D convolutions, combining features from multiple hierarchies. The overall architecture of ViNet is conceptually simple; it is causal and runs in real-time (60 fps). ViNet does not use audio as input and still outperforms the state-of-the-art audio-visual saliency prediction models on nine different datasets (three visual-only and six audio-visual datasets). ViNet also surpasses human performance on the CC, SIM and AUC metrics for the AVE dataset, and to our knowledge, it is the first model to do so. We also explore a variation of ViNet architecture by augmenting audio features into the decoder. To our surprise, upon sufficient training, the network becomes agnostic to the input audio and provides the same output irrespective of the input. Interestingly, we also observe similar behaviour in the previous state-of-the-art models [1] for audio-visual saliency prediction. Our findings contrast with previous works on deep learning-based audio-visual saliency prediction, suggesting a clear avenue for future explorations incorporating audio in a more effective manner. The code and pre-trained models are available at https://github.com/samyak0210/ViNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Video saliency prediction focuses on understanding and modeling human visual attention (HVA) while viewing a dynamic scene (determining where and what people pay attention to given visual stimuli). HVA empowers primates to analyze/interpret the complex surroundings rapidly, and naturally, we would like to extend these abilities to machines/robots. For instance, a robot that orients its eyes like humans gives impressions of an intelligent behaviour <ref type="bibr" target="#b1">[2]</ref>. Moreover, it may allow the robot to orient towards regions of the visual scene that are likely to be relevant. Upon compiling the ground truth regarding where viewers gaze in the scene via eye-tracking hardware, saliency prediction (SP) aims to mimic HVA given a novel video computationally. Previous works have shown that SP is valuable in a variety of applications like human-robot interaction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, robotic camera control <ref type="bibr" target="#b1">[2]</ref>, motion tracking <ref type="bibr" target="#b6">[7]</ref>, stream compression <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, video captioning <ref type="bibr" target="#b9">[10]</ref>, automated cinematic editing <ref type="bibr" target="#b10">[11]</ref>, etc.</p><p>Video SP models primarily employ visual information to predict gaze. Larger datasets like DHF1K <ref type="bibr" target="#b11">[12]</ref> discard audio during ground truth collection, and ask users to look at silent videos. End-to-end deep saliency models are then trained <ref type="bibr" target="#b0">1</ref> CVIT, KCIS, International Institute for Information Technology, Hyderabad samyak.j@research.iiit.ac.in <ref type="bibr" target="#b1">2</ref> University of Canberra <ref type="figure">Fig. 1</ref>. The core of our approach is a strong visual-only model ViNet. Here, we compare ViNet (third column) with state-of-the-art UNISAL model <ref type="bibr" target="#b12">[13]</ref> (fourth column). Note that ViNet better captures the action, while UNISAL focuses on objectness. In this example, ViNet focuses on the region being drawn, whereas UNISAL focuses on the completed portion. Best viewed in color and under zoom. using only visual information. State-of-the-art video SP models largely depend on Long Short-Term Memory (LSTM) networks to encode temporal dependencies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. These models build on image-based saliency and aggregate frame-wise prediction using an LSTM. Since both spatial decoding and temporal aggregation are performed separately, LSTM models cannot collectively leverage Spatio-temporal information, shown to be beneficial for video SP <ref type="bibr" target="#b15">[16]</ref>.</p><p>To this end, we propose a novel fully convolutional encoder-decoder architecture called ViNet for visual saliency detection. ViNet takes a set of frames as input and predicts a saliency map for the last frame. Following the methodology adopted in <ref type="bibr" target="#b15">[16]</ref>, it then employs a sliding window approach to predict saliency for the entire video. ViNet takes features learned from an action recognition network from multiple hierarchies, fuses them in a UNet <ref type="bibr" target="#b16">[17]</ref> like fashion, and outputs a saliency map using trilinear interpolations and 3D convolutions. The strength of ViNet is that it only comprises commonly used components, resulting in a minimal and conceptually simple model which is easy to train and interpret. ViNet is causal, runs in real-time, and surpasses the state-of-the-art on three popular vision-only saliency prediction datasets (a motivating example is illustrated in <ref type="figure">Fig. 1</ref>). At the time of submission, ViNet is also the topranked model on the private test-set of DHF1K, the most diverse video saliency prediction benchmark. Interestingly, ViNet also achieves state-of-the-art results on six audiovisual saliency datasets without using any audio information. More fundamentally, discarding audio information contrasts with our real-life behaviour, where we simultaneously perceive visual and audio modalities. Cognitive studies confirm that auditory and visual cues are correlated and jointly contribute to human attention <ref type="bibr" target="#b17">[18]</ref>. Coutrot et al. <ref type="bibr" target="#b18">[19]</ref> collect the human gaze on the same set of videos with and without the original soundtrack and observe that the soundtrack significantly affects the attention models in human perception, even when using a monophonic stimuli <ref type="bibr" target="#b18">[19]</ref>. Consequently, recent efforts have explored multi-modal (audio-visual) video SP <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b0">[1]</ref>, and claim audio as a strong cue for SP.</p><p>Consequently, we experiment with an audio-visual saliency prediction model obtained by augmenting ViNet with an audio branch. The resulting architecture called AViNet is end-to-end trainable and uses pre-trained audio features from SoundNet <ref type="bibr" target="#b20">[21]</ref>. We explore two fusion strategies, similar to <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[20]</ref> i.e. simple concatenation and bilinear fusion. We observe that when compared to ViNet, AViNet gives nil or marginal improvements on most audiovisual SP datasets. Our results suggest that current audiovisual saliency models <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b0">[1]</ref> are not optimal on the visual modality. Furthermore, when we dig deeper, we find out that the audio-visual network learns to ignore the audio signal entirely and gives the same result even while sending a zero vector as audio or by sending an unrelated random audio file ( <ref type="figure" target="#fig_0">Fig. 2)</ref>. Surprisingly, we observe the same behaviour with STAViS <ref type="bibr" target="#b0">[1]</ref>, the current state-of-the-art audio-visual saliency prediction model. Our findings contrast to the prevalent claims that the audio acts as a strong cue for SP. Overall, we make the following research contributions:</p><p>? We propose a novel visual-only architecture called ViNet for video saliency detection. Our model uses commonly known deep learning components/ideas, and the contributions are in their efficient amalgamation. We back the proposed architecture with thorough ablation studies. ? We present a comprehensive analysis on ten different datasets (three visual and seven audio-visual datasets). Our model achieves solid performance gains over the current state-of-the-art. ? We carefully explore the role of audio and find that the visual-only model almost recovers the underlying performance. Furthermore, the strategies mentioned in existing literature end up learning a prediction model agnostic to audio. This motivates the need for exploring novel architectures for audio-visual fusion for SP and possibly carefully curating datasets where audio plays a significant role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Saliency</head><p>The recent landscape in video saliency prediction is dominated by the end-to-end trainable deep networks. The availability of large datasets like Hollywood-2 <ref type="bibr" target="#b21">[22]</ref> and DHF1K <ref type="bibr" target="#b11">[12]</ref> have been instrumental in this progress. Hollywood-2 is the largest dataset, however, its content is limited to human actions and movie scenes. DHF1K is considered the most diverse and challenging dataset for saliency detection.</p><p>Majority of the recent approaches rely on an LSTM based architecture for sequential fixation prediction over successive frames. Wang et al. <ref type="bibr" target="#b11">[12]</ref> combine frame-level image features using a ConvLSTM. SalEMA <ref type="bibr" target="#b14">[15]</ref> model recurrence using a temporal exponential moving average (EMA) operation over the convolutional layer. They show such a simple moving average-based approach matches the performance achieved using a ConvLSTM. SALSAC <ref type="bibr" target="#b13">[14]</ref> adds further complexity to basic ConvLSTM architecture through a shuffled multilevel attention module and a frame correlation module. STRA-Net <ref type="bibr" target="#b22">[23]</ref> learns an alignment module, and then aligned frames are sent into a Bi-ConvLSTM. <ref type="bibr" target="#b23">[24]</ref> propose a novel construction of LSTM (2C-LSTM) with two sub-networks to focus on objectness and motion, respectively. UNISAL <ref type="bibr" target="#b12">[13]</ref> is a unified image and video saliency prediction model that uses MobileNet to extract spatial features and LSTMs for encoding temporal information. The method heavily relies on domain adaptive prior maps (different prior maps for image and video domains), domain adaptive batch-normalization, etc. Several of these video saliency prediction architectures <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref> borrow and extend ideas (hierarchical features, transfer learning, multi-branch architectures, etc.) from the models trained for static image saliency prediction <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>.</p><p>3D convolutional architectures have also been explored for the task. These methods typically rely on action detection networks as their backbone. TASED-Net <ref type="bibr" target="#b15">[16]</ref> uses S3D as an encoder to extract spatial features while jointly aggregating all the temporal information in order to produce a single full-resolution prediction map. They use transposed convolution layers with auxiliary pooling ( a variation of max-unpooling layers) for spatial upscaling in the decoder. Bellitto et al. <ref type="bibr" target="#b26">[27]</ref> use multiple decoders for features encoded at different levels to obtain multiple saliency instances that are finally combined to obtain final output saliency maps. <ref type="bibr" target="#b26">[27]</ref> is inspired by the DVA image saliency model <ref type="bibr" target="#b27">[28]</ref>. A combination of 3D convolutions and recurrent architecture has also been explored <ref type="bibr" target="#b29">[29]</ref>. STSConvNet <ref type="bibr" target="#b30">[30]</ref> explicitly computes optical flow and fuses the optical with the visual features into two-stream convolutional architecture.</p><p>In contrast, our ViNet method is a straightforward encoderdecoder architecture exploiting basic ideas of spatial hierarchy, feature concatenation, skip connections, trilinear upsampling, and 3D convolutions. It uses pretrained features from a network trained for action recognition as a backbone and is void of any explicit inputs like optical flow or any extra modules for detecting objectness, motion, attention, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Audio-Video Saliency</head><p>Research in cognitive neuroscience has led to interesting findings about audiovisual integration. If you ever watched a ventriloquist in action, you would agree how they trick our visual stimuli to guide the perceived location of the sound (and where we look at). Ventriloquist turns to face the puppet, they attend the puppet, use a different voice for the puppet, and make it seem that it is the puppet that is talking (although the sound is being generated from their stomach). McGurk effect <ref type="bibr" target="#b31">[31]</ref>, pip and pop effect <ref type="bibr" target="#b32">[32]</ref>, unity assumptions <ref type="bibr" target="#b33">[33]</ref> are other examples of how we jointly integrate and perceive visual and audio modalities. Coutrot et al. <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref> present some interesting studies on the influence of soundtrack on eye movements during video exploration.</p><p>Application-specific attempts have been made for visual saliency and audio localization <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>. The fusion of handcrafted attention models and pre-trained deep imagelevel models using canonical correlation analysis has been explored <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>. However, only a couple of attempts have been made towards an end-to-end deep learning-based audiovisual saliency fixation prediction. Tavakoli et al. <ref type="bibr" target="#b19">[20]</ref> trains two independent networks for the two modalities (audio and visual data), and their outputs are simply concatenated as a late fusion scheme. They use 3DResNet as the backbone for both modalities. STAViS <ref type="bibr" target="#b0">[1]</ref> extends the SUSiNet <ref type="bibr" target="#b41">[41]</ref> visual saliency model and investigates three different ways to fuse the audio modality.</p><p>Significant efforts have been made in the direction of selfsupervised learning and representation learning exploiting audio-visual data. SoundNet <ref type="bibr" target="#b20">[21]</ref> leverage the natural synchronization between vision and sound to learn an acoustic representation. They use a student-teacher training procedure to transfer discriminative visual knowledge (large-scale visual recognition) into the sound modality. On similar lines, audiovisual correspondence has been exploited for the task of cross-modal retrieval <ref type="bibr" target="#b42">[42]</ref>, sound classification <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b43">[43]</ref>, sound localization in images <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b44">[44]</ref>, scene analysis <ref type="bibr" target="#b45">[45]</ref>, temporal event localization <ref type="bibr" target="#b46">[46]</ref> etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED ARCHITECTURE</head><p>We propose an end-to-end architecture visual-only model called ViNet. It is a fully 3D-convolutional encoder-decoder architecture that predicts the saliency for the last frame of the corresponding set of sequential frames. Then we present an audio-visual saliency detection model called AViNet that fuses the visual features from ViNet and audio features from SoundNet. <ref type="figure" target="#fig_1">Fig. 3</ref> displays an overview of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Backbone</head><p>The architecture uses the S3D network <ref type="bibr" target="#b47">[47]</ref> as the video encoder. We use the model pre-trained on the Kinetics dataset which is an action-recognition dataset. We use S3D since it consists of 3D convolutional layers which efficiently encodes the spatio-temporal information. Moreover, it is light-weight and pre-trained on a large dataset, making it fast and effective for transfer-learning. It consists of 4 convolutional blocks base1, base2, base3 and base4 that provides outputs X 1 ,X 2 ,X 3 and X 4 in different spatial and temporal scales. X 1 ,X 2 and X 3 are referred as multi-level features that  <ref type="bibr" target="#b20">[21]</ref>, which is trained for audio/sound based scene classification. We pre-process the audio data similar to the STAViS <ref type="bibr" target="#b0">[1]</ref> (section 3.2). The sound module takes 1D pre-processed audio feature as input, y audio ? R 1?T ?1 and outputs audio features A ? R 1024?3?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Audio-Visual Fusion</head><p>Inspired by the recent works on audio-visual saliency prediction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we explore two types of fusion techniques. First is a simple concatenation of encoded audio and video features which was used in <ref type="bibr" target="#b19">[20]</ref>. We repeat the audio features to match the dimensions of visual features and combine them across the channel dimension. Then we apply 1 ? 1 Convolution to reduce the number of channels.</p><p>Secondly, we applied bilinear fusion which has been used in <ref type="bibr" target="#b0">[1]</ref>. The visual features are first passed through Max Pool to reduce the spatial and temporal dimension and then collapsed to represent it as a vector x 1 ? R 1024?x0 . Similarly, the audio features are collapsed as a vector x 2 ? R 1024?y0 . The bilinear fusion is defined as</p><formula xml:id="formula_0">y = x T 1 Ax 2 + b<label>(1)</label></formula><p>where A ? R x0?x?y0 and b ? R x?1 are parameters and x is the desired output dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prediction Network</head><p>The Prediction Network consists of 6 decoding layers consisting of 3D convolutional and upsampling layers. For ViNet, the input to the Prediction Network is the X 4 features from the Backbone and X 3 ,X 2 , and X 1 are passed in using skip connections, respectively. In the case of AViNet, the audio features are fused with X 4 and then sent to the decoder (skip connections are made similarly).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation</head><p>Both ViNet and AViNet follow a sliding window approach to generate a saliency map for all frames in the video. Given a window size of T frames, we predict saliency map S t at time step t by taking F t?T +1 , ...F t sequence of frames as input. To enable prediction in the first T frames, we simply repeat the first frame of the video at the start. A single inference of ViNet takes around 0.016 seconds (62.5 fps) to generate a saliency map, with T = 32 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>A. Datasets 1) Visual Datasets: The three most popular visual-only datasets in video saliency are DHF1K, Hollywood-2, and UCF-Sports <ref type="bibr" target="#b50">[50]</ref>. We carry out the tests and comparisons on these three datasets.</p><p>DHF1K <ref type="bibr" target="#b11">[12]</ref> contains 1000 videos where 700 videos are for training and 100 for validation. A test set of 300 videos is also released, however, without public ground truth. All our experiments and analysis are based on this dataset since it is the most general and diverse dataset.</p><p>Hollywood-2 <ref type="bibr" target="#b21">[22]</ref> is the largest video saliency prediction dataset in terms of the number of videos, consisting of 1707 videos. The dataset is focused on human actions. The videos in this dataset are short video sequences from a set of 69 Hollywood movies, containing 12 different human action classes, ranging from answering the phone, eating, driving, running and etc. We use the standard split of 823 training videos and 884 test videos.</p><p>UCF-Sports <ref type="bibr" target="#b50">[50]</ref> dataset consists of 150 videos focusing on human actions in sports. We use a standard split with 103 videos for training and 47 videos for testing.</p><p>2) Audio-Visual Datasets: There are seven audio-visual datasets in video saliency: DIEM, Coutrot1, Coutrot2, AVAD, ETMD, SumMe, and AVE dataset. We carry out the tests and comparisons on all these seven datasets.</p><p>DIEM <ref type="bibr" target="#b51">[51]</ref> consists of 81 movie clips of varying genres. They sourced from publicly accessible repositories, including advertisements, documentaries, game trailers, movie trailers, music videos, news clips, and time-lapse footage. It consists of 64 training videos and 17 test videos.</p><p>Coutrot databases <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref> are split into Coutrot1 and Coutrot2. Coutrot1 contains 60 clips with dynamic natural scenes split into four visual categories: one/several moving objects, landscapes, and faces. Coutrot2 contains 15 clips of 4 persons in a meeting and the corresponding eye-tracking data from 40 persons.</p><p>AVAD dataset <ref type="bibr" target="#b39">[39]</ref> contains 45 short clips of 5-10 sec duration with several audio-visual scenes, e.g., dancing, guitar playing, birds singing, etc.</p><p>ETMD dataset [52] contains 12 videos from six different hollywood movies.</p><p>SumMe dataset <ref type="bibr" target="#b53">[53]</ref> contains 25 unstructured videos, i.e. , mostly user-made videos and their corresponding multiplehuman created summaries, which were acquired in a controlled psychological experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>For training ViNet, clips with T consecutive frames were randomly selected from the dataset. Each frame is resized to 224 ? 384 and trained with a batch size of 8. The optimizer used is Adam, and the learning rate is set to be 1e-4. The network is initially trained on the DHF1K dataset. The validation set of DHF1K is used for early stopping. The trained model is then fine-tuned for Hollywood-2 and UCF-Sports dataset using their respective training sets. The test sets of Hollywood-2 and UCF-Sports are used for early stopping.</p><p>For our audio-visual extension AViNet, weights of ViNet pre-trained on DHF1K are used and fine-tuned on the audiovisual datasets. For DIEM, the standard split provided in the literature is used. For other datasets, there are no standard splits defined, so we evaluated our model on three different splits defined by <ref type="bibr" target="#b0">[1]</ref> and report the average metric values across various splits. For evaluating on AVE dataset, we finetune the model using its training set and use its validation for early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>We evaluate our method on five standard evaluations metrics <ref type="bibr" target="#b54">[54]</ref>. The CC metric computes the Pearson's Correlation between the ground truth and the predicted maps. The Similarity metric (SIM) computes the histogram intersection. The Area Under the ROC Curve (AUC) treats saliency map as a binary classifier of fixations at various threshold values, and an ROC curve is swept out by measuring the true and false positive rates under each binary classifier. sAUC is a variation of AUC where negatives are sampled from fixation locations from other images. NSS is computed as the average normalized saliency at fixated locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>We use the Kullback-Leibler divergence as the loss function, which is often used in saliency prediction tasks. KLDiv is an information-theoretic measure of the difference between two probability distributions:</p><formula xml:id="formula_1">KLdiv(P, Q) = i Q i log( + Q i P i + ),<label>(2)</label></formula><p>where P , Q are predicted and ground truth maps respectively and is a regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation studies</head><p>We present ablations studies that motivated our design choices in the ViNet model. All the ablations in this section are performed with training on the DHF1K training set and evaluation on its validation set. We examine the effects of (a) changing the clip size, (b) using multi-level features, (c) replacing upsampling with transpose convolutions, and (d) applying different concatenation techniques for fusing hierarchical features. <ref type="table" target="#tab_0">Table I</ref> illustrates the results on varying the clip size of the input and using clips of size 32 frames gave the best results. Ablation results by using hierarchical features can be found in <ref type="table" target="#tab_0">Table II</ref>. It clearly indicates that using multi-level features adds up to the performance. We also use transpose convolution instead of trilinear upsampling to increase the spatial dimension, but CC decreased to 0.5178 from 0.5212. The multi-level features extracted from the backbone are concatenated at each decoder block. We tried two ways of concatenating features -across temporal dimension and channel dimension. We observed that they gave a similar performance; therefore, we went ahead with the former approach due to fewer trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with state-of-the-art a) Visual-Only datasets:</head><p>We quantitatively compare our model with the top six state-of-the-art models on DHF1K, Hollywood-2, and UCF-Sports test set. <ref type="table" target="#tab_0">Table III</ref> shows the results on all three datasets in terms of CC, sAUC, AUC, NSS, and SIM metrics. We can observe that ViNet outperforms all the state-of-the-art models on the DHF1K dataset. ViNet also achieves top results on most metrics on Hollywood-2 and UCF-Sports datasets. At the time of the submission, ViNet is the top-performing model on the DHF1K challenge (evaluated on the private test set) <ref type="bibr" target="#b0">1</ref> . We show a qualitiative example in <ref type="figure">Fig. 4</ref> where we see that ViNet is able to produce much more accurate saliency maps as compared to TASED-Net and STAViS.</p><p>b) Audio-Visual Datasets: The comparison of ViNet and AViNet models with state-of-the-art methods on audiovisual datasets are presented in <ref type="table" target="#tab_0">Table IV</ref>. We also present results on ViNet(NF) baseline model, which is a trained of DHF1K dataset and not fine-tuned further on audiovisual datasets. The ViNet, ViNet(NF), ACLNet and TASED-Net models are trained without using any audio information. STAViS and AViNet models make use of the audio modality, both during training and inference. AViNet(B) and AViNet(C) present the two fusion methodologies discussed above i.e. concatenation and bilinear fusion respectively.</p><p>The ViNet model significantly outperforms STAViS on most datasets across most metrics. Surprisingly, the ViNet(NF) model is already competitive, indicating that models trained on DHF1K can generalize well to other datasets. Moreover, the results clearly suggest that the visualonly modality, when exploited well, is able to recover most of the underlying performance on the current datasets, as compared to existing state-of-the-art models. The improvements obtained by AViNet(B) and AViNet(C) models over ViNet are marginal at best (with an exception on Coutrot2 dataset, which is captured in highly specific settings). Hence, in contrast to previous works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[20]</ref>, our experimental results do not indicate any clear benefit of incorporating audio in the prediction pipeline.</p><p>We also evaluate the performance of our models on the AVE dataset <ref type="bibr" target="#b19">[20]</ref>. Although the AVE dataset is formed using the sequences from DIEM, Coutrot1, and Coutrot2 datasets, it is an interesting dataset because (a) it provides a human upper bound and a lower bound using dataset biases and (b) it provides video level categorization. The upper bound is named Human Infinite (HI) and is computed by splitting the eye-movements of observers into two groups and assessing one group against the other (human vs. human performance). The lower bound is called the Mean Eye Position map (MEP) and is computed from the training sequences. It depicts the center-bias that a model may learn by training on the dataset. It is, hence, a robust lower-bound baseline.</p><p>ViNet model outperforms the state-of-the-art approaches on the AVE dataset by a significant margin, resonating with the observations on other datasets. Notably, ViNet is able to cross the HI upper bound on AUC-J, CC, and SIM metrics. We further provide a category-wise analysis of both our models on this dataset. It is evident from Table V ViNet and AViNet give fairly similar performance across all three  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. The Impact of Audio</head><p>We conduct a simple experiment to investigate the role of audio in AViNet and STAViS models. We compare the output predictions obtained with original audio and by sending zeroed-out vector as audio (indicating the absence of audio). To our surprise, the network's prediction maps obtained with and without audio are nearly identical (as presented in <ref type="table" target="#tab_0">Table VI</ref>). A qualitative example is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. This indicates that the network learns to be agnostic to audio and gives the same output irrespective of the audio input (zero vector, corresponding audio, or random audio). In summary, the current state-of-the-art audio-saliency models end up learning a visual-only model and that also explains the marginal differences with ViNet and AViNet models in our results <ref type="table" target="#tab_0">(Table IV)</ref>. Such marginal differences might arise due to different instances of training or possibly due to a slight variation in the number of parameters. A deeper exploration is left for future work. Finally, the observations contrast with cognitive studies, which suggest clear differences in human gaze patterns when the videos are watched with or without audio <ref type="bibr" target="#b18">[19]</ref>. The findings open up an interesting avenue for future research for designing architectures that can make better use of the aural modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We propose ViNet, a novel spatio-temporal visual-only architecture that efficiently addresses the problem of saliency detection in videos. We also explored incorporating audio for the task with AViNet by the addition of an auditory module to ViNet. We explore two different fusion techniques for combining audio-visual cues. We perform a comprehensive analysis of both models on 10 different datasets (3 visual and 7 audio-visual). Our models brings significantly gains over the state-of-the-art models. We find that audio does not seem to be playing a major role in audio-visual saliency prediction, even in models that explicitly incorporate audio. Our findings clearly illustrate the need for further explorations in this direction, leading to better models as well as curating datasets which can better utilize the auditory modality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Sample frames from Coutrot-2 database with the corresponding ground-truth. The predicted saliency maps of AViNet with and without passing audio input turn out to be the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>AViNet Architecture overview. Removing the audio branch, the resulting architecture is ViNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>VI CC AND SIM METRICS FOR AVINET(B), AVINET(C) AND STAVIS WITH AND WITHOUT AUDIO(SENDING ZEROS FOR AUDIO) ON Coutrot2 DATASET. THE PREDICTIONS ARE NEARLY IDENTICAL AS REFLECTED IN solid gains over other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I VALIDATION</head><label>I</label><figDesc>RESULTS ON VARYING CLIP SIZE FOR TRAINING VINET ON DHF1K.</figDesc><table><row><cell>Clip Size (T )</cell><cell>CC</cell><cell>SIM</cell><cell>NSS</cell></row><row><cell>8</cell><cell>0.4978</cell><cell>0.363</cell><cell>2.8221</cell></row><row><cell>16</cell><cell>0.5112</cell><cell>0.378</cell><cell>2.9067</cell></row><row><cell>32</cell><cell>0.5212</cell><cell cols="2">0.3881 2.9565</cell></row><row><cell>48</cell><cell>0.5231</cell><cell cols="2">0.3807 2.9477</cell></row><row><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell></row><row><cell cols="4">VALIDATION RESULTS OF VINET WITH AND WITHOUT HIERARCHY ON</cell></row><row><cell></cell><cell>DHF1K.</cell><cell></cell><cell></cell></row><row><cell>Model Architecture</cell><cell>CC</cell><cell>SIM</cell><cell>NSS</cell></row><row><cell>Without Hierarchy</cell><cell>0.5002</cell><cell>0.361</cell><cell>2.7371</cell></row><row><cell>With Hierarchy</cell><cell cols="3">0.5212 0.3881 2.9565</cell></row></table><note>are extracted at three-levels of hierarchy. The input to the encoder is a video clip x clip ? R 3?T0?H0?W0 , where T 0 is 32. It generates a lower-resolution activation map X 4 ? R C?T ?H?W , where C = 1024 and T, H, W = T0 8 , H0 32 , W0 32 . For audio representation, we employ SoundNet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III COMPARISON</head><label>III</label><figDesc>RESULTS ON THE DHF1K, Hollywood-2 AND UCF-Sports TEST SETS. THE BEST SCORES ARE SHOWN IN RED AND SECOND BEST SCORES IN BLUE.FPS VALUES ARE TAKEN FROM THE LEADERBOARD AND ARE COMPUTED ON DIFFERENT HARDWARE, HENCE, A DIRECT COMPARISON CANNOT BE MADE. VINET IS EVALUATED ON A GTX 1080 TI GPU AND RUNS AT A SPEED ADEQUATE FOR MOST REAL-TIME APPLICATIONS.</figDesc><table><row><cell></cell><cell>FPS</cell><cell></cell><cell></cell><cell>DHF1K</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Hollywood-2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">UCF-Sports</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell></row><row><cell>SALEMA [15]</cell><cell>100</cell><cell>0.449</cell><cell>0.667</cell><cell>0.890</cell><cell>2.57</cell><cell>0.466</cell><cell>0.613</cell><cell>0.708</cell><cell>0.919</cell><cell>3.18</cell><cell>0.487</cell><cell>0.544</cell><cell>0.740</cell><cell>0.906</cell><cell>2.63</cell><cell>0.431</cell></row><row><cell>ACLNet [48]</cell><cell>50</cell><cell>0.434</cell><cell>0.601</cell><cell>0.890</cell><cell>2.35</cell><cell>0.315</cell><cell>0.623</cell><cell>0.757</cell><cell>0.913</cell><cell>3.08</cell><cell>0.542</cell><cell>0.510</cell><cell>0.744</cell><cell>0.897</cell><cell>2.56</cell><cell>0.406</cell></row><row><cell>STRA-Net [49]</cell><cell>50</cell><cell>0.458</cell><cell>0.663</cell><cell>0.895</cell><cell>2.55</cell><cell>0.355</cell><cell>0.662</cell><cell>0.774</cell><cell>0.923</cell><cell>3.47</cell><cell>0.536</cell><cell>0.593</cell><cell>0.751</cell><cell>0.910</cell><cell>3.01</cell><cell>0.479</cell></row><row><cell>SALSAC [14]</cell><cell>50</cell><cell>0.479</cell><cell>0.697</cell><cell>0.896</cell><cell>2.67</cell><cell>0.357</cell><cell>0.670</cell><cell>0.712</cell><cell>0.931</cell><cell>3.35</cell><cell>0.529</cell><cell>0.671</cell><cell>0.806</cell><cell>0.926</cell><cell>3.52</cell><cell>0.534</cell></row><row><cell cols="2">TASED-Net [16] 16.6</cell><cell>0.470</cell><cell>0.712</cell><cell>0.895</cell><cell>2.66</cell><cell>0.361</cell><cell>0.64</cell><cell>0.768</cell><cell>0.918</cell><cell>3.30</cell><cell cols="2">0.507 0.582</cell><cell>0.752</cell><cell>0.899</cell><cell>2.92</cell><cell>0.469</cell></row><row><cell>UNISAL [13]</cell><cell>111</cell><cell>0.490</cell><cell>0.691</cell><cell>0.901</cell><cell>2.77</cell><cell>0.390</cell><cell>0.673</cell><cell>0.795</cell><cell>0.934</cell><cell>3.90</cell><cell>0.542</cell><cell>0.644</cell><cell>0.775</cell><cell>0.918</cell><cell>3.38</cell><cell>0.523</cell></row><row><cell>ViNet</cell><cell>62.5</cell><cell>0.510</cell><cell>0.728</cell><cell>0.908</cell><cell>2.87</cell><cell>0.381</cell><cell>0.693</cell><cell>0.813</cell><cell>0.930</cell><cell>3.73</cell><cell>0.550</cell><cell>0.673</cell><cell>0.810</cell><cell>0.924</cell><cell>3.62</cell><cell>0.522</cell></row><row><cell cols="8">AVE dataset [20] consists of 150 hand-picked video se-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">quences from DIEM, Coutrot1 and Coutrot2 datasets. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">videos are divided into three categories -Nature, Social</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Events, and Miscellaneous. The dataset consists of 92 train-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">ing videos, 29 validation, and 29 test sequences.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>RESULTS ON THE DIEM, Coutrot1, Coutrot2, AVAD, ETMD AND SumMe TEST SETS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DIEM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Coutrot1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Coutrot2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell></row><row><cell cols="2">ACLNet [48]</cell><cell>0.522</cell><cell>0.622</cell><cell>0.869</cell><cell>2.02</cell><cell>0.427</cell><cell>0.425</cell><cell>0.542</cell><cell>0.85</cell><cell>1.92</cell><cell>0.361</cell><cell>0.448</cell><cell>0.594</cell><cell>0.926</cell><cell>3.16</cell><cell>0.322</cell></row><row><cell cols="2">TASED-Net [16]</cell><cell>0.557</cell><cell>0.657</cell><cell>0.881</cell><cell>2.16</cell><cell>0.461</cell><cell>0.479</cell><cell>0.58</cell><cell>0.867</cell><cell>2.18</cell><cell>0.388</cell><cell>0.437</cell><cell>0.611</cell><cell>0.921</cell><cell>3.17</cell><cell>0.314</cell></row><row><cell cols="2">STAViS [1]</cell><cell>0.579</cell><cell>0.674</cell><cell>0.883</cell><cell>2.26</cell><cell>0.482</cell><cell>0.472</cell><cell>0.584</cell><cell>0.868</cell><cell>2.11</cell><cell>0.393</cell><cell>0.734</cell><cell>0.71</cell><cell>0.958</cell><cell>5.28</cell><cell>0.511</cell></row><row><cell cols="2">ViNet(NF)</cell><cell>0.571</cell><cell>0.695</cell><cell>0.886</cell><cell>2.28</cell><cell>0.468</cell><cell>0.509</cell><cell>0.619</cell><cell>0.875</cell><cell>2.46</cell><cell>0.406</cell><cell>0.645</cell><cell>0.72</cell><cell>0.949</cell><cell>5.11</cell><cell>0.419</cell></row><row><cell cols="2">ViNet</cell><cell>0.626</cell><cell>0.723</cell><cell>0.898</cell><cell>2.47</cell><cell>0.483</cell><cell>0.551</cell><cell>0.633</cell><cell>0.886</cell><cell>2.68</cell><cell>0.423</cell><cell>0.724</cell><cell>0.739</cell><cell>0.95</cell><cell>5.61</cell><cell>0.466</cell></row><row><cell cols="2">AViNet(B)</cell><cell>0.632</cell><cell>0.719</cell><cell>0.899</cell><cell>2.53</cell><cell>0.498</cell><cell>0.56</cell><cell>0.635</cell><cell>0.889</cell><cell>2.73</cell><cell>0.425</cell><cell>0.754</cell><cell>0.742</cell><cell>0.951</cell><cell>5.95</cell><cell>0.493</cell></row><row><cell cols="2">AViNet(C)</cell><cell>0.631</cell><cell>0.720</cell><cell>0.897</cell><cell>2.50</cell><cell>0.497</cell><cell>0.556</cell><cell>0.636</cell><cell>0.887</cell><cell>2.68</cell><cell>0.426</cell><cell>0.753</cell><cell>0.743</cell><cell>0.951</cell><cell>5.81</cell><cell>0.486</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AVAD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETMD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SumMe</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell></row><row><cell cols="2">ACL-Net [48]</cell><cell>0.58</cell><cell>0.56</cell><cell>0.905</cell><cell>3.17</cell><cell>0.446</cell><cell>0.477</cell><cell>0.675</cell><cell>0.915</cell><cell>2.36</cell><cell>0.329</cell><cell>0.379</cell><cell>0.609</cell><cell>0.868</cell><cell>1.79</cell><cell>0.296</cell></row><row><cell cols="2">TASED-Net [16]</cell><cell>0.601</cell><cell>0.589</cell><cell>0.914</cell><cell>3.16</cell><cell>0.439</cell><cell>0.509</cell><cell>0.711</cell><cell>0.916</cell><cell>2.63</cell><cell>0.366</cell><cell>0.428</cell><cell>0.657</cell><cell>0.884</cell><cell>2.1</cell><cell>0.333</cell></row><row><cell cols="2">STAViS [1]</cell><cell>0.608</cell><cell>0.593</cell><cell>0.919</cell><cell>3.18</cell><cell>0.457</cell><cell>0.569</cell><cell>0.731</cell><cell>0.931</cell><cell>2.94</cell><cell>0.425</cell><cell>0.422</cell><cell>0.656</cell><cell>0.888</cell><cell>2.04</cell><cell>0.337</cell></row><row><cell cols="2">ViNet (NF)</cell><cell>0.665</cell><cell>0.651</cell><cell>0.923</cell><cell>3.67</cell><cell>0.501</cell><cell>0.544</cell><cell>0.719</cell><cell>0.924</cell><cell>2.92</cell><cell>0.404</cell><cell>0.455</cell><cell>0.687</cell><cell>0.893</cell><cell>2.35</cell><cell>0.349</cell></row><row><cell cols="2">ViNet</cell><cell>0.694</cell><cell>0.663</cell><cell>0.928</cell><cell>3.82</cell><cell>0.504</cell><cell>0.569</cell><cell>0.736</cell><cell>0.928</cell><cell>3.06</cell><cell>0.409</cell><cell>0.466</cell><cell>0.696</cell><cell>0.898</cell><cell>2.40</cell><cell>0.345</cell></row><row><cell cols="2">AViNet(B)</cell><cell>0.674</cell><cell>0.658</cell><cell>0.927</cell><cell>3.77</cell><cell>0.491</cell><cell>0.571</cell><cell>0.733</cell><cell>0.928</cell><cell>3.08</cell><cell>0.406</cell><cell>0.463</cell><cell>0.692</cell><cell>0.897</cell><cell>2.41</cell><cell>0.343</cell></row><row><cell cols="2">AViNet(C)</cell><cell>0.683</cell><cell>0.661</cell><cell>0.931</cell><cell>3.74</cell><cell>0.494</cell><cell>0.566</cell><cell>0.737</cell><cell>0.928</cell><cell>3.05</cell><cell>0.404</cell><cell>0.471</cell><cell>0.699</cell><cell>0.899</cell><cell>2.42</cell><cell>0.346</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">PERFORMANCE OF VARIOUS MODELS ON AVE TEST SET CATEGORIES.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">HI REPRESENTS HUMAN INFINITE (HI) REPRESENTS UPPER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">PERFORMANCE BOUND AND MEAN EYE POSITION (MEP) REPRESENTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">LOWER PERFORMANCE BOUND.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cat.</cell><cell cols="2">Model Name</cell><cell>CC</cell><cell>sAUC</cell><cell>AUC</cell><cell>NSS</cell><cell>SIM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Nature</cell><cell cols="2">HI AViNet ViNet DAVE [20]</cell><cell>0.669 0.649 0.680 0.539</cell><cell>0.762 0.729 0.735 0.723</cell><cell>0.866 0.895 0.900 0.877</cell><cell>3.32 2.37 2.47 2.27</cell><cell>0.538 0.515 0.538 0.450</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">ACLNet* [48] 0.517</cell><cell>0.723</cell><cell>0.884</cell><cell>2.03</cell><cell>0.401</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MEP</cell><cell></cell><cell>0.471</cell><cell>0.686</cell><cell>0.869</cell><cell>1.76</cell><cell>0.368</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Soc Ev.</cell><cell cols="2">HI AViNet ViNet DAVE [20]</cell><cell>0.655 0.688 0.688 0.545</cell><cell>0.759 0.765 0.760 0.726</cell><cell>0.855 0.914 0.910 0.885</cell><cell>3.63 2.96 2.88 2.65</cell><cell>0.516 0.536 0.544 0.442</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">ACLNet* [48] 0.449</cell><cell>0.683</cell><cell>0.869</cell><cell>2.02</cell><cell>0.359</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MEP</cell><cell></cell><cell>0.314</cell><cell>0.633</cell><cell>0.819</cell><cell>1.35</cell><cell>0.274</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>HI</cell><cell></cell><cell>0.597</cell><cell>0.748</cell><cell>0.837</cell><cell>3.23</cell><cell>0.481</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Misc.</cell><cell>AViNet ViNet</cell><cell></cell><cell>0.635 0.636</cell><cell>0.730 0.726</cell><cell>0.898 0.896</cell><cell>2.42 2.40</cell><cell>0.506 0.509</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Dave [20]</cell><cell>0.549</cell><cell>0.736</cell><cell>0.881</cell><cell>2.39</cell><cell>0.454</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">ACLNet* [48] 0.456</cell><cell>0.683</cell><cell>0.852</cell><cell>1.84</cell><cell>0.378</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MEP</cell><cell></cell><cell>0.438</cell><cell>0.675</cell><cell>0.845</cell><cell>1.73</cell><cell>0.342</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Overall</cell><cell cols="2">HI AViNet ViNet DAVE [20]</cell><cell>0.644 0.655 0.671 0.545</cell><cell>0.757 0.744 0.742 0.726</cell><cell>0.854 0.901 0.903 0.881</cell><cell>3.41 2.55 2.60 2.45</cell><cell>0.514 0.516 0.533 0.449</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">ACLNet* [48] 0.475</cell><cell>0.700</cell><cell>0.870</cell><cell>1.98</cell><cell>0.379</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MEP</cell><cell></cell><cell>0.403</cell><cell>0.662</cell><cell>0.844</cell><cell>1.59</cell><cell>0.326</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The challenge website can be found here https://mmcheng.net/ videosal/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stavis: Spatio-temporal audiovisual saliency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsiami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koutras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Visual saliency model for robot cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Butko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attentional mechanisms for socially interactive robots-a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating the effect of saliency detection and attention manipulation in human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schillaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodiro?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Hafner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="139" to="152" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Salgaze: Personalizing gaze estimation using visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Matias</forename><surname>Di Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Facial expression recognition using visual saliency and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Miyapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual saliency based object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Saliency-aware video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hadizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Baji?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="33" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual saliency guided video compression algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1006" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Static saliency vs. dynamic saliency: a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gazedgaze-guided cinematic editing of wide-angle monocular video recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
		<editor>CHI</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting video saliency: A large-scale benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unified image and video saliency modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Droste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Salsac: A video saliency prediction model with shuffled attentions and correlationbased convlstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simple vs complex temporal recurrences for video saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Linardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mohedano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01869</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tased-net: Temporally-aggregating spatial encoder-decoder network for video saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Audiovisual events capture attention: Evidence from temporal order judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Der Burg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Olivers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Bronkhorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2" to="2" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Influence of soundtrack on eye movements during video exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coutrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guyader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Eye Movement Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dave: A deep audio-visual embedding for dynamic saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10693</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Video saliency prediction using enhanced spatiotemporal alignment network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00292</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Predicting video saliency with object-to-motion cnn and two-layer convolutional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06316</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Salicon: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tidying deep saliency prediction architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yarlagadda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Video saliency detection with domain adaption using hierarchical gradient reversal layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bellitto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Salanitri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01220</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep visual attention prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sample frames from the Coutrot1 and AVAD datasets with the corresponding ground truth, ViNet, and previous state-of-the-art STAViS and TASED-Net visual saliency maps for comparisons. ViNet is able to capture the salient region in all of these 3 examples efficiently, whereas STAViS and TASED-Net are not able to capture the salient regions accurately</title>
		<imprint/>
	</monogr>
	<note>Complete video for all figures is available in the supplementary material</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recurrent mixture density network for spatiotemporal visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08199</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency networks for dynamic saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1688" to="1698" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">5588</biblScope>
			<biblScope unit="page" from="746" to="748" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pip and pop: nonspatial auditory signals improve spatial visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Der Burg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Olivers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Bronkhorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1053</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crossmodal binding: Evaluating the &quot;unity assumption&quot; using audiovisual speech stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vatakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="744" to="756" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How saliency, faces, and sound influence gaze in dynamic social scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coutrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guyader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5" to="5" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal saliency models for videos</title>
	</analytic>
	<monogr>
		<title level="m">From Human Attention to Computational Attention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="291" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multimodal saliency-based bottom-up attention a framework for the humanoid robot icub</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hornstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pfeifer</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multimodal saliency-based attention for object-based scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schauerte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>K?hn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kroschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Audio matters in visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1992" to="2003" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fixation prediction through multimodal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A multimodal saliency model for videos with high audio-visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3805" to="3819" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Susinet: See, understand and summarize it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koutras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to localize sound source in visual scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with selfsupervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Audio-visual event localization in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting video saliency prediction in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="220" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video saliency prediction using spatiotemporal residual attentive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1113" to="1126" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action mach a spatiotemporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Clustering of gaze during dynamic scene viewing is predicted by motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A perceptually based spatio-temporal computational framework for visual saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koutras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="15" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Creating summaries from user videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">What do different evaluation metrics tell us about saliency models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="740" to="757" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
