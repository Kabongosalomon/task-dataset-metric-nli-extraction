<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2022 FROM STARS TO SUBGRAPHS: UPLIFTING ANY GNN WITH LOCAL STRUCTURE AWARENESS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
							<email>lingxiao@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon Uni</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
							<email>jinwei2@msu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Michigan State Uni</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
							<email>lakoglu@andrew.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Carnegie Mellon Uni</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
							<email>nshah@snap.com</email>
							<affiliation key="aff3">
								<orgName type="department">Snap Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2022 FROM STARS TO SUBGRAPHS: UPLIFTING ANY GNN WITH LOCAL STRUCTURE AWARENESS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Message Passing Neural Networks (MPNNs) are a common type of Graph Neural Network (GNN), in which each node's representation is computed recursively by aggregating representations ("messages") from its immediate neighbors akin to a star-shaped pattern. MPNNs are appealing for being efficient and scalable, however their expressiveness is upper-bounded by the 1st-order Weisfeiler-Leman isomorphism test (1-WL). In response, prior works propose highly expressive models at the cost of scalability and sometimes generalization performance. Our work stands between these two regimes: we introduce a general framework to uplift any MPNN to be more expressive, with limited scalability overhead and greatly improved practical performance. We achieve this by extending local aggregation in MPNNs from star patterns to general subgraph patterns (e.g., k-egonets): in our framework, each node representation is computed as the encoding of a surrounding induced subgraph rather than encoding of immediate neighbors only (i.e. a star). We choose the subgraph encoder to be a GNN (mainly MPNNs, considering scalability) to design a general framework that serves as a wrapper to uplift any GNN. We call our proposed method GNN-AK (GNN As Kernel), as the framework resembles a convolutional neural network by replacing the kernel with GNNs. Theoretically, we show that our framework is strictly more powerful than 1&amp;2-WL, and is not less powerful than 3-WL. We also design subgraph sampling strategies which greatly reduce memory footprint and improve speed while maintaining performance. Our method sets new state-of-the-art performance by large margins for several well-known graph ML tasks; specifically, 0.08 MAE on ZINC, 74.79% and 86.887% accuracy on CIFAR10 and PATTERN respectively. networks: Possibility and impossibility results. arXiv preprint arXiv:2012.03174, 2020.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are permutation invariant, combinatorial structures used to represent relational data, with wide applications ranging from drug discovery, social network analysis, image analysis to bioinformatics <ref type="bibr" target="#b17">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b20">Fan et al., 2019;</ref><ref type="bibr" target="#b49">Shi et al., 2019;</ref><ref type="bibr">Wu et al., 2020)</ref>. In recent years, Graph Neural Networks (GNNs) have rapidly surpassed traditional methods like heuristically defined features and graph kernels to become the dominant approach for graph ML tasks.</p><p>Message Passing Neural Networks (MPNNs) <ref type="bibr" target="#b23">(Gilmer et al., 2017)</ref> are the most common type of GNNs owing to their intuitiveness, effectiveness and efficiency. They follow a recursive aggregation mechanism where each node aggregates information from its immediate neighbors repeatedly. However, unlike simple multi-layer feedforward networks (MLPs) which are universal approximators of continuous functions <ref type="bibr" target="#b26">(Hornik et al., 1989)</ref>, MPNNs cannot approximate all permutation-invariant graph functions <ref type="bibr" target="#b37">(Maron et al., 2019b)</ref>. In fact, their expressiveness is upper bounded by the first order Weisfeiler-Leman (1-WL) isomorphism test <ref type="bibr">(Xu et al., 2018)</ref>. Importantly, researchers have shown that such 1-WL equivalent GNNs are not expressive, or powerful, enough to capture basic structural concepts, i.e., counting motifs such as cycles or triangles <ref type="bibr">(Zhengdao et al., 2020;</ref><ref type="bibr" target="#b2">Arvind et al., 2020)</ref> that are shown to be informative for bio-and chemo-informatics <ref type="bibr" target="#b19">(Elton et al., 2019)</ref>.</p><p>The weakness of MPNNs urges researchers to design more expressive GNNs, which are able to discriminate graphs from an isomorphism test perspective; <ref type="bibr" target="#b13">Chen et al. (2019)</ref> prove the equivalence between such tests and universal permutation invariant function approximation, which theoretically justifies it. As k-WL is strictly more expressive than 1-WL, many works <ref type="bibr" target="#b40">(Morris et al., 2019;</ref><ref type="bibr" target="#b35">2020b)</ref> try to incorporate k-WL in the design of more powerful GNNs, while others approach k-WL </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extract subgraphs</head><p>Convolve subgraphs Aggregate embeddings from subgraphs to new node embedding <ref type="figure" target="#fig_1">Figure 1</ref>: Shown: one GNN-AK + layer. For each layer, GNN-AK + first extracts n (# nodes) rooted subgraphs, and convolves all subgraphs with a base GNN as kernel, producing multiple rich subgraph-node embeddings of the form Emb(i | Sub[j]) (node i's embedding when applying a GNN kernel on subgraph j). From these, we extract and concatenate three encodings for a given node j: (i) centroid Emb(j | Sub[j]), (ii) subgraph i Emb(i | Sub[j]), and (iii) context i Emb(j | Sub[i]). GNN-AK + repeats the process for L layers, then sums all resulting node embeddings to compute the final graph embedding. As a weaker version, GNN-AK only contains encodings (i) and (ii).</p><p>expressiveness indirectly from matrix invariant operations <ref type="bibr" target="#b36">(Maron et al., 2019a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b29">Keriven &amp; Peyr?, 2019)</ref> and matrix language perspectives <ref type="bibr">(Balcilar et al., 2021)</ref>. However, they require O(k)-order tensors to achieve k-WL expressiveness, and thus are not scalable or feasible for application on large, practical graphs. Besides, the bias-variance tradeoff between complexity and generalization <ref type="bibr" target="#b44">(Neal et al., 2018)</ref> and the fact that almost all graphs (i.e. O(2 ( n 2 ) ) graphs on n vertices, <ref type="bibr" target="#b4">Babai et al. (1980)</ref>) can be distinguished by 1-WL challenge the necessity of developing such extremely expressive models. In a complementary line of work, <ref type="bibr" target="#b34">Loukas (2020a)</ref> sheds light on developing more powerful GNNs while maintaining linear scalability, finding that MPNNs can be universal approximators provided that nodes are sufficiently distinguishable. Relatedly, several works propose to add features to make nodes more distinguishable, such as identifiers <ref type="bibr" target="#b34">(Loukas, 2020a)</ref>, subgraph counts <ref type="bibr" target="#b11">(Bouritsas et al., 2020)</ref>, distance encoding <ref type="bibr" target="#b33">(Li et al., 2020)</ref>, and random features <ref type="bibr" target="#b48">(Sato et al., 2021;</ref><ref type="bibr">Abboud et al., 2021)</ref>. However, these methods either focus on handcrafted features which lose the premise of automatic learning, or create permutation sensitive features that hurt generalization.</p><p>Present Work. Our work stands between the two regimes of extremely expressive but unscalable korder GNNs, and the limited expressiveness yet high scalability of MPNNs. Specifically, we propose a general framework that serves as a "wrapper" to uplift any GNN. We observe that MPNNs' local neighbor aggregation follows a star pattern, where the representation of a node is characterized by applying an injective aggregator function as an encoder to the star subgraph (comprised of the central node and edges to neighbors). We propose a design which naturally generalizes from encoding the star to encoding a more flexibly defined subgraph, and we replace the standard injective aggregator with a GNN: in short, we characterize the new representation of a node by using a GNN to encode a locally induced encompassing subgraph, as shown in <ref type="figure" target="#fig_1">Fig.1</ref>. This uplifts GNN as a base model in effect by applying it on each subgraph instead of the whole input graph. This generalization is close to Convolutional Neural Networks (CNN) in computer vision: like the CNN that convolves image patches with a kernel to compute new pixel embeddings, our designed wrapper convolves subgraphs with a GNN to generate new node embeddings. Hence, we name our approach GNN-AK (GNN As Kernel). We show theoretically that GNN-AK is strictly more powerful than 1&amp;2-WL with any MPNN as base model, and is not less powerful than 3-WL with <ref type="bibr">PPGN (Maron et al., 2019a)</ref> used. We also give sufficient conditions under which GNN-AK can successfully distinguish two non-isomorphic graphs. Given this increase in expressive power, we discuss careful implementation strategies for GNN-AK, which allow us to carefully leverage multiple modalities of information from subgraph encoding, and resulting in an empirically more expressive version GNN-AK + . As a result, GNN-AK and GNN-AK + induce a constant factor overhead in memory. To amplify our method's practicality, we further develop a subgraph sampling strategy inspired by Dropout <ref type="bibr" target="#b50">(Srivastava et al., 2014)</ref> to drastically reduce this overhead (1-3? in practice) without hurting performance. We conduct extensive experiments on 4 simulation datasets and 5 well-known real-world graph classification &amp; regression benchmarks <ref type="bibr" target="#b18">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b27">Hu et al., 2020)</ref>, to show significant and consistent practical benefits of our approach across different MPNNs and datasets. Specifically, GNN-AK + sets new state-of-the-art performance on ZINC, CIFAR10, and PATTERN -for example, on ZINC we see a relative error reduction of 60.3%, 50.5%, and 39.4% for base model being <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, <ref type="bibr">GIN (Xu et al., 2018)</ref>, and (a variant of) PNA <ref type="bibr" target="#b14">(Corso et al., 2020)</ref> respectively. To summarize, our contributions are listed as follows: ? A General GNN-AK Framework. We propose GNN-AK (and enhanced GNN-AK + ), a general framework which uplifts any GNN by encoding local subgraph structure with a GNN. ? Theoretical Findings. We show that GNN-AK's expressiveness is strictly better than 1&amp;2-WL, and is not less powerful than 3-WL. We analyze sufficient conditions for successful discrimination. ? Effective and Efficient Realization. We present effective implementations for GNN-AK and GNN-AK + to fully exploit all node embeddings within a subgraph. We design efficient online subgraph sampling to mitigate memory and runtime overhead while maintaining performance. ? Experimental Results. We show strong empirical results, demonstrating both expressivity improvements as well as practical performance gains where we achieve new state-of-the-art performance on several graph-level benchmarks. Our implementation is easy-to-use, and directly accepts any GNN from PyG  for plug-and-play use. See code at https://github.com/GNNAsKernel/GNNAsKernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Exploiting subgraph information in GNNs is not new; in fact, k-WL considers all k node subgraphs. <ref type="bibr" target="#b39">Monti et al. (2018)</ref>; <ref type="bibr" target="#b32">Lee et al. (2019)</ref> exploit motif information within aggregation, and others <ref type="bibr" target="#b11">(Bouritsas et al., 2020;</ref><ref type="bibr" target="#b6">Barcel? et al., 2021)</ref> augment MPNN features with handcrafted subgraph based features. MixHop <ref type="bibr" target="#b1">(Abu-El-Haija et al., 2019)</ref> directly aggregates k-hop information by using adjacency matrix powers, ignoring neighbor connections. Towards a meta-learning goal, G-meta <ref type="bibr" target="#b28">(Huang &amp; Zitnik, 2020)</ref> applies GNNs on rooted subgraphs around each node to help transferring ability. Tahmasebi &amp; Jegelka (2020) only theoretically justifies subgraph convolution with GNN by showing its ability in counting substructures. <ref type="bibr">Zhengdao et al. (2020)</ref> also represent a node by encoding its local subgraph, however using non-scalable relational pooling. k-hop GNN (Nikolentzos et al., 2020) uses k-egonet in a specially designed way: it encodes a rooted subgraph via sequentially passing messages from k-th hops in the subgraph to k ? 1 hops, until it reaches the root node, and use the root node as encoding of the subgraph. Ego-GNNs <ref type="bibr" target="#b47">(Sandfelder et al., 2021)</ref> computes a context encoding with <ref type="bibr">SGC (Wu et al., 2019)</ref> as the subgraph encoder, and only be studied on node-level tasks. Both k-hop GNN and Ego-GNNs can be viewed as a special case of <ref type="bibr">GNN-AK. You et al. (2021)</ref> designs ID-GNNs which inject node identity during message passing with the help of k-egonet, with k being the number of layers of <ref type="bibr">GNN (Hamilton et al., 2017)</ref>. Unlike GNN-AK which uses rooted subgraphs, <ref type="bibr">Thiede et al. (2021)</ref>; <ref type="bibr" target="#b9">Bodnar et al. (2021a)</ref> design GNNs to use certain subgraph patterns (like cycles and paths) in message passing, however their preprocessing requires solving the subgraph isomorphism problem. <ref type="bibr">Cotta et al. (2021)</ref> explores reconstructing a graph from its subgraphs. A contemporary work <ref type="bibr">(Zhang &amp; Li, 2021)</ref> also encodes rooted subgraphs with a base GNN but it essentially views a graph as a bag of subgraphs while GNN-AK modifies the 1-WL color refinement and has many iterations. Viewing graph as a bag of subgraphs is also explored in another contemporary work <ref type="bibr" target="#b8">(Bevilacqua et al., 2022)</ref>. To summarize, our work differs by (i) proposing a general subgraph encoding framework motivated from theoretical Subgraph-1-WL for uplifting GNNs, and (ii) addressing scalability issues involved with using subgraphs, which poses significant challenges for subgraph-based methods in practice. See additional related work in Appendix.A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENERAL FRAMEWORK AND THEORY</head><p>We first introduce our setting and formalisms. Let G = (V, E) be a graph with node features x i ? R d , ?i ? V. We consider graph-level problems where the goal is to classify/regress a target y G by learning a graph-level representation h G . Let N k (v) be the set of nodes in the k-hop egonet rooted at node v. N (v) = N 1 (v)\v denotes the immediate neighbors of node v. For S ? V, let G[S] be the induced subgraph: G[S] = (S, {(i, j) ? E|i ? S, j ? S}). Then G[N k (v)] denotes the k-hop egonet rooted at node v. We also define Star(v) = (N 1 (v), {(v, j) ? E|j ? N (v)}) be the induced star-like subgraph around v. We use {?} denotes multiset, i.e. set that allows repetition.</p><p>Before presenting GNN-AK, we highlight the insights in designing GNN-AK and driving the expressiveness boost. Insight 1: Generalizing star to subgraph. In MPNNs, every node aggregates information from its immediate neighbors following a star pattern. Consequently, MPNNs fail to distinguish any non-isomorphic regular graphs where all stars are the same, since all nodes have the same degree. Even simply generalizing star to the induced, 1-hop egonet considers connec-tions among neighbors, enabling distinguishing regular graphs. Insight 2: Divide and conquer. When two graphs are non-isomorphic, there exists a subgraph where this difference is captured (see <ref type="figure">Figure 3</ref>). Although a fixed-expressiveness GNN may not distinguish the two original graphs, it may distinguish the two smaller subgraphs, given that the required expressiveness for successful discrimination is proportional to graph size <ref type="bibr" target="#b35">(Loukas, 2020b)</ref>. As such, GNN-AK divides the harder problem of encoding the whole graph to smaller and easier problems of encoding its subgraphs, and "conquers" the encoding with the base GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FROM STARS TO SUBGRAPHS</head><p>We first take a close look at MPNNs, identifying their limitations and expressiveness bottleneck. MPNNs repeatedly update each node's embedding by aggregating embeddings from their neighbors a fixed number of times (layers) and computing a graph-level embedding h G by global pooling. Let h (l) v denote the l-th layer embedding of node v. Then, MPNNs compute h G by</p><formula xml:id="formula_0">h (l+1) v = ? (l) h (l) v , f (l) h (l) u |u ? N (v) l = 0, ..., L ? 1 ; hG = POOL({h (L) v |v ? V}) (1) where h (0) i = x i is the original features,</formula><p>L is the number of layers, and ? (l) and f (l) are the l-th layer update and aggregation functions. ? (l) , f (l) and POOL vary among different MPNNs and influence their expressiveness and performance. MPNNs achieve maximum expressiveness (1-WL) when all three functions are injective <ref type="bibr">(Xu et al., 2018)</ref>.</p><p>MPNNs' expressiveness upper bound follows from its close relation to the 1-WL isomorphism test <ref type="bibr" target="#b40">(Morris et al., 2019)</ref>. Similar to MPNNs which repeatedly aggregate self and neighbor representations, at t-th iteration, for each node v, 1-WL test aggregates the node's own label (or color) c </p><formula xml:id="formula_1">(t) v , {c (t) u |u ? N (v)} into a new, compressed label c (t+1) v</formula><p>. 1-WL outputs the set of all node labels c (T ) v |v ? V as G's fingerprint, and decides two graphs to be non-isomorphic as soon as their fingerprints differ.</p><p>The hash process in 1-WL outputs a new label c (t+1) v that uniquely characterizes the star graph Star(v) around v, i.e. two nodes u, v are assigned different compressed labels only if Star(u) and Star(v) differ. Hence, it is easy to see that when two non-isomorphic unlabeled (i.e., all nodes have the same label) d-regular graphs have the same number of nodes, 1-WL cannot distinguish them. This failure limits the expressiveness of 1-WL, but also identifies its bottleneck: the star is not distinguishing enough. Instead, we propose to generalize the star Star(v) to subgraphs, such as the egonet G[N 1 (v)] and more generally k-hop egonet G[N k (v)]. This results in an improved version of 1-WL which we call Subgraph-1-WL. Formally, Definition 3.1 (Subgraph-1-WL). Subgraph-1-WL generalizes the 1-WL graph isomorphism test algorithm by replacing color refinement (at iteration t) by c</p><formula xml:id="formula_2">(t+1) v = HASH(Star (t) (v)) with c (t+1) v = HASH(G (t) [N k (v)]), ?v ? V where HASH(?)</formula><p>is an injective function on graphs. Note that an injective hash function for star graphs is equivalent to that for multi-sets, which is easy to derive <ref type="bibr">(Zaheer et al., 2017)</ref>. In contrast, Subgraph-1-WL must hash a general subgraph , where an injective hash function for graphs is non-trivial (as hard as graph isomorphism). Thus, we derive a variant called Subgraph-1-WL * by using a weaker choice for HASH(?) -specifically, 1-WL. Effectively, we nest 1-WL inside Subgraph-1-WL. Formally,</p><formula xml:id="formula_3">Definition 3.2 (Subgraph-1-WL * ). Subgraph-1-WL * is a less expressive variant of Subgraph-1- WL where c (t+1) v = 1-WL(G (t) [N k (v)]).</formula><p>We further transfer Subgraph-1-WL to neural networks, resulting in GNN-AK whose expressiveness is upper bounded by Subgraph-1-WL. The natural transformation with maximum expressiveness is to replace the hash function with a universal subgraph encoder of G[N k (v)], which is non-trivial as it implies solving the challenging graph isomorphism problem in the worst case. Analogous to using 1-WL as a weaker choice for HASH(?) inside Subgraph-1-WL * , we can use use any GNN (most practically, MPNN) as an encoder for subgraph</p><formula xml:id="formula_4">G[N k (v)]. Let G (l) [N k (v)] = G[N k (v)|H (l) ]</formula><p>be the attributed subgraph with hidden features H (l) at the l-th layer. Then, GNN-AK computes h G by</p><formula xml:id="formula_5">h (l+1) v = GNN (l) G (l) [N k (v)] l = 0, ..., L ? 1 ; hG = POOL({h (L) v |v ? V})<label>(2)</label></formula><p>Notice that GNN-AK acts as a "wrapper" for any base GNN (mainly MPNN). This uplifts its expressiveness as well as practical performance as we demonstrate in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">THEORY: EXPRESSIVENESS ANALYSIS</head><p>We next theoretically study the expressiveness of GNN-AK, by investigating the expressiveness of Subgraph-1-WL. We first establish that GNN-AK and Subgraph-1-WL have the same expressiveness under certain conditions. A GNN is able to distinguish two graphs if its embeddings for two graphs are not identical. A GNN is said to have the same expressiveness as a graph isomorphism test when for any two graphs the GNN outputs different embeddings if and only if (iff) the isomorphism test deems them non-isomorphic. We give following theorems with proof in Appendix. Theorem 1. When the base model is an MPNN with sufficient number of layers and injective ?, f and POOL functions shown in Eq. (1), and MPNN-AK has an injective POOL function shown in Eq.</p><p>(2), then MPNN-AK is as powerful as Subgraph-1-WL * .</p><p>See Appendix.A.3 for proof. A more general version of Theorem 1 is that GNN-AK is as powerful as Subgraph-1-WL iff base GNN of GNN-AK is as powerful as the HASH function of Subgraph-1-WL in distinguishing subgraphs, following the same proof logic. The Theorem implies that we can characterize expressiveness of GNN-AK through studying Subgraph-1-WL and Subgraph-1-WL * . Theorem 2. Subgraph-1-WL * is strictly more powerful than 1&amp;2-WL 1 .</p><p>A direct corollary from Theorem 1&amp;2 is as follows, which is empirically verified in <ref type="table" target="#tab_2">Table 1</ref>. Corollary 2.1. When MPNN is 1-WL expressive, MPNN-AK is strictly more powerful than 1&amp;2-WL.</p><p>Theorem 3. When HASH(?) is 3-WL expressive, Subgraph-1-WL is no less powerful than 3-WL, that is, it can discriminate some graphs for which 3-WL fails.</p><p>A direct corollary from Theorem 1&amp;3 is as follows, which is empirically verified in <ref type="table" target="#tab_2">Table 1</ref>.</p><formula xml:id="formula_6">Corollary 3.1. PPGN-AK can distinguish some 3-WL-failed non-isomorphic graphs.</formula><p>Theorem 4. For any k ? 3, there exists a pair of k-WL-failed graphs that cannot be distinguished by Subgraph-1-WL even with injective HASH(?) when t-hop egonets are used with t ? 4.</p><p>Theorem 4 is proven (Appendix.A.6) by observing that with limited t all rooted subgraphs of two non-isomorphic graphs from CF I(k) family <ref type="bibr" target="#b12">(Cai et al., 1992)</ref> are isomorphic, i.e. local rooted subgraph is not enough to capture the "global" difference. This opens a future direction of generalizing rooted subgraph to general subgraph (as in k-WL) while keeping number of subgraphs in O(|V|). Proposition 1 (sufficient conditions). For two non-isomorphic graphs G, H, Subgraph-1-WL with k-egonet can successfully distinguish them if: 1) for any node reordering</p><formula xml:id="formula_7">v G 1 , ..., v G |V G | and v H 1 , ..., v H |V H | , ?i ? [1, max(|V G |, |V H |)] that G[N k (v G i )] and H[N k (v H i )] are non-isomorphic 2 ; and 2) HASH(?) is discriminative enough that HASH(G[N k (v G i )]) = HASH(H[N k (v H i )]).</formula><p>This implies subgraph size should be large enough to capture difference, but not larger which requires more expressive base model <ref type="bibr" target="#b34">(Loukas, 2020a)</ref>. We empirically verify Prop. 1 in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCRETE REALIZATION</head><p>We first realize GNN-AK with two type of encodings, and then present an empirically more expressive version, GNN-AK + , with (i) an additional context encoding, and (ii) a subgraph pooling design to incorporate distance-to-centroid, readily computed during subgraph extraction. Next, we discuss a random-walk based rooted subgraph extraction for graphs with small diameter to reduce memory footprint of k-hop egonets. We conclude this section with time and space complexity analysis.</p><formula xml:id="formula_8">Notation. Let G = (V, E) be the graph with N = |V|, G (l) [N k (v)] be the k-hop egonet rooted at node v ? V in which h (l)</formula><p>u denotes node u's hidden representation for u ? N k (v) at the l-th layer of GNN-AK. To simplify notation, we use Sub <ref type="bibr">(l)</ref> [v] instead of G <ref type="bibr">(l)</ref>  <ref type="bibr">[N k (v)</ref>] to indicate the the attributeenriched induced subgraph for v. We consider all intermediate node embeddings across rooted subgraphs. Specifically, let Emb(i | Sub (l) [j]) denote node i's embedding when applying base GNN (l) on Sub (l) [j]; we consider node embeddings for every j ? V and every i ? Sub (l) <ref type="bibr">[j]</ref>. Note that the base GNN can have multiple convolutional layers, and Emb refers to the node embeddings at the last layer before global pooling POOL GNN that generates subgraph-level encoding.</p><p>Realization of GNN-AK. We can formally rewrite Eq. (2) as</p><formula xml:id="formula_9">h (l+1)|Subgraph v = GNN (l) Sub (l) [v] := POOL GNN (l) Emb(i | Sub (l) [v]) | i ? N k (v)<label>(3)</label></formula><p>We refer to the encoding of the rooted subgraph Sub <ref type="bibr">(l)</ref> [v] in Eq.</p><p>(3) as the subgraph encoding. Typical choices of POOL GNN (l) are SUM and MEAN. As each rooted subgraph has a root node, POOL <ref type="bibr">GNN (l)</ref> can additionally be realized to differentiate the root node by self-concatenating its own representation, resulting in the following realization as each layer of GNN-AK:</p><formula xml:id="formula_10">h (l+1) v = FUSE h (l+1)|Centroid v , h (l+1)|Subgraph v where h (l+1)|Centroid v := Emb(v | Sub (l+1) [v]) (4)</formula><p>where FUSE is concatenation or sum, and h (l)|Centroid v is referred to as the centroid encoding. The realization of GNN-AK in Eq.4 closely follows the theory in Sec.3.</p><p>Realization of GNN-AK + . We further develop GNN-AK + , which is more expressive than GNN-AK, based on two observations. First, we observe that Eq.4 does not fully exploit all information inside the rich intermediate embeddings generated for Eq.4, and propose an additional context encoding.</p><formula xml:id="formula_11">h (l+1)|Context v := POOLContext Emb(v | Sub (l) [j]) | ?j s.t. v ? N k (j)<label>(5)</label></formula><p>Different from subgraph and centroid encodings, the context encoding captures views of node v from different subgraph contexts, or points-of-view. Second, GNN-AK extracts the rooted subgraph for every node with efficient k-hop propagation (complexity O(k|E|)), along which the distance-tocentroid (D2C) 3 within each subgraph is readily recorded at no additional cost and can be used to augment node features; <ref type="bibr" target="#b33">(Li et al., 2020)</ref> shows this theoretically improves expressiveness. Therefore, we propose to uses the D2C by default in two ways in GNN-AK + : (i) augmenting hidden representation h (l) v by concatenating it with the encoding of D2C; (ii) using it to gate the subgraph and context encodings before POOL Subgraph and POOL Context , with the intuition that embeddings of nodes far from v contribute differently from those close to v.</p><p>To formalize the gate mechanism guided by D2C, let d (l) i|j be the encoding of distance from node i to j at l-th layer 4 . Applying gating changes Eq. (5) to</p><formula xml:id="formula_12">h (l+1)|Context gated,v := POOLContext Sigmoid(d (l) v|j ) Emb(v | Sub (l) [j]) | ?j s.t. v ? N k (j)<label>(6)</label></formula><p>where denotes element-wise multiplication. Similar changes apply to Eq.</p><p>(3) to get h</p><formula xml:id="formula_13">(l)|Subgraph gated,v .</formula><p>Formally, each layer of GNN-AK + is defined as</p><formula xml:id="formula_14">h (l+1) v = FUSE d (l+1) i|j , h (l+1)|Centroid v , h (l+1)|Subgraph gated,v , h (l+1)|Context gated,v<label>(7)</label></formula><p>where FUSE is concatenation or sum. We illustrate in <ref type="figure" target="#fig_1">Figure 1</ref> the l-th layer of GNN-AK( + ). Proposition 2. GNN-AK + is at least as powerful as GNN-AK. See Proof in Appendix.A.A.8.</p><p>Beyond k-egonet Subgraphs. The k-hop egonet (or k-egonet) is a natural choice in our framework, but can be too large when the input graph's diameter is small, as in social networks <ref type="bibr" target="#b31">(Kleinberg, 2000)</ref>, or when the graph is dense. To limit subgraph size, we also design a random-walk based subgraph extractor. Specifically, to extract a subgraph rooted at node v, we perform a fixed-length random walk starting at v, resulting in visited nodes N rw (v) and their induced subgraph</p><formula xml:id="formula_15">G[N rw (v)].</formula><p>In practice, we use adaptive random walks as in <ref type="bibr" target="#b24">Grover &amp; Leskovec (2016)</ref>. To reduce randomness, we use multiple truncated random walks and union the visited nodes as N rw (v). Moreover, we employ online subgraph extraction during training that re-extracts subgraphs at every epoch, which further alleviates the effect of randomness via regularization.</p><p>Complexity Analysis. Assuming k-egonets as rooted subgraphs, and an MPNN as base model. For each graph G = (V, E), the subgraph extraction takes O(k|E|) runtime complexity, and outputs |V| subgraphs, which collectively can be represented as a union graph</p><formula xml:id="formula_16">G ? = (V ? , E ? ) with |V| disconnected components, where |V ? | = v?V |N k (v)| and |E ? | = v?V |E G[N k (v)] |. GNN-AK( + )</formula><p>can be viewed as applying base GNN on the union graph. Assuming base GNN has O(|V| + |E|) runtime and memory complexity, GNN-AK( + ) has O(|V ? | + |E ? |) runtime and memory cost. For rooted subgraphs of size s, GNN-AK( + ) induces an O(s) factor overhead over the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPROVING SCALABILITY: SUBGRAPHDROP</head><p>The complexity analysis reveals that GNN-AK( + ) introduce a constant factor overhead (subgraph size) in runtime and memory over the base GNN. Subgraph size can be naturally reduced by choosing smaller k for k-egonet, or by ranking and truncating visited nodes in a random walk setting. However limiting to very small subgraphs tends to hurt performance as we empirically show in <ref type="table" target="#tab_9">Table 6</ref>. Here, we present a different subsampling-based approach that carefully selects only a subset of the |V| rooted subgraphs. Further more, inspired from Dropout <ref type="bibr" target="#b50">(Srivastava et al., 2014)</ref>, we only drop subgraphs during training while still use all subgraphs when evaluation. Novel strategies are designed specifically for three type of encodings to eliminate the estimation bias between training and evaluation. We name it SubgraphDrop for dropping subgraphs during training. SubgraphDrop significantly reduces memory overhead while keeping performance nearly the same as training with all subgraphs. We first present subgraph sampling strategies, then introduce the designs of aligning training and evaluation. <ref type="figure" target="#fig_2">Fig. 2</ref> in Appendix.A.2 provides a pictorial illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SUBGRAPH SAMPLING STRATEGIES</head><formula xml:id="formula_17">Intuitively, if u, v are directly connected in G, subgraphs G[N k (u)] and G[N k (v)]</formula><p>share a large overlap and may contain redundancy. With this intuition, we aim to sample only m |V| minimally redundant subgraphs to reduce memory overhead. We propose three fast sampling strategies (See Appendix.A.2) that select subgraphs to evenly cover the whole graph, where each node is covered by ?R (redundancy factor) selected subgraphs; R is a hyperparameter used as a sampling stopping condition. Then, GNN-AK( + )-S (with Sampling) has roughly R times the overhead of base model (R?3 in practice). We remark that our subsampling strategies are randomized and fast, which are both desired characteristics for an online Dropout-like sampling in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TRAINING WITH SUBGRAPHDROP</head><p>Although dropping redundant subgraphs greatly reduces overhead, it still loses information. Thus, as in Dropout <ref type="bibr" target="#b50">(Srivastava et al., 2014)</ref>, we only "drop" subgraphs during training while still using all of them during evaluation. Randomness in sampling strategies enforces that selected subgraphs differ across training epochs, preserving most information due to amortization. On the other hand, it makes it difficult for the three encodings during training to align with full-mode evaluation. Next, we propose an alignment procedure for each type of encoding. |v ? S}. To estimate uncomputed encodings of u ? V \ S, we propose to propagate encodings from S to V \ S. Formally, let k max = max u?V\S Dist(u, S) where Dist(u, S)=min v?S ShortestPathDistance(u, v). Then we partition U = V \ S into {U 1 , ..., U kmax } with U d = {u ? U|Dist(u, S) = d}. We propose to spread vector encodings of S out iteratively, i.e. compute vectors of U d from U d?1 . Formally, we have</p><formula xml:id="formula_18">h (l)|Subgraph u = Mean {h (l)|Subgraph v |v ? U d?1 , (u, v) ? E} for d = 1, 2 . . . kmax, ?u ? U d ,<label>(8)</label></formula><formula xml:id="formula_19">h (l)|Centroid u = Mean {h (l)|Centroid v |v ? U d?1 , (u, v) ? E} for d = 1, 2 . . . kmax, ?u ? U d ,<label>(9)</label></formula><p>Context Encoding in Training. Following Eq. (5), context encodings can be computed for every node v ? V as each node is covered at least R times during training with SubgraphDrop. However when POOL Context is SUM(?), the scale of h (l)|Context v is smaller than the one in full-mode evaluation. Thus, we scale the context encodings up to align with full-mode evaluation. Formally,</p><formula xml:id="formula_20">h (l)|Context v = |{j ? V|N k (j) v}| |{j ? S|N k (j) v}| ? SUM Emb(v | Sub (l) [j]) | ?j ? S s.t. v ? N k (j)<label>(10)</label></formula><p>When POOL Context is MEAN(?), the context encoding is computed without any modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section we (1) empirically verify the expressiveness benefit of GNN-AK( + ) on 4 simulation datasets;</p><p>(2) show GNN-AK( + ) boosts practical performance significantly on 5 real-world datasets;</p><p>(3) demonstrate the effectiveness of SubgraphDrop; (4) conduct ablation studies of concrete designs. We mainly report the performance of GNN-AK + , while still keep the performance of GNN-AK with GIN as base model for reference, as it is fully explained by our theory.</p><p>Simulation Datasets: 1) EXP <ref type="bibr">(Abboud et al., 2021)</ref> contains 600 pairs of 1&amp;2-WL failed graphs that are splited into two classes where each graph of a pair is assigned to two different classes. 2) SR25 <ref type="bibr">(Balcilar et al., 2021)</ref> has 15 strongly regular graphs (3-WL failed) with 25 nodes each. SR25 is translated to a 15 way classification problem with the goal of mapping each graph into a different class. 3) Substructure counting (i.e. triangle, tailed triangle, star and 4-cycle) problems on random graph dataset <ref type="bibr">(Zhengdao et al., 2020)</ref>. 4) Graph property regression (i.e. connectedness, diameter, radius) tasks on random graph dataset <ref type="bibr" target="#b14">(Corso et al., 2020)</ref>. All simulation datasets are used to empirically verify the expressiveness of GNN-AK( + ). Large Real-world Datasets: ZINC-12K, CIFAR10, PATTER from Benchmarking GNNs <ref type="bibr" target="#b18">(Dwivedi et al., 2020)</ref> and MolHIV, and MolPCBA from Open Graph Benchmark <ref type="bibr" target="#b27">(Hu et al., 2020)</ref>. Small Real-world Datasets: MUTAG, PTC, PRO-TEINS, NCI1, IMDB, and REDDIT from TUDatset <ref type="bibr">(Morris et al., 2020a)</ref> (their results are presented in Appendix A.12). See <ref type="table" target="#tab_8">Table 5</ref> in Appendix for all dataset statistics.</p><p>Baselines. We use <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, <ref type="bibr">GIN (Xu et al., 2018)</ref>, PNA * 5 <ref type="bibr" target="#b14">(Corso et al., 2020)</ref>, and 3-WL powerful <ref type="bibr">PPGN (Maron et al., 2019a)</ref> directly, which also server as base model of GNN-AK to see its general uplift effect. <ref type="bibr">GatedGCN (Dwivedi et al., 2020)</ref>, DGN <ref type="bibr" target="#b7">(Beani et al., 2021)</ref>, PNA <ref type="bibr" target="#b14">(Corso et al., 2020)</ref>, GSN <ref type="bibr" target="#b11">(Bouritsas et al., 2020)</ref>, HIMP , and CIN <ref type="bibr" target="#b9">(Bodnar et al., 2021a)</ref> are referenced directly from literature for real-world datasets comparison. Hyperparameter and model configuration are described in Appendix.A.10.  <ref type="table" target="#tab_2">Table 1</ref> presents the results on simulation datasets. To save space we present GNN-AK + with different base models but only one one version of GNN-AK: GIN-AK. All GNN-AK( + ) variants perform perfectly on EXP, while only PPGN alone do so previously. Moreover, PPGN-AK + reaches perfect accuracy on SR25, while PPGN fails. Similarly, GNN-AK( + ) consistently boosts all MPNNs for substructure and graph property prediction (PPGN-AK + is OOM as it is quadratic in input size). In <ref type="table" target="#tab_3">Table 2</ref> we look into PPGN-AK's performance on SR25 as a function of kegonets (k?[1, 2]), as well as the number of PPGN (inner) layers and (outer) iterations for PPGN-AK. We find that at least 2 inner layers is needed with 1egonet subgraphs to achieve top performance. With 2-egonets more inner layers helps, although performance is sub-par, attributed to PPGN's disability to distinguish larger (sub)graphs, aligning with Proposition 1 (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">EMPIRICAL VERIFICATION OF EXPRESSIVENESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">COMPARING WITH SOTA AND GENERALITY</head><p>Having studied expressiveness tasks, we turn to performance on real-world datasets, as shown in <ref type="table" target="#tab_4">Table 3</ref>. We observe similar performance lifts across all datasets and base GNNs (we omit PPGN due to scalability), demonstrating our framework's generality. Remarkably, GNN-AK + sets new SOTA performance for several benchmarks -ZINC, CIFAR10, and PATTERN -with a relative error reduction of 60.3%, 50.5%, and 39.4% for base model being GCN, GIN, and PNA * respectively. Notably, GNN-AK + -S models, shown at the end of <ref type="table" target="#tab_4">Table 3</ref>, do not compromise and can even improve performance as compared to their non-sampled counterpart, in alignment with Dropout's benefits <ref type="bibr" target="#b50">Srivastava et al. (2014)</ref>. Next we evaluate resource-savings, specifically on ZINC-12K and CIFAR10. <ref type="table" target="#tab_5">Table 4</ref> shows that GIN-AK + -S with varying R provides an effective handle to trade off resources with performance. Importantly, the rate in which performance decays with smaller R is much lower than the rates at which runtime and memory decrease. We present ablation results on various structural components of GNN-AK( + ) in Appendix A.11. <ref type="table" target="#tab_9">Table 6</ref> shows the performance of GIN-AK + for varying size egonets with k. <ref type="table" target="#tab_10">Table 7</ref> illustrates the added benefit of various encodings and D2C feature. <ref type="table" target="#tab_11">Table 8</ref> extensively studies the effect of context encoding and D2C in GNN-AK + , as they are not explained by Subgraph-1-WL * . <ref type="table" target="#tab_12">Table 9</ref> studies the effect of base model's depth (or expressiveness) for GNN-AK + with and without D2C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Our work introduces a new, general-purpose framework called GNN-As-Kernel (GNN-AK) to uplift the expressiveness of any GNN, with the key idea of employing a base GNN as a kernel on induced subgraphs of the input graph, generalizing from the star-pattern aggregation of classical MPNNs.</p><p>Our approach provides an expressiveness and performance boost, while retaining practical scalability of MPNNs-a highly sought-after middle ground between the two regimes of scalable yet less-expressive MPNNs and high-expressive yet practically infeasible and poorly-generalizing korder designs. We theoretically studied the expressiveness of GNN-AK, provided a concrete design and the more powerful GNN-AK + , introducing SubgraphDrop for shrinking runtime and memory footprint. Extensive experiments on both simulated and real-world benchmark datasets empirically justified that GNN-AK( + ) (i) uplifts base GNN expressiveness for multiple base GNN choices (e.g. over 1&amp;2-WL for MPNNs, and over 3-WL for PPGN), (ii) which translates to performance gains with SOTA results on graph-level benchmarks, (iii) while retaining scalability to practical graphs. Connections to CNN and k-WL: GNN-AK has a similar convolutional structure as CNN, and in fact historically many spatial GNNs are inspired by CNN; see <ref type="bibr">Wu et al. (2020)</ref> for a detailed survey. The non-Euclidean nature of graphs makes such generalizations non-trivial. MoNet <ref type="bibr" target="#b38">(Monti et al., 2017)</ref> introduces pseudo-coordinates for nodes, while PatchySAN <ref type="bibr" target="#b45">(Niepert et al., 2016)</ref> learns to order and truncate neighboring nodes for convolution purposes. However, both methods aim to mimic the formulation of the CNN without admitting the inherent difference between graphs and images. In contrast, GNN-AK, generalizes CNN to graphs with a base GNN kernel, similar to how a CNN kernel encodes image patches. GNN-AK also shares connections with two variants of k-WL test algorithms: depth-k 1-dim WL <ref type="bibr" target="#b12">(Cai et al., 1992;</ref><ref type="bibr">Weisfeiler, 1976)</ref> and deep WL <ref type="bibr" target="#b2">(Arvind et al., 2020)</ref>. The former recursively applies 1-WL to all size-k subgraphs, with slightly weaker expressiveness than k-WL, and the latter reduces the number of such subgraphs for the k-WL test. The descriptions of subgraph sampling strategies are as follows. <ref type="figure" target="#fig_2">Fig. 2</ref> shows an overview of SubgraphDrop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>? Random sampling selects subgraphs randomly until every node is covered ?R times.</p><p>? Farthest sampling selects subgraphs iteratively, starting at a random one and greedily selecting each subsequent one whose root node is farthest w.r.t. shortest path distance from those of already selected subgraphs, until every node is covered ?R times. ? Min-set-cover sampling initially selects a subgraph randomly, and follows the greedy minimum set cover algorithm to iteratively select the subgraph containing the maximum number of uncovered nodes, until every node is covered ?R times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 PROOF OF THEOREM 1</head><p>Proof. <ref type="bibr">(Xu et al., 2018)</ref> proved that with sufficient number of layers and all injective functions, MPNN is as powerful as 1-WL. Then Eq.2 outputs different vectors for two graphs iff Subgraph-1-WL * encodes different labels with c</p><formula xml:id="formula_21">(t+1) v = 1-WL(G (t) [N k (v)]).</formula><p>With POOL in Eq.2 also to be injective, MPNN-AK outputs different vectors iff Subgraph-1-WL * outputs different fingerprints for two graphs. Then MPNN-AK is as powerful as Subgraph-1-WL * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 PROOF OF THEOREM 2</head><p>Proof. We first prove that if two graphs are identified as isomorphic by Subgraph-1-WL * , they are also determined as isomorphic by 1-WL. Then we present a pair of non-isomorphic graphs that can be distinguished by Subgraph-1-WL * but not by 1-WL. Together these two imply that Subgraph-1-WL * is strictly more powerful than 1-WL. Comparing with 2-WL can be concluded from the fact that 1-WL and 2-WL are equivalent in expressiveness <ref type="bibr" target="#b36">(Maron et al., 2019a)</ref>. In the proof we use 1-hop egonet subgraphs for Subgraph-1-WL * .</p><p>Assume graphs G and H have the same number of nodes (otherwise easily determined as nonisomorphic) and are two non-isomorphic graphs but Subgraph-1-WL * determines them as isomorphic. Then for any iteration t, set 1-WL(G <ref type="bibr">(t)</ref>  </p><formula xml:id="formula_22">[N 1 (v)])|v ? V G is the same as set 1-WL(H (t) [N 1 (v)])|v ? V H . Then there existing an ordering of nodes v G 1 , ..., v G n and v H 1 , ..., v H N with N = |V G | = |V H |, such that for any node order i = 1, ..., N , 1-WL(G (t) [N 1 (v G i )]) = 1-WL(H (t) [N 1 (v H i )]). This implies that structure G[N 1 (v G i )] and H[N 1 (v H i )] are</formula><formula xml:id="formula_23">v G i ) and Star(v H i ) can also distin- guish G[N 1 (v G i )] and H[N 1 (v H i )].</formula><p>Then for any iteration t and any node with order i,</p><formula xml:id="formula_24">HASH Star(v G (t) i ) = HASH Star(v H (t) i</formula><p>) implies that 1-WL fails in distinguishing G and H. In fact if we replace the 1-WL hashing function in Subgraph-1-WL * to a stronger version HASH HASH(Star(v G (t) )), 1-WL(G (t) [N 1 (v)]) , this directly implies the above statement.  <ref type="figure">Figure 3</ref>: Two 4-regular graphs that cannot be distinguished by 1-WL. Colored edges are the difference between two graphs. Two 1-hop egonets are visualized while all other rooted egonets are ignored as they are same across graph A and graph B.</p><p>In <ref type="figure">Figure 3</ref>, two 4-regular graphs are presented that cannot be distinguished by 1-WL but can be distinguished by Subgraph-1-WL * . We visualize the 1-hop egonets that are structurally different among graphs A and B. It's easy to see that A's egonet A[N 1 (1)] and B's egonet B[N 1 (1)] can be distinguished by 1-WL, as degree distribution is not the same. Hence, A and B can be distinguished by Subgraph-1-WL * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 PROOF OF THEOREM 3</head><p>Proof. We prove by showing a pair of 3-WL failed non-isomorphic graphs can be distinguished by Subgraph-1-WL (see definition of "no less powerful" in <ref type="bibr">Zhengdao et al. (2020)</ref>: we call A is no/not less powerful than B if there exists a pair of non-isomorphic graphs that cannot be distinguished by B but can be distinguished by A.), assuming HASH(?) is 3-WL discriminative. <ref type="figure">Figure 4</ref> shows two strongly regular graphs that can not be distinguished by 3-WL (any strongly regular graphs are not distinguishable by 3-WL <ref type="bibr" target="#b2">(Arvind et al., 2020)</ref>), along with their 1-hop egonet rooted subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B</head><p>A's subgraph B's subgraph <ref type="figure">Figure 4</ref>: Two non-isomorphic strongly regular graphs that cannot be distinguished by 3-WL.</p><p>Notice that all 1-hop egonet rooted subgraphs in A (also in B) are the same, resulting that Subgraph-1-WL can successfully distinguish A and B if HASH can distinguish the showed two 1-hop egonets. Now we prove that 3-WL can distinguish these two subgraphs. 3-WL constructs a coloring of 3tuples of all vertices in a graph, and uses the histogram of colors of all k-tuples as fingerprint of the graph. Then, different 3-tuples correspond to different colors or bins in the histogram. As a triangle is a unique type of 3-tuple, at iteration 0 of 3-WL, the histogram of all 3-tuples counts the number of triangles in the graph. Notice that A's subgraph has 2 ? 4 3 = 8 triangles, and B's subgraph contains 6 triangles. This implies that even at iteration 0, 3-WL can distinguish these two subgraphs. Hence, when HASH(?) is 3-WL expressive, Subgraph-1-WL can distinguish A and B. Therefore there exists a pair of 3-WL failed non-isomorphic graphs that can be distinguished by Subgraph-1-WL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 PROOF OF THEOREM 4</head><p>Proof. For any k ? 3, <ref type="bibr" target="#b12">Cai et al. (1992)</ref> provided a scheme to construct a pair of k-WL-failed nonisomorphic graphs based on a base graph G with separator size k + 1 (more specifically, G can be chosen as a degree-3 regular graph with separator size k + 1, see Corollary 6.5 in <ref type="bibr" target="#b12">Cai et al. (1992)</ref>).</p><p>We describe the proposed scheme of constructing the two graphs A and B briefly, for details see Section 6 in <ref type="bibr" target="#b12">Cai et al. (1992)</ref>. At high level, this scheme first constructs A by replacing every node (with degree d) in G by a carefully designed graph X d , and then rewires two edges in A to its non-isomorphic counter graph B. Specifically, the small graph X d = (V d , E d ) is defined as follows.</p><formula xml:id="formula_25">V d =A d ? B d ? M d , where A d = {a i |1 ? i ? d}, B d = {b i |1 ? i ? d}, and M d = {m S |S ? {1, ..., d}, |S| is even} (11) E d ={(m S , a i )|i ? S} ? {(m S , b i )|i /</formula><p>? S} Hence each vertex v with degree d in G is replaced by a graph X(v) = X d of size 2 d?1 + 2d, including a "middle" section M d of size 2 d?1 and d pairs of vertices (a i , b i ) representing "endpoints" of each edge incident with v in the original graph G. Let (u, v) ? E(G), then two small graphs X(v) and X(u) in A are connected by the following. Let (a u,v , b u,v ), (a v,u , b v,u ) be the pair of vertices associated with (u, v) in X(u) and X(v) respectively, then a u,v is connected to a v,u , and b u,v is connected to b v,u . After constructing A, B is modified from A by rewiring a single edge <ref type="bibr">(u, v)</ref> in the original graph G. Specifically, now in B, a u,v is connected to b v,u and b u,v is connected to a v,u . <ref type="bibr" target="#b12">Cai et al. (1992)</ref> proved that A and B are non-isomorphic, and when base graph G is a degree-3 regular graph with separator size k + 1, A and B are not distinguishable by k-WL. See a visualization in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>We provide several useful lemmas related to graph X d in the following, which are then used in proving Theorem 4.</p><p>Lemma 5 <ref type="bibr" target="#b12">(Cai et al. (1992)</ref>). X d has exactly 2 d?1 automorphisms, and each automorphism is determined by interchanging a i and b i for each i in some subset S ? {1, ..., d} of even cardinality. Lemma 5 is directly taken from Lemma 6.1 in <ref type="bibr" target="#b12">Cai et al. (1992)</ref>. Lemma 6. The shortest path distance between a i and b i in X d for any i is exactly 4, and diameter of X d is 4.</p><p>Proof. Based on the construction shown in Eq.11, for any i, a i and b i are not directly connected, and any middle nodes share no connection. This implies the shortest path between a i and b i is larger than 3. Let m S be one middle node connected with a i . We construct a new set S = {p, q}, with p = i, p ? S and q / ? S, then m S connects with b i based on Eq.11. Additionally, a p connects to both m S and m S , which implies the shortest path distance between a i and b i ? 4. Hence, the shortest path distance between a i and b i is exactly 4. To show the diameter of X d is 4, it's easy to observe that distance(a i , b i ) only decreases when replacing a i with other nodes in X d (or reverse, replacing b i with other nodes in X d ).</p><p>Lemma 7. For any k ? 1, k-egonets of a i and b i in X d are isomorphic.</p><p>Proof. When k ? 4, X d [N k (a i )] = X d [N k (b i )] = X d based on Lemma 6. Now for 1 ? k ? 3, we analyze the k-hop egonets of a i and b i sequentially. First, X d [N 1 (a i )] and X d [N 1 (b i )] are two depth-1 trees with equal number of leaves (middle nodes in X d ), and of course are isomorphic.</p><p>Similarly, X d [N 2 (a i )] and X d [N 2 (b i )] are two depth-2 trees with equal number of depth-1 nodes and equal number of leaves, which are also isomorphic. Last, X d [N 3 (a i )] and X d [N 3 (b i )] are the graphs by removing b i from X d and a i from X d , respectively. It's trivial to observe that they are also isomorphic. Combining all cases proves the Lemma.</p><p>To prove the main theorem, we visualize the two graphs A and B zoomed at edge (u, v) of base graph in <ref type="figure" target="#fig_4">Figure 5</ref> to help understanding. To prove that Subgraph-1-WL with t ? 4-egonet (we use t to prevent notation conflict) cannot distinguish A and B, we mainly analyze the subgraph around a u,v for both graph A and B. For other subgraphs with different root nodes within t shortest-path-distance around nodes a u,v , a v,u , b u,v , b v,u , they follow exactly the same argument. For all subgraphs with other root nodes, it's trivial to observe that there is no difference among two subgraphs in A and B.</p><p>We introduce some notation first. Let A[N t (a u,v )] and B[N t (a u,v )] be the two t-egonet subgraphs around a u,v in A and B respectively. Let A lef t [N t (a u,v )] be the "left" part graph (visualized in <ref type="figure" target="#fig_4">Figure 5</ref>, the left side of a u,v , include a u,v ) of A[N t (a u,v )], and A right [N t (a u,v )] be the "right" part graph (visualized in <ref type="figure" target="#fig_4">Figure 5</ref>, the right side of a u,v , excluding</p><formula xml:id="formula_26">a u,v ) of A[N t (a u,v )]. Then A[N t (a u,v )] is reconstructed by connecting A lef t [N t (a u,v )] and A right [N t (a u,v )] with a single edge (a u,v , a v,u ). B lef t [N t (a u,v )] and B right [N t (a u,v )] are defined in the same way, such that B[N t (a u,v )] is reconstructed by connecting (a u,v , b v,u ) in B.</formula><p>When t ? 4, we show that there exists an isomorphism that maps A[N t (a u,v )] to B[N t (a u,v )]. We construct this isomorphism by constructing an isomorphism between We remark that the theorem doesn't imply that Subgraph-1-WL with t ? 5-egonet can distinguish A and B. This should depend on the base graph. We give an informal sketch. Suppose that for t ? 5, the left part</p><formula xml:id="formula_27">A lef t [N t (a u,v )] and B lef t [N t (a u,v )] and an isomorphism between A right [N t (a u,v )] and B right [N t (a u,v )] first. Triv- ially, the current node ordering in A lef t [N t (a u,v )] and B lef t [N t (a u,v )] already defines an iso- morphism among them, that is, mapping any a i (b i , m S ) in A lef t [N t (a u,v )] to a i (b i , m S ) in B lef t [N t (a u,v )]. To find an isomorphism between A right [N t (a u,v )] and B right [N t (a u,v )], one can easily observe that for t ? 3, A right [N t (a u,v )] can be 1-to-1 mapped (with a v,u in A be- ing mapped to a i ) to X 3 [N t?1 (a i )] and B right [N t (a u,v )] can be 1-to-1 mapped (with b v,u in A being mapped to b i ) to X 3 [N t?1 (b i )] for some i. When t = 4, A right [N t (a u,v )] is mapped to X 3 [N 3 (a i )] with</formula><formula xml:id="formula_28">A lef t [N t (a u,v )] and right part A right [N t (a u,v )] are only connected through (a u,v , a v,u ), that is, removing (a u,v , a v,u ) from A[N t (a u,v )] resulting in A lef t [N t (a u,v )] and A right [N t (a u,v )]</formula><p>being disconnected. Then we can apply Lemma 5 multiple times with mapping some a v,u in A to b v,u in B and b v,u in A to a v,u in B, until this kind of switches reach the boundary of A[N t (a u,v )], resulting in an isomorphism between A[N t (a u,v )] and B[N t (a u,v )]. We refer the reader to Lemma 6.2 in <ref type="bibr" target="#b12">Cai et al. (1992)</ref>, which also uses this line of argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 PROOF OF PROPOSITION 1</head><p>Proof. The proof is by contradiction. Let G and H be two non-isomorphic graphs and |V G | = |V H | (if graph sizes are different then Subgraph-1-WL can trivially distinguish them). Now suppose that Prop. 1 is incorrect, i.e. Subgraph-1-WL cannot distinguish G and H even provided that the two conditions <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_5">(2)</ref>   While hyperparameters may not be optimal for other GNN models, the evaluation is fair as there is no benefit for GNN-AK( + ). Next, we search GNN-AK( + ) exclusive ones (encoding types) over validation set using GIN-AK( + ) and keep them fixed for other GNN-AK( + ).</p><p>We use a 1-layer base model for GNN-AK( + ), with exceptions that we tune number of layers of base model (while keeping total number of layers fixed) for GNN-AK in simulation datasets (presented in <ref type="table" target="#tab_2">Table 1</ref>). We use 3-hop egonets for GNN-AK( + ), with exceptions that CIFAR10 uses 2-hop egonet due to memory constraint; PATTERN and RDT-B use random walk based subgraph with walk length=10 and repeat times=5 as their graphs are dense. For GNN-AK( + )-S, R = 3 is set as default. We use farthest sampling for molecular datasets ZINC, MolHIV, and MolPCBA; to speed up further, random sampling is used for CIFAR10 whose graphs are k-NN graphs; min-set-cover sampling is used for PATTERN to adapt random walk based subgraph. We use Batch Normalization and ReLU activation in all models. For optimization we use Adam with learning rate 0.001 and weight decay 0. All experiments are repeated 3 times to calculate mean and standard derivation. All experiments are conducted on RTX-A6000 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 ABLATION STUDY</head><p>We present ablation results on various structural components of GNN-AK. <ref type="table" target="#tab_9">Table 6</ref> shows the performance of GIN-AK for varying size egonets with k. We find that larger subgraphs tend to yield improvement, although runtime-performance trade-off may vary by dataset. Notably, simply 1-egonets are enough for CIFAR10 to uplift performance of the base GIN considerably.  Next <ref type="table" target="#tab_10">Table 7</ref> illustrates the added benefit of various node encodings. Compared to the full design, eliminating any of the subgraph, centroid, or context encodings (Eq.s (3)-(5)) yields notably inferior results. Encoding without distance awareness is also subpar. These justify the design choices in our framework and verify the practical benefits of our design.</p><p>As GNN-AK + is not directly explained by Subgraph-1-WL * (though D2C is supported by Subgraph-1-WL, by strengthening HASH), we conduct additional ablation studies over GNN-AK + with different combinations of removing context encoding and D2C, shown in <ref type="table" target="#tab_11">Table 8</ref>. Note that all experiments in <ref type="table" target="#tab_11">Table 8</ref> are using a 1-layer base GIN model. We summarize the observations as follows.</p><p>? For real-world datasets (ZINC-12K, CIFAR10), We observe that the largest performance improvement over base model comes from wrapping base GNN with Eq.2 (the performance of GIN-AK), while adding context encoding and D2C monotonically improves performance of GNN-AK + . ? For substructure counting and graph property regression tasks, we observe D2C significantly increases the performance of GIN-AK + w/o Ctx (supported by the theory of Subgraph-1-WL where the HASH is required to be injective). Specifically, the cost-free D2C feature enhances the expressiveness of the base 1-layer GIN model (similar to the benefit of distance encoding shown in <ref type="bibr" target="#b33">Li et al. (2020)</ref>), resulting in a more expressive GNN-AK + , which lies between Subgraph-1-WL and Subgraph-1-WL * . We leave the exact theoretical analysis of D2C's expressiveness benefit in future work. Notice that GIN-AK improves the performance over the base model but not in a large margin, we next show this is due to the insufficiency of the number of layers of base GIN. According to Prop. 1, the base model must be discriminative enough such that HASH(G[N k (v G i )]) = HASH(H[N k (v H i )]), for Subgraph-1-WL with k-egonet to enjoy expressiveness benefits. In addition to using D2C to increase the expressiveness of base model, another way is to practically increase the number of layers of the base model (akin to increasing the number of iterations of 1-WL, as in the definition of Subgraph-1-WL * ). We study the effect of base model's number of layers in GNN-AK + , with and without D2C in <ref type="table" target="#tab_12">Table 9.</ref> ? Firstly, GNN-AK + with D2C is insensitive to the depth of base model, with 1-layer base model being enough to achieve great performance in counting substructures and the best performance in regressing graph properties. We hypothesize that D2C-enhanced 1-layer GIN base model is discriminative enough for subgraphs in the dataset, and without expressiveness bottleneck of base model, increasing GNN-AK + 's depth benefits expressiveness, akin to increasing iterations of Subgraph-1-WL. Besides, unlike counting substructure that needs local information within subgraphs, regressing graph properties needs the graph's global information which can only be accessed with increasing GNN-AK + 's (outer) depth. ? Secondly, GNN-AK( + ) without D2C suffers from a trade-off between the base model's expressiveness (depth of base model) and the number of GNN-AK( + ) layers (outer depth), which is clearly observed in regressing graph properties. We hypothesize that without D2C the 1-layer GIN base model is not discriminative enough for subgraphs in the dataset, and with this bottleneck of base model, GNN-AK( + ) cannot benefit from increasing the outer depth. Hence the number of layers of the base model are important for the expressiveness of GNN-AK( + ) when D2C is not used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>u</head><label></label><figDesc>|u ? N (v)}, and hashes this multi-set of labels c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A. 1</head><label>1</label><figDesc>ADDITIONAL RELATED WORK Improving Expressiveness of GNNs: Several works other than those mentioned in Sec.1 tackle expressive GNNs. Murphy et al. (2019) achieve universality by summing permutation-sensitive functions across a combinatorial number of permutations, limiting feasibility. Dasoulas et al. (2020) adds node indicators to make them distinguishable, but at the cost of an invariant model, while Vignac et al. (2020) further addresses the invariance problem, but at the cost of quadratic time complexity. Corso et al. (2020) generalizes MPNN's default sum aggregator, but is still limited by 1-WL. Beani et al. (2021) generalizes spatial and spectral aggregation with &gt;1-WL expressiveness, but using expensive eigendecomposition. Recently, Bodnar et al. (2021b) introduce MPNNs over simplicial complexes that shares similar expressiveness as GNN-AK. Ying et al. (2021) studies transformer with above 1-WL expressiveness. Azizian &amp; Lelarge (2021) surveys GNN expressiveness work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Instead of working on all O(n k ) size-k subgraphs, we keep linear scalability by only applying 1-WL-equivalent MPNNs to O(n) rooted subgraphs. GNN-AK-S with SubgraphDrop used in training. GNN-AK-S first extracts subgraphs and subsamples m |V| subgraphs to cover each node at least R times with multiple strategies. The base GNN is applied to compute all intermediate node embeddings in selected subgraphs. Context encodings are scaled to match evaluation. Subgraph and centroid encodings initially only exist for root nodes of selected subgraphs, and are propagated to estimate those of other nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>A pair of CFI graphs (A and B) zoomed at the (rewired) edge (u, v) of a base graph. The base graph is a degree-3 regular graph with separator size k + 1 for k-WL-failed case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>additional 2 nodes and B right [N t (a u,v )] is mapped to X 3 [N 3 (b i )] with additional 2 nodes. Then applying Lemma 7, when t ? 4 we can construct an isomorphism between A right [N t (a u,v )] and N right [N t (a u,v )] with a v,u in A being mapped to b v,u in B. We remark that when t = 4, the two additional nodes outside X 3 [N 3 (a i )] and X 3 [N 3 (b i )] does not affecting applying Lemma 7. Last, the combination of the isomorphism between A lef t [N t (a u,v )] and B lef t [N t (a u,v )] and the isomorphism between A right [N t (a u,v )] and B right [N t (a u,v )] is actually a new isomorphism between A[N t (a u,v )] and B[N t (a u,v )], as the edge (a u,v , a v,u ) is mapped to edge (a u,v , b v,u ). Thus, when t ? 4, A[N t (a u,v )] and B[N t (a u,v )] are isomorphic and HASH(A[N t (a u,v )]) = HASH(B[N t (a u,v )]) no matter what HASH(?) function is used. Applying the same argument to all other pairs of subgraphs in A and B implies that the histogram encodings of A and B are the same. Hence Subgraph-1-WL with t ? 4 cannot distinguish the two graphs A and B that are k-WL-failed for any k ? 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Subgraph and Centroid Encoding in Training. Let S ? V be the set of root nodes of the selected subgraphs. When sampling during training, subgraph and centroid encoding can only be computed for nodes v ? S, following Eq. (3) and Eq. (4), resulting incomplete subgraph encodings {h</figDesc><table><row><cell>(l)|Subgraph v</cell><cell>(l)|Centroid |v ? S} and centroid encodings {h v</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>EXP</cell><cell>SR25</cell><cell cols="4">Counting Substructures (MAE)</cell><cell cols="2">Graph Properties (log 10 (MAE))</cell></row><row><cell></cell><cell>(ACC)</cell><cell>(ACC)</cell><cell cols="2">Triangle Tailed Tri.</cell><cell>Star</cell><cell cols="3">4-Cycle IsConnected Diameter Radius</cell></row><row><cell>GCN</cell><cell cols="3">50% 6.67% 0.4186</cell><cell>0.3248</cell><cell cols="2">0.1798 0.2822</cell><cell>-1.7057</cell><cell>-2.4705 -3.9316</cell></row><row><cell>GCN-AK +</cell><cell cols="3">100% 6.67% 0.0137</cell><cell>0.0134</cell><cell cols="2">0.0174 0.0183</cell><cell>-2.6705</cell><cell>-3.9102 -5.1136</cell></row><row><cell>GIN</cell><cell cols="3">50% 6.67% 0.3569</cell><cell>0.2373</cell><cell cols="2">0.0224 0.2185</cell><cell>-1.9239</cell><cell>-3.3079 -4.7584</cell></row><row><cell>GIN-AK</cell><cell cols="3">100% 6.67% 0.0934</cell><cell>0.0751</cell><cell cols="2">0.0168 0.0726</cell><cell>-1.9934</cell><cell>-3.7573 -5.0100</cell></row><row><cell>GIN-AK +</cell><cell cols="3">100% 6.67% 0.0123</cell><cell>0.0112</cell><cell cols="2">0.0150 0.0126</cell><cell>-2.7513</cell><cell>-3.9687 -5.1846</cell></row><row><cell>PNA  *</cell><cell cols="3">50% 6.67% 0.3532</cell><cell>0.2648</cell><cell cols="2">0.1278 0.2430</cell><cell>-1.9395</cell><cell>-3.4382 -4.9470</cell></row><row><cell cols="4">PNA  *  -AK + 100% 6.67% 0.0118</cell><cell>0.0138</cell><cell cols="2">0.0166 0.0132</cell><cell>-2.6189</cell><cell>-3.9011 -5.2026</cell></row><row><cell>PPGN</cell><cell cols="3">100% 6.67% 0.0089</cell><cell>0.0096</cell><cell cols="2">0.0148 0.0090</cell><cell>-1.9804</cell><cell>-3.6147 -5.0878</cell></row><row><cell cols="3">PPGN-AK + 100% 100%</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell></row></table><note>Simulation dataset performance: GNN-AK( + ) boosts base GNN across tasks, empirically verifying expressiveness lift. (ACC: accuracy, MAE: mean absolute error, OOM: out of memory)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>PPGN-AK expressiveness on SR25.</figDesc><table><row><cell>Base PPGN's #L</cell><cell></cell><cell>1-hop egonet</cell><cell></cell><cell></cell><cell>2-hop egonet</cell><cell></cell></row><row><cell>PPGN-AK's #L</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>1</cell><cell cols="6">26.67% 100% 100% 26.67% 26.67% 46.67%</cell></row><row><cell>2</cell><cell cols="6">33.33% 100% 100% 26.67% 26.67% 53.33%</cell></row><row><cell>3</cell><cell cols="6">26.67% 100% 100% 33.33% 26.67% 53.33%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Real-world dataset performance: GNN-AK + achieves SOTA performance for ZINC-12K, CIFAR10, and PATTERN. (OOM: out of memory, ?: missing values from literature)</figDesc><table><row><cell>Method</cell><cell cols="3">ZINC-12K (MAE) CIFAR10 (ACC) PATTERN (ACC)</cell><cell>MolHIV (ROC)</cell><cell>MolPCBA (AP)</cell></row><row><cell>GatedGCN</cell><cell>0.363 ? 0.009</cell><cell>69.37 ? 0.48</cell><cell>84.480 ? 0.122</cell><cell>?</cell><cell>?</cell></row><row><cell>HIMP</cell><cell>0.151 ? 0.006</cell><cell>?</cell><cell>?</cell><cell>0.7880 ? 0.0082</cell><cell>?</cell></row><row><cell>PNA</cell><cell>0.188 ? 0.004</cell><cell>70.47 ? 0.72</cell><cell>86.567 ? 0.075</cell><cell cols="2">0.7905 ? 0.0132 0.2838 ? 0.0035</cell></row><row><cell>DGN</cell><cell>0.168 ? 0.003</cell><cell>72.84 ? 0.42</cell><cell>86.680 ? 0.034</cell><cell cols="2">0.7970 ? 0.0097 0.2885 ? 0.0030</cell></row><row><cell>GSN</cell><cell>0.115 ? 0.012</cell><cell>?</cell><cell>?</cell><cell>0.7799 ? 0.0100</cell><cell>?</cell></row><row><cell>CIN</cell><cell>0.079 ? 0.006</cell><cell>?</cell><cell>?</cell><cell>0.8094 ? 0.0057</cell><cell>?</cell></row><row><cell>GCN</cell><cell>0.321 ? 0.009</cell><cell>58.39 ? 0.73</cell><cell>85.602 ? 0.046</cell><cell cols="2">0.7422 ? 0.0175 0.2385 ? 0.0019</cell></row><row><cell>GCN-AK +</cell><cell>0.127 ? 0.004</cell><cell>72.70 ? 0.29</cell><cell>86.887 ? 0.009</cell><cell cols="2">0.7928 ? 0.0101 0.2846 ? 0.0002</cell></row><row><cell>GIN</cell><cell>0.163 ? 0.004</cell><cell>59.82 ? 0.33</cell><cell>85.732 ? 0.023</cell><cell cols="2">0.7881 ? 0.0119 0.2682 ? 0.0006</cell></row><row><cell>GIN-AK</cell><cell>0.094 ? 0.005</cell><cell>67.51 ? 0.21</cell><cell>86.803 ? 0.044</cell><cell cols="2">0.7829 ? 0.0121 0.2740 ? 0.0032</cell></row><row><cell>GIN-AK +</cell><cell>0.080 ? 0.001</cell><cell>72.19 ? 0.13</cell><cell>86.850 ? 0.057</cell><cell>0.7961 ? 0.0119</cell><cell>0.2930 ? 0.0044</cell></row><row><cell>PNA  *</cell><cell>0.140 ? 0.006</cell><cell>73.11 ? 0.11</cell><cell>85.441 ? 0.009</cell><cell cols="2">0.7905 ? 0.0102 0.2737 ? 0.0009</cell></row><row><cell>PNA  *  -AK +</cell><cell>0.085 ? 0.003</cell><cell>OOM</cell><cell>OOM</cell><cell cols="2">0.7880 ? 0.0153 0.2885 ? 0.0006</cell></row><row><cell>GCN-AK + -S</cell><cell>0.127 ? 0.001</cell><cell>71.93 ? 0.47</cell><cell>86.805 ? 0.046</cell><cell cols="2">0.7825 ? 0.0098 0.2840 ? 0.0036</cell></row><row><cell>GIN-AK + -S</cell><cell>0.083 ? 0.001</cell><cell>72.39 ? 0.38</cell><cell>86.811 ? 0.013</cell><cell cols="2">0.7822 ? 0.0075 0.2916 ? 0.0029</cell></row><row><cell>PNA  *  -AK + -S</cell><cell>0.082 ? 0.000</cell><cell>74.79 ? 0.18</cell><cell>86.676 ? 0.022</cell><cell cols="2">0.7821 ? 0.0143 0.2880 ? 0.0012</cell></row><row><cell cols="3">6.3 SCALING UP BY SUBSAMPLING</cell><cell></cell><cell></cell><cell></cell></row></table><note>In some cases, GNN-AK ( + )'s overhead leads to OOM, especially for complex models like PNA* that are resource-demanding. Sampling with SubgraphDrop enables training using practical resources.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Resource analysis of SubgraphDrop.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>R=1</cell><cell>R=2</cell><cell>GIN-AK + -S R=3</cell><cell>R=4</cell><cell>R=5</cell><cell>GIN-AK +</cell><cell>GIN</cell></row><row><cell>ZINC-12K</cell><cell>MAE Runtime (S/Epoch)</cell><cell cols="5">0.1216 0.0929 0.0846 0.0852 0.0854 10.8 11.2 12.0 12.4 12.5</cell><cell>0.0806 9.4</cell><cell>0.1630 6.0</cell></row><row><cell></cell><cell>Memory (MB)</cell><cell>392</cell><cell>811</cell><cell>1392</cell><cell>1722</cell><cell>1861</cell><cell>1911</cell><cell>124</cell></row><row><cell>CIFAR10</cell><cell>ACC Runtime (S/Epoch)</cell><cell>71.68 80.7</cell><cell>72.07 89.1</cell><cell>72.39 100.5</cell><cell>72.20 110.9</cell><cell>72.32 119.7</cell><cell>72.19 241.1</cell><cell>59.82 55.0</cell></row><row><cell></cell><cell>Memory (MB)</cell><cell>2576</cell><cell>4578</cell><cell>6359</cell><cell>8716</cell><cell>10805</cell><cell>30296</cell><cell>801</cell></row><row><cell cols="2">6.4 ABLATION STUDY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>WL that includes the hashed result of Star(</figDesc><table><row><cell>not dis-</cell></row><row><cell>tinguishable by 1-WL. Hence Star(v G i ) and Star(v H i ) are hashed to the same label other-</cell></row><row><cell>wise the 1-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>are satisfied. Then, for any iteration of Subgraph-1-WL, G and H would have the same histogram of subgraph colors. Now we focus on iteration 0. Formally, let colorc G v,i = HASH(G[N k (v G i )]) and c H v,i = HASH(H[N k (v H i )])for a node order v (v G i maps index i to a node in G). According to Condition (1): for any node reorderingv G 1 , ..., v G N G and v H 1 , ..., v H N H , ?i ? [1, max(N G , N H )] that G[N k (v G i )] and H[N k (v G i )] are non-isomorphic, then we know that ?i that G[N k (v G i )] and H[N k (v H i )] are non-isomorphic, hence c G v,i = c H v,i since by Condition(2)HASH can distinguish these two subgraphs. However as two graphs have the same histogram of subgraph colors, there must be a j = i such that c . This process can be repeated until having a new node order w such that ?i ? 1, ..., |V G |, c G w,i = c H w,i . As HASH is discriminative enough according to Condition (2), this implies ?i ? 1, ..., |V G |, G[N k (w Proof. When a pair of non-isomorphic graphs G and H cannot be distinguished by GNN-AK + , for any layer l, the histogram of h in G and H in Eq. 4 should be the same. Then GNN-AK cannot distinguish G and H. So for any pair of non-isomorphic graphs, GNN-AK cannot distinguish them if GNN-AK + cannot distinguish them. Thus GNN-AK + is more powerful than GNN-AK.</figDesc><table><row><cell></cell><cell>G v,i = c H v,j and c G v,j = c H v,i . Then we can create a</cell></row><row><cell cols="2">new node order m by swapping v G i and v G j , resulting c G m,i = c H m,i and c G m,j = c H m,j G i )] and H[N k (w H i )] are isomorphic, which contradicts with Condition (1). Thus, Prop. 1 must be true.</cell></row><row><cell>A.8 PROOF OF PROPOSITION 2</cell><cell></cell></row><row><cell>(l)) v</cell><cell>in G and H in Eq. 7 should be the same. Which implies that for</cell></row><row><cell>(l)) any layer l, the histogram of h v</cell><cell></cell></row><row><cell cols="2">A.9 DATASET DESCRIPTION &amp; STATISTICS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Dataset statistics. MEAN] for each dataset using GIN based on validation performance, and fix it for any other GNN and GNN-AK( + ).</figDesc><table><row><cell>Dataset</cell><cell>Task Semantic</cell><cell># Cls./Tasks</cell><cell># Graphs</cell><cell cols="2">Ave. # Nodes Ave. # Edges</cell></row><row><cell>EXP</cell><cell>Distinguish 1-WL failed graphs</cell><cell>2</cell><cell>1200</cell><cell>44.4</cell><cell>110.2</cell></row><row><cell>SR25</cell><cell>Distinguish 3-WL failed graphs</cell><cell>15</cell><cell>15</cell><cell>25</cell><cell>300</cell></row><row><cell cols="2">CountingSub. Regress num. of substructures</cell><cell>4</cell><cell>1500 / 1000 / 2500</cell><cell>18.8</cell><cell>62.6</cell></row><row><cell cols="2">GraphProp. Regress global graph properties</cell><cell>3</cell><cell>5120 / 640 / 1280</cell><cell>19.5</cell><cell>101.1</cell></row><row><cell cols="2">ZINC-12K Regress molecular property</cell><cell>1</cell><cell>10000 / 1000 / 1000</cell><cell>23.1</cell><cell>49.8</cell></row><row><cell>CIFAR10</cell><cell>10-class classification</cell><cell>10</cell><cell>45000 / 5000 / 10000</cell><cell>117.6</cell><cell>1129.8</cell></row><row><cell cols="2">PATTERN Recognize certain subgraphs</cell><cell>2</cell><cell>10000 / 2000 / 2000</cell><cell>118.9</cell><cell>6079.8</cell></row><row><cell>MolHIV</cell><cell>1-way binary classification</cell><cell>1</cell><cell>32901 / 4113 / 4113</cell><cell>25.5</cell><cell>54.1</cell></row><row><cell cols="2">MolPCBA 128-way binary classification</cell><cell>128</cell><cell>350343 / 43793 / 43793</cell><cell>25.6</cell><cell>55.4</cell></row><row><cell>MUTAG</cell><cell>Recognize mutagenic compounds</cell><cell>2</cell><cell>188</cell><cell>17.93</cell><cell>19.79</cell></row><row><cell>PTC-MR</cell><cell>Classify chemical compounds</cell><cell>2</cell><cell>344</cell><cell>14.29</cell><cell>14.69</cell></row><row><cell cols="2">PROTEINS Classify Enzyme &amp; Non-enzyme</cell><cell>2</cell><cell>1113</cell><cell>39.06</cell><cell>72.82</cell></row><row><cell>NCI1</cell><cell>Classify molecular</cell><cell>2</cell><cell>4110</cell><cell>29.87</cell><cell>32.30</cell></row><row><cell>IMDB-B</cell><cell>Classify movie</cell><cell>2</cell><cell>1000</cell><cell>19.77</cell><cell>96.53</cell></row><row><cell>RDT-B</cell><cell>Classify reddit thread</cell><cell>2</cell><cell>2000</cell><cell>429.63</cell><cell>497.75</cell></row><row><cell cols="2">A.10 EXPERIMENTAL SETUP DETAILS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">To reduce the search space, we search hyperparameters in a two-phase approach: First, we search</cell></row><row><cell cols="6">common ones (hidden size from [64, 128], number of layers L from [2,4,5,6], (sub)graph pooling</cell></row><row><cell>from [SUM,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Effect of various k-egonet size.</figDesc><table><row><cell>k of GIN-AK +</cell><cell>ZINC-12K</cell><cell>CIFAR10</cell></row><row><cell>GIN</cell><cell cols="2">0.163 ? 0.004 59.82 ? 0.33</cell></row><row><cell>k = 1</cell><cell cols="2">0.147 ? 0.006 71.37 ? 0.28</cell></row><row><cell>k = 2</cell><cell cols="2">0.120 ? 0.005 72.19 ? 0.13</cell></row><row><cell>k = 3</cell><cell>0.080 ? 0.001</cell><cell>OOM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Effect of various encodings</figDesc><table><row><cell>Ablation of GIN-AK +</cell><cell>ZINC-12K</cell><cell>CIFAR10</cell></row><row><cell>Full</cell><cell cols="2">0.080 ? 0.001 72.19 ? 0.13</cell></row><row><cell>w/o Subgraph encoding</cell><cell cols="2">0.086 ? 0.001 67.76 ? 0.29</cell></row><row><cell>w/o Centroid encoding</cell><cell cols="2">0.084 ? 0.003 72.20 ? 0.96</cell></row><row><cell>w/o Context encoding</cell><cell cols="2">0.088 ? 0.003 69.25 ? 0.30</cell></row><row><cell>w/o Distance-to-Centroid</cell><cell cols="2">0.085 ? 0.001 71.91 ? 0.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Study GNN-AK without context encoding (Ctx) and without distance-to-centroid (D2C). Base model is 1-layer GIN for all methods.</figDesc><table><row><cell>Method</cell><cell>ZINC-12K</cell><cell>CIFAR10</cell><cell>EXP</cell><cell>SR25</cell><cell cols="2">Counting Substructures (MAE) Graph Properties (log 10 (MAE))</cell></row><row><cell></cell><cell>(MAE)</cell><cell>(ACC)</cell><cell>(ACC)</cell><cell>(ACC)</cell><cell cols="2">Triangle Tailed Tri. Star 4-Cycle IsConnected Diameter Radius</cell></row><row><cell>GIN</cell><cell>0.163</cell><cell>59.82</cell><cell cols="3">50% 6.67% 0.3569 0.2373 0.0224 0.2185 -1.9239</cell><cell>-3.3079 -4.7584</cell></row><row><cell>GIN-AK</cell><cell>0.094</cell><cell cols="4">67.51 100% 6.67% 0.2311 0.1805 0.0207 0.1911 -1.9574</cell><cell>-3.6925 -5.0574</cell></row><row><cell>GIN-AK + w/o Ctx</cell><cell>0.088</cell><cell cols="4">69.25 100% 6.67% 0.0130 0.0108 0.0177 0.0131 -2.7083</cell><cell>-3.9257 -5.2784</cell></row><row><cell cols="2">GIN-AK + w/o D2C 0.085</cell><cell cols="4">71.91 100% 6.67% 0.1746 0.1449 0.0193 0.1467 -2.0521</cell><cell>-3.6980 -5.0984</cell></row><row><cell>GIN-AK +</cell><cell>0.080</cell><cell cols="4">72.19 100% 6.67% 0.0123 0.0112 0.0150 0.0126 -2.7513</cell><cell>-3.9687 -5.1846</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Study the effect of base model's number of layers while keeping total number of layers in GNN-AK fixed. Different effect is observed for GNN-AK and GNN-AK without D2C.</figDesc><table><row><cell>Method</cell><cell>GIN-AK's</cell><cell>Base GIN's</cell><cell cols="2">Counting Substructures (MAE)</cell><cell cols="2">Graph Properties (log 10 (MAE))</cell></row><row><cell></cell><cell>#Layers</cell><cell>#Layers</cell><cell cols="4">Triangle Tailed Tri. Star 4-Cycle IsConnected Diameter Radius</cell></row><row><cell>GIN</cell><cell>0</cell><cell>6</cell><cell>0.3569</cell><cell>0.2373 0.0224 0.2185</cell><cell>-1.9239</cell><cell>-3.3079 -4.7584</cell></row><row><cell>GIN-AK</cell><cell>6 3</cell><cell>1 2</cell><cell>0.2311 0.1556</cell><cell>0.1805 0.0207 0.1911 0.1275 0.0172 0.1419</cell><cell>-1.9574 -1.9134</cell><cell>-3.6925 -5.0574 -3.7573 -5.0100</cell></row><row><cell></cell><cell>2</cell><cell>3</cell><cell>0.1064</cell><cell>0.0819 0.0168 0.1071</cell><cell>-1.9259</cell><cell>-3.7243 -4.9257</cell></row><row><cell></cell><cell>1</cell><cell>6</cell><cell>0.0934</cell><cell>0.0751 0.0216 0.0726</cell><cell>-1.9916</cell><cell>-3.6555 -4.9249</cell></row><row><cell></cell><cell>6</cell><cell>1</cell><cell>0.1746</cell><cell>0.1449 0.0193 0.1467</cell><cell>-2.0521</cell><cell>-3.6980 -5.0984</cell></row><row><cell>GIN-AK + w/o D2C</cell><cell>3 2</cell><cell>2 3</cell><cell>0.1244 0.1021</cell><cell>0.1052 0.0219 0.1121 0.0830 0.0162 0.0986</cell><cell>-2.1538 -2.2268</cell><cell>-3.7305 -4.9250 -3.7585 -5.1044</cell></row><row><cell></cell><cell>1</cell><cell>6</cell><cell>0.0885</cell><cell>0.0696 0.0174 0.0668</cell><cell>-2.0541</cell><cell>-3.6834 -4.8428</cell></row><row><cell></cell><cell>6</cell><cell>1</cell><cell>0.0123</cell><cell>0.0112 0.0150 0.0126</cell><cell>-2.7513</cell><cell>-3.9687 -5.1846</cell></row><row><cell>GIN-AK + with D2C</cell><cell>3 2</cell><cell>2 3</cell><cell>0.0116 0.0119</cell><cell>0.0100 0.0168 0.0122 0.0102 0.0146 0.0127</cell><cell>-2.6827 -2.6197</cell><cell>-3.8407 -5.1034 -3.8745 -5.1177</cell></row><row><cell></cell><cell>1</cell><cell>6</cell><cell>0.0131</cell><cell>0.0123 0.0174 0.0162</cell><cell>-2.5938</cell><cell>-3.7978 -5.0492</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">1-WL and 2-WL are known to be equally powerful, see Azizian &amp; Lelarge (2021) and Maron et al. (2019a). 2 When |VG| &lt; |VH | , ?i ? {|VG|, |VG| + 1, ..., |VH |}, let G[N k (v G i )] denote an empty subgraph.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We record D2C value for every node in every subgraph, and the value is categorical instead of continuous.4  The categorical D2C does not change across layers, but is encoded with different parameters in each layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">PNA * is a variant of PNA that changes from using degree to scale embeddings to encoding degree and concatenate to node embeddings. This eliminates the need of computing average degree of datasets in PNA.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Published as a conference paper at ICLR 2022 A.12 ADDITIONAL RESULTS ON TU DATASETS We also report additional results on several smaller datasets from TUDataset <ref type="bibr">(Morris et al., 2020a)</ref>, with their statistics reported in last block of <ref type="table">Table 5</ref>. The training setting and evaluation procedure follows <ref type="bibr">Xu et al. (2018)</ref> exactly, where we perform 10-fold cross-validation and report the average and standard deviation of validation accuracy across the 10 folds within the cross-validation. We take results of existing baselines directly from <ref type="bibr" target="#b9">Bodnar et al. (2021a)</ref> with their method labeled as CIN, for references to all baselines see <ref type="bibr" target="#b9">Bodnar et al. (2021a)</ref>. The result is shown in <ref type="table">Table 10</ref>. We mark that the performance of GIN-AK + over IMDB-B is not improved because each graph in the dataset is an egonet, hence all nodes have the same rooted subgraph -the whole graph. The performance of MUTAG and PTC is very unstable, given these datasets are too small: 188 and 344, respectively, and the evaluation method is based on average 10 validation curves over 10 folds.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The surprising power of graph neural networks with random node initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?smaililkan</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artifical Intelligence (IJCAI)</title>
		<meeting>the Thirtieth International Joint Conference on Artifical Intelligence (IJCAI)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On weisfeiler-leman invariance: subgraph counts and related graph properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikraman</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Fuhlbr?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Expressive power of invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waiss</forename><surname>Azizian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lelarge</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=lxHgXYN4bwl" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">M</forename><surname>Selkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random graph isomorphism. SIaM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Breaking the limits of message passing graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph neural networks with local graph parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barcel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Floris</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksimilian</forename><surname>Ryschkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Equivariant subgraph aggregation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Balamurugan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=dFbKQaRk15w" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go cellular: Cw networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Mont?far</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go topological: Message passing simplicial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1026" to="1037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<title level="m">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>F?rer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reconstruction for powerful graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Cotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coloring graph neural networks for node disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aladin</forename><surname>Virmaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2126" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for molecular design-a review of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zois</forename><surname>Daniel C Elton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boukouvalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter W</forename><surname>Mark D Fuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Systems Design &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="828" to="849" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Graph Representation Learning and Beyond (GRL+) Workhop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph meta learning via local subgraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7092" to="7101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The small-world phenomenon: An algorithmic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-second annual ACM symposium on Theory of computing</title>
		<meeting>the thirty-second annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with motif-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Boaz</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1l2bp4YwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How hard is to distinguish graphs with graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2156" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Motifnet: a motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Science Workshop</title>
		<imprint>
			<biblScope unit="page" from="225" to="228" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="URLwww.graphlearning.io" />
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Tantia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Scicluna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08591</idno>
		<title level="m">Simon Lacoste-Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">and Michalis Vazirgiannis. k-hop graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dasoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="195" to="205" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ego-gnns: Exploiting ego structures in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Sandfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyesh</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8523" to="8527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2021 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
