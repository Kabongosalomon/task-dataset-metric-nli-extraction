<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difei</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixian</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Captioning</term>
					<term>Generic Event Understanding</term>
					<term>Status Changes</term>
					<term>Difference Modelling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cognitive science has shown that humans perceive videos in terms of events separated by the state changes of dominant subjects. State changes trigger new events and are one of the most useful among the large amount of redundant information perceived. However, previous research focuses on the overall understanding of segments without evaluating the fine-grained status changes inside. In this paper, we introduce a new dataset called Kinetic-GEB+. The dataset consists of over 170k boundaries associated with captions describing status changes in the generic events in 12K videos. Upon this new dataset, we propose three tasks supporting the development of a more fine-grained, robust, and human-like understanding of videos through status changes. We evaluate many representative baselines in our dataset, where we also design a new TPD (Temporal-based Pairwise Difference) Modeling method for visual difference and achieve significant performance improvements. Besides, the results show there are still formidable challenges for current methods in the utilization of different granularities, representation of visual difference, and the accurate localization of status changes. Further analysis shows that our dataset can drive developing more powerful methods to understand status changes and thus improve video level comprehension. The dataset is available at https://github.com/showlab/GEB-Plus</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>According to cognitive science <ref type="bibr" target="#b26">[27]</ref>, humans perceive videos in terms of different events, which are separated by the status changes of dominant subjects in the video. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>, humans perceive the process of "javelin sport" by the action events such as "walking", "running" and "throwing". These events are triggered by the athlete's status changes, like the instantaneous change from "walking" to "running". The moment that instantly triggers status changes of persons, objects, or scenes often conveys useful and interesting information among a large amount of repeated, static, or regular events. Therefore, developing the understanding of the salient, instantaneous status changes is another step towards a more fine-grained and robust video understanding. Previous works, like Dense Video Captioning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b12">13]</ref> and Video Grounding <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> attempt to develop the understanding of events in video or video segments. However, these works only focus on developing an overall understanding of events rather than delving into the fine-grained status changes in the video. Other researches focusing on image level changes <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref> employ the visual difference modeling to capture the status changes in image pairs. However, since the image contains only static information, the state changes exhibited by the two images involve only a few simple patterns, e.g., appear, move. These tasks are hard to evaluate the ability on understanding generic status changes.</p><p>More recently, Shou et al. <ref type="bibr" target="#b30">[31]</ref> proposes Kinetic-GEBD dataset with annotated boundary timestamps for detection in Kinetic-400 videos <ref type="bibr" target="#b6">[7]</ref>, where a boundary is defined as the splitter between two status of the subject. Though the videos in Kinetic-400 <ref type="bibr" target="#b6">[7]</ref> are categorized, the events selected inside are generic and mostly independent from the whole video's category. However, in addition to letting the model predict where is the boundary, it is more important to understand why this is the boundary, which associates the visual information of boundaries with natural human languages.</p><p>Motivated by this idea, we build a new dataset called Kinetic-GEB+ (Generic Event Boundary Captioning, Grounding and Retrieval) which includes the video boundaries indicating status changes happening in generic events. For every boundary, our Kinetic-GEB+ provides the temporal location and a natural language description, which consists of the dominant Subject, Status Before and Status After the boundary. In total, our dataset includes 176,681 boundaries in 12,434 videos selected from all categories in Kinetic-400 <ref type="bibr" target="#b6">[7]</ref>. The detailed definition of our boundary is described in Sec. 3.1. For future applications like AI assistant robots, with the comprehension developed from the visual status changes and natural language captions, they could understand the real time, instantaneous occurrences without hints to assist the users. In order to comprehensively evaluate the machine's understanding of our boundaries, we further propose three downstream tasks shown in <ref type="figure" target="#fig_0">Fig. 1:</ref> (1) Boundary Captioning. Provided with the timestamp of a boundary, the machine is required to generate sentences describing the status change at the boundary.</p><p>(2) Boundary Grounding. Provided with a description of a boundary, the machine is required to locate that boundary in the video. (3) Boundary Caption-Video Retrieval. Provided with the description of a boundary, the machine is required to retrieve the video containing that boundary from video corpus.</p><p>In the experiment, we compare several state-of-the-art methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref> along with many variants on our datasets to analyze the limitation of current methods and show the challenges of the proposed tasks. Due to the need of visual difference for understanding the status changes, we further propose a Temporalbased Pairwise Difference (TPD) Modeling method representing a finegrained visual difference before and after the boundary. This method brings a significant performance improvement. On the other hand, the results show that there are still formidable challenges for current SOTA methods in developing the comprehension of status changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Captioning is a conventional task with many benchmarks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42]</ref> established which aim to caption trimmed videos with natural language descriptions. More recently, several works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b12">13]</ref>, e.g., Dense Video Captioning <ref type="bibr" target="#b15">[16]</ref>, focus on captioning the self-proposed event segments in videos. All tasks above are evaluating the overall understanding of an event, whether the event is presented in the form of a trimmed video or a video segment. In contrast, our Boundary Captioning task is to develop the comprehension of instantaneous status changes happening at boundaries, i.e., describing the important moment that caused a dramatic change in the state of persons, objects or scenes. As a result, there is a more urgent need for models to understand the changes in various granularity of visual concepts, e.g., action, attributes, scene status, etc. In Tab. 1, we compare the most relevant video captioning datasets with ours.</p><p>Image Change Captioning is a task evaluating the ability on capturing and describing the difference between two images. There are many existing benchmarks targeting at this task. Early works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21]</ref> focus on changes in aerial imagery for monitoring disaster. Some other datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> are about captioning the changes in street scenes, e.g., Spot-the-diff <ref type="bibr" target="#b13">[14]</ref>. Recently, <ref type="bibr" target="#b24">[25]</ref> proposes a more challenging change caption dataset, CLEVR-change, which utilizes the CLEVR engine to construct complicated synthetic scenes to evaluate models on finding more subtle change. One crucial limitation of previous works is that images can only present static information, thus status changes presented by two images can only involve a limited number of patterns, e.g., "appear", "disappear", "add" and "move". Towards a generic understanding of change, we extend the setting from images to videos which supports a open set of change pattern, including human action change, scene state change, etc.</p><p>Video Retrieval and Grounding are both language-to-vision tasks. Given a text description of a video or event, Video Retrieval requires models to select the target video from the corpus <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>, and Video Grounding requires models to locate the target event segment (i.e. start and end boundaries) from an untrimmed video <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. These tasks are based on the event level understanding to find the best matching video or time span. Compared with previous works, our Boundary Caption-Video Retrieval and Boundary Grounding tasks requires locating the two states of the subject, while traditional grounding only localizes one event. Besides, our captions are more fine-grained (describing detailed status changes) than those in traditional tasks (describing a general event).</p><p>Generic Understanding is a popular topic aiming to drive models from understanding predefined classes to open world vocabulary. Many pioneer works <ref type="bibr" target="#b4">[5]</ref> propose open-set recognition tasks, which extend image classification tasks to generic understanding versions. Some works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42]</ref> introduce datasets for the generic event understanding requiring models to describe videos with natural language. More recently, a new dataset called Kinetic-GEBD (Generic Event Boundary Detection) <ref type="bibr" target="#b30">[31]</ref> is proposed, which focuses on detecting the status changes between generic events. Our work is an extension to Kinetic-GEBD. We also study the boundary between events. However, we believe a sophisticated model should not only know where is the boundary but also understand why it is a boundary. Thus, this paper constructs a dataset with a large scale of boundary captions and introduces new boundary language-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subject: boy in blue t-shirt and blue jeans pant</head><p>Status Before: /0 (*not in the scene) Status After: /1 (*in the scene) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Change of Subject</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Boundary Collection</head><p>When annotating Kinetic-GEB+, one simple way would be directly captioning the boundaries in Kinetic-GEBD <ref type="bibr" target="#b30">[31]</ref>. However, our annotators did their jobs quicker when being asked to re-annotate the boundary positions than to interpret GEBD's boundaries. Yet, the boundaries from GEBD and GEB+ are highly consistent: when following Supp. Sec. 6.2 , nearly 90%/70% boundary positions in GEB+ reaching f1 scores higher than 0.5/0.7 with the boundaries in GEBD. Format and Guideline. Following GEBD <ref type="bibr" target="#b30">[31]</ref>, a boundary is defined as the splitter between two status of the subject in the video. Generally, we categorized our boundaries into five types: Change of Action, Change of Subject, Change of Object, Change of Color and Multiple Changes. When annotating, we accept both single timestamps and time ranges as in <ref type="bibr" target="#b30">[31]</ref>, and each video is allocated to at least five annotators. Each annotator could independently decide whether to accept or reject the video following the criteria. The statistical results of annotation numbers and formats is shown in Tab. <ref type="bibr" target="#b7">8</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Caption Collection</head><p>In our Kinetic-GEB+, annotators are supposed to add a language description to each boundary they annotated in Sec. 3.1. To clearly and comprehensively represent humans' understanding of the status changes, we randomly sampled 300 videos for pilot annotation to design the formats and guidelines of captioning.</p><p>Format. Our final format of caption consists of three compulsory items: (1) Dominant Subject that performs the status changes. (2) Subject's Status Before the boundary. (3) Subject's Status After the boundary. In the pilot stage, we compare different versions of annotation formats as shown in <ref type="figure">Fig. 3</ref>:</p><p>One-Sentence format: Annotators use a single sentence to describe the status change happening at each boundary. In order to obtain an open-vocab description close to daily language, we do not restrict or request anything to the expression and annotators have full autonomy in narrating. Though this format enables fluent and natural descriptions, there are significant problems in the annotations: (1) Ambiguity of subject: Annotators tend to describe the subject shortly without further restriction, causing ambiguity, e.g., in a scene full of people, a short description like "a man" might indicate multiple persons. (2) Dual changes: Without restriction, annotators could wrongly combine two state changes of different subjects together in caption, like "Musician stops playing and an auditor starts clapping". (3) Low efficiency: Long sentences costs annotators more time to construct and takes our raters more time to understand.</p><p>Two-Items format: To address the problems in the one-sentence format, we separate the sentence into a Subject item and its Change item as shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Set</head><p>Test Set Val Set For Subject, annotators should fill in a noun phrase. We notice that this separation makes it easier for annotators to check the singularity and specification of Subject. Although we see that the efficiency of both annotation and evaluation are improved, this scheme still have some shortcomings: (1) Incomplete status:</p><p>Annotators sometimes forgot to describe the status before the boundary. For example, when describing an athlete's changing from walking to running, an annotator only filled "starts to run on the track" in Change and forgot to mention the "walking" status before the boundary.</p><p>(2) Low efficiency: Even though this separation improves the efficiency, the Change item could still be too long for auditors to evaluate. Therefore, we further separate Change into Status Before and Status After to ensure the completeness of the status change description. Finally, we found this fully separated format the most efficient and robust for annotation, as shown in <ref type="figure">Fig. 3</ref>. Guideline. In our Kinetic-GEB+, the caption is defined as the reason why the annotator separates the preceding and succeeding segment of the boundary. Following the format of annotation, we brought up some specific guidance for annotating the items. Specifically, when annotating the Subject item, annotators are required to provide distinguishable attributes of the dominant subject. However, in complex cases where the subject is difficult to describe without ambiguity (e.g. many people dressing similarly in the scene), the annotator could just describe some attributes to avoid verbose descriptions.</p><p>When annotating Status Before and Status After, annotators are required to limit their attention to the time range between the proceeding boundary and succeeding boundary, thus to ensure all the status changes in the same video are at the same temporal level. To further improve the consistency of expressions, we employ the symbol /1 and /0 to represent the appearance and disappearance Finally, we embrace all the tenses only if the annotators feel natural. In this way, we ensure the specification of descriptions while keeping their naturalness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistics</head><p>Splitting. When splitting our Kinetic-GEB+ into train, validation and test sets, the boundary type is the most important characteristic of consistency, since it determines which granularities the model should depend on to understand the state change. We allocate videos containing different types of boundaries by proportion to ensure the distribution is the same in all splits. The final distribution is shown in <ref type="figure">Fig. 4</ref>, where we see the distribution is consistent in three splits. More details of splitting criteria is discussed in Supp. Sec. 6. Boundary number. To quantify the density of annotated boundaries, we make a statistics of the boundary number in each piece of annotation. Notably, due to the variant understanding of annotator, annotations of the same video could have different numbers of boundaries. The bottom left side of <ref type="figure">Fig. 4</ref> shows the counts of annotations versus their boundary numbers, from which we could see that most of annotations have 1 to 4 boundaries.</p><p>Boundary interval. Furthermore, to investigate the duration of events located between two boundaries, we conduct the statistics on the length of intervals. For the first boundary in the video, we take the distance to the start of the video as its interval duration. The result is shown in the bottom right side of <ref type="figure">Fig. 4</ref> which is similar to the statistic of boundary numbers.</p><p>Part of speech comparison in caption. For the captions in our dataset, we first analyze and compare the part of speech distributions in the subject and two status parts. In <ref type="figure" target="#fig_1">Fig. 5(a)</ref>, the comparison result indicates that the status parts contain more verbs and focus more on actions than the subject part. On the other hand, the subject part includes more nouns and adjectives than the two status parts, suggesting it focuses more on appearance information.</p><p>Frequent subjects and actions in caption. To further analyze the different aspects of information in the three parts. In <ref type="figure" target="#fig_1">Fig. 5(b</ref> After, and then illustrate the 20 most frequent words. Same with Kinetic-400, we see that both the nouns and verbs in our datasets are mainly correlated with the appearance and motions of humans. This conforms to the scenarios of practical application, since humans are also the dominant subject in most of the scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adjustment for downstream tasks</head><p>For downstream tasks, we select one annotator whose labeled boundaries are mostly consistent with others to reduce noise and duplication. Then, we use these boundaries' timestamps as the anchors to merge other annotators' captions, preserving the diversity of different opinions. Thus, one video corresponds to multiple boundaries, and each boundary could be with multiple captions. Finally, this selection includes 40k anchors from all videos. Furthermore, we find two different boundaries in the same video could be occasionally too similar in semantics for even humans to tell. For Boundary Grounding, we mark these pair of boundaries as equal in the ground truth. More details are discussed in Supp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Kinetic-GEB+ dataset enables us to benchmark how well current mainstream methods could comprehend the instantaneous status changes in videos. For each task, we implement and compare among SOTA models with our modifications, as well as further explorations on ablation and visual difference modeling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>Granularities of Input Features. We extract multiple granularities of features and utilize different combinations of them in experiments. Given each boundary, we sampled multiple frames before and after the timestamp and one frame at the timestamp for further extraction. Our features include: (1) ResNet: Firstly, we extract a 1024 dimensional ResNet-roi feature using ResNet <ref type="bibr" target="#b11">[12]</ref> followed by Region of Interest (RoI) pooling. Then we extract another ResNet-conv feature to fit <ref type="bibr" target="#b24">[25]</ref>: We sample one frame before and another frame after the boundary, then extract the Conv features from the two frames. (2) TSN : For frames before and after the timestamp, we extract a 2048 dimensional TSN feature for the before and after snippets using pre-trained TSN <ref type="bibr" target="#b34">[35]</ref> network. (3) Faster R-CNN : For every sampled frame, we employ Faster R-CNN <ref type="bibr" target="#b29">[30]</ref> to extract the 1024 dimensional R-CNN feature by selecting 20 objects with highest confidence. (4) C3D: Similar to the TSN feature, we extract 4096 dimensional C3D features with pre-trained C3D <ref type="bibr" target="#b32">[33]</ref> network for the before and after snippets to fit <ref type="bibr" target="#b40">[41]</ref>.</p><p>These features are categorized into two granularities: Instant-granularity features extracted from the instantaneous appearance in a single frame, such as the R-CNN and ResNet features, are to provide fine-grained visual information of instants. Event-granularity features, like the TSN and C3D feature, could provide an overall representation of appearance and motion information in event snippets. We assume that developing a fine-grained understanding of status changes requires both the granularities.</p><p>Backbones. We implement the following backbones with various adoption and modification according to the tasks: (1) CNN+LSTM : A rudimentary backbone that simply uses a vanilla LSTM which takes the CNNs extracted features as input. The output of LSTM is mapped to caption tokens in Boundary Captioning, or is max-pooled to be the matching score in other two tasks. (2) Dual Dynamic Attention Model (DUDA): The baseline method in <ref type="bibr" target="#b24">[25]</ref> which consists of a CNN-based Change Detector and a LSTM-based Dynamic Speaker. Besides, it utilizes a simple visual difference modeling by subtraction. (3) ActionBERTrevised : A one-stream BERT architecture using early fusion from <ref type="bibr" target="#b42">[43]</ref>. We modify the structure by applying difference modeling after the embedding and employing different feature combinations. (4) UniVL-revised : A two-stream BERT architecture from <ref type="bibr" target="#b21">[22]</ref>, which includes a caption encoder, a context encoder and a cross-encoder for late fusion. We apply difference modeling to the context encoder with different feature combinations. (5) FROZEN-revised : A two-stream BERT architecture from <ref type="bibr" target="#b2">[3]</ref>, which includes a caption encoder and a context encoder with no fusion. The revision is the same as UniVL-revised. (6) TVQA: The baseline method in <ref type="bibr" target="#b16">[17]</ref>, where we remove all the "answer" substreams and process each visual granularity with one stream. (7) 2D-TAN : The baseline method in <ref type="bibr" target="#b40">[41]</ref>, where we only keep the diagonal elements in the 2D map.</p><p>Visual Difference Modeling. Developing a fine-grained understanding of status changes at the boundary requires visual difference information. Most existing methods are focused on image-pair differences <ref type="bibr" target="#b24">[25]</ref>, where the difference is obtained by simply subtracting the "before" image from the "after" image. A simple inference of this method on video tasks is by pooling the sampled frames then doing subtraction. However, this method only provides an event-granularity representation of the visual difference between the before and after snippets, and thus loses the instant-granularity visual differences.</p><p>To address this problem, we design a new method of Temporal-based Pairwise Difference (TPD) Modeling for BERT models. As shown in <ref type="figure">Fig. 6</ref>, we first compute the pairwise subtraction between the embedding of frames in "before" and "after" as Part a, where the embeddings of the frames are sampled in Sec. 4.1. This provides us a fine-grained and wide-viewing visual comparison between the status before and after. To represent the visual difference between the boundary and other sampled timestamps, we further compute Part b and Part c, which includes the pairwise subtraction between the frame embeddings at the boundary and that before or after the boundary. Finally, we concatenate all these differences together as the output of TPD Modeling.</p><p>The advantage of our TPD Modeling is that, compared with previous methods designed for image tasks, it provides multiple granularities of information and ensures the fine-grained representation of visual differences. In the ablation study of Boundary Captioning, we design an experiment to explore the difference modeling methods and verify our perceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Boundary Captioning</head><p>For Boundary Captioning, we first implement and compare the performance of CNN+LSTM, DUDA, UniVL-revised and ActBERT-revised. To further explore how different input granularities support the understanding, we design a series of ablation studies using ActBERT-revised for all combinations of input features. In these two experiments, we apply our TPD Modeling as shown in <ref type="figure">Fig. 6</ref>.</p><p>To find the best schemes to represent visual difference, we further compare the performances of three schemes on ActBERT-revised : (1) Embedding with no difference modeling. (2) Max-pooling the frames before and after the boundary and simply subtracting one from another, which is inferred from the current method in <ref type="bibr" target="#b24">[25]</ref>. (3) Using TPD Modeling to represent the visual differences. In Supp. Sec. 8, we conduct an ablation study of different parts of TPD Modeling and explore on several other methods for visual difference representation.</p><p>Implementation. For CNN+LSTM and DUDA, we utilize the ResNet-conv feature following <ref type="bibr" target="#b24">[25]</ref>. For UniVL-revised and ActBERT-revised, we utilize the ResNet-roi feature and TSN feature described in Sec. 4.1, where the sampling range is from the preceding boundary to the succeeding boundary. In evaluation, we separate the prediction into the three items, and then compute the similarity score of each item with the ground truth. After that, we employ CIDEr <ref type="bibr" target="#b33">[34]</ref>, SPICE <ref type="bibr" target="#b1">[2]</ref> and ROUGE L <ref type="bibr" target="#b18">[19]</ref> as evaluation metrics, which are widely utilized in image and video captioning benchmarks. Further details are discussed in Supp.</p><p>Result. From Tab. 4, we see that the ActBERT-revised backbone performs the best. However, the results in are still far from satisfactory, thus we further analyze the challenges of our task through the result in Tab. 5:</p><p>Accurate captioning of the status changes requires both the instant and event granularities. First, the event-granularity features perform as the base of the understanding. In Tab. 5, the "ResNet-roi+TSN" combination outperforms all the groups using only the instant-granularity features (e.g. the combinations of ResNet features). Second, a proper usage of the instant-granularity features could help to enrich the understanding. As in Tab. 5, the "ResNet-roi+TSN" combination outperforms the single TSN feature.</p><p>Our task requires adaptive usage of different granularities. Machines need to know when to look at which granularity. Simply assembling different features together could sometimes disturb the attention resulting in worse performance. In Tab. 5, when only utilizing the TSN feature, the performance is better than using either "ResNet-roi+TSN" or "ResNet-roi+ResNet-conv+TSN" combination.</p><p>Understanding the status changes requires effective modeling of visual differences In the comparison of difference modeling schemes in Tab. 5, the plain embedding without difference modeling performs the worst, while the utilization of simple-subtraction difference modeling brings little improvement to the performance. At the same time, the group with our TPD Modeling method significantly outperforms others. This gap in performance conforms to our perspective that learning a fine-grained understanding of status changes requires not only an overall but also a fine-grained representation of visual differences.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Boundary Grounding</head><p>In Boundary Grounding, we compare the performance of four backbones: TVQA, 2D-TAN, FROZEN-revised and ActionBERT-revised. Given a video and a caption query, the model computes the matching scores of each candidate sampled from the video, followed with post-processing to finalize the prediction.</p><p>Implementation. In the training period, we use the ground truth boundaries processed in Sec. 3.4 and their timestamps. In testing, we employ two strategies to sample the timestamp candidates for groups as specified in their suffix. More details are discussed in Supp. For 2D-TAN, we utilize the C3D feature as in <ref type="bibr" target="#b40">[41]</ref>. For TVQA, we utilize the R-CNN and ResNet-roi features as context. Besides, we build the triplets consisting of one positive and two negative pairs, and then compute the cross-entropy loss for each triplet in training. In ActBERT-revised and FROZEN-revised, we apply the contrastive loss in <ref type="bibr" target="#b2">[3]</ref> as objective and implemented a batch-randomed sequential sampler in training. The batch-randomed sampler allocates the boundaries in the same video to the same batch, encouraging the model to learn the visual differences within videos.</p><p>After the models generate the matching scores of all candidate timestamps, we apply the Laplace-of-Gaussian filter in <ref type="bibr" target="#b30">[31]</ref> to derive local maximas of the scores. Then we select the top-k maximas as final prediction, where k is subject to the statistical number of ground truth timestamps marked in Sec. 3.4. To evaluate the accuracy of the prediction, we compute F1 scores based on the absolute distance between ground truth timestamps and predicted timestamps, with the threshold varying from 0.1s to 3s. Further details are discussed in Supp.</p><p>Result. We see that FROZEN-revised performs the best in the comparison of SOTA methods in Tab. 6. However, all the SOTA methods struggle when the threshold is less than 1s, indicating that improving the temporal resolution of understanding is still a main challenge of our task. Future improvements still need to focus on how to delve deeper into the temporal details and prevent the models from taking a glance and learning a rough impression of status changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Boundary Caption-Video Retrieval</head><p>We implement and compare the performance of the CNN+LSTM, FROZENrevised and ActionBERT-revised backbones. Same as in Boundary Grounding, the backbones is to compute the matching score between the query and context.</p><p>Implementation. In order to find the target video from the corpus, each query is to be tried to match with every boundary candidate from all videos. Considering the corpus size, we only apply the baseline in <ref type="bibr" target="#b30">[31]</ref> to generate the boundary candidates. When implementing CNN+LSTM, we take the R-CNN and ResNet-roi features as visual contexts. For FROZEN-revised and ActBERTrevised, we utilize the same configuration with Boundary Grounding. To evaluate the retrieval accuracy, for each query, we sort all the videos by the highest scores of their boundary candidates and then compute the mAP and recall metrics.</p><p>Result. In Tab. 7, FROZEN-revised with difference modeling performs the best, but the performance gap is significantly smaller than in Boundary Grounding, suggesting that this video-level retrieval task relies less on the fine-grained visual differences. It is natural since the overall video-level understanding is already enough to distinguish the target among different videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have introduced our new dataset Kinetic-GEB+ with the methods of benchmark construction and proposed three tasks that aim to develop a more fine-grained, robust and human-like understanding of videos based on status changes. We further explore the challenges with designed experiments, where we design a new Temporal-based Pairwise Difference (TPD) modeling method to represent visual differences and obtain significant improvement in performance. Concluding the results from the experiments, we summarize the challenges of our benchmarks as three parts: (1) How to adaptively utilize multiple granularities of features and exclude the disturbance. (2) How to effectively represent the visual differences around the boundary. (3) How to improve the temporal resolution of understanding. We believe our work could be a stepping stone for the following works to develop more powerful methods to understand status changes and thus improve video-level comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supp.: Overview</head><p>In the supplementary material, we provide more details of annotations (Sec. 6) and more implementation details of the baselines (Sec. 7). Moreover, we conduct more experiments of Boundary Captioning and Grounding for more visual difference representation methods as well as the ablation study for our TPD Modeling method (Sec. 8). Finally, we release some common failure cases in our prediction of Boundary Captioning and further discussion on the benchmark (Sec. 9).</p><p>6 More Details of Annotations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Boundary Definition</head><p>Specifying the level of details. A great number of our video sources from Kinetic-400 contain more than one actor or object with different levels of status changes, and different annotators could have high-variance opinions on the boundary positions. According to <ref type="bibr" target="#b30">[31]</ref>, to reduce the variance among annotators, the highest priority is to specify the level of the spatial and temporal details we take into consideration. For the level of spatial details, we only focus on the event changes that are performed by dominant subjects. Specifically, in the Example 2 of <ref type="figure" target="#fig_2">Fig. 2</ref>, the two girls are repeating the same event, the status of which is unchanged (no event boundary). Instead, the boy performs different events before and after the marked timestamp. For the level of temporal details, we only consider the "one-level-deeper" granularity as in <ref type="bibr" target="#b30">[31]</ref>. By specifying this, we ensure that most of the boundaries are in the same granularity, rendering it possible for annotators to basically reach an agreement on the boundary location without predefined classes.</p><p>Embracing the Ambiguity. Knowing the specified level of details, however, different annotators could still have some disagreements on the dominant subjects and the "one-step-deeper" granularity events. Following <ref type="bibr" target="#b30">[31]</ref>, we embrace this varsity when annotating. For each video, we take all the annotations as correct. Then we supervise the consistency among different annotations towards the same video by calculating the F1 score in Sec. 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Quality Assurance</head><p>Criteria for Rejecting a Video. To ensure the quality of videos, we designed a rejection criteria for annotators to filter the video sources. Each video is simultaneously allocated to at least 5 annotators, and each annotator could independently decide whether to annotate or reject the video. Following <ref type="bibr" target="#b30">[31]</ref>, the criteria is designed based on the understandability and the boundary number of the video. Specifically, a video is expected to be rejected in four cases: (1) Not understandable due to blurry or overspeeding.    <ref type="figure">Fig. 7</ref>. Distribution of consistency F1 scores in all annotations. We first compute the F1 scores with different thresholds from 0.2s to 1s, and then average the scores in all thresholds as the final score (4) Violating content. The statistics on the number of annotations in all selected videos is shown in Tab. 8. We could see that a majority of videos are accepted by at least 5 annotators, indicating the consistency of annotators' opinions on our annotated videos. Evaluation of Annotators' Consistency. Following <ref type="bibr" target="#b30">[31]</ref>, we compute F1 score to evaluate the consistency of the annotations towards the same videos. When computing, we take the timestamps of each annotation as the "prediction" and all other annotations in the same video as the "ground truth". Then for each threshold varying from 0.2s to 1s, we compute the precision and recall for the "prediction" to obtain its F1 score. Finally, we average the F1 scores under all thresholds as the final result of the evaluation. The distribution of the average F1 score is shown in <ref type="figure">Fig. 7</ref>, where over 92% percent of annotations are scored higher than 0.4, suggesting that our annotators have very high consistency in determining event boundary positions. They also tend to focus on the same subject on the agreed boundary, providing captions for the same event change with little bias. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Statistics on the Similarity between Captions</head><p>To further investigate the similarity between the captions annotated in the same videos. We first randomly selected 1,000 videos with over 103K captions from our dataset, then computed the Sentence-BERT similarity of status parts' captions in these videos. The results with three examples for different level of scores are shown in <ref type="figure" target="#fig_4">Fig. 8</ref>. We see that nearly 80% of the caption pairs are less than 0.7 score (only a few words are shared), indicating our captions are unique and fine-grained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Statistics based on the Video Categories in Kinetic-400</head><p>Since we take the "one-step-deeper" events in videos as <ref type="bibr" target="#b30">[31]</ref>, the video-level categories in Kinetic-400 could not determine the pattern of events. However, the category provides a higher-level background for our events, thus we conduct further statistics towards it.</p><p>Boundary Number and Interval Duration. Firstly we investigate the distribution of boundary numbers in each category of videos. Given a Kinetic-400 category, we compute the average number of boundaries per video in the category. From the result in <ref type="figure">Fig. 9</ref>, we see that the boundary numbers slightly vary with the category and most categories have 2 to 3 boundaries per video. We also illustrate the interval durations versus categories in the right of <ref type="figure">Fig. 9</ref>. In most categories of videos, we could see the average duration of boundary intervals is around 2s.</p><p>Distributions in Splits. Furthermore, we conduct statistics on the video numbers of each category in our train/val/test splits. The percentage distribution is shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, where the categories are sorted by their video numbers in the entire dataset. We see that the categories' distribution in the three splits are consistent with the distribution in the entire dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Details of the Adjustment for Downstream Tasks</head><p>In the raw annotation of Kinetic-GEB+, each video is allocated to more than 5 annotators. Due to the variance of annotators' opinions, the boundary locations in different annotations towards the same video are not the same. When preparing the data for downstream tasks, we select one annotator whose labeled boundaries have highest F1 score (computed in Sec. 6.2) for each video. Then, we use these boundaries' timestamps as the anchors to merge other annotators' captions, preserving the diversity of different opinions. Thus, one video corresponds to multiple boundaries, and each boundary could be with multiple captions. Finally, we collected 40k anchors/boundaries and from the total 176,681 boundaries in 12,434 videos, where 80% anchors/boundaries have more than 3 captions and around 10% of anchors have only 1 unique caption.</p><p>As mentioned in main body, the videos in our Kinetic-GEB+ could sometimes contain repeated events or actions, which could disturb the Boundary Grounding task. We found that the difference among some pairs of boundaries within a video is too subtle even for humans to distinguish. Therefore, we need to find these "equal" pairs of boundaries and mark them as "equal" boundaries to each other for the Boundary Grounding task. Specifically, when querying with the caption of one boundary in the pair, the timestamps of other "equal" boundaries are also correct answers. When queried by a boundary caption, the machine is supposed to answer the locations of that boundary as well as all its "equal" boundaries. An example is shown in <ref type="figure" target="#fig_0">Fig. 13</ref>, where the man changes his status from sitting to standing twice in the video, thus these two status changes are marked as an "equal" pair.</p><p>To find and mark these "equal" pairs, we employ Sentence-BERT <ref type="bibr" target="#b28">[29]</ref> to compute the similarity score between the annotated captions of every two different boundaries inside a video. Firstly, we take the filtered annotations. Then for each video, we combine every two of its boundaries to form all the possible pairs. After that, we separate each pair of captions into subject, status before and status after items, and then compute the similarity score for each item using Sentence-BERT. The range of similarity scores is from 0 to 1. In order to distinguish these "equal" pairs, we need to set a maximum threshold for similarity scores. First we find that the item pairs scoring less than 0.9 usually have significant differences that are easy for humans to recognize. Hence, we collect the pairs of all the three items that score higher than 0.9, and then we annotate manually to classify if each pair is an "equal" pair. After that, we simulate the decision accuracy of different candidate thresholds varying from 0.9 to 1.0 and finally choose 0.93 as the threshold, where the corresponding accuracy is 95.5% (i.e. the 2-sigma probability in normal distribution). Finally, we found and marked 4,426 "equal" pairs consisting of 4,295 boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">More Examples of Kinetic-GEB+</head><p>Here we illustrate more examples from our Kinetic-GEB+ in <ref type="figure" target="#fig_0">Fig.11</ref>. The Example 1 to 4 are all based on Change of Action. The Example 5 is based on Change of Subject, since the man was at first appearing in the scene and then disappears after the boundary. In Example 6, the color of the stage suddenly changes from blue to pink, causing the boundary based on Change of Color. In Example 7, the woman was first interacting with the trophy and then retreats her hands to stop interacting after the boundary. This boundary is thus due to the Change of Object being interacted with. Finally in Example 8, the boundary is based on Multiple types of status changes. The man in the scene changes his action and simultaneously stops interacting with the iron ball at the boundary.</p><p>7 More Details of Implementation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Schemes for frame sampling</head><p>In all our experiment groups, if not specified, we employ the two following schemes of frame sampling when extracting visual information for boundary timestamps: Scheme 1. In most cases, when using the ground truth boundaries, we set two sampling ranges before and after each boundary timestamp. For the range before the boundary, we set the preceding boundary as the start and the current boundary as the end. Similarly, the range after is between the current boundary and the succeeding boundary. Notably, the predecessor of the first boundary in videos is set to 0, and the successor of the last boundary in videos is set to the end of videos. Finally, we sample 10 frames in each range and 1 frame at the timestamp of the current boundary. This scheme is also employed when using  <ref type="figure" target="#fig_0">Fig. 11</ref>. More samples from Kinetic-GEB+ dataset the proposal timestamps generated by GEBD baseline <ref type="bibr" target="#b30">[31]</ref>, like in the testing period of Boundary Grounding specified with "GEBD" suffix. Scheme 2. Sometimes there is no predefined boundary or proposal, and thus the locations of the preceding and succeeding timestamps are unknown. Therefore, we replace the predecessor and successor with the timestamps 1s before and after the current timestamp. Then we sample 10 frames in each range and 1 frame at the candidate timestamp for further extraction. For example, in the testing period of Boundary Grounding (in the groups without "GEBD" suffix), this scheme is employed by sampling a timestamp candidate every 0.1s for all videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Further Details in Training</head><p>For each backbone utilized in our experiments, we trained for 50 epochs. For all the BERT based models, we used AdamW optimizer with a linearly decreasing learning rate starting from 5e ?5 . Notably, in Boundary Grounding we modify the original contrastive loss in FROZEN <ref type="bibr" target="#b2">[3]</ref> by adding an additional intra loss. Given a batch of embeddings, the intra loss is computed in the same way yet only among the caption and context embeddings from the same videos. Besides, as mentioned in previous sections, we design a batch-random sequential sampler for Boundary Grounding. It ensures more boundaries in the same video to be collected in the same batch, since the boundaries are sequentially sorted by their videos in the dataset. This intra loss and new sampler encourage the model to learn the differences among the boundaries in the same videos, which conforms to the goal of Video Grounding that is selecting the best match among all timestamps in a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Post-processing and Evaluation</head><p>In Boundary Captioning, we separate and evaluate the Subject, Status Before and Status After items of the generated captions. We found that the conventional BLEU <ref type="bibr" target="#b23">[24]</ref> metric is not suitable for our task and its scores are often inconsistent with humans' impression, since it only considers the simple repetition of word grams. Samples of predicted captions in a video are illustrated in <ref type="figure" target="#fig_0">Fig. 12</ref>. We see that the first two generated captions are relatively great, while the caption generated from the last boundary is not satisfying. For Boundary Grounding, we conduct a post-processing after the models generating the matching scores of all candidates. First we apply the LoG filter <ref type="bibr" target="#b19">[20]</ref> to find the local maximas following <ref type="bibr" target="#b30">[31]</ref>. Then we select the top-K maximas as final prediction following the statistics of the ground truth timestamp numbers for all queries. After that, we evaluate the finalized prediction by calculating the F1 score under different thresholds, where the computation is the same as in Sec. 6.2. Samples of predictions are shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. Notably, the boundaries at 00:00.93 and 00:06.11 are a pair of equal boundaries, thus we mark both of their timestamps as the ground truths for their caption queries. For Boundary Caption-Video Retrieval, several samples of predicted ranking are illustrated in <ref type="figure" target="#fig_0">Fig. 14.</ref> For the first three samples Equal: Equal: <ref type="figure" target="#fig_0">Fig. 13</ref>. Samples of Prediction in Boundary Grounding </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Common Failures in Boundary Captioning</head><p>In the task of Boundary Captioning, we find some failure cases happened in our prediction. Here we present two types in <ref type="figure" target="#fig_0">Fig. 15:</ref> (1) the model misses the target subject due to another subject without event change is visually salient. (2) the action in status before or after is subtle and the model mistakenly considers there is nothing happening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">"</head><p>Can we replace Kinetic-GEB+ with existing video captioning datasets simply by concatenating two segments together?"</p><p>It may not work well as (1) previous and next captions from existing datasets could correspond to different subjects while our boundary caption targets one</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of generic event boundaries with captions in Kinetic-GEB+, as well as three downstream tasks designed upon the boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>(a) The parts of speech distribution of the Status Before and Status After compared with that of the Subject part. The two status parts contains more verbs and focus more on motions. (b) The 20 most frequent nouns in Subject. (c) The 20 most frequent verbs in Status Before and Status After of a subject in the scene, as shown in Example 2 of Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )</head><label>2</label><figDesc>Contains no boundary or too many boundaries. (3) Includes shot changes like zooming, panning or cutting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Status A: walk towards the group of people Score: 0.1201 Status B: turn to the left while playing trumpet Status A: lift the hand up from right Score: 0.5656 Status B: walk to the right while playing trumpet Status A: sitting in vehicle in front of the camera Score: Left. Distribution of Sentence-BERT scores for the sampled captions in the same videos. Right. Intuitive examples of different level of Sentence-BERT scores selected from captions in the same videos in Kinetic-GEB+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Left. Average number of boundaries in videos in each Kinetic-400 category. Right. Average duration of boundary intervals in videos in each Kinetic-400 category. Percentage distributions of the videos in each Kinetic-400 category in the entire dataset and the train/val/test splits. The categories are sorted by their video numbers in the entire dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with most relevant Video Captioning datasets. Our Kinetic-GEB+ has comparable scale and is the only one targeting the generic boundaries, while conventional datasets focus on entire videos or video segments #Videos Video Domain #Captions Caption Target Target Type</figDesc><table><row><cell>Annotation in Segments</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The man in black t-shirt and pant used to walk on the running track holding the javelin in hand from left, then at the boundary, he starts to running with the javelin in his hand.</figDesc><table><row><cell cols="2">Valid Percentage (%) Avg. Annotation Time (s) Avg. Evaluation Time (s) Sentence: Valid Percentage (%) Subject: man in black t-shirt and pant</cell><cell>93.87 74.3 20.3 97.60</cell></row><row><cell>Change: first walked on the running track holding the javelin in hand from</cell><cell>Avg. Annotation Time (s)</cell><cell>58.0</cell></row><row><cell>left, then at the boundary, starts to running with the javelin in his hand.</cell><cell>Avg. Evaluation Time (s)</cell><cell>17.9</cell></row><row><cell>Subject: man in black t-shirt and pant</cell><cell>Valid Percentage (%)</cell><cell>99.53</cell></row><row><cell>Status Before: walking on the running track holding javelin in hand from left</cell><cell>Avg. Annotation Time (s)</cell><cell>50.2</cell></row><row><cell>Status After: run on the running track holding javelin in hand from left</cell><cell>Avg. Evaluation Time (s)</cell><cell>12.5</cell></row><row><cell cols="3">Fig. 3. Three candidate formats of Boundary Captions and their evaluation results,</cell></row><row><cell cols="3">respectively One-Sentence format, Two-Item format and Our Finalized format</cell></row><row><cell cols="3">we set a minimum threshold for both temporal and spatial details' level to ensure</cell></row><row><cell cols="3">the consistency among different annotators. Further details are shown in Supp.</cell></row><row><cell></cell><cell cols="2">and Tab. 9. Following [31],</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Annotation</figDesc><table><row><cell></cell><cell></cell><cell cols="4">number per video</cell><cell>Table 3. Timestamp v.s. Time Range</cell></row><row><cell cols="2">#Annotations 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Boundary Timestamp Time Range</cell></row><row><cell>#Videos</cell><cell cols="5">605 536 582 928 9783</cell><cell>Num.</cell><cell>172103</cell><cell>4578</cell></row><row><cell>Per. (%)</cell><cell cols="5">4.87 4.31 4.68 7.46 78.68</cell><cell>Per. (%)</cell><cell>97.41</cell><cell>2.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>)(c), we extract the first Subject: man in black t shirt and pant //Status Before: walking on the running track holding javelin in hand from left //Status After: run on the running track holding javelin in hand from left[SEP]    </figDesc><table><row><cell cols="4">MLM Head Word Embedding [CLS] Subject: man in black t shirt and [MASK]</cell><cell></cell><cell cols="6">BERT Encoder CNN TPD Modeling [CLS] TSN</cell><cell>Subtraction</cell></row><row><cell cols="4">//Status Before: walking on the [MASK] track holding javelin in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">hand [MASK] left //Status After: run on the running [MASK] holding [MASK] in</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell cols="2">hand from left [SEP]</cell><cell></cell><cell cols="2">Boundary Caption</cell><cell></cell><cell cols="2">Before</cell><cell></cell><cell>Boundary</cell><cell>After</cell></row><row><cell></cell><cell cols="7">TPD (Temporal-based Pairwise Difference) Modeling :</cell><cell></cell><cell></cell><cell>Simple Subtraction:</cell></row><row><cell>Input:</cell><cell></cell><cell></cell><cell></cell><cell>Part a:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input:</cell></row><row><cell cols="2">Before ?</cell><cell>Boundary</cell><cell>After ?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>Before</cell><cell>After</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Output:</cell><cell>? Part a</cell><cell>? Part b</cell><cell>? Part c</cell><cell>Part b: ?</cell><cell>?</cell><cell cols="2">Part c: ?</cell><cell>?</cell><cell>Output:</cell><cell>Difference</cell></row></table><note>Fig. 6. Top. A general modification for BERT model showing on ActBERT-revised. Bottom. Our difference modeling methods designed for BERT model noun in every Subject as well as the first verb in all Status Before and Status</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Performance of Different Methods in Boundary Captioning. For UniVLrevised and ActBERT-revised, we apply the TPD Modeling and take the "ResNet-roi+TSN" combination as input feature Bef. Aft. Avg. Sub. Bef. Aft. Avg. Sub. Bef. Aft. CNN+LSTM 49.73 80.11 34.39 34.69 13.62 18.84 9.92 12.10 26.46 39.77 20.77 18.83 DUDA 58.56 104.41 47.12 24.14 16.34 21.72 14.63 12.68 27.57 42.76 21.76 18.18 UniVL-revised (two-stream) 65.74 91.51 56.58 49.13 18.06 21.08 17.06 16.05 26.12 40.67 19.42 18.28 ActBERT-revised (one-stream) 74.71 85.33 75.98 62.82 19.52 20.10 20.66 17.81 28.15 39.16 23.70 21.60</figDesc><table><row><cell>Method</cell><cell>Avg.</cell><cell>CIDEr Sub.</cell><cell>SPICE</cell><cell>ROUGE L</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Upper. Ablation study results of the Boundary Captioning utilizing ActBERT-revised with TPD Modeling employed to all rows with "ResNet-roi". Lower. The performance comparison of visual difference modeling methods, where the TPD Modeling is employed to the last row Sub. Bef. Aft. Avg. Sub. Bef. Aft. Avg. Sub. Bef. Aft. ResNet-roi 51.93 67.79 46.59 41.42 14.30 16.01 13.54 13.34 24.20 35.42 19.04 18.13 ResNet-conv 66.18 96.86 54.77 46.91 17.07 20.58 15.82 14.8 26.30 40.38 19.71 18.82 TSN 70.80 92.54 65.64 54.21 19.00 20.97 18.98 17.04 26.89 40.53 20.82 19.32 ResNet-roi + ResNet-conv 56.64 83.82 45.64 40.45 15.68 19.17 13.77 14.1 25.46 38.64 19.26 18.47 ResNet-conv + TSN 69.58 83.56 68.88 56.3 18.95 20.15 19.51 17.2 27.14 38.52 22.36 20.53 ResNet-roi + TSN 74.71 85.33 75.98 62.82 19.52 20.10 20.66 17.81 28.15 39.16 23.70 21.60 ResNet-roi + ResNet-conv + TSN 65.83 80.9 63.22 53.38 18.69 19.37 19.25 17.46 26.84 37.82 22.11 20.59 ResNet-roi + TSN (w/o Diff.) 67.38 85.59 63.06 53.49 18.47 19.84 18.69 16.87 24.23 31.65 21.14 19.90 ResNet-roi + TSN (simple) 67.75 85.31 64.28 53.65 18.96 20.35 19.13 17.39 26.78 39.14 21.20 20.00 ResNet-roi + TSN (TPD) 74.71 85.33 75.98 62.82 19.52 20.10 20.66 17.81 28.15 39.16 23.70 21.60</figDesc><table><row><cell>Input Granularity</cell><cell>Avg.</cell><cell>CIDEr</cell><cell>SPICE</cell><cell>ROUGE L</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Performance comparison among different methods in Boundary Grounding. For UniVL-revised and ActBERT-revised, we apply TPD Modeling and take the "ResNet-roi+TSN" combination as input feature.54 18.33 31.04 40.48 47.86 54.81 61.45 33.35 FROZEN-revised-GEBD 4.20 8.48 18.49 29.91 39.54 48.37 55.29 61.55 33.23</figDesc><table><row><cell>Method</cell><cell>0.1 0.2</cell><cell>0.5</cell><cell>1</cell><cell>Threshold (s) 1.5 2</cell><cell>2.5</cell><cell>3</cell><cell>Avg.</cell></row><row><cell>Random Guess</cell><cell cols="7">2.14 4.56 11.46 22.81 31.63 40.43 48.06 54.37 26.93</cell></row><row><cell>TVQA</cell><cell cols="7">2.60 5.30 12.90 23.73 32.94 41.33 48.56 55.17 27.82</cell></row><row><cell>2D-TAN</cell><cell cols="7">2.91 6.32 15.04 26.95 36.94 45.34 51.87 58.22 30.45</cell></row><row><cell>ActBERT-revised</cell><cell cols="7">3.12 6.14 14.79 26.78 36.61 45.45 52.99 59.41 30.66</cell></row><row><cell>FROZEN-revised</cell><cell>4.28 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison of different methods in Boundary Caption-Video Retrieval. For FROZEN-revised, we add another group without difference modeling</figDesc><table><row><cell>Method</cell><cell>mAP R@1 R@5 R@10 R@50</cell></row><row><cell>Random</cell><cell>0.39 0.05 0.23 0.44 2.52</cell></row><row><cell>CNN+LSTM</cell><cell>9.25 4.08 12.49 19.53 42.26</cell></row><row><cell>ActBERT-revised (one-stream)</cell><cell>19.14 9.52 28.89 40.14 64.50</cell></row><row><cell>FROZEN-revised (two-stream)</cell><cell>23.39 12.80 34.81 45.66 68.10</cell></row><row><cell cols="2">FROZEN-revised (two-stream) w/o diff 22.44 12.12 33.42 43.89 65.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Annotation number per video Timestamp v.s. Time Range</figDesc><table><row><cell cols="2">#Annotations 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Boundary Timestamp Time Range</cell></row><row><cell>#Videos</cell><cell cols="5">605 536 582 928 9783</cell><cell>Num.</cell><cell>172103</cell><cell>4578</cell></row><row><cell>Per. (%)</cell><cell cols="5">4.87 4.31 4.68 7.46 78.68</cell><cell>Per. (%)</cell><cell>97.41</cell><cell>2.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Results of more exploration on Boundary Captioning, including the comparison among different fusion methods and the ablation study on our TPD Modeling method Sub. Bef. Aft. Avg. Sub. Bef. Aft. Avg. Sub. Bef. Aft. Concat 68.16 86.35 62.99 55.15 18.92 20.13 19.08 17.57 26.71 39.25 20.96 19.93 Tucker 67.25 85.15 63.08 53.51 18.71 20.39 18.97 16.78 26.91 39.28 21.42 20.02 TPD (part a) 72.45 89.7 70.45 57.20 19.39 20.64 19.87 17.67 27.74 39.49 22.86 20.87 TPD (part b) 70.78 90.04 66.59 55.70 19.22 20.46 19.67 17.54 27.27 39.6 21.93 20.27 TPD (part c) 69.01 86.9 66.11 54.03 19.11 20.34 19.47 17.53 27.31 39.69 21.98 20.27 TPD 74.71 85.33 75.98 62.82 19.52 20.10 20.66 17.81 28.15 39.16 23.70 21.60</figDesc><table><row><cell>CIDEr Avg. 9 More Discussions Method</cell><cell>SPICE</cell><cell>ROUGE L</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This project is supported by the National Research Foundation, Singapore under its NRFF Award NRF-NRFF13-2021-0008, and Mike Zheng Shou's Start-Up Grant from NUS. The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>in the figure, the prediction result is relatively satisfying and the ground truth video is within the top-5 of the ranking. However, given the caption of the last sample in the figure, the machine could not clearly recognize the target video from the corpus, and the ground truth video is ranked to #42.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">More Exploration on Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Boundary Captioning</head><p>Here we delve deeper on the design of fusion mechanism for status changes. In Tab. 10, we further compare subtraction operation (TPD method) with another 2 operations: Simple concatenation (denoted as Concat) and Multimodal Tucker Fusion <ref type="bibr" target="#b3">[4]</ref>. As shown, our TPD outperforms the other fusion methods. Furthermore, to investigate the contribution made by different parts in our TPD Modeling method, we conduct an ablation study. In Tab. 10, we see that part a contributes more than part b and part c, while the combination of the three parts enables the model to have the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Boundary Grounding</head><p>In order to investigate the contribution of different captioning parts in Boundary Grounding task, we evaluate some variants of FROZEN-revised-GEBD which take no caption (i.e. random guess based on boundary proposals from GEBD) or only subject parts as input for grounding. The F1 scores of no caption/only subject/full caption under 0.1s are 3.09/3.25/4.20 respectively. The performance doesn't improve much with only the subject. The reason is that boundaries in the same video are often caused by the same subject, requiring the model to understand captions depicting detailed status changes to ground the video.  <ref type="figure">Fig. 15</ref>. Two Common Failure Cases in the Task of Boundary Captioning subject; (2) Event caption usually summarises the whole time span, while boundary caption focuses on detailed, fine-grained status change of the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Caption: Output Ranking of Videos by Boundaries:</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Street-view change detection with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gherardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1301" to="1322" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2612" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards open world recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On pursuit of designing multimodal transformer for video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. pp</title>
		<imprint>
			<biblScope unit="page" from="9810" to="9823" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. In: CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dolan</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P11-1020" />
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04290</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5267" to="5275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mac: Mining activity concepts for languagebased temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="245" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-modal dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="958" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to describe differences between pairs of similar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Jointly localizing and describing events for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="7492" to="7500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature detection with automatic scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Change detection in heterogenous remote sensing images via homogeneous pixel transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1822" to="1834" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local-global video-text interactions for temporal grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10810" to="10819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Robust change captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4624" to="4633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Event perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Radvansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Zacks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="608" to="620" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Grounding action descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="671" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generic event boundary detection: A benchmark for event segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8075" to="8084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building change detection based on satellite stereo imagery and digital surface models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="406" to="417" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-to-end dense video captioning with parallel decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6847" to="6857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">To find where you talk: Temporal sentence localization in video with attention based location regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9159" to="9166" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dense regression network for video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10287" to="10296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning 2d temporal adjacent networks formoment localization with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
