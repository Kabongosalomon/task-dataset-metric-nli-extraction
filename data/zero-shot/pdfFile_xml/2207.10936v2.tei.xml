<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-tailed Instance Segmentation using Gumbel Optimized Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Panagiotis</surname></persName>
							<email>konstantinos.alexandridis@kcl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<postCode>L69 3BX</postCode>
									<settlement>Liverpool</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandridis</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
							<email>j.deng16@imperial.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
							<email>anguyen@liverpool.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<postCode>L69 3BX</postCode>
									<settlement>Liverpool</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Luo</surname></persName>
							<email>shan.luo@kcl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<postCode>L69 3BX</postCode>
									<settlement>Liverpool</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long-tailed Instance Segmentation using Gumbel Optimized Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Long-tailed distribution</term>
					<term>long-tailed instance segmentation</term>
					<term>Gumbel activation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Major advancements have been made in the field of object detection and segmentation recently. However, when it comes to rare categories, the state-of-the-art methods fail to detect them, resulting in a significant performance gap between rare and frequent categories. In this paper, we identify that Sigmoid or Softmax functions used in deep detectors are a major reason for low performance and are suboptimal for long-tailed detection and segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for long-tailed detection and segmentation. It aligns with the Gumbel distribution of rare classes in imbalanced datasets, considering the fact that most classes in long-tailed detection have low expected probability. The proposed GOL significantly outperforms the best state-of-the-art method by 1.1% on AP , and boosts the overall segmentation by 9.0% and detection by 8.0%, particularly improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS dataset. Code available at: https://github.com/kostas1515/ GOL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There have been astonishing advancements in the fields of image classification, object detection and segmentation recently. They have been made possible by using curated and balanced datasets, e.g., CIFAR <ref type="bibr" target="#b19">[20]</ref>, ImageNet <ref type="bibr" target="#b8">[9]</ref> and COCO <ref type="bibr" target="#b21">[22]</ref> and by using deep Convolutional Neural Networks (CNNs). Despite that, all these advancements could be in vain if they are not usable in real-world applications. For example, the performance of classifiers in ImageNet is similar to humans, however, ImageNet pretrained detectors still struggle as they suffer from various sources of imbalance <ref type="bibr" target="#b26">[27]</ref>. Moreover, existing instance segmentation models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref> fail to generalize for long-tailed datasets and their performance <ref type="figure">Fig. 1</ref>. (i) Gumbel activation function (blue) is asymmetric and it aligns better with the long-tailed instance segmentation distribution due to the extreme background and foreground class imbalance, whereas Sigmoid activation (red) is symmetric and more appropriate for balanced distributions. (ii) Gumbel activation (blue) produces more balanced weight norms in comparison to Softmax activation (orange) in the LVIS <ref type="bibr" target="#b10">[11]</ref> dataset using Mask-RCNN <ref type="bibr" target="#b11">[12]</ref>. significantly decreases for the rare categories <ref type="bibr" target="#b10">[11]</ref>. As a result, it is difficult to exploit the advancements in image classification and transfer them to applications like object detection and segmentation due to the imbalance problem. Furthermore, there is a significant gap in performance between frequent (head) and infrequent (tail) classes in long-tailed datasets, as the state-of-the-art (SOTA) methods only detect the frequent classes <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21]</ref>. All such problems may deteriorate the reliability of autonomous systems that rely on object detection and segmentation and raise concerns.</p><p>One possible solution for improving the long-tailed instance segmentation performance is to gather more samples for rare classes, as it is known that CNNs can achieve better results by using more data. Unfortunately, data collection will be not only costly but also intractable. The physical world contains objects that follow the Zipfian distribution <ref type="bibr" target="#b22">[23]</ref>. This means that by increasing the distinct classes of a dataset, it is unavoidable that some will be frequent while others will be rare.</p><p>The main reason for the low performance of instance segmentation in longtailed datasets is class imbalance. As discussed in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b23">24]</ref>, head classes dominate during training and they cause large discouraging gradients for tail classes. Since tail classes have fewer training samples, the amount of positive feedback is scarce and in the end, the model is trained effectively only for the head classes. It is also reflected by the norms of the classifier's weights <ref type="bibr" target="#b15">[16]</ref>: classifiers trained under the long-tailed paradigm have classification weights whose norms are larger for head classes and lower for tail classes. As larger weights produce larger probabilities, the classifiers are therefore biased towards head classes. For these reasons, many prior works focus on balancing either the weight norms or the gradients caused by head and tail classes or performing two-stage training where the model is first trained for all classes and then fine-tuned for tail categories.</p><p>In contrast, we argue that the low performance in long-tailed instance segmentation is partially due to the use of sub-optimal activation functions in bounding box classification. Most classes in this long-tailed distribution have extremely low expected probabilities due to imbalance <ref type="bibr" target="#b26">[27]</ref>, making the widely Object distributions in the LVIS long-tailed object detection dataset <ref type="bibr" target="#b10">[11]</ref>. (1): The distribution of objects P (obj, u) in the dataset (irrespective of their class); (2.a): the class probability conditioned on object and its location P (y|obj, u), and (2.b): the expected class distribution P (y, u), for the tail class bait; (3.a): the class probability conditioned on object and its location P (y|obj, u), and (3.b): the expected class distribution P (y, u), for the head class banana. As shown in the figures, the distributions of objects in a long-tailed object dataset have a normal distribution as a whole and also for the head classes, whereas follows a Gumbel distribution for tail classes.</p><p>used activation functions such as Sigmoid and Softmax unsuitable. For this reason, we develop a new activation function, namely Gumbel activation and a new Gumbel loss function to model the long-tailed distribution in instance segmentation. Gumbel activation is an asymmetric function that aligns better with the long-tailed instance segmentation distribution as shown in <ref type="figure">Figure 1</ref>(i). Moreover, Gumbel loss allows the gradient of positive samples to grow exponentially while suppressing the gradient of negative samples. This is especially useful for rare category learning, in which positive feedback is scarce. At the same time, it produces more balanced classification weight norms in comparison to Softmax as shown in <ref type="figure">Figure 1</ref>(ii), suppressing the classification bias. Both head and tail categories can benefit from Gumbel loss, without the need of gradient re-balancing, exhausting parameter tuning, weight normalization or complex two-stage training. Furthermore, Gumbel is agnostic to frameworks, it can be used alongside with other loss functions and datasets, which makes it widely applicable. Based on the proposed Gumbel loss, we have developed Gumbel optimized methods, that outperform the state-of-the-art instance segmentation methods on the LVIS <ref type="bibr" target="#b10">[11]</ref> dataset. We list our contributions as follows:</p><p>-We identify the problem of activation functions in long-tailed instance segmentation for the first time, via extensive experiments; -We propose a new loss, i.e., Gumbel Optimized Loss (GOL), for long-tailed instance segmentation; -We have validated the effectiveness of GOL on real-world long-tailed instance segmentation datasets, outperforming the SOTA methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Long-tailed object classification. It has been a hot topic to address the imbalance problem in object classification. Long-tailed object classification datasets of CIFAR10, CIFAR100 and ImageNet have been investigated to tackle imbalanced classification using techniques such as data re-sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref>, Cost Sensitive Learning (CSL) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, margin adjustment <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref> and two stage training <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36]</ref>. Data re-sampling methods re-sample the rare classes and have been most widely used and investigated <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref>. However, such methods cost more training effort and pose the risk of overfitting for rare classes, while under-sampling under-fits heads class and deteriorates the overall performance. The CSL methods construct a cost matrix so that the cost function can be more sensitive to the rare classes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, so as to exploit the data available. But CSL methods are dependent on the dataset and require careful calibration to avoid exploding gradients caused by excessive costs. Margin adjustment techniques change the decision boundary of the classifiers by either normalizing the classifier weight norms, engineering appropriate losses or modifying the classification prediction a-posteriori <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref>, which do not cost additional training time.</p><p>Their drawback is that the margins are difficult to compute, and they are based on dataset statistics. Long-tailed object detection. Some methods addressing the long-tailed image classification could be applied in long-tailed object detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b15">16]</ref>. However, many SOTA long-tailed classification methods obtain low performance for tasks that include the special background class <ref type="bibr" target="#b25">[26]</ref>. Under this realistic scenario, the performance drop is caused by the extreme imbalance between the dominant background class and other foreground classes. The same applies in long-tailed object detection where the imbalance factor is ? 1000 larger than the imbalance factor in image classification. For this reason, not all long-tailed classification methods are transferable to long-tailed instance segmentation. Instead, many methods are developed to tackle long-tailed object detection, directly. Some of them include the creation of specialized loss functions that balance the gradient contribution of positive and negative samples <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Others construct hierarchical groups <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref>, enforce margins in the classifier's prediction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref> or use two-stage strategies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref>. These methods have produced promising results but they suffer from limitations: Two-stage methods are complex and laborious; hierarchical methods require pre-processing and careful grouping; loss engineering methods have many hyper-parameters that need tuning. All these methods use Sigmoid or Softmax as their activation function, which is not close to the target distribution and not a good choice as we discuss in Section 3.</p><p>To the best of our knowledge, we are the first to tackle long-tailed segmentation by using Gumbel loss function. The most related work is <ref type="bibr" target="#b0">[1]</ref>, where they used the general extreme value distribution to classify Covid-19 cases. In contrast, we develop Gumbel for long-tailed instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>Assume a dataset X = {x i , y i }, i ? {1, ..., N }, where x i , y i are images and annotations respectively and N is the total number of training images. We can train a convolutional neural network f (X, ?) = z, where z is the latent representation and ? is the network's weight parameters. To calculate the prediction?, one first can use a fully connected layer q(z) = W T z + b, where W is the classification weights and b is the bias term, to calculate the score q i . Thenp i = ?(q i ) is used to transform the score q i into probabilityp i , using the activation function ?(?) and finally the prediction? is calculated using? = arg max i (p i ).</p><p>In image classification and instance segmentation, the Sigmoid activation, i.e., ? sigmoid (q i ) = 1 1+e ?q i , or the Softmax activation, i.e., ? softmax (q i ) = e q i e q j , has been commonly used. For the binary case, it assumes thatp i follows a Bernoulli distribution as the score q i = logp i 1?pi and it can be interpreted as the odds-ratio of the eventp i , i.e., how many times an event happensp i divided by how many times it does not happen 1 ?p i in a log scale.</p><p>It would be reasonable to use the Sigmoid or Softmax activation function for image classification, where the expected probability distribution P is a Bernoulli distribution and all classes are mutually exclusive, thus one can use Sigmoid or Softmax to effectively model the data. However, we argue that it would not be well suitable to use these activation functions for long-tailed instance segmentation as the expected distribution of objects is not the same as the expected image distribution in classification. Object distribution is more complex as it is affected by class imbalance and location imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Object Distribution</head><p>To make this clear, we first calculate the ground truth object distribution. To this end, we calculate the expected number of objects P (obj, u) whose centers fall inside the cell u = [i, j] of the normalized grid as follows:</p><formula xml:id="formula_0">P (obj, u) = x=N x=1 1(obj, x)1(obj, u) M (1)</formula><p>where obj is the object occurrence, 1 is the indicator function and M is the total number of objects in the dataset. Next, we calculate the class membership P (y|obj, u) for each location u, which summarizes the uncertainty of an object belonging to each class y in the dataset for the specific location u (i.e., it holds that y=C y=1 P (y|obj, u) = 1). Finally, we calculate P (y, u) 4 as:</p><formula xml:id="formula_1">P (y, u) = P (y|obj, u)P (obj, u)<label>(2)</label></formula><p>The final target distribution is a distribution that we aim to estimate by minimizing the cross entropy between the target and the data distribution.</p><p>In <ref type="figure" target="#fig_0">Figure 2</ref>, we visualize P (obj, u), P (y|obj, u) and P (y, u) for one head class and one tail class of LVIS dataset. The probability of detection for a head class i.e., banana in the example, is low in each location and varies in different locations of the image. For a tail class, i.e., bait in the example, is even lower and zero for most locations. This is different from long-tailed image classification, in which the expected class probabilities are not affected by location imbalance, only by class imbalance. On the other hand, in long-tailed instance segmentation, classes have even lower expected probabilities as they are affected by both location imbalance and class imbalance. For example, even head classes like banana have even lower expectation for locations far away from the center of the image, making the long-tailed segmentation more challenging for both head and tail classes. This highlights the magnitude of imbalance in long-tailed segmentation and motivates us to develop Gumbel activation.</p><p>Using Gumbel, we assume that the target distribution follows Gumbel distribution and this is a better choice than using Sigmoid or Softmax because the expected classification probabilities are minuscule. In fact, by using any activation function, one assumes how the ground truth is distributed. It is a common practice to use Sigmoid or Softmax and this assumes that the target distribution is Bernoulli. While this is a rational choice for image classification, it is unrealistic for long-tailed instance segmentation as the expected classification probabilities are infinitesimal. For this reason, we assume that the target distribution is Gumbel and we use Gumbel activation. We further explain why choosing an activation, that implicitly assumes the target distribution, below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Activations as Priors</head><p>To understand why choosing an activation implicitly assumes the target distribution, we consider an example of binary classification, however, it can be extended to multi-class classification easily. In a binary classification problem, the true variable y relates with the representations z as follows:</p><formula xml:id="formula_2">y = 1, if W T z + b + ? &gt; 0 0, otherwise<label>(3)</label></formula><p>where ? is the error that is a random variable. The classification boundary is set to 0, but it could be any other value as it is adjusted by the bias term b in optimization. We are interested in the probability P (y = 1) and this is calculated as:</p><formula xml:id="formula_3">P (y = 1) = P (W T z + b + ? &gt; 0) P (? &gt; ?W T z ? b) = 1 ? F (?q)<label>(4)</label></formula><p>where F is the cumulative distribution function. Many practitioners use Sigmoid to activate q and estimate P (y = 1) and this means that:</p><formula xml:id="formula_4">P (y = 1) = ? sigmoid (q) = 1 1 + e ?q ? sigmoid (q) = F logistic (q; 0, 1) P (y = 1) = 1 ? F logistic (?q; 0, 1)<label>(5)</label></formula><p>By comparing Eq. 4 and the last expression of Eq. 5, it is understood that by using Sigmoid activation, one assumes that the error term ? follows the standard logistic distribution (with ? = 0 and ? = 1), as one chooses F to be logistic. In practice, when any activation function ?(q) is applied, it is implicitly assumed how ? is distributed and as a result the target distribution y ? P is assumed. For example, if a Sigmoid function is used, the variable q is transformed into a binomial probability distribution, which implies that it has a success rat? p and a failure rate 1 ?p, like a coin toss. In this case, it is assumed that y follows Bernoulli Distribution and the error ? follows Logistic Distribution. Finally, the training of the model is to minimize the discrepancy between the target distribution P and the predicted distribution Q using Cross Entropy:</p><formula xml:id="formula_5">H(P(x), Q(x)) = ? x?X P(x)log(Q(x))<label>(6)</label></formula><p>In training, as Stochastic Gradient Descent is an iterative algorithm, the starting conditions play a significant role. This suggests that it is preferable to have a good starting prior so that the initial estimation of P is reasonable and the choice of the activation function will facilitate the initial estimation. This has a similar concept to the prior distribution in Bayesian Inference, where it is important to choose a suitable prior for optimal results. Hypothesis. In long-tailed instance segmentation, the expected classification probabilities are significantly low due to imbalance problems as mentioned by <ref type="bibr" target="#b26">[27]</ref> and explained in Eq. 2. For this reason, we hypothesize that Gumbel activation will produce superior results as it models the data using Gumbel Distribution which is closer to the real object distribution.</p><p>In conclusion, long-tailed instance segmentation is far more challenging than image classification and naive usage of Sigmoid or Softmax activation infringes on the underlying assumptions. Nevertheless, one can use Cross Entropy to minimize the discrepancy between the target distribution P and the predicted distribution Q but if the prior guess is significantly different than the actual distribution P then the results might be sub-optimal. By choosing the activation function, one guesses how the target P is distributed. For Sigmoid or Softmax, one believes that P is Bernoulli distribution and while this is reasonable for image classification, we argue that in long-tailed instance segmentation it is not optimal and we empirically show that Gumbel can produce superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Gumbel Activation for Long-tailed Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sigmoid Activation for Object Classification</head><p>It is useful to remind the readers about Sigmoid activation as we can make clear distinctions between this and our suggested activation. The formula is ? ? (q i ) = 1 1+exp(?qi) . If one encodes the ground truth y as a one-hot vector then the gradient using Eq. 6 is dL(??(qi),yi)</p><formula xml:id="formula_6">dqi = y i (? ? (q i ) ? 1) + (1 ? y i )? ? (q i ).</formula><p>Sigmoid is a symmetric activation function: the positive gradient (i.e., when y = 1) takes values from (?1, 0), while the negative gradient (i.e., when y = 0) takes values from (0, 1) and the response value grows with the same rate for both positive and negative input, as shown in <ref type="figure">Figure 3</ref>(i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gumbel Activation for Rare-class Segmentation</head><p>We notice that P (y, u) has infinitesimal probabilities. For tail classes it has a maximum value in the scale of 1e-7, for head classes it has a maximum 1e-5 and for both cases, it has even lower probabilities for edge locations in the image. This motivates us to model P (y, u) using the extreme value distribution Gumbel. Gumbel is useful for modelling extreme events, i.e., those with very low probabilities. For example, Gumbel can be used to predict the next great earthquake because this has a much lower probability than regular earthquakes. Other applications of Gumbel can be found in finance and biology and the readers are referred to this work <ref type="bibr" target="#b18">[19]</ref> for more information on extreme value distribution. In long-tailed instance segmentation, one can use the cumulative density function of the standard Gumbel distribution as the activation function (we further study the choice of non-standard Gumbel activation in supplementary material):</p><formula xml:id="formula_7">? ? (q i ) = F Gumbel (q; 0, 1) = exp(? exp(?q i ))<label>(7)</label></formula><p>Combining Eq. 6 and Gumbel activation we derive Gumbel Loss (GL) as:</p><formula xml:id="formula_8">GL(? ? (q i ), y i ) = ? log(? ? (q i )), if y i = 1 ? log(1 ? ? ? (q i )), if y i = 0<label>(8)</label></formula><p>The gradient of Eq. 8 is:</p><formula xml:id="formula_9">dL(? ? (q i ), y i ) dq i = ? exp(?q i ), if y i = 1 exp(?qi) exp(exp(?qi))?1 , if y i = 0<label>(9)</label></formula><p>The Gumbel activation is asymmetric as illustrated in <ref type="figure">Figure 3</ref>(i). This means that the positive gradients (i.e., when y = 1) will take values from (??, 0) while the negative gradients (i.e., when y = 0) will take values from (0, 1). This is a beneficial property that allows the positive feedback to grow exponentially while it suppresses the negative feedback. It is especially useful in long-tailed segmentation as the positive feedback for rare categories is scarce. With Gumbel activation, positive gradients can grow faster than by using Sigmoid activation as shown in <ref type="figure">Figure 3</ref>(ii) and this can boost the performance of rare classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Gumbel Optimised Loss</head><p>Gumbel activation can be used not only with Cross Entropy loss but with other state-of-the-art loss functions as well. We select the recently proposed Droploss <ref type="bibr" target="#b13">[14]</ref> as the loss function of our enhanced Gumbel-based model, other loss functions are shown in <ref type="figure" target="#fig_1">Fig. 4</ref>(ii). Using Gumbel activation and Droploss we propose Gumbel Optimised Loss GOL as follows:</p><formula xml:id="formula_10">L GOL = ? C j=1 w Drop j log(p j ),p j = ? ? (q i ), if y j = 1 1 ? ? ? (q i ), if y j = 0<label>(10)</label></formula><p>where w Drop j are class specific weights proposed by DropLoss <ref type="bibr" target="#b13">[14]</ref>. We show the full equation in supplementary material due to space limitations. <ref type="figure">Fig. 3</ref>. Activation properties for Gumbel (blue dashed lines), Sigmoid (red solid lines) activation functions. Their activation behaviours are illustrated in (i). The Gumbel activation is asymmetric, whereas Sigmoid is symmetric. Their positive gradients and negative gradients are illustrated in (ii) and (iii) respectively, using inverted y-axis ?dL dq . The positive gradients of the Gumbel activation (i.e., when y = 1) ranges in (??, 0), while the negative gradients (i.e., when y = 0) ranges in (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Dataset and evaluation metrics. For our experiments on long-tailed instance segmentation, we use LVIS (Large Vocabulary Instance Segmentation) dataset <ref type="bibr" target="#b10">[11]</ref>. We mainly use version 1 which contains 100k images for training and 19.8k images for validation. LVISv1 dataset contains 1, 203 categories that are grouped according to their image frequency: rare categories (those with 1-10 images in the dataset), common categories (11 to 100 images in the dataset) and frequent categories (those with &gt; 100 images in the dataset.) Some previous methods use the LVISv0.5 dataset, which has 1230 classes instead of 1203. For fairness, we also show results in this dataset. We report our results using average segmentation performance AP , average box performance AP b and average segmentation performance for rare AP r , common AP c and frequent categories AP f . Implementation details. For our experiments, we use 4 V100 GPUs, a total batch size of 16 images, a learning rate of 0.02, weight decay of 0.0001, Stochastic Gradient Descent and momentum of 0.9. We use random horizontal flipping and multi-scaling as data augmentations for training, following the conventions of the community. We train our models using the mmdetection framework <ref type="bibr" target="#b5">[6]</ref> and during inference, we use score threshold of 0.0001 as in <ref type="bibr" target="#b10">[11]</ref>.</p><p>For our intermediate experiments, we use Gumbel activation and a plethora of architectures, backbones, loss functions and sampling strategies using the 1x schedule. For our enhanced GOL model, we use Mask-RCNN <ref type="bibr" target="#b11">[12]</ref>, the 2x schedule, Normalised Mask <ref type="bibr" target="#b34">[35]</ref>, RFS sampler <ref type="bibr" target="#b10">[11]</ref> and a stricter Non-Maximum Suppression threshold that is 0.3. When using Gumbel activation, we initialize the weights of the classifier to 0.001 and the bias terms to -2.0, to enable stable training. More implementation details, design choices and results are discussed in our supplementary material. <ref type="table">Table 1</ref>. Comparative results for LVISv1 using schedule 1x, random sampler (left) and RFS <ref type="bibr" target="#b10">[11]</ref> sampler (right). Gumbel activation is superior than Softmax, especially for the case of random sampling when the distribution is unaltered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampler</head><p>Random </p><formula xml:id="formula_11">RFS [11] Method AP AP r AP c AP f AP b Method AP AP r AP c AP f AP b<label>Sigmoid</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results with Different Sampling Strategies</head><p>We compare activations when using a random sampler and 1x schedule. Under this configuration, the target distribution is unaltered and the probability of sampling is equal to the dataset's class probability. As <ref type="table">Table 1</ref> suggests, the best activation function using a random sampler is Gumbel which largely outperforms Sigmoid and Softmax. It increases overall AP by 2.6%, AP r and AP c by 4.1% and AP b by 1.9% compared to Sigmoid and AP by 3.8%, AP r by 4.9%, AP c by 6.2 % and AP b by 3.0% compared to Softmax. Noticeably, the gap in mask and box performance is smaller with Gumbel activation at 19.0% and 19.1% respectively, while other activations have larger gaps between box and segmentation performance. This suggests that Gumbel is more suitable for long-tailed segmentation than other activation functions.</p><p>We apply the state-of-the-art RFS <ref type="bibr" target="#b10">[11]</ref> method, which is an oversampling method and report the results in <ref type="table">Table 1</ref>. With RFS, images containing rare categories are up-sampled, thus the original distribution is distorted. Under this scenario, Gumbel activation does not boost the performance as much as before, as the object distribution becomes more balanced with oversampling. Nevertheless, Gumbel improves overall performance by a respective margin of 0.7% in overall segmentation and by 0.8% AP r which is attributed to the fact that firstly, there are still classes in the dataset that suffer from imbalance after using RFS, thus Gumbel can model them better than Sigmoid or Softmax; secondly, Gumbel activation has a lower gap in bounding box and segmentation performance than Sigmoid or Softmax, which results in higher segmentation performance (i.e., 0.7% increase) given similar box performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Integrating Gumbel Activation</head><p>We conduct experiments using training schedule 1x with larger backbones, i.e., Resnet101 <ref type="bibr" target="#b12">[13]</ref> and ResNeXt101 <ref type="bibr" target="#b38">[39]</ref> and architectures i.e., Cascade Mask RCNN <ref type="bibr" target="#b1">[2]</ref> and Hybrid Task Cascade <ref type="bibr" target="#b4">[5]</ref> to determine if the proposed activation generalizes for deeper models. As shown in <ref type="figure" target="#fig_1">Figure 4</ref> (i), Gumbel activation is a better choice than the Softmax activation function as it achieves better overall AP and AP in rare categories. Next, we examine the behavior of the Gumbel activation function using SOTA loss functions Equalization loss (EQL) <ref type="bibr" target="#b32">[33]</ref>, DropLoss <ref type="bibr" target="#b13">[14]</ref> and Federated Loss <ref type="bibr" target="#b40">[41]</ref>. For EQL and DropLoss, we use the hyperparameter ? = 0.0011 which is more appropriate for LVISv1 as described in <ref type="bibr" target="#b31">[32]</ref> and we change only the activation function from Sigmoid to Gumbel. For Federated Loss, we use the same hyper-parameters as described in <ref type="bibr" target="#b40">[41]</ref> changing only the activation from Sigmoid to Gumbel. As <ref type="figure" target="#fig_1">Figure 4</ref> (ii) indicates, Gumbel significantly boosts the performance of all models in both overall AP and rare category AP and this highlights its applicability and efficacy. We show more detailed results in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">GOL Components</head><p>We conduct an ablation study using a 2x-schedule and we report the most significant findings, a more detailed ablation study is provided in our supplementary material. We use the standard Mask-RCNN, EQL <ref type="bibr" target="#b32">[33]</ref> and RFS <ref type="bibr" target="#b10">[11]</ref> as the basis and examine the behavior of Gumbel. As shown in <ref type="table" target="#tab_1">Table 2</ref>, Gumbel can significantly boost the performance of this pipeline by 0.8%. To further boost the performance, we use a stricter Non-Maximum Suppression threshold that is 0.3 and Mask normalization, we denote these enhancements in the Table as (Enh). Next, we adopt DropLoss which is a recent improvement in the loss function of EQL, proposed by <ref type="bibr" target="#b13">[14]</ref>. Finally, the best performance is achieved using RFS, Gumbel, Enh and DropLoss, we codename this as pipeline GOL. Total performance. In the end, our GOL method significantly improves the vanilla Mask-RCNN AP by 9.0%, and it largely improves AP r by 20.3%, AP c by 11.5%, AP f by 1.2% and AP b by 8.0%. Comparison with other methods. As shown in <ref type="table" target="#tab_2">Table 3</ref>, GOL significantly surpasses the state-of-the-art in LVIS Dataset using the standard Mask-RCNN benchmark. In detail, in LVISv0.5, GOL achieves 29.5% AP , surpassing RFS <ref type="bibr" target="#b10">[11]</ref>    <ref type="bibr" target="#b31">[32]</ref> by 2.2% using ResNet50 backbone. It also achieves the best AP , which is at least 0.9% higher than other methods, using the larger ResNet101 backbone. Finally, it consistently outperforms all other methods for rare and common categories in both Resnet50 and Resnet101 backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Model Analysis</head><p>We analyze the behavior of Mask-RCNN using Gumbel Loss. In detail, we visualize Mask-RCNN predicted object distributions in the validation set, for two random classes chandelier and frisbee. We compare, the predicted distributions Q of Softmax, Sigmoid and Gumbel against the ground truth P using the Kullback Leibler divergence. As <ref type="figure">Figure 5</ref> suggests, Gumbel produces object distributions that are closer to the target distribution, as they have smaller Kullback-Leibler (KL) divergence. Moreover, as <ref type="bibr" target="#b15">[16]</ref> has suggested, there is a positive correlation between the weight norms of the classifier and the image frequency of categories, which results in classification bias. In our case, we visualize the weight norms of the Mask-RCNN classifier trained with Softmax (baseline model), and Gumbel respectively. As <ref type="figure">Figure 1</ref> (ii) suggests, the weight norm distribution of the Mask-RCNN classifier trained with Gumbel is more uniform than the distribution of vanilla Mask-RCNN. This suggests that the classifier norms are more balanced when using Gumbel loss which validates its efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Long-tailed Image Classification</head><p>We further test Gumbel activation in long-tailed image classification benchmarks. For all classification experiments, we use random sampling and decoupled strategy. In decoupled strategy, the model is first trained with Softmax activation and then only the classifier is re-trained with Gumbel activation. CIFAR100-LT <ref type="bibr" target="#b2">[3]</ref>. We train a ResNet32 for 240 epochs using Auto-Augment <ref type="bibr" target="#b6">[7]</ref>, SGD, weight decay 0.0002, batch size 64 and learning rate 0.1 that decays at epoch 200 and 220 by 0.01. In the second stage, we retrain the classifier using a learning rate of 1e-4 for 15 epochs. ImageNet-LT, Places-LT <ref type="bibr" target="#b22">[23]</ref>. For ImageNet-LT, we train ResNet50 with and without Squeeze and Excite <ref type="bibr" target="#b14">[15]</ref> modules for 200 epochs using Auto-Augment. For Places-LT, we finetune an ImageNet pretrained ResNet152 for 30 epochs.</p><p>For both datasets, we use SGD, weight decay 0.0005, batch size 256, and learning rate 0.2 with cosine scheduler. In the second stage, we retrain the classifier using a learning rate of 1e-5 for 10 epochs. As <ref type="table" target="#tab_3">Table 4</ref> indicates, Gumbel activation can boost the classification performance of all models in all datasets consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Discussions</head><p>We hypothesize that real-world long-tailed detection and segmentation data follows a distribution that is closer to Gumbel distribution and not Bernoulli. For this reason, we propose to use Gumbel activation instead of Sigmoid or Softmax. We validate the superiority of Gumbel against Sigmoid and Softmax under different sampling strategies, deeper models and loss functions and we develop the GOL method based on Gumbel activation that significantly outperforms the state-of-the-art. Our extensive experiments validate that Gumbel is a superior activation function that can be used as a component with both off-the-shelf methods and state-of-the-art models to further increase their performance.</p><p>We have also tested Gumbel activation in long-tailed classification benchmarks and saw consistent improvements when Gumbel is used as a decoupled method. Finally, Gumbel could also be used for dense object detection and we have seen a 0.4% increase in AP when using RetinaNet on COCO and 1x schedule. Currently, Gumbel cannot be used with Softmax-based loss functions and it does not take full advantage of oversampling methods. In the future, we will develop a custom loss function and sampling mechanism tailored to Gumbel activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Gumbel activation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Weights and biases initialization</head><p>Gumbel activation has exponential positive gradients, making it difficult to initialize due to arithmetic errors caused by the gradient overflow. For this reason, one should initialize the bias and weight terms of the classification layer with values that will produce small initial gradient. First, all weight terms W T are initialized to a small value of 0.001, which will result in that all q i = W T z+b ? b, then the total gradient will be:</p><formula xml:id="formula_12">?H(? ? (q), y) ? ? exp(?b) + (C ? 1) exp(?b) exp(exp(?b)) ? 1<label>(11)</label></formula><p>where C is the total number of classes in the dataset. As the total gradient should be zero initially, we have:</p><formula xml:id="formula_13">?H(? ? (q), y) = 0 (C ? 1) exp(?b) exp(exp(?b)) ? 1 = exp(?b) b = ? log(log(C)<label>(12)</label></formula><p>For the case of LVIS dataset that has 1,203 classes plus one for the background, we set the weights W T equal to 0.001 and the bias equal to ? log(log(1204) ? ?2.</p><p>These values produce small initial gradients and they prevent gradient overflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Temperature in Gumbel activation</head><p>We have also studied the choice of non Standard Gumbel activation, as shown in <ref type="figure" target="#fig_3">Figure 6</ref>.i, for different choices of temperature ?:</p><formula xml:id="formula_14">? ? (q i ; ?) = exp(? exp(? q i ? ))<label>(13)</label></formula><p>We observe that, choosing a larger temperature flattens Gumbel activation curve, while choosing a smaller temperature steepens the curve. Gumbel activation has a double exponent as shown in Eq. 13, which makes it difficult to select values of ? due to arithmetic instability. In our case, we choose values [0.8, 0.9, 1.0, 1.1, 1.2] and we observe that the best choice is ? = 1 as it has better overall AP and AP r as shown in 6.ii.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Gumbel activation and cut-off error</head><p>Gumbel activation has a double exponent, as shown in Eq. 13, this makes it numerically unstable for large inputs and hinders training. For this reason, we tested different ranges of values and decided to clip the input space to be within the range of <ref type="bibr">[?4, 10]</ref>. Using this range of values the cut-off error is e-5 and training commences without overflow errors. In the future, we will develop a solution that prevents numerical instability, so that we do not have to clip the input space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Average Positive Gradient</head><p>We visualize the average positive gradient g, each category receives during training for 12 epochs using MaskRCNN. We use logarithmic scale to measure g in dB because the average gradient is small, especially for rare categories. As <ref type="figure" target="#fig_4">Figure 7</ref> indicates, using Gumbel activation, the positive gradient is on average 7dB larger than the case of using Sigmoid, while for the case of rare categories, Gumbel produces gradients that are 10dB larger.</p><p>In conclusion, the network learns better the rare categories by using Gumbel activation than by using Sigmoid activation, as the gradient is larger with Gumbel. This is also reflected in the formula of the positive Sigmoid gradient and the positive Gumbel gradient. In detail, Sigmoid positive gradient is bounded to values (?1, 0), while Gumbel positive gradient is exponential and has values that reach (??, 0). This enables Gumbel activation to produce larger gradients than Sigmoid and it is useful for rare categories, where the gradient updates are scarce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Gumbel Optimised Loss</head><p>Our GOL method is based in DropLoss <ref type="bibr" target="#b13">[14]</ref>. It is described as follows:</p><formula xml:id="formula_15">L GOL = ? C j=1 w Drop j log(p j ),p j = ? ? (q i ), if y j = 1 1 ? ? ? (q i ), if y j = 0 (14) w Drop j = 1 ? T ? (f j )(1 ? y j ), if E(r) = 1 w ? Ber(? fj ), otherwise<label>(15)</label></formula><p>? fj = (n rare + n common )/n all , if T ? (f j ) = 1 n f requent /n all , otherwise where E(r) is a binary indicator function that outputs 1 if a region proposal r is foreground, T ? (f j ) is a rare category indicator that outputs 1 if the frequency of category j is lower than ?, w ? {0, 1} is a random variable drawn from Bernoulli distribution and ? fj is the shape parameter that is computed according to the foreground region proposals in the training batch.</p><p>B Long-tailed instance segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Ablation Study</head><p>In <ref type="table" target="#tab_4">Table 5</ref>, we conduct an ablation study of Gumbel activation, RFS <ref type="bibr" target="#b10">[11]</ref>, EQL <ref type="bibr" target="#b32">[33]</ref>, DropLoss <ref type="bibr" target="#b13">[14]</ref>, Normalised Mask <ref type="bibr" target="#b34">[35]</ref> and stricter Non Maximum Suppression (NMS) threshold. We denote the stricter NMS threshold and Normalised Mask enhancements as (Enh). As shown in <ref type="table" target="#tab_4">Table 5</ref> the best overall performance is achieved with Gumbel, RFS, Enh and DropLoss, we denote this pipeline as Gumbel Optimised Loss (GOL). The best performance on AP f is achieved using Gumbel, RFS, Enh and EQL, we denote this pipeline as GOL*.</p><p>Total Performance Our GOL method significantly boosts the vanilla MaskR-CNN AP by 9.0%, and it largely improves AP r by 20.3%, AP c by 11.5%, AP f by 1.2% and AP b by 8.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Results on Larger Frameworks and SOTA Losses</head><p>In <ref type="table">Table 6</ref>, we show detailed results when using Gumbel activation and SOTA long-tailed instance segmentation loss functions. In <ref type="table" target="#tab_5">Table 7</ref>, we show detailed experimental results using Gumbel activation and common instance segmentation  <ref type="table">Table 6</ref>. MaskRCNN with Resnet50, schedule 1x, EQLv1 loss <ref type="bibr" target="#b32">[33]</ref>, DropLoss <ref type="bibr" target="#b13">[14]</ref>, ACSL <ref type="bibr" target="#b36">[37]</ref> and Federated Loss <ref type="bibr" target="#b40">[41]</ref>. Gumbel activation boosts AP of all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Activation AP AP r AP c AP f AP b EQL ? <ref type="bibr" target="#b31">[32]</ref> Sigmoid 18.  frameworks. In all cases, Gumbel activation improves the overall segmentation performance of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Results on Larger Models</head><p>We report the performance of our methods using larger models such as MaskR-CNN with ResNet-101. As shown in <ref type="table" target="#tab_6">Table 8</ref>, using MaskRCNN ResNet-50, GOL significantly outperforms the best method, LOCE [10] by 1.1% on AP , by 2.9% on AP r and by 1.5% on AP c , using smaller training budget and the same enhancements.</p><p>Using MaskRCNN ResNet-101, GOL largely surpasses the best state-of-theart Seesaw <ref type="bibr" target="#b34">[35]</ref> by 0.9% in overall AP , 2.8% in AP r , 1.0% in AP c and 0.3% in AP b using the same enhancements and RFS sampler. It also surpasses LOCE by 1.0% in overall AP using fewer training epochs.</p><p>Finally, our GOL* method has the best AP f in both MaskRCNN ResNet-50 and MaskRCNN ResNet-101 backbones, thus it is useful if AP f is most important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Object Distributions</head><p>We further show more examples of object distributions in LVIS v1 validation set. As shown in <ref type="figure">Figure 8</ref>, Gumbel activation produces object distributions that are closer to the target distribution as they have lower K-L divergence. <ref type="figure">Fig. 8</ref>. Comparison of four object distributions in LVIS validation set, using Softmax (second column), Sigmoid (third column) and Gumbel (fourth column). Gumbel predicts distributions that have smaller K-L divergence than Sigmoid or Softmax.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Object distributions in the LVIS long-tailed object detection dataset [11]. (1): The distribution of objects P (obj, u) in the dataset (irrespective of their class); (2.a): the class probability conditioned on object and its location P (y|obj, u), and (2.b): the expected class distribution P (y, u), for the tail class bait; (3.a): the class probability conditioned on object and its location P (y|obj, u), and (3.b): the expected class distribution P (y, u), for the head class banana. As shown in the figures, the distributions of objects in a long-tailed object dataset have a normal distribution as a whole and also for the head classes, whereas follows a Gumbel distribution for tail classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>(i): Comparison of Softmax against Gumbel activation using common instance segmentation frameworks. (ii): Comparison of Sigmoid-based SOTA losses against the Gumbel-based alternatives. All models use 1x schedule and random sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>by 4.1% and the best state-of-art LOCE [10] by 1.1% in AP . Moreover,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>(i): Gumbel activation using different temperature ?. Selecting a larger ? flattens the curve, while selecting a smaller ? makes the curve steeper. (ii): Performance of MaskRCNN-R50 on LVISv1 using training schedule 1x and random sampler, for different choices of temperature ?. The best performance is observed for ? = 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Average Positive Gradient g per category, measured in decibel, (dB). Gumbel activation produces larger gradients for rare categories and facilitates rare category learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Gumbel (ours) 22.7 12.2 21.2 28.0 22.9</figDesc><table><row><cell></cell><cell>16.4 0.8 12.7 27.3 17.2 Sigmoid</cell><cell>22.0 11.4 20.9 27.9 23.0</cell></row><row><cell>Softmax</cell><cell>15.2 0.0 10.6 26.9 16.1 Softmax</cell><cell>21.5 9.7 20.7 27.6 22.4</cell></row><row><cell cols="2">Gumbel (ours) 19.0 4.9 16.8 27.6 19.1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study, using Mask-RCNN, Resnet50 and training schedule 2x. RFS EQL Gumbel Enh DropLoss AP AP r AP c AP f AP b Comparison of two object distributions in LVIS validation set, using Softmax (second column), Sigmoid (third column) and Gumbel (fourth column). Gumbel predicts distributions that have smaller KL divergence than Sigmoid or Softmax.our method achieves the best performance at rare and common categories and it consistently surpasses many other recent works. In LVISv1, GOL achieves 27.7% overall AP , surpassing RFS<ref type="bibr" target="#b10">[11]</ref> by 4.0%, LOCE by 1.1%, Seesaw Loss [35] by 1.3% and EQLv2</figDesc><table><row><cell>18.7 1.1 16.2 29.2 19.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison against SOTA on the LVIS dataset.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>Framework</cell><cell>AP AP r AP c AP f AP b</cell></row><row><cell>RFS [11]</cell><cell></cell><cell></cell><cell>25.4 16.3 25.7 28.7 25.4</cell></row><row><cell>DropLoss[14]</cell><cell></cell><cell></cell><cell>26.4 17.3 28.7 27.2 25.8</cell></row><row><cell>BAGS [21]</cell><cell></cell><cell></cell><cell>26.2 18.0 26.9 28.7 25.8</cell></row><row><cell>BALMS[30]</cell><cell></cell><cell></cell><cell>27.0 19.6 28.9 27.5 27.6</cell></row><row><cell>Forest-RCNN[38]</cell><cell cols="2">LVIS v0.5 Mask-RCNN R50-FPN</cell><cell>25.6 18.3 26.4 27.6 25.9</cell></row><row><cell>EQLv2[32]</cell><cell></cell><cell></cell><cell>27.1 18.6 27.6 29.9 27.0</cell></row><row><cell>LOCE[10]</cell><cell></cell><cell></cell><cell>28.4 22.0 29.0 30.2 28.2</cell></row><row><cell>DisAlign [40]</cell><cell></cell><cell></cell><cell>27.9 16.2 29.3 30.8 27.6</cell></row><row><cell>GOL (ours)</cell><cell></cell><cell></cell><cell>29.5 22.5 31.3 30.1 28.2</cell></row><row><cell>RFS[11]</cell><cell></cell><cell></cell><cell>23.7 13.3 23.0 29.0 24.7</cell></row><row><cell>EQLv2[32]</cell><cell></cell><cell></cell><cell>25.5 17.7 24.3 30.2 26.1</cell></row><row><cell>LOCE[10] NorCal with RFS [28]</cell><cell cols="2">LVIS v1.0 Mask-RCNN R50-FPN</cell><cell>26.6 18.5 26.2 30.7 27.4 25.2 19.3 24.2 29.0 26.1</cell></row><row><cell>Seesaw[35]</cell><cell></cell><cell></cell><cell>26.4 19.5 26.1 29.7 27.6</cell></row><row><cell>GOL (ours)</cell><cell></cell><cell></cell><cell>27.7 21.4 27.7 30.4 27.5</cell></row><row><cell>RFS [11]</cell><cell></cell><cell></cell><cell>25.7 17.5 24.6 30.6 27.0</cell></row><row><cell>EQLv2[32]</cell><cell></cell><cell></cell><cell>27.2 20.6 25.9 31.4 27.9</cell></row><row><cell>LOCE[10] NorCal with RFS [28]</cell><cell cols="2">LVIS v1.0 Mask-RCNN R101-FPN</cell><cell>28.0 19.5 27.8 32.0 29.0 27.3 20.8 26.5 31.0 28.1</cell></row><row><cell>Seesaw[35]</cell><cell></cell><cell></cell><cell>28.1 20.0 28.0 31.8 28.9</cell></row><row><cell>GOL(ours)</cell><cell></cell><cell></cell><cell>29.0 22.8 29.0 31.7 29.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Top-1 accuracy on long-tailed classification datasets.</figDesc><table><row><cell>Dataset</cell><cell>CIFAR100-LT</cell><cell cols="2">ImageNet-LT</cell><cell>Places-LT</cell></row><row><cell cols="2">Imbalance factor 50 100 200</cell><cell>256</cell><cell></cell><cell>996</cell></row><row><cell>Model</cell><cell cols="4">ResNet-32 ResNet-50 SE-ResNet-50 ResNet-152</cell></row><row><cell>Softmax</cell><cell>46.2 42.4 38.3</cell><cell>45.2</cell><cell>45.9</cell><cell>28.7</cell></row><row><cell>Gumbel</cell><cell>49.0 45.5 41.5</cell><cell>48.2</cell><cell>48.5</cell><cell>30.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study, using MaskRCNN, Resnet50 and training schedule 2x. RFS Gumbel EQL Enh DropLoss AP AP r AP c AP f AP b 18.7 1.1 16.2 29.2 19.5 ? 22.0 8.9 20.3 29.6 22.4 ? 21.6 3.8 21.7 29.2 22.5</figDesc><table><row><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>23.9 11.4 23.4 29.9 24.2</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>23.7 13.3 23.0 29.0 24.7</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>23.5 13.8 22.2 29.2 24.3</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>25.3 17.4 24.9 29.2 26.0</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>26.1 18.4 25.9 29.8 26.8</cell></row><row><cell>?</cell><cell>?</cell><cell>? ?</cell><cell></cell><cell>26.9 18.1 26.5 31.3 26.8</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>25.6 14.5 26.1 29.9 25.1</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>27.7 21.4 27.7 30.4 27.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>6 2.1 17.4 27.2 19.3 EQL Gumbel 21.7 9.6 20.6 28.2 21.8 DropLoss ? [14] Sigmoid 19.8 3.5 20.0 26.7 20.4 DropLoss Gumbel 22.0 10.0 22.1 27.1 21.9 ACSL [37] Sigmoid 20.7 9.6 19.7 26.6 21.2 ACSL Gumbel 21.0 10.9 19.8 26.7 21.1 Federated Loss [41] Sigmoid 17.6 1.8 14.9 27.5 18.2 Federated Loss Gumbel 20.1 6.0 18.5 28.0 20.5 Comparison of activations in various frameworks using 1x schedule.MethodFramework AP AP r AP c AP f AP b</figDesc><table><row><cell>Sigmoid</cell></row><row><cell>MaskRCNN-ResNet50[12]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Comparative results on LVISv1 using MaskRCNN-FPN and schedule 2x.MethodSampler Backbone AP AP r AP c AP f AP b</figDesc><table><row><cell>Softmax</cell></row><row><cell>random MaskRCNN ResNet50</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Here we omit obj for simplicity since P (y, obj, u) = P (y, u) due to that y shows there is object occurrence obj.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introducing the gev activation function for highly unbalanced data to develop covid-19 diagnostic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2776" to="2786" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring classification equilibrium in long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3417" to="3426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Droploss for long-tail instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1549" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3573" to="3587" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adjusting decision boundary for class imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="81674" to="81685" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Extreme value distributions: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nadarajah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Learning multiple layers of features from tiny images</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10991" to="11000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV). pp</title>
		<meeting>the European conference on computer vision (ECCV). pp</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=37nvvqkCo5" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Background splitting: Finding rare classes in a sea of background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Mullapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8043" to="8052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Imbalance problems in object detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3388" to="3415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On model calibration for long-tailed object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-scale object detection in the wild from imbalanced multi-labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9709" to="9718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems(NeurIPS)</title>
		<meeting>Neural Information Processing Systems(NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Equalization loss v2: A new gradient balance approach for long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1685" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1513" to="1524" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The devil is in classification: A simple framework for long-tail instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="728" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive class suppression loss for long-tail object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3103" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Forest r-cnn: Large-vocabulary long-tailed object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1570" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distribution alignment: A unified framework for long-tail visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2361" to="2370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Probabilistic two-stage detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigmoid</forename><surname>Maskrcnn-Resnet101</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigmoid</forename><surname>Maskrcnn-Resnext101</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
