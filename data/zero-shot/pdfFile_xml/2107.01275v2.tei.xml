<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-15">15 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Lohrenz</surname></persName>
							<email>t.lohrenz@tu-bs.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Braunschweig Institute for Communications Technology</orgName>
								<address>
									<addrLine>Schleinitzstr. 22</addrLine>
									<postCode>38106</postCode>
									<settlement>Braunschweig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schwarz</surname></persName>
							<email>patrick.schwarz@tu-bs.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Braunschweig Institute for Communications Technology</orgName>
								<address>
									<addrLine>Schleinitzstr. 22</addrLine>
									<postCode>38106</postCode>
									<settlement>Braunschweig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Li</surname></persName>
							<email>zhengyang.li@tu-bs.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Braunschweig Institute for Communications Technology</orgName>
								<address>
									<addrLine>Schleinitzstr. 22</addrLine>
									<postCode>38106</postCode>
									<settlement>Braunschweig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
							<email>t.fingscheidt@tu-bs.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Braunschweig Institute for Communications Technology</orgName>
								<address>
									<addrLine>Schleinitzstr. 22</addrLine>
									<postCode>38106</postCode>
									<settlement>Braunschweig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-15">15 Dec 2021</date>
						</imprint>
					</monogr>
					<note>arXiv:2107.01275v2 [eess.AS] RELAXED ATTENTION: A SIMPLE METHOD TO BOOST PERFORMANCE OF END-TO-END AUTOMATIC SPEECH RECOGNITION</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, attention-based encoder-decoder (AED) models have shown high performance for end-to-end automatic speech recognition (ASR) across several tasks. Addressing overconfidence in such models, in this paper we introduce the concept of relaxed attention, which is a simple gradual injection of a uniform distribution to the encoder-decoder attention weights during training that is easily implemented with two lines of code 1 . We investigate the effect of relaxed attention across different AED model architectures and two prominent ASR tasks, Wall Street Journal (WSJ) and Librispeech. We found that transformers trained with relaxed attention outperform the standard baseline models consistently during decoding with external language models. On WSJ, we set a new benchmark for transformer-based end-to-end speech recognition with a word error rate of 3.65%, outperforming state of the art (4.20%) by 13.1% relative, while introducing only a single hyperparameter.</p><p>Index Terms-End-to-end speech recognition, encoderdecoder models, relaxed attention, speech transformer 1 Code contributed to http://github.com/freewym/espresso</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>End-to-end automatic speech recognition (ASR) gained a lot of interest in the research community as it makes phonetic modeling obsolete and significantly simplifies the processing pipeline while achieving superior performance compared to hidden Markov model (HMM)-based (hybrid) approaches in many prominent ASR benchmark tasks, especially those that comprise large amounts of data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Common endto-end ASR approaches that directly translate acoustic input sequences into graphemic output sequences are based on connectionist temporal classification (CTC) <ref type="bibr" target="#b2">[3]</ref>, recurrent neural network transducers (RNN-T) <ref type="bibr" target="#b3">[4]</ref>, or attentionbased encoder-decoder (AED) models <ref type="bibr" target="#b4">[5]</ref>. The latter approach emerged from neural machine translation and was soon adopted for ASR <ref type="bibr" target="#b5">[6]</ref>. In contrast to early encoderdecoder models that used a fixed-length intermediate repre- sentation <ref type="bibr" target="#b6">[7]</ref>, the attention mechanism uses variable-length attention weight vectors to draw attention to relevant parts in the input sequence, yielding significant improvements for long sentences. Prominent AED model architectures are the RNN-based listen-attend-and-spell (LAS) <ref type="bibr" target="#b7">[8]</ref>, the allattention-based transformer model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and, as a variant of the latter, the conformer model <ref type="bibr" target="#b10">[11]</ref>.</p><p>The problem of overconfidence in AED models is demonstrated in <ref type="bibr" target="#b11">[12]</ref>, where utterances with high confidence scores of an LAS model contributed to high word error rates (WER). One reason for such behavior is the use of cross entropy between the predicted output token and the ground truth label as a loss function, as it promotes sparse softmax distribu-  </p><formula xml:id="formula_0">B ? 1 B ? 1 ? E B ? 1 ? dd B ? 1 ? dd B ? 1 ? dd B ? 1 ? (dd + de) B ? 1 ? D P ? output token probs Bahdanau Attention (2), (3) z ??1 previous context vector V B ? 1 ? de z ? Q ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAS Encoder LAS Decoder</head><p>(b) Listen-attend-and-spell (LAS) model. tions <ref type="bibr" target="#b12">[13]</ref>. This leads to two problems: First, beam search decoding (especially with language models) is less effective as alternatives to a given output token are harder to explore. Second, it is unfavorable for gradient learning as the derivative of the loss function approaches zero when a correct prediction with high confidence is made by the model <ref type="bibr" target="#b13">[14]</ref>. Methods to deal with sharp state probability distributions in (hybrid) ASR (often necessary for stream fusion) are stream weighting <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, limiting <ref type="bibr" target="#b17">[18]</ref>, and the use of temperature in the softmax function <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref>. One effective method to deal with overconfidence in end-to-end ASR, introduced in <ref type="bibr" target="#b19">[20]</ref>, is label smoothing that blends the one-hot label with a uniform distribution or assigns part of the probability mass to tokens that are neighbors of the labeled token in the target sequence <ref type="bibr" target="#b13">[14]</ref>. Interestingly, label smoothing is also effective against overfitting <ref type="bibr" target="#b20">[21]</ref> and thus is commonly used for AED end-to-end ASR besides related regularization methods such as spectral augmentation <ref type="bibr" target="#b21">[22]</ref>, dropout <ref type="bibr" target="#b22">[23]</ref>, multi-task learning with an additional CTC loss <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, and the recently proposed multi-encoder-learning that uses additional encoders only during training <ref type="bibr">[?]</ref>. Regularization methods applied to the crucial encoder-decoder attention mechanism in AED models were only recently discovered in <ref type="bibr" target="#b25">[26]</ref>, where CTC predictions in a multi-task learning setup are used to focus the attention in transformer models <ref type="bibr" target="#b25">[26]</ref> to relevant frames in the encoded input sequence.</p><p>In this paper we introduce relaxed attention, a simple adjustment to the encoder-decoder attention weights during training to reduce overconfidence in AED-based end-to-end speech recognition without adding learnable parameters to the standard model architecture. Different to label smooth-ing, relaxed attention injects a uniform distribution to the probabilistic attention weights (here: not the labels!) to prevent the attention from being overly focused on the encoder input frames. Relaxed attention can easily be implemented in end-to-end ASR toolkits with two lines of code. We investigate the effect of relaxed attention across several attentionbased encoder-decoder models, namely the LAS and the transformer model, and across two different tasks (i.e., Wall Street Journal and Librispeech). We also investigate the influence of relaxed attention on the overconfidence problem by analyzing the AED models with and without integration of external RNN-based language models. The paper is structured as follows: Section 2 revises AED models and the attention mechanism to introduce our relaxed attention approach. Section 3 provides details of our conducted experiments, whose results are presented and discussed in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELAXED ATTENTION 2.1. Attention-Based Encoder-Decoder Models</head><p>Attention-based encoder-decoder (AED) approaches to endto-end automatic speech recognition (e.g., the herein used transformer and LAS architectures, shown in Figures 2 (a) and 2 (b), respectively) comprise an encoder and a decoder network to transform an input feature vector sequence xT 1 of dimension F and lengthT to a sequence of output tokens c L 1 with c ? ? C = {c <ref type="bibr" target="#b0">(1)</ref> , c <ref type="bibr" target="#b1">(2)</ref> , . . . , c (D) } being a single output token (i.e., grapheme-based characters or subword units <ref type="bibr" target="#b26">[27]</ref>) at output sequence index ? ? {1, . . . , L} from a vocabulary of size D. First, the original feature sequence xT 1 is commonly preprocessed by several convolutional neural network (CNN) layers to a subsampled representation that is indexed by t ? {1, . . . , T }, with T &lt;T . While first approaches towards streaming encoder-decoder models exist <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">29]</ref>, in this work the encoder network computes a hidden representation h T 1 = ENC(xT 1 ) for all T frames that must be available at the start of decoding. For each decoding step (starting at ? = 1), the decoder of the respective model uses the encoded input sequence h T 1 and the previous output token c ??1 to output a vector P ? = DEC(h T 1 , c ??1 ) comprising probabilities of all output tokens c ? . These probabilities are then subject to a greedy or beam search algorithm which step-by-step invokes the decoder until some end-of-sentence (EOS) threshold is reached and the final hypothesis is emitted.</p><p>To gather information, which timesteps t in the encoded input sequence are relevant for decoding of the output sequence at step ?, AED models use the attention mechanism that internally computes attention weights containing probabilistic information about relevant input times t.</p><p>In the following, we will revise attention types for the two most common AED models that we used in our work. As our proposed relaxed attention is applied only during training, the notations in the following sections hold for the training scenario, where the transformer model is able to train all L output timesteps during decoding in parallel, while the LAS model decodes the output sequence step-by-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Scaled Dot Product Attention (Transformer)</head><p>The standard scaled dot product multi-head attention (MHA), introduced in <ref type="bibr" target="#b8">[9]</ref>, is the common attention type in several layers of transformer AED models (i.e., each encoder and decoder block cf. <ref type="figure" target="#fig_2">Figure 2</ref> (a)) to model temporal dependencies without using recurrent layers. While it is implemented as self-attention employed in the encoder blocks, here we focus on the encoder-decoder attention in the decoder blocks (cf. <ref type="figure">Figure 3</ref>) which draws the decoders' attention to relevant parts in the encoded input sequence h T 1 ? R T ?d . The standard MHA (yellow block) employs multiple attention heads </p><formula xml:id="formula_1">Z i (Q, K, V) = softmax ? ? ? QW (Q) i KW (K) i T ? d ? ? ? attention weights =Gi(Q,K) ? VW (V) i value projections =Yi(V) ? R L? d N h (1) with W (Q) i , W (K) i , W (V) i ? R d? d N</formula><formula xml:id="formula_2">+ B ?1?d Q ? K ? V ? B ?1?d Q B ?1?d K V from encoder B ? T ? d Z B ?1?d B ?1?d B ?1?4d B ? 1 ? d</formula><p>Decoder Block <ref type="figure">Fig. 3</ref>: Single decoder block as used in the decoder of the transformer model (cf. <ref type="figure" target="#fig_2">Fig. 2 (a)</ref>) during inference, yielding a singleton dimension (...?1?...) in the decoder block tensors, as the decoder is invoked step by step. Dropout layers <ref type="bibr" target="#b22">[23]</ref> are in dashed-line boxes. Details of the multi-head attention block (yellow) are shown in <ref type="figure" target="#fig_0">Fig. 1</ref> for the training case.</p><p>to the relevance of a time frame t to the decoding at step ?. The outputs Z i of all N h separate attention heads are concatenated and subject to a fully connected output layer, yielding the MHA output Z ? R L?d . Note that for brevity of notation the attention dropout commonly applied to the attention weights in transformer models is not shown in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Bahdanau Attention (LAS)</head><p>Additive attention, also known as Bahdanau attention, was proposed in <ref type="bibr" target="#b4">[5]</ref> and is the common attention type for the LAS model, shown here in <ref type="figure" target="#fig_2">Figure 2</ref> (b). One of the main differences of the LAS model compared to the more recent transformer model is the employment of recurrent neural networks (RNNs) in each of the encoder and decoder blocks. Details on the RNN encoder and decoder blocks will be given in Section 3.3.1. Note that for the purpose of clarity, in the following we reuse some notations of the transformer model (e.g., weight matrices), even though these entities depend on their corresponding architecture. Unlike the previously described scaled dot product attention used in transformer models, the LAS model does not incorporate multiple attention heads, but computes a single vectorial context</p><formula xml:id="formula_3">z ? = T t=1 g t,? h t ? R 1?de<label>(2)</label></formula><p>for each timestep ?, with g t,? being an element of the vectorial attention weights g ? ? I 1?T , and h t ? R 1?de being a single vector of the encoded input h T 1 = V ? R T ?de at time index t. The attention weights</p><formula xml:id="formula_4">g ? softmax v?tanh 1?diag Q ? W (Q) +b +VW (V ) T (3) are computed utilizing the learnable weights W (Q) ? R d d ?da , W (V) ? R de?da , v, b ? R 1?da</formula><p>, with 1 being a T ?d a matrix with all-ones, diag(r) of 1 ? d a vector r being its d a ? d a diagonal matrix, and ( ) T being the transpose. The query input vector Q ? ? R 1?d d stems from the first RNN decoder block of the LAS decoder, and d e , d a , d d are dimensions of the encoder, attention, and decoder tensors, respectively (cf. <ref type="figure" target="#fig_2">Fig. 2 (b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Novel Relaxed Attention</head><p>According to <ref type="bibr" target="#b0">(1)</ref> and <ref type="formula">(3)</ref>, the attention weights for both previously described attention types (i.e., G i (Q, K) ? I L?T for the scaled dot product MHA, and g ? (Q ? , V) ? I 1?T for the Bahdanau attention) are of probabilistic nature after the softmax activation function. To prevent overly sharp attention distributions applied in training to the encoded input sequence, our novel relaxed attention weights for the transformer model are defined as simple as</p><formula xml:id="formula_5">G i (Q, K) = (1 ? ?)G i (Q, K) + ? 1 T , i ? N h ,<label>(4)</label></formula><p>gradually injecting a uniform distribution (with 1 here being an L?T matrix of ones) into the standard attention weights, controlled by a relaxation coefficient ? ? [0, 1], as shown here in <ref type="figure" target="#fig_0">Figure 1</ref>. For the LAS model with relaxed Bahdanau attention, which we also use for our experiments, the relaxed attention weights for training are defined analogously as</p><formula xml:id="formula_6">g ? = (1 ? ?)g ? + ? ? 1 T ,<label>(5)</label></formula><p>with 1 here being a length T row vector of ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP 3.1. Databases</head><p>We investigate our relaxed attention method on two prominent ASR tasks. First is the 81-hour Wall Street Journal (WSJ) dataset <ref type="bibr" target="#b30">[30]</ref> using the dev93 and eval92 splits for evaluation. Second is the 980-hour LibriSpeech dataset <ref type="bibr" target="#b31">[31]</ref> with the clean and other conditions of the dev and test datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Acoustic Frontend</head><p>For all experiments the encoder receives a sequence xT 1 of T feature vectors, each of dimension F = 83, composed of standard 80-dimensional filterbank features, extended with 3dimensional pitch features, both extracted with the Kaldi toolkit <ref type="bibr" target="#b32">[32]</ref>. The convolutional neural networks (CNNs) at the input layer, shown as CNN block in <ref type="figure" target="#fig_2">Figure 2</ref>, consist of a total of four convolutional layers, each using 3 ? 3 filter kernels. As the second and forth convolutional layer use a stride of 2 in both temporal and frequency direction, the input sequence length is compressed to T =T /4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Configurations</head><p>In the following, we will describe all used model architectures and training configurations. All models were trained using the PyTorch-based Espresso and fairseq toolkits <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref>. For the approaches dubbed Baseline, the model architectures are configured exactly according to the recipes available within the Espresso toolkit. For our new Relaxed Attention approach, we extended the respective baseline models with our simple modification according to <ref type="bibr" target="#b3">(4)</ref> or <ref type="bibr" target="#b4">(5)</ref>. All models were trained using the Adam optimizer with crossentropy loss and temporal label smoothing of 0.1 for Librispeech, and 0.05 for WSJ. We follow <ref type="bibr" target="#b9">[10]</ref> for tri-stage learning rate scheduling with a maximum learning rate of 0.001. For Librispeech experiments, we also used spectral augmentation <ref type="bibr" target="#b21">[22]</ref>. No speed perturbation or multi-task learning (as common in <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>) was employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Listen-Attend-and-Spell (LAS)</head><p>The encoder of the LAS model incorporates three RNN encoder blocks (cf. 2 (b)) each consisting of a dropout layer followed by a single bidirectional long-short term memory (LSTM) layer of size d e = 640. The Bahdanau attention uses an internal attention dimension of d a = 320. Each of the three RNN decoder blocks first concatenates both inputs before applying a single unidirectional LSTM layer with output size d d = 320. All RNN decoder blocks also employ residual connections and dropout layers before the LSTM layers. The LAS model was trained for 35 epochs and employed a dropout of 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Transformer</head><p>The transformer used in our work follows the standard architecture as introduced in <ref type="bibr" target="#b8">[9]</ref> and is shown in <ref type="figure" target="#fig_2">Figures 2 (a)</ref>, 3, and 1. The encoder uses absolute position embedding on the input that has been preprocessed by the acoustic frontend (cf. Sec. 3.2) and incorporates 12 encoder blocks, each consisting of multi-head self-attention (MHSA) and pointwise fully connected layers, while the transformer decoder stacks 6 decoder blocks (cf. <ref type="figure">Figure 3)</ref>  Librispeech we use a large transformer setting with d = 512, N h = 8, and dropout of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Conformer</head><p>To further extend our investigations to a larger variety of model architectures, we also employed the recent conformer model <ref type="bibr" target="#b10">[11]</ref> for experiments on Librispeech. While using the exact same decoder as the transformer (cf. <ref type="figure" target="#fig_2">Fig. 2 (a)</ref> and Section 3.3.2), the conformer model adds a convolutional module after the MHSA in each encoder block, as well as an additional fully connected module before. For the sake of comparability, our implementation uses absolute positional encoding but otherwise follows <ref type="bibr" target="#b37">[37]</ref> with a total of 12 encoder blocks and a convolution kernel size of 31.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">Tokenization and Language Model</head><p>For WSJ experiments, we trained all acoustic models to output tokens on character level, with a total amount of D = 52 tokens (including special end-of-sentence and blank symbols). We apply a word-based language model (LM) that is able to output character-level tokens by using the lookahead method from <ref type="bibr" target="#b38">[38]</ref>. The LM is composed of three LSTM layers, each comprising 1200 neurons totaling in an amount of 112M parameters. For Librispeech we used a total amount of D = 5000 subword output tokens generated with SentencePiece <ref type="bibr" target="#b26">[27]</ref>, and employ a token-level LM with  <ref type="table">Table 2</ref>: Results on WSJ with and without language model (LM) using transformer models without (baseline) or with relaxation (coefficient ? = 0.35). Training of each model was repeated 5 times and averaged.</p><p>four LSTM layers and 800 neurons each. During decoding, we use shallow fusion <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40]</ref> for LM integration according to log P final ? = log P ? + ? log P LM ? with ? being the language model weight that we keep fixed to the values from the recipes in Espresso (? = 0.9 for WSJ, ? = 0.4 for Librispeech).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSION 4.1. Wall Street Journal (WSJ)</head><p>Experimental results on the Wall Street Journal task are shown in <ref type="table" target="#tab_2">Table 1</ref>. To achieve statistically profound results, model trainings of both Baseline and Relaxed Attention approaches were repeated 5 times (with different seeds for weight initialization) and results were averaged. First, we observe that the WERs for both AED model types (LAS and transformer) are consistently lower with relaxed attention for a wide range of relaxation coefficients. For the LAS model, a WER reduction of 0.28% absolute (from 5.7% to 5.42%) on dev93 corresponds to a relative WER reduction of 7.4% on eval92, when using relaxed attention with ? = 0.1. For the transformer model, the best average result on dev93 is achieved with ? = 0.35, yielding an average WER of 5.80% on dev93 and 3.65% on eval92, which is an 18% relative improvement on eval92 compared to our own Baseline model (4.45%), and exceeds the current WSJ transformer state of the art by Moriya et al. <ref type="bibr" target="#b24">[25]</ref> (4.20%) by 13.1% relative, without adding any model complexity.</p><p>Interestingly, we note that the WERs on the eval92 set are consistently decreasing with increasing relaxation (until ? = 0.35) and the single-best result for the transformer model (before averaging, not shown in <ref type="table" target="#tab_2">Table 1</ref>) even reaches 5.65% on dev93, with a benchmark WER of 3.19% on eval92.</p><p>In a small ablation study shown in <ref type="table">Table 2</ref>, we investigate the behavior of both Baseline and Relaxed Attention approaches with and without language model (LM). <ref type="bibr" target="#b1">2</ref> For the optimal transformer relaxation coefficient ? = 0.35 that has been found before under use of an LM, relaxed attention training performs suboptimal w/o LM, while with LM we obtain the benchmark results from   LM on WSJ, it helps decreasing overconfidence and makes the model perfectly suitable for language model integration.</p><p>We additionally performed a further analysis of entropy in the transformer MHA weights w/o LM. During training, by application of (4), the relaxed attention weightsG i have higher entropy compared to G i , as expected. During inference on dev93, the attention weights of the best Relaxed Attention model (? = 0.35) yield a 4% higher entropy as compared to the Baseline model, thereby confidence is decreased even without relaxation (4) in inference, giving important degrees of freedom to the LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Librispeech</head><p>We choose Librispeech to validate our relaxed attention approach on a large dataset and also evaluate performance on increasing training set sizes in <ref type="table" target="#tab_5">Table 3</ref>. We report on transformer-based models as they yield superior performance compared to LAS models on Librispeech (e.g., in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">41]</ref>). We also use a re-simulated conformer model, which holds the benchmark WERs of 1.9%/3.9% in <ref type="bibr" target="#b10">[11]</ref> on the clean / other portions of the test set. All re-simulated models are compared without and with relaxed attention.</p><p>In our experiments we note that even without LM, relaxed attention helps in some cases on the larger training sets for both model types, while showing similar behavior as on WSJ on the similar-sized 100 h training set. With LM on the dev set, in 7 out of 8 cases relaxed attention leads to improvements over all training set sizes and models, with particularly consistent improvements in the other condition. On the test set with LM, our relaxed attention for transformer models exceeds Baseline performance in all conditions and all training set sizes, while for the conformer model only an improvement in the other condition is achieved (? hasn't been optimized for the conformer). Using a standard transformer model trained with the entire 960 h training set, relaxed attention achieves a relative improvement of 4.9% averaged across both test set conditions with LM (5.76% vs. 5.48% absolute WER).</p><p>In <ref type="table" target="#tab_6">Table 4</ref>, we learned the relaxation coefficient ? during training and observe that performance of the learned Relaxed Attention is close to-but yet still lower-than the Baseline approach. This is expected, as relaxed attention (similar to other generalization techniques, e.g., dropout) harms the training loss and thus the learned ? values in each decoder block converged towards small values in a range of [0, 0.03] during training in our experiments. We conclude, however, that ? should not be learned but manually set to put stress on the network to still learn under relaxed attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work we introduced relaxed attention for end-to-end ASR, a simple method that smoothes attention weights in attention-based encoder-decoder models during training to decrease overconfidence of these models. Across a variety of encoder-decoder models, we observe performance gains when our method is used in combination with external language models. Particularly on the WSJ task, transformer models trained with relaxed attention reduce the average word error rate by 13.1% relative compared to state of the art, setting a new benchmark of 3.65% WER on WSJ for transformer-based automatic speech recognition without adding any model complexity in inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Encoder-decoder multi-head attention as used in decoder blocks of transformer models (cf.Fig. 3) with relaxed attention (red block) during training; N h = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Standard end-to-end model architectures used for relaxed attention experiments in this work during inference. Transformer decoder block details are shown in Fig. 3. For faster inference, a batch of B hypotheses are processed in parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>h being linear projection weight matrices for the query Q, key K, and value V inputs, i ? N h = {1 . . . N h } being the index of the in total N h attention heads, and d is the feature vector size being used in most layers of the transformer model. For encoder-decoder attention, key and value inputs stem from the encoder's last layer, yielding K V h T 1 . The entries in each of the L rows of the attention weight matrix G i (Q, K) ? I L?T , with I = [0, 1], sum up to one and are treated as probabilities that correspond</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>We measure system performance in terms of word error rate WER = 1 ? N ?D?I?S N , as well as w.r.t. character error rate (CER) for some experiments, where the number of units N , deletions D, insertions I and substitutions S are calculated on character-level instead of on word-level as for the WER. All raw speech signals are sampled at 16 kHz and analyzed with a 25 ms window and a frame shift of 10 ms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. For the WSJ experiments, as well as on the 100 h training subset of Librispeech, we employ a smaller model size with d = 256, N h = 4 attention heads, and dropout of 0.1, while for the larger (460 and 960 h) datasets of</figDesc><table><row><cell>AED</cell><cell></cell><cell># of</cell><cell></cell><cell>dev93</cell><cell>eval92</cell></row><row><cell>model type</cell><cell>Approach</cell><cell>AM par's</cell><cell>?</cell><cell cols="2">WER CER WER CER</cell></row><row><cell></cell><cell cols="3">Baseline 17.8M 0</cell><cell cols="2">5.70 2.97 4.18 2.11</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.05 5.53 2.80 4.02 2.06</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.10 5.42 2.75 3.87 1.88</cell></row><row><cell>LAS</cell><cell cols="2">Relaxed Attention 17.8M</cell><cell cols="3">0.15 5.43 2.72 3.77 1.90 0.20 5.59 2.87 3.92 1.96 0.25 5.99 3.14 4.02 2.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.30 6.00 3.02 3.88 1.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.35 7.10 3.81 4.73 2.55</cell></row><row><cell></cell><cell cols="2">Moriya et al. [25]</cell><cell></cell><cell>6.90</cell><cell>4.20</cell></row><row><cell></cell><cell cols="3">Baseline 16.8M 0</cell><cell cols="2">6.69 3.91 4.45 2.46</cell></row><row><cell>Transformer</cell><cell cols="2">Relaxed Attention 16.8M</cell><cell cols="3">0.05 6.41 3.70 4.28 2.34 0.10 6.54 3.79 4.10 2.26 0.15 6.09 3.54 3.96 2.16 0.20 6.14 3.45 3.91 2.16 0.25 6.02 3.32 3.83 2.11</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.30 6.32 3.58 3.65 1.91</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.35 5.80 3.24 3.65 1.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.40 5.83 3.19 3.74 1.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on WSJ using LAS and transformer models with various relaxation coefficients ? with language model. The number of acoustic model (AM) parameters is shown. Training of each model was repeated 5 times and averaged. Best results for each model type are in bold font.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Baseline 14.92 5.32 11.89 3.95 6.69 3.91 4.45 2.46</figDesc><table><row><cell></cell><cell></cell><cell>dev93</cell><cell>eval92</cell></row><row><cell>Approach</cell><cell>LM</cell><cell cols="2">WER CER WER CER</cell></row><row><cell></cell><cell></cell><cell cols="2">16.14 5.48 12.73 4.04</cell></row><row><cell>Relaxed Attention</cell><cell></cell><cell cols="2">5.80 3.24 3.65 1.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Baseline 19.31M 13.51 28.23 14.71 29.56 11.18 24.51 12.48 26.65 Relaxed Attention 19.31M 14.21 28.81 15.50 30.17 9.83 21.99 10.66 23.48</figDesc><table><row><cell>This indicates that even</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>WER results on Librispeech using transformer and conformer models; relaxation coefficient ? = 0.25 for the 100 h training set, and ? = 0.2 for all others. Best results for each training set size and model type are in bold font.</figDesc><table><row><cell></cell><cell></cell><cell>dev</cell><cell>test</cell></row><row><cell>Approach</cell><cell>?</cell><cell cols="2">clean other clean other</cell></row><row><cell>Baseline</cell><cell>0</cell><cell cols="2">11.18 24.51 12.48 26.65</cell></row><row><cell cols="4">Relaxed Attention learned 10.76 24.29 11.46 26.52</cell></row><row><cell cols="2">Relaxed Attention 0.25</cell><cell cols="2">9.83 21.99 10.66 23.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>WER results of learned relaxation on Librispeech (100 h training set) using transformer model with LM.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In supplementary experiments we smoothed attention weights with a tempered softmax and obtained results inferior to Baseline performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The research leading to these results has received funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) for project number 414091002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">State-of-the-Art Speech Recognition with Sequence-to-Sequence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Comparative Study on Transformer vs RNN in Speech Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards End-to-End Speech Recognition With Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech Recognition With Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS<address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SSST</title>
		<meeting>of SSST<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-Augmented Transformer for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTER-SPEECH</title>
		<meeting>of INTER-SPEECH<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Confidence Estimation for Attention-Based Sequence-to-Sequence Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04" />
			<biblScope unit="page" from="6388" to="6392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing Neural Networks by Penalizing Confident Output Distributions</title>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards Better Decoding and Language Model Integration in Sequence to Sequence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
		<meeting>of INTERSPEECH<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="523" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Turbo Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Receveur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="846" to="862" />
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparing Fusion Models for DNN-Based Audiovisual Continuous Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdelaziz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Dynamic Stream Weights for Coupled-HMM-Based Audio-Visual Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="863" to="876" />
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Turbo Fusion of Magnitude and Phase Information for DNN-Based Phoneme Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lohrenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distilling Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS -Workshops</title>
		<meeting>of NIPS -Workshops<address><addrLine>Montr?al, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">When Does Label Smoothing Help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02629</idno>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
		<meeting>of INTERSPEECH<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint CTC-Attention Based End-to-End Speech Recognition Using Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-Distillation for Improving CTC-Transformer-Based ASR Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ashihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Masumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTER-SPEECH</title>
		<meeting>of INTER-SPEECH<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="546" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focus on the Present: A Regularization Method for the ASR Source-Target Attention Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>?elasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="5994" to="5998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<title level="m">SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing</title>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention Based On-Device Streaming Speech Recognition with Large Speech Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="956" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Streaming Automatic Speech Recognition with the Transformer Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="6074" to="6078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Design for the Wall Street Journal-Based CSR Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 5th DARPA Speech and Natural Language Workshop</title>
		<meeting>of 5th DARPA Speech and Natural Language Workshop<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-02" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR Corpus Based on Public Domain Audio Books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>South Brisbane, QLD, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motl?cek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solovsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesel?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">fairseq: A Fast, Extensible Toolkit for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT 2019: Demonstrations</title>
		<meeting>of NAACL-HLT 2019: Demonstrations<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Espresso: A Fast End-to-End Neural Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving Transformer-Based End-to-End Speech Recognition With Connectionist Temporal Classification and Language Model Integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
		<meeting>of INTERSPEECH<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="1408" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recent Developments on Espnet Toolkit Boosted By Conformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="5874" to="5878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-End Speech Recognition With Word-Based RNN Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
		<meeting>of SLT<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12" />
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On Using Monolingual Corpora in Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
		<meeting>of SLT<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12" />
			<biblScope unit="page" from="369" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Comparison of Transformer and LSTM Encoder Decoder Models for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
