<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FBI-Pose: Towards Bridging the Gap between 2D Images and 3D Human Poses using Forward-or-Backward Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Cloudream Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong(Shenzhen)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Cloudream Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Cloudream Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Cloudream Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FBI-Pose: Towards Bridging the Gap between 2D Images and 3D Human Poses using Forward-or-Backward Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although significant advances have been made in the area of human poses estimation from images using deep Convolutional Neural Network (ConvNet), it remains a big challenge to perform 3D pose inference in-the-wild. This is due to the difficulty to obtain 3D pose groundtruth for outdoor environments. In this paper, we propose a novel framework to tackle this problem by exploiting the information of each bone indicating if it is forward or backward with respect to the view of the camera(we term it Forwardor-Backward Information abbreviated as FBI). Our method firstly trains a ConvNet with two branches which maps an image of a human to both the 2D joint locations and the FBI of bones. These information is further fed into a deep regression network to predict the 3D positions of joints. To support the training, we also develop an annotation user interface and labeled such FBI for around 12K in-the-wild images which are randomly selected from MPII (a public dataset of 2D pose annotation). Our experimental results on the standard benchmarks demonstrate that our approach outperforms state-of-the-art methods both qualitatively and quantitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reasoning 3D human pose from a single RGB image in the wild has drawn great attentions in the past three decades due to its broad application scenarios, such as autonomous driving, virtual reality, human-computer interaction and video surveillance. Recently, leveraging on largescale well-annotated datasets such as MPII <ref type="bibr" target="#b0">[1]</ref>, significant progresses have been made in 2D human pose estimation using Convolutional Neural Networks(ConvNets) <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b6">7]</ref>. Although ConvNet models can be directly used for fitting the function f that maps an image I of a human to the 3D positions of the human skeleton joints y as proposed in <ref type="bibr" target="#b11">[11]</ref>, challenges remain for two reasons: 1) <ref type="figure">Figure 1</ref>. Input an image of a human, our approach firstly locates its 2D joints and predicts the Forward-or-Backward Information (FBI) for each bone. And then, the 3D pose is estimated by considering both of the two terms.</p><p>preparing such in-the-wild images dataset is extremely difficult whether be it by capturing using 3D sensors or annotating manually; 2) f is hyper nonlinear and is hard to approximate.</p><p>In order to tackle this problem, state-of-the-art methods, such as <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b9">9]</ref>, usually separate the mapping y = f (I) into two functions and try to fit them using ConvNets individually. The two functions are the mapping from the image to 2d joint locations x = g(I) and the mapping from 2D joint locations to 3D joint positions y = h(x). They are connected by a compound operation and we have y = h(g(I)). This strategy greatly reduces the difficulty in obtaining annotated ground-truth data mainly in two aspects: Firstly, due to the ease of annotating 2D joints, it is possible to have a large amount of labeled images (e.g. MPII <ref type="bibr" target="#b0">[1]</ref>) for training a deep network to approximate the function g. Secondly, due to the flexibility of projecting a 3D pose using arbitrary viewpoints, it is also possible to have a large amount of synthetically generated 2D-to-3D pose pairs (as shown in <ref type="bibr" target="#b9">[9]</ref>) for training a deep network to approximate the function h. Moreover, this two-step frame-work also alleviates the difficulty of fitting f by decomposing it into two easier tasks of fitting g and h independently.</p><p>However, such a two-step framework is fundamentally flawed by oversimplifying the 3D pose estimation problem. Firstly, due to the weak supervision of 2D annotated poses, this approach causes loss of 3D-aware features during learning procedure. On the other hand, recovering 3D poses from 2D joint locations only is an ill-posed problem. Ambiguity exists since different yet valid 3D poses can explain the same observed 2D joints. In other words, although decomposing the learning procedure into two independent phases makes data annotation and function learning tractable, it produces a gap between the 2D human image and its corresponding 3D pose.</p><p>In this paper, we attempt to bridge this gap by involving Forward-or-Backward Information (FBI) of each bone associated with pair-wise connecting joints. Taking a forearm as example, the information whether this bone is forward or backward shows which end-joint on it (say, the elbow or the wrist) is closer to the view of camera. As validated in <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b32">32]</ref>, the 3D joint positions can be uniquely determined by their 2D locations plus such FBI if taking the length ratios of bones as prior knowledge. In detail, our method decomposes the mapping y = f (I) into three subfunctions: the mapping from the input image to 2D joint locations g(I), the mapping from the input image to FBI g (I), and the mapping from these two terms to 3D coordinates of the joints y = h(g(I), g (I)). In our work, all these sub-functions are approximated using deep neural networks. To support the training, we randomly selected 12K in-the-wild images from MPII <ref type="bibr" target="#b0">[1]</ref> and annotated the FBI for them using a well-designed user interface. Compared with previous works, our approach shows advantages in three manifolds: At first, using FBI as a kind of 3D-aware supervision to fit g digs out more effective information for predicting 3D human poses. Secondly, taking both 2D joint locations and FBI as input greatly reduces the ambiguity of 3D joints lifting. Thirdly, to label the FBI of an image, the annotators only need to do a binary selection for each bone. This makes it possible to build an infinitely large training dataset. <ref type="figure">Fig 1 shows</ref> an example of our 3D pose estimation.</p><p>In summary, our major contributions in this paper are:</p><p>? the first work to exploit Forward-or-Backward Information (FBI) of bones for 3D human pose estimation, with which our method outperforms all previous works.</p><p>? we design a novel deep learning architecture that consists of two ConvNets to map an image of a human to the 2D joint locations and the FBI of all bones individually and a deep regression network to predict 3D joint positions using both 2D joint locations and FBI.</p><p>? we have labeled the FBI for 12K in-the-wild images with a well-designed user interface. They will be released to public to benefit other researchers working in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>To capture human pose from a single image is a longstanding research topic in computer vision, readers can refer to <ref type="bibr" target="#b31">[31]</ref> and <ref type="bibr" target="#b22">[22]</ref> for literature reviews of 2D and 3D human pose estimation respectively. In this paper, we only give a review of the algorithms using deep nets for 3D human pose prediction.</p><p>Inferring 3D Pose by Encoder-Decoder With a dataset of human images and their well-labeled 3D skeleton joints, Li and Chan <ref type="bibr" target="#b11">[11]</ref> trained a deep ConvNets for image encoding and followed by two deep regression networks for 3D pose prediction and body part detection simultaneously. The algorithm in <ref type="bibr" target="#b24">[24]</ref> firstly trained an auto-encoder to represent the 3D joints into a high-dimensional space and then learned a ConvNets to map the input image to this space for 3D pose generation. To decode the 3D coordinates of the joints, instead of using a regression method, Pavlakos et al. <ref type="bibr" target="#b20">[20]</ref> represented 3D joints in a volume and utilized a set of 3D deconvolutional layers for pose prediction in a coarse-to-fine way. Because of the challenges to obtain 3D groundtruth poses, these methods are difficult to be applied on in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inferring 3D Pose by 2D Joints Estimation</head><p>To avoid collecting 2D-3D paired data in the wild, a large portion of recent works (such as <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b18">18]</ref>) decomposed the task of 3D pose inference into two independent stages: generating 2D poses firstly and then lifting them into 3D space. For example, Martinez et al. <ref type="bibr" target="#b13">[13]</ref> proposed to directly regress the 3D coordinates of the joints from their 2D locations with a sequence of fully connected layers. The method is very simple yet achieves state-of-theart performance. Fang et al. <ref type="bibr" target="#b9">[9]</ref> conducted 3D pose lifting from 2D joints by considering the prior knowledges of the relationships in-between skeleton joints. By exploiting geometric constraints, Zhou et al. <ref type="bibr" target="#b33">[33]</ref> proposed a weaklysupervised approach making these two stages possible to be trained together using in-the-wild images which have no 3D pose groundtruth. Most recently, adversarial learning framework was adopted in <ref type="bibr" target="#b30">[30]</ref> to ensure the anthropometrical validity of the output pose and further improved the performance. Our approach follows such two-stage framework but differs from it in two folds: 1) our first stage not only outputs the 2D joints but also the FBI; 2) correspondingly, the second stage takes 2D joints as well the FBI as input for 3D pose prediction.</p><p>Building Training Dataset There exist some works attempting to obtain 3D groundtruth poses for images to build training dataset. H3.6M <ref type="bibr" target="#b10">[10]</ref> is one of such works in which the 3D pose of a human is captured using a Mocap system within an indoor environment. As a result, the learning-based methods trained with this dataset are difficult to be generalized to images in-the-wild. The work of <ref type="bibr" target="#b14">[14]</ref>, therefore, captured actors in a green screen studio and synthesized new images by composting the segmented foregrounds with arbitrary backgrounds. Both <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b27">[27]</ref> took use of graphics methods to generate synthetic 3D human bodies and created images by overlaying them on a real background photo. Rogez et al. <ref type="bibr" target="#b21">[21]</ref> proposed an image-based synthesis approach that firstly searched appropriate images according to a known 3D pose and then stitched them together. Although unlimited images can be synthesized using these methods, they still have very different appearances as real photos. This causes the challenges of domain adaptation to remain. Another kind of solutions for data collection is designing interactive annotation tools. The work called Poselets from Bourdev and Malik <ref type="bibr" target="#b2">[3]</ref> belongs to this category. However, the method shows extremely heavy user interventions and restricts the ability to construct a large dataset. In this work, we propose to annotate the FBI of bones instead of the 3D positions of joints. This change helps to reach a balance by not only reducing the amount of user interactions but also improving the performance by involving an extra supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given an image of a human I, we represent the 2D pose as a set of skeleton joints J 2d = {p 1 , p 2 , ..., p n } where n denotes the number of joints (n = 16 in this paper) and p i = (x i , y i ) means the pixel the i th joint located at. Accordingly, the 3D pose is denoted as J 3d = {P 1 , P 2 , ..., P n } and P i = (X i , Y i , Z i ) means the 3D coordinates (in this work, we use the coordinate system of the camera) of the i th joint located at. The 3D bones can be represented as</p><formula xml:id="formula_0">B 3d = {B 1 , B 2 , ..., B m }</formula><p>where m is the number of bones(it equals 14 in this work). B i indicates a directed vector which starts from one of its end joint B 0 i to the other one B 1 i . The order of end joints associated with each bone is fixed and highlighted using an arrow as illustrated in <ref type="figure">Fig. 1</ref>. So as to construct the mapping from I to J 3d , our approach is built upon a ConvNets based deep learning architecture. Our methodology is introduced in the following three parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">FBI annotation</head><p>What is FBI?</p><formula xml:id="formula_1">Each bone B i = ???? B 0 i B 1 i , w.</formula><p>r.t the camera view, has three states: forward, backward and parallel to sight. This shows the depth order of B 0 i and B 1 i w.r.t camera frame reference. Therefore, the FBI of an image can be expressed using a binary matrix F = {f 1 , f 2 , ..., f m } where f i is a one-hot 3-dimensional vector, i.e., f i (j) = 1 means the i th bone has the j th status and j = 0, 1, 2.</p><p>Annotating procedure We recruited 6 paid annotators for FBI labeling. The images for annotation are randomly selected from MPII dataset <ref type="bibr" target="#b0">[1]</ref> where the 2D bones (projection of B 3d ) are provided. Using our developed user interface, annotators were shown images one by one with the associated 2D bones overlayed on them (as illustrated in <ref type="figure">Fig.  1</ref>). The bones are also shown one by one with highlight each time. For each one, the annotator is asked to make a choice from three options: forward, backward or uncertain. Considering the difficulties to give an accurate judgement whether a bone is "parallel to sight" or not, we replace this option by "uncertain". On average, one image can be annotated in 20 seconds. Note that, we randomly chose a set of images from H3.6M <ref type="bibr" target="#b10">[10]</ref> and mixed them into the images for labeling. These images are shown to annotators in a ran- dom order. Based on 3D groundtruth poses of these images, the whole labeling procedure is monitored automatically: the annotator would be given a feedback when his or her labeled FBI is conflicting with the groundtruth. This greatly helps the annotators to learn how to make a good annotation and ensures the quality of labeled information.</p><p>Our FBI-Dataset In all, we successfully annotated the FBI for around 12K in-the-wild images from MPI <ref type="bibr" target="#b0">[1]</ref>. Among them, around 20% bones are marked as uncertain. We illustrate the distributions of out-of-plane angles for all uncertain bones in <ref type="figure" target="#fig_1">Fig 3,</ref> from which we can see that people show more uncertainty when the bone is closer to parallelling with the view plane. Both our dataset and the annotating user interface will be released to public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>In short, our network consists of three components: a 2D pose estimator, a FBI predictor and a 3D pose regressor. Our 2D pose estimator used the same architecture as in <ref type="bibr" target="#b0">[1]</ref> aiming to take an image I of a human as input and output the 2D locations of all 16 joints of the human in I. The detailed design of other two components are described as follows.</p><p>FBI Predictor This module inputs an image I and outputs the FBI of all 14 bones, where f i can have three statuses: forward, backward or uncertain. This problem thus is formulated as a per-bone classification task. Taking I as in-put, our network for this component starts from a sequence of convolutional layers that are followed by two successive stacked hourglass modules(refer to <ref type="bibr" target="#b0">[1]</ref> for the design of such modules). The extracted feature maps are then fed into a set of convolutional layers. Finally, these are followed by a fully connected layer with a softmax layer to output classification results. The number of neurons for these layers are set as same as in <ref type="bibr" target="#b33">[33]</ref>.</p><p>3D Pose Regressor In our work, we learn a deep regression network to infer the 3D coordinates of the joints by taking both their 2D locations and the FBI as input. Instead of using the discrete F , this regressor takes the generated probability matrix of the softmax layer as input which provides more information. We first concatenate the 2D locations and the probability matrix together and then map them to the 3D pose by exploiting two cascaded blocks as used in <ref type="bibr" target="#b9">[9]</ref>. Specifically, each block maps the input feature into a higher dimension using two fully connected layers (1024 neurons are used in our work) interleaved with Batch Normalization, Dropout layers, and ReLU activation. At the end of the first block, an extra linear layer is utilized to output a coarse 3D pose. This is further re-projected into a 1024-dimension space and serves as a part of the input for the second block. We apply residual connections between the two blocks for sake of taking fully use of the information.</p><p>Weighted FBI Supervision By observing the FBI labeling procedure, we found that it shows high difficulty to separate the forward(or backward) status from uncertain one. In other words, these two statuses have no sharp gap. This heavily limits us to extract proper features due to the blurry supervision. To deal with this problem, we propose to use two kinds of weighted FBI supervision to extract features with different focuses and then combine them together for 3D pose regression. We first use a fixed weighting strategy and assign different weights for the bones with different states when doing FBI classification. In this work, 1 is set for the bones with forward and backward status and 0.05 is set for the bones with uncertain status. Such weighting generates a probability map P f ws (fws is a shorthand for fixed weighting supervision) that mostly captures the information of the bones with large out-of-plane angle. Another weighting strategy we used is in an adaptive way that assigns dynamic weights for bones during the training procedure. This is implemented using focal loss <ref type="bibr" target="#b12">[12]</ref>. For each bone B i , we use a one-hot 3-dimensional vector g i to denote its groundtruth label and use a 3-dimensional vector p i to represent the output probability vector. Thus, the loss function for the FBI classification of B i has the following formula, where ? is set to be 2 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss(p</head><formula xml:id="formula_2">i ) = 2 j=0 (?(1 ? p i (j)) ? log(p i (j))) * g i (j) (1)</formula><p>Intuitively speaking, using this method, a bone would have lower weight if it is already of a high probability to be classified into one category. We denote the output probability map using this strategy as P aws (aws is a shorthand for adaptive weighting supervision) which takes more care about the bones owning small out-of-plane angle. Our final 3D pose regressor takes both P f ws and P aws as input. The efficiency of this strategy is validated in Section 4.2. The whole network architecture of our method is illustrated in <ref type="figure" target="#fig_0">Fig 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>Our training procedure includes three steps from local to global and from coarse to fine. At first, we train FBI predictor and 3D pose regressor respectively from scratch and use the pre-trained model provided by <ref type="bibr" target="#b17">[17]</ref> for the 2D pose estimator. In this step, the FBI predictor is trained using the images in H3.6M <ref type="bibr" target="#b10">[10]</ref> where the 3D groundtruth poses are converted into FBI formula. Afterwards, the images from H3.6M are fed into both the trained 2D pose estimator and FBI predictor. We take their outputs and the 3D groundtruth poses as paired data to train the 3D pose regressor. As the second step, to reach a global optimal, all these three components are connected and the parameters are finetuned simultaneously with the training images in H3.6M. Finally, we finetune both the FBI predictor and the 3D pose regressor using our annotated FBI-dataset for generalizing our network into in-the-wild domain. To support this training, our approach is performed in a weakly-supervised way: the output of the 3D pose regressor is followed by a set of fully connected layers and a softmax layer for FBI classification where such layers are pre-trained using synthetic data.</p><p>Converting 3D pose into FBI formula To support our training, a critical problem is how to convert the 3D groundtruth pose in H3.6M into a FBI formula. We exploit a thresholding method: for a bone, it is marked as forward if its out-of-plane angle is larger than alpha, backward if the angle is smaller than ?? and uncertain for other cases. Intuitively, learning the FBI predictor would be getting harder if ? is getting smaller while the useful information from uncertain bones would be getting fewer when alpha is getting larger. In our work, ? is set as 35 ? to reach a balance based on our experiments in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Our approach was implemented based on the code released by <ref type="bibr" target="#b13">[13]</ref> that uses tensorflow. During the first train-ing stage described in Section 3.3, it took 100K iterations with a batch size of 8 to train the FBI predictor for both fixed weighting and adaptive weighting strategy based on the pretrained model from <ref type="bibr" target="#b33">[33]</ref>. The batch size of the second global finetuning stage is set as 8 and the procedure took 100K iterations. Our final FBI-predictor finetuning stage was conducted in 120K iterations by taking a batch size of 8. The whole training procedure took about two days in one Titan X GPU with CUDA 8.0 and cudnn 5. During testing phase, one forward passing takes 30ms. To verify the efficiency of our proposed algorithm, we conduct the evaluation in the following manifolds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparisons against existing methods</head><p>Our numerical evaluation is conducted on Human3.6M <ref type="bibr" target="#b10">[10]</ref>. As far as we know, this is one of the largest public dataset where both the 2D pose and 3D groundtruth pose are available. It contains 3.6 millions of RGB images captured by a MoCap System in an indoor environment, in which 7 professional actors performed 15 activities such as walking, eating, sitting, making a phone call and engaging in a discussion. The videos are down-sampled from 50fps to 10fps in order to reduce redundancy. Following the standard protocol as in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b17">17]</ref>, we use 5 subjects(S1, S5, S6, S7, S8) for training and the rest 2 subjects(S9, S11) for evaluation. The mean per joint position error(MPJPE) between the ground truth and our prediction (after alignment of the central hip joint) is used as our evaluation metric. This is denoted as protocol #1. For some of previous works, the prediction has been further aligned with the ground truth via a rigid transformation. This post-processing is named protocol #2.</p><p>The comparison results on protocol #1 are reported in <ref type="table">Table 3</ref>.3 while the results on protocol #2 are shown in <ref type="table">Table 4</ref>.1. As seen from the table, our method outperforms all previous works almost on all actions. It is worth mentioning that our approach makes considerable improvements on some complicated actions like sitting and sitting down which show more challenges than others. Thanks to the FBI information, our method is very good at for the cases with large poses. Before our work, the best performance is achieved by Fang et al. <ref type="bibr" target="#b9">[9]</ref> that involved the relationship priors between skeleton joints into the learning procedure. Compared with this, our results are 4mm more accurate on average. And we also believe their strategy can be integrated into our framework to get further improvement. It is worth mentioning that, concurrently, the work in <ref type="bibr" target="#b19">[19]</ref> exploited a similar strategy as ours and achieved comparable results which are also shown in <ref type="table">Table 3.3 and Table  4</ref>.1. Specifically, it proposed an annotation tool for collecting the depth relations for all joints. Such ordinal depth information is further used for training a neural network in a weakly supervised way. Comparing with this work, our annotation procedure is much easier for two reasons: 1) us-  <ref type="bibr" target="#b19">[19]</ref> is a concurrent work with our method. The best score without consideration of this work is marked in blue bold. We also use black bold to highlight the best score when taking this work for comparison.</p><p>ing our tool, annotators only need to mark the ordinal depth information for two joints with rigid connection (as a bone). This is more intuitive than the task of marking the depth relations for two separate joints as in <ref type="bibr" target="#b19">[19]</ref>. 2) for each image, to obtain the global depth orders of all joints, the annotation in <ref type="bibr" target="#b19">[19]</ref> usually requires the annotators to answer dozens of questions while ours has only 16 questions to be answered. In addition to, Yang et al. <ref type="bibr" target="#b30">[30]</ref> proposed another concurrent method which applied an adversarial learning framework and reached comparable performance as ours. It is convenient to combine their idea into our framework and we believe this will produce better results.</p><p>Comparisons on in-the-wild generalization To validate the efficiency of our in-the-wild generalization, we also conduct comparisons against previous works on images in the wild both qualitatively and quantitatively. To our best knowledge, there is no dataset owning 3D pose groundtruth.</p><p>To support the evaluation, we take 1K images from our FBIdataset as the test data. We take the method <ref type="bibr" target="#b33">[33]</ref> to attend this comparison because of its state-of-the-art performance and accessible code. With each method, the 3D pose results are generated for all the 1K images firstly and then we use the correctness ratio of FBI derived from the 3D pose as the evaluation metric. Here, we only do the statistic on the bones with clear status, say forward or backward. The method of <ref type="bibr" target="#b33">[33]</ref> has 75% correctness ratio while ours reaches 78%. This is also verified by some qualitative comparison results as shown in <ref type="figure" target="#fig_2">Fig 4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head><p>We study the influence on final performance of different choices made in our network design and the training procedure. The method without weighted supervision and inthe-wild FBI finetuning is denoted as "Ours-baseline" representing a baseline of our approach. Specifically, it used equal weighted loss for the training of FBI classification. Adopting weighted FBI supervision on this baseline method gives rise to "Ours-WS". After that, the strategy to exploit our in-the-wild FBI dataset for network finetuning is applied on "Ours-WS" and derives our final method "Ours-Final".</p><p>With/without weighted supervision We first study the effectiveness of our weighted FBI supervision strategy by  <ref type="bibr" target="#b19">[19]</ref> is a concurrent work with our method. The best score without consideration of this work is marked in blue bold. We also use black bold to highlight the best score when taking this work for comparison. evaluate "Ours-Baseline" and "Ours-WS". All these methods are evaluated using the same way as in Section 4.1 and the results are also presented in <ref type="table">Table 3.3 and Table 4</ref>.1.</p><p>It is not difficult to find that "Ours-WS" shows better performances than "Ours-baseline" for the actions with large poses (e.g. sitting down) and also keeps comparable accuracy for other actions. This validated the effectiveness of the design with two kinds of weighting supervision. With/without in-the-wild FBI boosting Without our FBI-dataset, our approach ("Ours-WS") also outperforms all existing methods. However, the model is only trained on images from H3.6M and would have low performance on in-the-wild images. To overcome this problem, we use a large amount of images with labeled FBI to finetune our model. As shown in <ref type="table">Table 3.3 and Table 4</ref>.1, with such finetuning, our final method also has a performance increasing based on the evaluation on H3.6M. We also perform the comparison between the method with and without finetuning on in-the-wild generalization as did in Section 4.1, where our method without in-the-wild boosting only produces 58% correct rate. This is also validated by some qualitative results as illustrated in <ref type="figure" target="#fig_2">Fig 4.</ref> The best thresholding to convert 3D pose into FBI As mentioned in Section 3.3, a critical issue for converting the 3D pose into FBI representation is determining the threshold angle alpha. To make a better choice, we generate several samples as (5 ? , 20 ? , 25 ? , 30 ? , 35 ? , 40 ? , 60 ? ) and conduct evaluation on all of them individually. The curve representing changes of accuracy along with alpha is shown in <ref type="figure" target="#fig_3">Fig 5,</ref> from which we found that 35 ? leads to the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we exploited a new information called Forward-Backward Information(FBI) of bones targeting an end-to-end framework to generate 3D human poses from a single RGB image. The biggest challenge in this area is the lack of images dataset with 3D groundtruth poses. The previous works solve this problem by decomposing the task into two stages: performing 2D pose estimation and inferring 3D pose from only the 2D joints. These two stages are usually treated separately while incurs a gap between images and 3D poses. The involving of FBI tackles this issue in three ways: 1) with FBI supervision, we dig out more 3D-aware features from images; 2) taking both FBI and 2D joints as input to infer the 3D pose greatly reduces the ambiguity; 3) more importantly, the FBI is very easy to annotate where we marked such information for around 12K images which are keeping updated. The experiments demonstrate the effectiveness of our approach both qualitatively and quantitatively. We also believe our proposed approach can inspire others to involve some weakly yet easily annotated information for weakly supervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The network architecture of our method. It consists of a 2D pose estimator, a FBI classifier and a 3D pose regressor. means concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The distributions of out-of-plane angles for all bones marked as "uncertain". Each bar shows the percentage of the bones whose out-of-plane angle lies in a certain range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Quantitative comparison results of our method against others on some in-the-wild images which are chosen from MPII<ref type="bibr" target="#b0">[1]</ref>. ITW is shorthand for In The Wild.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Our performance changes with the thresholding angle selection. All the values are obtained by using "Ours-baseline" on H3.6M protocol #1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons of the mean per joint position error(MPJPE) between our prediction and the ground truth on Human3.6M under Protocol #1. Note that, Ordinal</figDesc><table><row><cell>Protocol #1</cell><cell>Direct</cell><cell>Discuss</cell><cell cols="4">Eating Greet Phone Photo</cell><cell>Pose</cell><cell>Purch.</cell></row><row><cell>LinKDE[10]</cell><cell>132.7</cell><cell>183.6</cell><cell cols="2">132.3 164.4</cell><cell>162.1</cell><cell>205.9</cell><cell>150.6</cell><cell>171.3</cell></row><row><cell>Tekin et al. [24]</cell><cell>102.4</cell><cell>147.2</cell><cell>88.8</cell><cell>125.3</cell><cell>118.0</cell><cell>182.7</cell><cell>112.4</cell><cell>129.2</cell></row><row><cell>Du et al. [8]</cell><cell>85.1</cell><cell>112.7</cell><cell cols="2">104.9 122.1</cell><cell>139.1</cell><cell>135.9</cell><cell>105.9</cell><cell>166.2</cell></row><row><cell>Chen &amp; Ramanan [4]</cell><cell>89.9</cell><cell>97.6</cell><cell>89.9</cell><cell>107.9</cell><cell>107.3</cell><cell>139.2</cell><cell>93.6</cell><cell>136.0</cell></row><row><cell>Pavlakos et al. [20]</cell><cell>67.4</cell><cell>71.9</cell><cell>66.7</cell><cell>69.1</cell><cell>72.0</cell><cell>77.0</cell><cell>65.0</cell><cell>68.3</cell></row><row><cell>Zhou et al. [33]</cell><cell>54.8</cell><cell>60.7</cell><cell>58.2</cell><cell>71.4</cell><cell>62.0</cell><cell>65.5</cell><cell>53.8</cell><cell>55.6</cell></row><row><cell>Martinez et al. [13]</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell></row><row><cell>Fang et al. [9]</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>73.3</cell><cell>53.4</cell><cell>55.7</cell></row><row><cell>Ordinal [19]</cell><cell>48.5</cell><cell>54.4</cell><cell>54.4</cell><cell>52.0</cell><cell>59.4</cell><cell>65.3</cell><cell>49.0</cell><cell>52.9</cell></row><row><cell>Ours-Baseline</cell><cell>49.7</cell><cell>53.9</cell><cell>53.4</cell><cell>56</cell><cell>62.6</cell><cell>70.5</cell><cell>53.4</cell><cell>52.2</cell></row><row><cell>Ours-WS</cell><cell>50.2</cell><cell>53.2</cell><cell>54.0</cell><cell>56.4</cell><cell>62.7</cell><cell>71.3</cell><cell>53.4</cell><cell>52.3</cell></row><row><cell>Ours-Final</cell><cell>49.1</cell><cell>52.7</cell><cell>52.0</cell><cell>55.2</cell><cell>60.5</cell><cell>69.8</cell><cell>52.3</cell><cell>51.5</cell></row><row><cell></cell><cell cols="8">Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Average</cell></row><row><cell>LinKDE[10]</cell><cell>151.6</cell><cell>243.0</cell><cell cols="2">162.1 170.7</cell><cell>177.1</cell><cell>96.6</cell><cell>127.9</cell><cell>162.1</cell></row><row><cell>Tekin et al. [24]</cell><cell>138.9</cell><cell>224.9</cell><cell cols="2">118.4 138.8</cell><cell>126.3</cell><cell>55.1</cell><cell>65.8</cell><cell>125.0</cell></row><row><cell>Du et al. [8]</cell><cell>117.5</cell><cell>226.9</cell><cell cols="2">120.0 117.7</cell><cell>137.4</cell><cell>99.3</cell><cell>106.5</cell><cell>126.5</cell></row><row><cell cols="2">Chen &amp; Ramanan [4] 133.1</cell><cell>240.1</cell><cell cols="2">106.6 106.2</cell><cell>87.0</cell><cell>114.0</cell><cell>90.5</cell><cell>114.1</cell></row><row><cell>Pavlakos et al. [20]</cell><cell>83.7</cell><cell>96.5</cell><cell>71.7</cell><cell>65.8</cell><cell>74.9</cell><cell>59.1</cell><cell>63.2</cell><cell>71.9</cell></row><row><cell>Zhou et al. [33]</cell><cell>75.2</cell><cell>111.6</cell><cell>64.1</cell><cell>66.0</cell><cell>51.4</cell><cell>63.2</cell><cell>55.3</cell><cell>64.9</cell></row><row><cell>Martinez et al. [13]</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Ordinal [19]</cell><cell>65.8</cell><cell>71.1</cell><cell>56.6</cell><cell>52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8</cell><cell>56.2</cell></row><row><cell>Fang et al. [9]</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Ours-Baseline</cell><cell>66.5</cell><cell>80.7</cell><cell>57.5</cell><cell>56</cell><cell>60.9</cell><cell>45.9</cell><cell>50.7</cell><cell>58.0</cell></row><row><cell>Ours-WS</cell><cell>64.7</cell><cell>77.0</cell><cell>57.4</cell><cell>55.5</cell><cell>61.0</cell><cell>45.6</cell><cell>50.4</cell><cell>57.7</cell></row><row><cell>Ours-Final</cell><cell>64.3</cell><cell>74.8</cell><cell>56.4</cell><cell>55.1</cell><cell>60.0</cell><cell>44.4</cell><cell>48.9</cell><cell>56.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons of the mean per joint position error(MPJPE) between our prediction and the ground truth on Human3.6M under Protocol #2. Note that, Ordinal</figDesc><table><row><cell>Protocol #2</cell><cell>Direct</cell><cell>Discuss</cell><cell cols="4">Eating Greet Phone Photo</cell><cell>Pose</cell><cell>Purch.</cell></row><row><cell>Bogo et al. [2]</cell><cell>62.0</cell><cell>60.2</cell><cell>67.8</cell><cell>76.5</cell><cell>92.1</cell><cell>77.0</cell><cell>73.0</cell><cell>75.3</cell></row><row><cell>Moreno-Noguer [16]</cell><cell>66.1</cell><cell>61.7</cell><cell>84.5</cell><cell>73.7</cell><cell>65.2</cell><cell>67.2</cell><cell>60.9</cell><cell>67.3</cell></row><row><cell>Pavlakos et al. [20]</cell><cell>47.5</cell><cell>50.5</cell><cell>48.3</cell><cell>49.3</cell><cell>50.7</cell><cell>55.2</cell><cell>46.1</cell><cell>48.0</cell></row><row><cell>Martinez et al. [13]</cell><cell>39.5</cell><cell>43.2</cell><cell>46.4</cell><cell>47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4</cell><cell>40.6</cell></row><row><cell>Fang et al. [9]</cell><cell>38.2</cell><cell>41.7</cell><cell>43.7</cell><cell>44.9</cell><cell>48.5</cell><cell>55.3</cell><cell>40.2</cell><cell>38.2</cell></row><row><cell>Ordinal [19]</cell><cell>34.7</cell><cell>39.8</cell><cell>41.8</cell><cell>38.6</cell><cell>42.5</cell><cell>47.5</cell><cell>38.0</cell><cell>36.6</cell></row><row><cell>Ours-Baseline</cell><cell>38.6</cell><cell>41.8</cell><cell>42.1</cell><cell>44.9</cell><cell>45.8</cell><cell>52.5</cell><cell>40.4</cell><cell>38.4</cell></row><row><cell>Ours-WS</cell><cell>38.9</cell><cell>41.5</cell><cell>43.0</cell><cell>45.2</cell><cell>46.2</cell><cell>52.4</cell><cell>40.6</cell><cell>38.3</cell></row><row><cell>Ours-Final</cell><cell>38.4</cell><cell>41.3</cell><cell>41.9</cell><cell>44.7</cell><cell>45.4</cell><cell>51.7</cell><cell>40.0</cell><cell>37.9</cell></row><row><cell></cell><cell cols="8">Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Average</cell></row><row><cell>Bogo et al. [2]</cell><cell>100.3</cell><cell>137.3</cell><cell>83.4</cell><cell>77.3</cell><cell>86.8</cell><cell>79.7</cell><cell>87.7</cell><cell>82.3</cell></row><row><cell cols="2">Moreno-Noguer [16] 103.5</cell><cell>74.6</cell><cell>92.6</cell><cell>69.6</cell><cell>71.5</cell><cell>78.0</cell><cell>73.2</cell><cell>74.0</cell></row><row><cell>Pavlakos et al. [20]</cell><cell>61.1</cell><cell>78.1</cell><cell>51.1</cell><cell>48.3</cell><cell>52.9</cell><cell>41.5</cell><cell>46.4</cell><cell>51.9</cell></row><row><cell>Martinez et al. [13]</cell><cell>56.5</cell><cell>69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Fang et al. [9]</cell><cell>54.5</cell><cell>64.4</cell><cell>47.2</cell><cell>44.3</cell><cell>47.3</cell><cell>36.7</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell>Ordinal [19]</cell><cell>50.7</cell><cell>56.8</cell><cell>42.6</cell><cell>39.6</cell><cell>43.9</cell><cell>32.1</cell><cell>36.5</cell><cell>41.8</cell></row><row><cell>Ours-Baseline</cell><cell>50.2</cell><cell>63.1</cell><cell>45.6</cell><cell>42.2</cell><cell>46.3</cell><cell>34.1</cell><cell>39.3</cell><cell>44.3</cell></row><row><cell>Ours-WS</cell><cell>49.3</cell><cell>60.2</cell><cell>45.6</cell><cell>41.9</cell><cell>46.7</cell><cell>34.1</cell><cell>39.5</cell><cell>44.2</cell></row><row><cell>Ours-Final</cell><cell>49.1</cell><cell>59.5</cell><cell>45.2</cell><cell>41.9</cell><cell>46.2</cell><cell>33.6</cell><cell>38.8</cell><cell>43.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Marker-less 3d human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2017 Fifth International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04095</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3108" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">3d human pose estimation: A review of the literature and analysis of covariates. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="363" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05180</idno>
		<title level="m">Structured prediction of 3d human pose with deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<biblScope unit="page" from="2500" to="2509" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3073" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-N</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Automation &amp; Soft Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parametric reshaping of human bodies in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">126</biblScope>
			<date type="published" when="2010" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
