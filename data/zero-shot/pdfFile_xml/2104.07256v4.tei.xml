<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba DAMO Academy</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, significant progress has been made on semantic segmentation. However, the success of supervised semantic segmentation typically relies on a large amount of labeled data, which is time-consuming and costly to obtain. Inspired by the success of semi-supervised learning methods for image classification, here we propose a simple yet effective semi-supervised learning framework for semantic segmentation. We demonstrate that the devil is in the details: a set of simple designs and training techniques can collectively improve the performance of semisupervised semantic segmentation significantly. Previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref> fail to effectively employ strong augmentation in pseudo-label learning, as the large distribution disparity caused by strong augmentation harms the batch normalization statistics. We design a new batch normalization, namely distribution-specific batch normalization (DSBN) to address this problem and show the importance of strong augmentation for semantic segmentation. Moreover, we design a self-correction loss, which is effective in terms of noise resistance. We conduct a series of ablation studies to show the effectiveness of each component. Our method achieves state-of-the-art results in the semi-supervised settings on the Cityscapes and Pascal VOC datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation/pixel labelling is one of the core tasks in visual understanding, which is widely used in scene parsing, human body parsing, and many downstream applications. It is a per-pixel classification problem, which classifies each pixel in the image into a predefined set of categories. In the past few years, semantic segmentation methods based on deep convolution neural networks (CNNs) have made tremendous progress <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. Note that, the common prerequisite for all these successes is the availability of a massive amount of pixel-level labeled data. Unfortu-* JY and YL contributed equally to this work. Accepted to Proc. Int. Conf. Computer Vision (ICCV) 2021.  nately, labeling such datasets is very expensive and timeconsuming, especially in dense prediction problems, such as semantic segmentation. As we need to label each pixel, it can be 60 times more expensive than image-level annotation.</p><p>Recent research reveals that semi-supervised learning (SSL), which uses a large amount of unlabeled data together with a small amount of labeled data, is greatly beneficial to classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>. These methods can be grouped into consistency methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28]</ref>, pseudolabelling methods <ref type="bibr" target="#b35">[36]</ref>, representation learning <ref type="bibr" target="#b13">[14]</ref>. A few works attempted to apply SSL to semantic segmentation. The naive student <ref type="bibr" target="#b2">[3]</ref> uses a large model to generate the pseudo-labels with unlabeled video sequences, and apply iterative training for further improvement. They only consider the pseudo-labels with the original input images without perturbations. Recent work of <ref type="bibr" target="#b24">[25]</ref> considers adding perturbations to the images for semantic segmentation. They forward images with different perturbations in different sub-decoders and enforce the consistency between sub-decoders and the main decoder. Other work pays attention to generative methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. AdvSemiSeg <ref type="bibr" target="#b16">[17]</ref> and the work in <ref type="bibr" target="#b22">[23]</ref> both employ Generative Adversarial Network (GAN) and train the model with a discriminative loss over unlabeled data and a supervised loss over labeled data.</p><p>Here, we propose an effective and efficient framework to apply SSL to semantic segmentation. We employ strong augmentation to make better use of the unlabeled data. Previous works based on consistency learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35]</ref> have shown that adding noise in the procedure of learning, the pseudo-labels help improve the performance in image classification. Motived by those works, we propose to apply strong augmentation to semantic segmentation. However, strong augmentation would inevitably affect the distribution of mean and variance in the batch normalization (BN) as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. As a consequence, one needs to be careful with computing the BN statistics in this scenario. This may be the reason why strong augmentation was not used in SSL semantic segmentation <ref type="bibr" target="#b2">[3]</ref>. Instead, multiple branch networks were employed to process different perturbations, as in <ref type="bibr" target="#b24">[25]</ref>. In order to avoid the distribution shift caused by strong data augmentations, we propose a simple yet effective method, namely, the distribution-specific batch normalization (DSBN). We forward the strongly augmented data and the weakly-augmented data with different batch statistics during training, and inference the model with the batch statistics calculated by the weakly-augmented data.</p><p>Furthermore, as the teacher network may not perform well under all situations, some unreliable regions may be included in the generated pseudo-labels. Thus, directly learning from all the pixels will inevitably introduce label noise. Inspired by previous work in <ref type="bibr" target="#b31">[32]</ref>, which selects the unreliable image-pairs, and exchanges the prediction with the learning target to suppress the negative impact of label noise, we design a new self-correction loss (SCL) and dynamically modify the weight and learning target for each pixel for semantic segmentation.</p><p>Our approach achieves state-of-the-art performance on the Cityscapes <ref type="bibr" target="#b6">[7]</ref> and Pascal VOC datasets <ref type="bibr" target="#b10">[11]</ref> under a semi-supervised setting. Our main contributions are summarized as follows.</p><p>? We propose an effective and efficient semi-supervised learning framework for semantic segmentation. We employ strong augmentation during training without modifying the network structure, such as introducing sub-networks. ? A distribution-specific batch normalization is proposed to accommodate the discrepancy of data distribution in a batch caused by strong data augmentation. Besides, we design a self-training robust loss to alleviate the negative impact of label noise. The loss function is able to correct the noisy label to some extent, thus resisting noise. ? We empirically demonstrate the effectiveness of our approach, including a comparison with state-of-theart methods, and an in-depth analysis of our approach with a detailed ablation study in the semi-supervised setting.</p><p>Next, we review some work relevant to ours. Semi-supervised classification. Most previous works on SSL focus on classification tasks. Current state-of-the-art representative methods include consistency based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28]</ref>, Pseudo labeling <ref type="bibr" target="#b35">[36]</ref> and representation learning methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. Consistency based methods exploit the fact that the prediction of an unlabeled image should not change significantly with minor perturbations. UDA <ref type="bibr" target="#b34">[35]</ref> and ReMixMatch <ref type="bibr" target="#b0">[1]</ref> both use a weakly-augmented example to generate an artificial label and enforce consistency against strongly-augmented examples. FixMatch implements consistency training by applying strong (CutOut <ref type="bibr" target="#b9">[10]</ref>, CTAugment <ref type="bibr" target="#b0">[1]</ref>, and RandAugment <ref type="bibr" target="#b7">[8]</ref>) and weak data augmentation to the inputs in both the unlabeled loss and the pseudo label generation, thus encouraging the outputs to be consistent for both augmented inputs. Pseudo labeling methods rely on the assumption that the pseudo-labels generated by a teacher model can benefit the training of new models. Noisy student training <ref type="bibr" target="#b35">[36]</ref> is an iterative self-training method in this category. The correspondence among different transformations is employed for representation learning. Then a small amount of data is used to finetune the model. Semi-supervised semantic segmentation. Early methods use a GAN model in semi-supervised segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>, where an adversarial loss is trained on unlabeled data. Consistency regularization based approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> have also been proposed recently, and achieve promising results. They depend on the fact that unlabeled data and labeled data have the same distribution, so it expects the trained model to have consistent and reliable predictions for both the unlabeled data and the labeled data. In these works, different unlabeled images under different transformations are used as the input to the model, and consistency loss is enforced on the prediction mask of the model. To date, self-training pushes the performance of the state-of-the-art <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref>. Note that none of them attempted to solve the negative impact caused by noisy pseudo-labels. Data augmentation. Data augmentation is an effective regularization technique. The basic policy of data augmentation contains random flip, random crop, etc., which are commonly used in training vision models. In addition, the inception preprocess <ref type="bibr" target="#b29">[30]</ref> is more sophisticated when the input image color is randomly disturbed. Recently, Au-toAugment <ref type="bibr" target="#b8">[9]</ref> improves the inception preprocess for image classification, using reinforcement learning to search for the optimal combination of augmentation strategies. Ran-dAugment <ref type="bibr" target="#b7">[8]</ref> proposes a significantly reduced search space which allows it to be trained on the target task and dataset, thus removing the need for a separate proxy task or dataset. All of these works tackle image classification. In this paper, we propose to apply strong augmentation to semantic segmentation. Separated batch normalization layers. It was observed in the literature that employment of a separated batch nor-malization to process out-of-distribution samples can lead to performance improvement. AdvProp <ref type="bibr" target="#b33">[34]</ref> uses an auxiliary batch normalization for accommodating domain shift coming from adversarial examples, which are proposed for more effective adversarial training in the context of supervised image-level classification. We are inspired by this approach but employ an auxiliary batch normalization to process strongly augmented training samples which exhibit different statistics from that of the weakly augmented samples. Split-BN <ref type="bibr" target="#b37">[38]</ref>, which shares ? and ? parameters, is proposed for class mismatch and image distortions of unlabeled datasets. TransNorm <ref type="bibr" target="#b30">[31]</ref> is designed for unsupervised domain adaptation. TransNorm calculates ? for a different domain. Here, our motivation is to solve the negative impact of BN statistics discrepancy caused by strong augmentation in image segmentation tasks. Robust loss functions for training with noisy data. A few works pay attention to the loss function for learning with noisy labels, and achieve improved results. The generalized cross-entropy <ref type="bibr" target="#b38">[39]</ref> was proposed to achieve the advantages of both mean absolute error (MAE) and categorical cross-entropy losses (CCE). Inspired by the symmetry of the Kullback-Leibler divergence, the symmetric crossentropy <ref type="bibr" target="#b31">[32]</ref> was proposed by combining a noise tolerance term, namely reverse cross-entropy loss, with the standard CCE loss. Nevertheless, since semantic segmentation is a pixel-level classification task, directly applying these methods to segmentation, which are designed for image-level classification, is unlikely to yield satisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-supervised Semantic Segmentation</head><p>In this section, we introduce our simple semi-supervised learning framework for semantic segmentation. Different from CCT <ref type="bibr" target="#b24">[25]</ref>, which needs to design auxiliary decoders for different perturbations, our method can be applied to any existing segmentation network, as shown in Algorithm 1.</p><p>Given a small set of labeled training examples and a large set of unlabeled training examples, the small set of labeled training examples are exploited to train an initial teacher model using the standard cross-entropy loss. We then use the teacher model to generate pseudo-labels on the unlabeled images with a test-time augmentation. Following the Naive student method <ref type="bibr" target="#b2">[3]</ref>, we only generate hard pseudolabels, i.e., one-hot vectors. Then, we train a student model with strong augmentation to make better use of the unlabeled data. Thus, the data distribution in a batch is disturbed by the strong augmentation process, causing mismatch to that of the samples processed by weak/standard augmentation. Here, we propose the distribution-specific batch normalization (DSBN) to alleviate the negative impact of this batch distribution mismatch. Moreover, motivated by the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Semi-supervised Learning Framework</head><p>Labeled images: n pairs of images x i and corresponding labels y i Unlabeled images: m images without labels {( x 1 , x 2 , ..., xm} Step1: Optimize the teacher model with the cross-entropy loss on labeled images; while iteration &lt;maximum number of iterations do</p><p>Step2: Use the teacher model to generate hard pseudo-labels (one-hot encodings) for clean (i.e., not distorted) unlabeled images with multi-scale and flip inference to get m pairs of images with pseudo-labels {(</p><formula xml:id="formula_0">x 1 , y 1 ), ( x 2 , y 2 ), ..., ( xm, ym)} for epoch = 1, ..., n epoch d? x i = SDA ( x i ), ?i = 1, 2, ..., m; [y * , y * ] = f ([x,</formula><p>x]); loss = CE(y * , y) + SCL( y * , y); minimizing the loss and update student parameters ?s; end Re-initialize the teacher parameters ?t = ?s; Jump to step 2. end symmetric cross entropy <ref type="bibr" target="#b31">[32]</ref>, in order to learn with potentially noisy labels, we develop a new loss function, namely, self-correction loss (SCL). SCL is used to accommodate learning with noisy labels. Finally, as in <ref type="bibr" target="#b2">[3]</ref>, we iterate the process by putting back the student as a teacher to generate new pseudo-labels and train a new student.</p><p>Strong data augmentation for semantic segmentation (SDA). While augmentation strategies for supervised and semi-supervised image classification were extensively studied <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>, much less effort has been made for semantic segmentation. We apply random augmentation to semantic segmentation. As illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>   tion methods. At each training iteration, we randomly select a series of operations from the pool, and then combine them with standard transformations of semantic segmentation (random scale, random crop, random flip, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Distribution-Specific Batch Normalization</head><p>As mentioned in some previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40]</ref>, batch normalization (BN) is crucial for semantic segmentation. BN computes the mini-batch mean (? B ) and standard deviation (? B ) <ref type="bibr" target="#b17">[18]</ref>, formulated in Equation <ref type="formula" target="#formula_1">(1)</ref>. Then, as shown in Equation <ref type="formula" target="#formula_2">(2)</ref>, BN uses a learnable scale (?) and shift (?) parameter to transform the normalized distribution. Meanwhile, moving mean and moving variance are updated by Equation <ref type="formula" target="#formula_3">(3)</ref>. In the testing phase, BN uses the moving mean and moving variance for whitening input activations. However, strong augmentations are likely to shift the distribution of natural images and lead to a domain gap between training images and testing images. Thus, the moving mean and moving variance calculated during the training phase would harm the testing performance. To deal with this issue, we propose the distribution-specific batch normalization (DSBN). Specifically, we have two separate BNs: BN w for samples of weak/standard augmentation; and BN s for samples of strong augmentation (see <ref type="figure" target="#fig_4">Figure 3</ref>). In the training phase, if a data sample comes from pseudo-labels, then BN s is used in the forward; otherwise, BN w is used. In the testing phase, the BN s is deprecated, and only the BN w is used for normalization:</p><formula xml:id="formula_1">? B = 1 m m i=1 x i ; ? B = 1 m m i=1 (x i ? ? B ) 2 + 0.5 ,<label>(1)</label></formula><formula xml:id="formula_2">y i = ? (x i ? ? B ) ? B + ?,<label>(2)</label></formula><formula xml:id="formula_3">? t+1 = ?? t + (1 ? ?)? t+1 , ? t+1 = ?? t + (1 ? ?)? t+1 .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-correction Loss</head><p>Previous work <ref type="bibr" target="#b31">[32]</ref> has shown that cross-entropy loss masks the model overfit to noisy labels on some easier classes and under learning on some harder classes in image classification. As semantic segmentation is a per-pixel classification problem, similar challenges also exist. Moreover, in the semi-supervised learning framework, the learning targets are pseudo-labels, which may contain unreliable regions caused by the limited capacity of the current teacher model.</p><p>To avoid overfitting to the noise contained in the pseudolabels, we propose a Self-Correction Loss (SCL) for semantic segmentation. It assigns an adaptive weight by the confidence of the network output to each pixel during the training phase. Besides, we apply a noise-robust term Reverse Cross Entropy <ref type="bibr" target="#b31">[32]</ref> to the unreliable regions. Different from previous work, which chooses the reversed learning pairs by fixed confidence from human heuristics, we dynamically change the learning targets by comparing the output of the student network and the teacher network.</p><p>As shown in Equation <ref type="formula" target="#formula_4">(4)</ref> (i is the location index), y * i denotes the prediction of a pixel, y i denotes the pseudo-label generated by the teacher, and w i represents the dynamic weight which is the largest activation after softmax among all c classes (refer to Equation <ref type="formula">(5)</ref>). We use the prediction to automatically adjust the confidence during training. If the confidence is very high, we adopt positive learning. If the confidence is very low, we believe that the pseudo-label for this pixel is unreliable. Thus, reverse learning is applied. In addition, SCL is only applied to pseudo-labels:</p><formula xml:id="formula_4">i = w i ? y i log(y * i ) + (1 ? w i ) ? y * i log(y i ),<label>(4)</label></formula><formula xml:id="formula_5">w i = max( exp(y * i0 ) c j=0 exp(y * ij ) , ..., exp(y * ic ) c j=0 exp(y * ij )</formula><p>). (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we first report the implementation details. Then we perform a series of ablation experiments, and analyze the results in detail. Finally, we report our results compared with other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>Training. Our implementation is built on Pytorch <ref type="bibr" target="#b25">[26]</ref>. Following previous work <ref type="bibr" target="#b3">[4]</ref>, we use the "poly" learning rate policy, where the base learning rate is multiplied by (1 ? iter/iter max ) power . The initial learning rate is set to 0.01, and power is set to 0.9. We train the network using mini-bath stochastic gradient descent (SGD). We employ the crop size of 769 ? 769 on Cityscapes with DeepLabv3Plus. More details is same as <ref type="bibr" target="#b3">[4]</ref>. In the addition study, following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>, we adopt the mean intersection of union (mIoU) as the evaluation metrics. Data augmentation. Following <ref type="bibr" target="#b3">[4]</ref>, we use mean subtraction, and apply data augmentation by random resize between 0.5 and 2 and random left-right mirror during initialization training. In the semi-supervised learning phrase, we use strong augmentation for improving the performance of self-training. Datasets. Following <ref type="bibr" target="#b2">[3]</ref>, We conduct the main experiments and ablation studies on the Cityscapes dataset <ref type="bibr" target="#b6">[7]</ref>. This large-scale data set contains different stereo video sequences recorded in 50 different urban street scenes. In addition to 20k weak annotation frames, there are 5, 000 highquality pixel level annotation frames, in which, 2, 975; 500 and 1, 525 were used for training, validation and testing. In addition, there are 20k images with rough annotations, namely train-coarse. We randomly subsample 1 /8, 1 /4, and 1 /2 of images in the standard training set to construct the pixel-level labeled data. The remaining images in the training set are used as unlabeled data.</p><p>At last, we report our results on the Pascal VOC 2012 dataset <ref type="bibr" target="#b10">[11]</ref>, which contains 21 classes including background. The standard Pascal VOC 2012 dataset has 1, 449 images as the training set and 1, 456 images as the validation set. We construct 1, 449 images as the pixel-level labeled data. The images in the augmented set <ref type="bibr" target="#b14">[15]</ref> (around 9k images), are used as unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation Study</head><p>In this subsection, we conduct the experiments to explore the effectiveness of each proposed module under different semi-supervised settings. First, we establish the baseline for our experiments. We evaluate DeepLabv3Plus based on Xception65 with cross entropy on the validation set. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>, self training adopts the baseline model as the teacher model and the teacher model generates pseudo labels on the unlabeled dataset, then the student model which is not smaller than the teacher model is trained on pseudo labels and original labeled images. We use the scales including 0.5, 0.75, 1.0, 1.5, 1.75 and flip for the remaining images to generate pseudo labels. For fair comparison and quick training, the student model is the same model initialized by the teacher model. The main results are shown in <ref type="table">Table 1</ref>, and we will give detail discussions on each module in the following sections. All the ablation study are conducted on Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Different semi-supervised settings</head><p>We follow previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref> to divide the train set into labeled data and unlabeled data according to different proportions. We use 1 /8, 1 /4, and 1 /2 of the training dataset as labeled data and other images as the unlabeled data. The evaluation results are shown in <ref type="table">Table 1</ref> as mIoU-1 /8, mIoU-1 /4, mIoU-1 /2, respectively. From the <ref type="table">Table 1</ref>, we can see that our algorithm can effectively improve the performance by 5.2%, 4.4%, 1.8%, 1.8%, respectively. When the total number of labeled data and unlabeled data does not change, the semi-supervised learning framework will have a  larger improvement, if we have less labeled images. In the meantime, the final results will be better if we have more high-quality labeled images. Besides, we can see that only half the amount of labeled data with the help of proposed semi-supervised learning framework can achieve the same results (mIoU of 78.7%) as we use all labeled training data with supervised learning (row 1 and column 10).</p><p>When the total number of available data increases, semisupervised learning will play a more valuable role. We use all high-quality labeled data in the training set as the labeled data and employ the train-coarse as unlabeled data to conduct the experiment, and the evaluation results are referred to as mIoU-Full in <ref type="table">Table 1</ref>. We can see that under a strong baseline with large amount of labeled data, if extra unlabeled data can be obtained, the performance will be further improved with the proposed semi-supervised learning framework.</p><p>In practical applications, labeled data is often limited, but unlabeled data is easy to obtain. To explore the impact of increasing the ratio of unlabeled data in the training set. We fix the labeled data as 1 /8 of the training set, and increase the number of the unlabeled data. The results are shown in <ref type="figure" target="#fig_6">Figure 4</ref>. From <ref type="figure" target="#fig_6">Figure 4</ref>, we can see that increasing ratios of the unlabeled data to the labeled data would improve the results, but the growth trend of the performance is gradually flattening limited by the capacity of the initial teacher model trained on the fixed labeled data. The performance even slightly drops if too many unlabeled images are introduced during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Impact of the strong augmentation</head><p>In this section, we will show the impact of the proposed strong augmentation and the DSBN. First, as shown in Table 1, we compare the results of the naive self-training and the self-training combined with strong augmentation and the proposed DSBN. Although self-training can improve the performance over the baseline, self training directly uses pseudo labels that makes the network difficult to learn and  <ref type="table">Table 1</ref> -Ablation study on the proposed semi-supervised learning framework. The model here is Deeplabv3plus with Xception65 backbone. 's.t.' denotes self-training with pseudo-labels without strong augmentation. 'SDA' means the strong data augmentation. 'DSBN' means the distribution specify batch normalization. 'SCL' is the proposed self correction loss. 'iter.' represents iterative training. The results are evaluated on the validation set with the single-scale input. 'mIoU-1 /n' means that we use 1 /n labeled data and the remaining images in the training set are used as unlabeled data. 'mIoU-Full' means that we use all the labeled data in the training set, and the train-coarse are employed as the unlabeled dataset.</p><p>introduces noise, as discussed in Section 2. Meanwhile, in the case of only using strong augmentation, there is no significant improvement in performance. On the contrary, our method performs favorably against naive self training with strong augmentation and DSBN. In particular, compared with naive self training, our method improves the mIoU by 1.1%, 0.7%, 0.7% and 0.5% based on DeepLabv3Plus. In the following sections, we analyze the distribution of the BN statistics, and then we compare the strong augmentation with different BN settings. Finally, we provided some discussions on how does the strong augmentation work with ground truth labels and the pseudo labels. Visualization of the BN statistics. To verify our statement that the distribution of the BN will be affect by the strong augmentation, we show the statistics of BN training with strong augmentation input images and the weakly augmentation input images. The visualization results can be found in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>The distribution of the mean and variance obtained from the whole training statistics deviates from the test set after adding strong augmentation. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, we calculate the distribution of the mean and the variance of the BN with the weakly augmentation and the strong augmentation. The blue line denotes the distribution of the BN with weakly augmentation, while the red line represents the BN with strong augmentation. We can see that strong augmentation has changed the distribution of BN statistics, which may lead to a domain gap. We think this is the reason why strong augmentation has not brought much performance improvement, as shown in <ref type="table">Table 1</ref>. Impact of the DSBN. To show the effectiveness of proposed DSBN, we conduct experiments to compare three different BN settings as follows:</p><p>? Trainable BN: the mean and the variance are updated during training with both weak augmentation data and strong augmentation data. ? Fixed BN: the mean and the variance are fixed during training, and initialized with the pre-trained weights from the ImageNet classification.</p><p>? DSBN: different mean and the variance are updated during training for weak augmentation data and strong augmentation data, separately.</p><p>The baseline model is DeepLabV3Plus trained with 1/8 labeled data on the training set. The other 7/8 images are used as the unlabeled dataset in the semi-supervised learning. The results are shown in <ref type="table">Table 2</ref>. We can see that with the proposed DSBN, the performance can be improved by 0.7%, which indicates the DSBN can help to solve the negative effect caused by the strong augmentation. Discussions. To further understand the impact of the strong augmentation and DSBN on different data groups, we apply strong augmentation to only ground truth labels and pseudo labels.</p><p>? 1 /8 ground truth. We train the model with only 1/8 images of the training dataset with corresponding ground truth labels. ? Full-ground truth. We train the model with all images of the training dataset with corresponding ground truth labels. ? 7 /8 pseudo labels. We train the model with 7/8 images of the training dataset with corresponding pseudo labels.</p><p>For each settings, we apply weekly augmentation, strong augmentation and DSBN during training. The experiment results are shown in <ref type="table">Table 3</ref>. We can see that under the fully supervised settings, the strong augmentation can not bring extra improvements. Meanwhile, when applying to the unlabeled data with pseudo labels, the mIOU improved  <ref type="table">Table 3</ref> -Effect of the strong augmentation on different data. We apply the strong augmentation to the baselines trained with only labeled data, and only unlabeled data with pseudo-labels, separately.</p><p>by 0.8%. These observations indicate that improvements under semi-supervised settings are not come from extra image transformation types on labeled data, but benefit from make further use of unlabeled data with strong augmentations. From <ref type="table">Table 3</ref>, we also find that the DSBN can contribute to the training. In addition, to our knowledge, this is the first time to apply the strong augmentation to semantic segmentation under fully-supervised setting, and improvement is observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Impact of the self-correction loss</head><p>To further improve the performance and reduce the impact of noise, we employ a self correction loss (SCL). From Table 1, we can see that the proposed SCL will play a more important role when the pseudo labels contain more noise. The teacher model trained with a limited labeled dataset may get less useful knowledge, and can not generalize well on unlabeled dataset. Thus, the level of the noise is higher than that training on more labeled data. When we train with 1 /8 or 1 /4 labeled images, the SCL can improve the mIoU by 0.5%. Furthermore, we show the mIoU during the training phase with and without the proposed SCL in <ref type="figure" target="#fig_7">Figure 5</ref>. When training without the SCL, the result will be divergent quickly. On the contrary, the result is relatively stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Impact of iterative training</head><p>Following Naive Students <ref type="bibr" target="#b2">[3]</ref>, we also test the impact of the iterative training in our semi-supervised learning framework. <ref type="table">Table 1</ref> has shown that the iteration training will become less useful if more labeled data can be obtained. Because the initial teacher model trained with more labeled data can already produce satisfactory pseudo labels. We show the change of the performance for the number of iterations in <ref type="figure" target="#fig_9">Figure 6</ref>. The baseline model is DeepLabV3Plus  <ref type="bibr" target="#b31">[32]</ref>. In addition, we set the same ratio between CE and RCE. The red line represents our proposed loss with the same initial weights as the SCE loss.  trained with 1 /8 labeled images. We can see that more iterations indeed help improve the performance, but the growth trend of the performance is gradually flattening. Visualization results Several visualization results are shown in <ref type="figure" target="#fig_10">Figure 7</ref>. We can see that the baseline and selftraining can not well separate the objects (especially largesize objects such as bus, truck, train, sidewalk) completely while ours corrects these errors. Our method also performs well on these small-size object classes, such as pole and bicycle, compared to the baseline model and self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with State-of-the-art Methods</head><p>Cityscapes. We conduct the comparison experiments with other state-of-the-art algorithms on the <ref type="table" target="#tab_2">Table 4</ref>. To make a fair comparison, we also apply our proposed framework to DeepLabV2 following <ref type="bibr" target="#b16">[17]</ref>. In particular, DeepLabV2 is based on ResNet-101 initialized with the pre-trained weights on ImageNet classification. The proposed framework achieves 67.6%, 69.3%, 70.7% with DeepLabV2 and 74.1%, 77.8%, 78.7% with DeepLabV3Plus, respectively, which performs favorably against previous state-of-   <ref type="table">Table 5</ref> -Comparison with state-of-the-art on the Pascal VOC val set (w/ unlabeled data). We use the official training set (1.4k) as labeled data, and the augmented set (9k) as unlabeled data. samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Additional Ablation Study with DeepLabV2</head><p>We conduct experiments to explore the effectiveness of each proposed module under 1 /8 labeled image settings. First, we establish the baseline for our experiments. We evaluate DeepLabV2 based on ResNet-101 on the validation set. Same as the experiments with the DeepLabV3Plus model, self-training adopts the baseline model as the teacher model and the teacher model generates pseudo-labels on the unlabeled dataset, then the student model which is not smaller than the teacher model is trained on pseudo labels and original labeled images. We use the scales including {0.5, 0.75, 1.0, 1.5, 1.75} and mirror for the remaining images to generate pseudo labels. For fair comparison, the student model is the same model ini-  tialized by the teacher model. The main results are shown in <ref type="table" target="#tab_3">Table 6</ref>. We also report detailed performance under different iterations in <ref type="figure" target="#fig_11">Figure 8</ref>. We can see that more iterations in general help improve the performance, but the growth trend of the performance is gradually flattening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we construct a simple semi-supervised learning framework for semantic segmentation. It employs strong augmentation with distribution-specific batch normalization, together with pseudo-label learning. In particular, DSBN can effectively avoid the BN statistics shift caused by strong augmentation. Meanwhile, we have also designed a self-correction loss, which effectively alleviates the label noises introduced by pseudo labeling. Quanti-tative and qualitative comparisons show that the proposed method performs favorably against recent state-of-the-art semi-supervised approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 -</head><label>1</label><figDesc>Distribution mismatch of BN statistics between weak augmentation and strong augmentation. Red and blue lines denote BN statistics trained with weak and strong data augmentation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, we build a pool of operations, which contains 16 image transforma-Transforms = ['contrast gamma', 'contrast linear', ' brightness', 'brightness channel', 'equalize', ' hsv', 'invert channel', 'blur','noise gau', ' noise pos', 'Channel shuffle', 'dropout', ' coarse dropout', 'multiply', 'salt pepper', ' solarize', 'jpeg compression'] Basic transforms = ['random scale', 'random flip', ' random crop', 'Normalization'] # data augmentation func def SDA(N): """ Generate a set of distortions. Args: N: Num. of augment. transform. to apply sequentially """ sampled ops = np.random.choice(Transforms, N) return sampled ops + Basic transforms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 -</head><label>2</label><figDesc>Python code for Strong Data Augmentation in Numpy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 -</head><label>3</label><figDesc>Distribution-Specific Batch Normalization (DSBN). During the training phase, images with strong augmentation are passed through BNs, while images with weak augmentation are passed through standard BNw. During the test phase, BNs is discarded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 -</head><label>4</label><figDesc>Different ratios of unlabeled data to labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 -</head><label>5</label><figDesc>This curve shows the validation mIoU w.r.t. the number of iterations during training. The green line represents the SCE loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 -</head><label>6</label><figDesc>The performance of the DeepLabV3Plus under different number of iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 -</head><label>7</label><figDesc>Qualitative results on the Cityscapes dataset. The baseline method is trained with 1 /8 of all labeled images. 'ST' represents the self-training trained with 7 /8 pseudo-labels without our methods. 'Ours' is our framework with 1 /8 labelled data. The proposed semi-supervised approach produces improved results compared to the baseline and the naive self training. We highlight the details in yellow boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 -</head><label>8</label><figDesc>Performance vs. number of iterations, using the DeepLabV2 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Model s.t. SDA DSBN SCL iter. mIoU-1 /8 (%) mIoU-1 /4 (%) mIoU-1 /2 (%) mIoU-Full (%)</figDesc><table><row><cell>Deeplabv3plus</cell><cell>68.9</cell><cell>73.4</cell><cell>76.9</cell><cell>78.7</cell></row><row><cell>Deeplabv3plus</cell><cell>71.2</cell><cell>76.2</cell><cell>78.0</cell><cell>79.5</cell></row><row><cell>Deeplabv3plus</cell><cell>71.3</cell><cell>76.3</cell><cell>78.2</cell><cell>79.7</cell></row><row><cell>Deeplabv3plus</cell><cell>72.3</cell><cell>76.9</cell><cell>78.7</cell><cell>80.0</cell></row><row><cell>Deeplabv3plus</cell><cell>72.8</cell><cell>77.4</cell><cell>78.7</cell><cell>80.4</cell></row><row><cell>Deeplabv3plus</cell><cell>74.1</cell><cell>77.8</cell><cell>78.7</cell><cell>80.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 -</head><label>4</label><figDesc>Compared with state-of-the-art methods on the Cityscapes val set. Here ' 1 /n' means that we use 1 /n labeled data and the remaining images in the training set are used as unlabeled data.the-art methods. More details of ablation studies of with DeepLabV2 can be found in the supplementary materials. Pascal VOC. We follow DeepLabV3Plus<ref type="bibr" target="#b3">[4]</ref> training details with the 513?513 crop size on Pascal VOC<ref type="bibr" target="#b10">[11]</ref>.Table 5lists the performance results of other state-of-the-art methods and ours. We adopt the single-scale testing on our experiments. In order to make a fair comparison, we did experiments under different backbones. Our framework achieves 75.0% mIoU based on Resnet-101<ref type="bibr" target="#b15">[16]</ref> and 79.3% mIoU based on Xception-65<ref type="bibr" target="#b3">[4]</ref>, which outperforms the PseudoSeg [42] by 1.8%. At the same time, we find that our results based on Xception-65 can achieve the same performance as that of the fully supervised setting. In addition, we find that the improvement of iterative training on Pascal VOC is limited, which might be due to the small number of</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell>1 /8</cell><cell>1 /4</cell><cell>1 /2</cell><cell>Full</cell><cell>Method</cell><cell>Model</cell><cell>mIoU</cell></row><row><cell>AdvSemiSeg [17] S4GAN + MLMT [23] CutMix [13] DST-CBC [12] ClassMix [24]</cell><cell>DeepLabV2 DeepLabV2 DeepLabV2 DeepLabV2 DeepLabV2</cell><cell cols="4">58.8 62.3 65.7 66.0 59.3 61.9 -65.8 60.3 63.87 -67.7 60.5 64.4 -66.9 61.4 63.6 66.3 66.2</cell><cell cols="2">GANSeg [29] AdvSemSeg [17] ResNet-101 VGG16 CCT [25] ResNet-50 PseudoSeg [42] ResNet-101</cell><cell>64.1 68.4 69.4 73.2</cell></row><row><cell cols="6">ECS [22] DeepLabv3Plus 67.4 70.7 72.9 74.8</cell><cell cols="2">Ours ResNet-101</cell><cell>75.0</cell></row><row><cell>Baseline</cell><cell>DeepLabV2</cell><cell cols="4">60.6 66.7 69.3 70.1</cell><cell cols="3">Ours Xception-65 79.3</cell></row><row><cell cols="6">Ours Baseline DeepLabV3Plus 68.9 73.4 76.9 78.7 DeepLabV2 67.6 69.3 70.7 70.1 Ours DeepLabV3Plus 74.1 77.8 78.7 78.7</cell><cell cols="3">Fully supervised ResNet-101 Fully supervised Xception-65 79.2 78.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 -</head><label>6</label><figDesc>Model s.t. SDA DSBN SCL iter. mIoU (%) Ablation study on the proposed semi-supervised learning framework. Baselines are ResNet101-based Deeplabv2. 's.t.denotes self-training with pseudo labels without strong augmentation. SDA means the strong data augmentation. DSBN means the distribution specify batch normalization. SCL is the proposed self correction loss. 'iter.' represents iterative training. The results are evaluated on the validation set with the singlescale input. Except the last row, all other experiments use 1 /8 labeled data and the remaining images in the training set are used as unlabeled data. '? Full' means the fully supervised setting.</figDesc><table><row><cell>DeepLabV2</cell><cell>60.6</cell></row><row><cell>DeepLabV2</cell><cell>62.1</cell></row><row><cell>DeepLabV2</cell><cell>62.2</cell></row><row><cell>DeepLabV2</cell><cell>63.8</cell></row><row><cell>DeepLabV2</cell><cell>64.5</cell></row><row><cell>DeepLabV2</cell><cell>67.6</cell></row><row><cell>DeepLabV2-Full</cell><cell>70.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst., 2020</title>
		<meeting>Adv. Neural Inform. ess. Syst., 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation via dynamic self-training and classbalanced curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform</title>
		<meeting>Adv. Neural Inform</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial learning for semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ting</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Guided collaborative training for pixel-wise semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05258</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation via strong-weak dual-branch network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised segmentation based on error-correcting supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Antonio De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="141" to="157" />
		</imprint>
	</monogr>
	<note>Jo?o Paulo Papa, and Christoph Palm</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and lowlevel consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ClassMix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Conf. Appl. Comput. Vis</title>
		<meeting>Winter Conf. Appl. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12674" to="12684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FixMatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst., 2020. 1</title>
		<meeting>Adv. Neural Inform. ess. Syst., 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5688" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transferable normalization: Towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst., 2020</title>
		<meeting>Adv. Neural Inform. ess. Syst., 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi receptive field network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Conf. Appl. Comput. Vis</title>
		<meeting>Winter Conf. Appl. Comput. Vis</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1883" to="1892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Split batch normalization: Improving semi-supervised learning under domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Zajkac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzkebski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. 2nd Learning from Limited Labeled Data Workshop</title>
		<meeting>Int. Conf. Learn. Represent. 2nd Learning from Limited Labeled Data Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PseudoSeg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Jia-Bin Huang, and Tomas Pfister</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
