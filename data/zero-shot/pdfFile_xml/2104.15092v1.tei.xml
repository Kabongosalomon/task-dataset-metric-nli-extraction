<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Faster Meta Update Strategy for Noise-Robust Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Xu</surname></persName>
							<email>youjiangxu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
							<email>lujiang@google.com</email>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Faster Meta Update Strategy for Noise-Robust Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It has been shown that deep neural networks are prone to overfitting on biased training data. Towards addressing this issue, meta-learning employs a meta model for correcting the training bias. Despite the promising performances, super slow training is currently the bottleneck in the meta learning approaches. In this paper, we introduce a novel Faster Meta Update Strategy (FaMUS) to replace the most expensive step in the meta gradient computation with a faster layer-wise approximation. We empirically find that FaMUS yields not only a reasonably accurate but also a low-variance approximation of the meta gradient. We conduct extensive experiments to verify the proposed method on two tasks. We show our method is able to save two-thirds of the training time while still maintaining the comparable or achieving even better generalization performance. In particular, our method achieves the stateof-the-art performance on both synthetic and realistic noisy labels, and obtains promising performance on long-tailed recognition on standard benchmarks. Code are released at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have achieved impressive results in various computer vision applications such as image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref>, object detection <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29]</ref>, and semantic segmentation <ref type="bibr" target="#b11">[12]</ref>. A notable issue is that DNNs are prone to memorizing the training data <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b46">47]</ref>, aggravating training set bias such as noisy training labels <ref type="bibr" target="#b59">[60]</ref> or imbalanced class distributions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b63">64]</ref>. This significantly degrades the generalization capabilities and results in skewed classifiers or degenerated feature representations.</p><p>Numerous works have been proposed to tackle this issue (e.g. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>). Among them, meta-learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref> has recently emerged as an effective framework to mitigate the training data bias. In a nutshell, it employs a meta-model to correct bias by providing a more precise estimation of the training data. The meta-model is updated by stochastic gradient descent using the meta gradient (or  <ref type="bibr" target="#b25">[26]</ref>. We apply our method on the MW-Net model <ref type="bibr" target="#b42">[43]</ref> and train them using the identical hardware platform of four NVIDIA V100 GPUs. (b) The average GPU running time (in seconds) of each step in MW-Net per training iteration. Inception-ResNet V2 is used as the backbone. (c) The meta gradient during the training process. The solid line denotes the mean and the shaded region show the standard deviation.</p><p>the high-order gradient) computed on a small proportion of validation data that is assumed available during training <ref type="bibr" target="#b0">1</ref> .</p><p>Recently, meta-learning approaches such as L2R <ref type="bibr" target="#b39">[40]</ref>, MW-Net <ref type="bibr" target="#b42">[43]</ref>, and MLC <ref type="bibr" target="#b50">[51]</ref> have shown superior performance on several public benchmarks such as CIFAR <ref type="bibr" target="#b21">[22]</ref>, WebVision <ref type="bibr" target="#b25">[26]</ref>, and Clothing1M <ref type="bibr" target="#b53">[54]</ref>. Despite the promising empirical results <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b43">44]</ref>, slow training is currently the bottleneck that prevents metalearning from being applied in many applications. The training time of the meta-learning model is approximately 3?7 times more than the regular DNN training time. For instance, it could take 4 days with 4 NVIDIA V100 GPUs to train MW-Net <ref type="bibr" target="#b42">[43]</ref> on a mini subset of WebVision <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20]</ref> of only ?50K images.</p><p>To understand why the meta-learning approaches are computationally intensive, we may divide the training into three stages: Virtual-Train, Meta-Train, and Actual-Train <ref type="bibr" target="#b50">[51]</ref>, where each stage consists of a forward and a backward step. <ref type="figure" target="#fig_0">Figure 1(b)</ref> summarizes the GPU time for each stage using a representative meta-learning model called MW-Net <ref type="bibr" target="#b42">[43]</ref>. We find more than 80% of the total computation comes from the Meta-Train backward step in which the meta gradient is computed with respect to the loss on the validation data. In this step, the meta gradient is back-propagated through every layer of the network all the way back to the meta-model to update its parameters. Since the regular training does not have such a step, this overhead cost rapidly becomes significant as the number of layers grows in the deep networks.</p><p>In this work, we aim at improving the training efficiency of meta-learning while maintaining the generalization capability. We propose a new Meta-Train step, named Faster Meta Update Strategy (FaMUS), to efficiently compute the meta gradient. The plausibility of our method relies on the important finding that the total meta gradient can be reasonably approximated by the meta gradient accumulated from only a few network layers. As a result, instead of accumulating meta gradients from all layers in the Meta-Train step, we design a gradient sampler that is learned to decide, whether or not, to aggregate the meta gradient for each layer. When the learnable gradient sampler is turned off, the meta gradient computation is hence circumvented for the corresponding layer. This saves a considerable amount of computation especially when the gradient samplers for lower layers are turned off.</p><p>More importantly, we find the meta gradient yielded by the FaMUS has lower variance. <ref type="figure" target="#fig_0">Figure 1</ref>(c) shows the total meta gradient of the ground-truth (blue curve) and the approximation by the FaMUS (red curve). It shows that our approximation is reasonably close to the mean but has a much lower variance. We hypothesize this is because the FaMUS learns to select a small number of most informative layers which hence reduces the noisy or redundant signals in the meta gradient. As shown in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>, reduction in gradient variance results in faster and more stable optimization. We observe similar results in our experiments where our method is able to improve the generalization performance of the recent meta-learning methods on noisy training data.</p><p>We conduct extensive experiments to verify the efficiency and efficacy of the proposed method. We demonstrate two benefits of our method in overcoming corrupted training labels. First, it speeds up the recent meta-learning methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref> by at least three times while main-taining the comparable or even better generalization performance. For example, <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows a faster and better convergence when we applied our method on the MW-Net model. Second, our method achieves new state-of-the-art performance on multiple benchmarks for both synthetic label noise and realistic label noise, including the challenging CNWL benchmark <ref type="bibr" target="#b17">[18]</ref>. The comparison is fair as our meta-model is learned without using any extra data. In addition, we also validate our method on the long-tailed recognition task. On the long-tailed CIFAR dataset <ref type="bibr" target="#b5">[6]</ref>, our method yields competitive performance compared to the recent strong baseline methods.</p><p>The contributions of this paper are three-fold. (1) We propose a new Faster Meta Update Strategy to efficiently learn to approximate the meta gradient, which halves twothirds of the training time of the recent meta-learning methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>. <ref type="bibr" target="#b1">(2)</ref> We empirically show our approach reduces the variance of the meta gradient and improves the generalization performance of the meta-learning model. (3) Our method achieves state-of-the-art performance on several benchmarks with noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Corrupted/Noisy training labels. Numerous methods have been recently proposed to learn robust deep networks that can overcome corrupted or noisy training labels. These methods address this problem from a variety of directions. For example, several works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b50">51]</ref> modeled the noise distribution or the transition matrix to correct noisy training samples. Other approaches tried to reduce the weights assigned to noisy samples <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref>. Another effective strategy is to directly identify the clean samples and only select them to train the models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b51">52]</ref>. Other contributions in this direction include data augmentation <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref>, semi-supervised learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b61">62]</ref>, etc.</p><p>Among them, meta-learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51]</ref> has recently emerged as an effective framework for addressing the noisy labels. These methods all learn a meta-model from clean validation examples but differ in the specific ways to correct the biased training labels. For example, L2R <ref type="bibr" target="#b39">[40]</ref> directly adjusts the weight for each example. MLNT <ref type="bibr" target="#b24">[25]</ref> simulates regular training with synthetic noisy labels. MW-Net <ref type="bibr" target="#b42">[43]</ref> learns an explicit weighting function. MLC <ref type="bibr" target="#b50">[51]</ref> estimates the noise transition matrix.</p><p>This paper aims at improving the training efficiency of the meta-learning models. The results show our method not only significantly reduces the training time of three recent meta-learning approaches but also improves their robustness to noisy labels on several standard benchmarks. Long-tailed recognition. Long-tailed recognition has been an active research field in computer vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. For example, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> aimed to increase the number of minority classes by oversampling, while Drummond et al. <ref type="bibr" target="#b6">[7]</ref> solved this problem by reducing the number of data in majority classes. Some recent studies <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b31">32]</ref> proposed to balance the number of data for each class. <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b29">30]</ref> applied the knowledge learned from the head classes to the tail. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b62">63]</ref> aimed to manipulate the loss on the class-level based on the data distribution.</p><p>Meta-learning based methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b15">16]</ref> have recently achieved promising results on the long-tailed recognition task, in which the meta-model is learned to assign larger weights to the examples of the long-tailed classes. Similar to the noisy labels, meta-learning suffers from slow training speed <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b15">16]</ref>. We show our method improves the efficiency and accuracy of the meta-learning methods on the long-tailed recognition task, and achieves competitive performance compared with recent strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary on Meta-learning</head><p>In this section, we briefly introduce the preliminary on meta-learning methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref> that learn robust deep neural networks from noisy labels by reweighting the training data. We follow the notation in the MW-Net [43] model using corrupted labels as an example. Alternative formulation can be found in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b49">50]</ref>.</p><formula xml:id="formula_0">Let D train = {(x tra i , y tra i )} N i=1 be a noisy training set of N examples, where x tra i is the i-th training image and y tra i ? {0,</formula><p>1} c is its one-hot label over c classes. Consider a deep neural network (DNN) as the base model ?(?; w) with w denoting its parameters. Generally, we can derive the optimal parameter w * by minimizing the softmax cross-entropy loss (?, y) over the training data, wher? y = ?(x; w) is the prediction of the DNN and y is the given label for the input image x.</p><p>In the meta-learning methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref>, there is an outof-sample validation set</p><formula xml:id="formula_1">D val = {(x val j , y val j )} M j=1 , where (x val j , y val j ) denote the j-th example. M is the size of D val and M N .</formula><p>The extra validation dataset is not always required in meta-learning. See the discussion in Section 5.3.</p><p>The meta-learning method employs a meta-model (e.g., instanced by a multilayer perceptron network (MLP) with only one hidden layer <ref type="bibr" target="#b42">[43]</ref>) to learn a weight for each training example. Let ?(?; ?) denote the meta-model, parametrized by ?, which maps a loss to a weight scalar. A meta-model can be regarded as a learnable derivation of the self-paced function in SPCL <ref type="bibr" target="#b18">[19]</ref>. Let L tra i (w) = (?(x tra i ; w), y tra i ) be the loss for the i-th example in D train . The optimal parameter w * can be obtained by computing the weighted loss:</p><formula xml:id="formula_2">w * (?) = argmin w 1 N N i=1 V tra i (?)L tra i (w),<label>(1)</label></formula><p>where V tra i (?) = ?(L tra i (w); ?) is the generated weight for the i-th training example. The meta-model is optimized by minimizing the validation loss:</p><formula xml:id="formula_3">? * = argmin ? 1 M M j=1 L val j (w * (?)),<label>(2)</label></formula><p>where L val j (w * (?)) = (?(x val j ; w * (?)), y val j ) is the loss for the j-th example in the validation set.</p><p>Solving Eq. (1) and Eq. (2) by alternating minimization is intractable for mini-batch gradient descent. Alternatively, an online optimization method is used instead which comprises three steps: Virtual-Train, Meta-Train, and Actual-Train <ref type="bibr" target="#b51">[52]</ref>.</p><p>Consider the t-th iteration. Given a training mini-batch</p><formula xml:id="formula_4">B train = {(x tra i , y tra i )} n i=1 and a validation mini-batch B val = {(x val j , y val j )} m j=1</formula><p>, n and m stand for the number of the examples in the mini-batch. For the Virtual-Train, an one-step "virtually" updated DNN can be derived by:</p><formula xml:id="formula_5">w(?) = w ? ? 1 n n i=1 V tra i (?)? w L tra i (w),<label>(3)</label></formula><p>where ? is the learning rate for the DNN. w is the parameter of the base DNN at the current iteration. This step is called Virtual-Train because?(?) will not be used to update the parameter of the base DNN. Then for the Meta-Train, with the latest?(?), the metamodel is updated by:</p><formula xml:id="formula_6">? = ? ? ? 1 m m j=1 ? ? L val j (?(?)).<label>(4)</label></formula><p>Similarly, ? is the learning rate for the meta-model. ? is the parameter of the updated meta-model. Notice that 1 m m j=1 ? ? L val j (?(?)) is called meta gradient, which is expensive to compute. More details will be discussed in Section 4.1.</p><p>Finally, in the last step (Actual-Train), the updated metamodel ?(?; ? ) is used to update the base DNN model using:</p><formula xml:id="formula_7">w = w ? ? 1 n n i=1 V tra i (? )? w L tra i (w),<label>(5)</label></formula><p>where V tra i (? ) is the weight for the i-th example computed by the latest meta-model. This step is called Actual-Train because w will be used to actually update the parameter of base DNN. Therefore, w becomes the w in Eq. (3) in the (t + 1)-th iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Faster Meta Update Strategy</head><p>In this section, we introduce a Faster Meta Update Strategy (FaMUS) to efficiently approximate the total meta gradients by a layer-wise meta gradient sampling procedure.</p><formula xml:id="formula_8">! " reweight ! #$% &amp; '()*+ ! % ! # ? ? '()*+ ?(/ ; 1) Meta Model &amp; 3)4 5 ! % 5 ! " 5 ! #$% 5 ! # ? ? 7 % '() ? 7 " '() ? 7 #$% '() ? 7 # '() ? ? 3)4 ? ? ? ? 9 #$% = 0 9 # = 1 9 " = 0 9 % = 0 9 4 = 1 ? 7 4 '()</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual-Train Forward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual-Train Backward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Train Forward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Train Backward</head><p>One-step update</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer-wise Gradient Sampling</head><p>Accumulating Stop Accumulating <ref type="figure">Figure 2</ref>: Illustration of the proposed method. We propose a new Meta-Train step, named Faster Meta Update Strategy (i.e., the red line 4 ), which learns a gradient sampler (denoted as ?) to aggregate the meta gradient for each layer. In this figure, the meta gradients from the l-th and L-th layers would be aggregated to compute g and used to update the meta-model ?. <ref type="figure">Figure 2</ref> presents the overall training process, where the red line indicates the proposed method. Specifically, we learn a gradient sampler to decide, whether or not, to aggregate the meta gradient for each layer. In the following, we first explain how the meta gradient can be calculated in a layerwise fashion in Section 4.1. Next, we detail the gradient sampler in Section 4.2 and the final objective for the metamodel in Section 4.3. The full algorithm for Faster Meta Update Strategy is shown in the supplementary materials.</p><formula xml:id="formula_9">update g = { 5 ! 4 } 4@% # Ave-Pool ! 4 ? 5 ! 4 ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Layer-wise meta gradient computation</head><p>In this section, we discuss the meta gradient computation and show how it can be calculated in a layer-wise fashion. For notational convenience, we simplify Eq. (4) as:</p><formula xml:id="formula_10">? = ? ? ? ? g,<label>(6)</label></formula><p>where g = 1 m m j=1 ? ? L val j (?(?)) denotes the meta gradient, which has shown to be computational intensive in recent studies <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Without loss of generality, suppose that the base DNN has L layers denoted as ?(?; {w l } L l=1 ), where w l represents the parameter for the l-th layer.</p><p>We rewrite the computation of meta gradient using the chain rule:</p><formula xml:id="formula_11">g = 1 m m j=1 ?L val j (?(?)) ??(?) n i=1 ??(?) ?V tra i (?) ?V tra i (?) ?? ? ?? nm L l=1 n i=1 m j=1 G i,j,l ?V tra i (?) ?? ,<label>(7)</label></formula><p>where G i,j,l = (</p><formula xml:id="formula_12">?L val j (?) ?? l ) T ?L tra i (w) ?w l</formula><p>is the dot product between the gradient from the j-th validation loss w.r.t.? l and the gradient from the i-th training loss w.r.t. w l . Intuitively, G i,j,l can be viewed as the similarity between the i-th training example and the j-th validation example according to the l-th layer of the base network. The derivation of Eq. <ref type="formula" target="#formula_11">(7)</ref> can be found in the supplementary materials.</p><p>Two observations can be drawn from Eq. <ref type="bibr" target="#b6">(7)</ref>. First, it explains the slow Meta-Train step in meta-learning, i.e. computing the meta gradient involves enumerating all training examples, all validation examples, and all layers. Second, it shows that the meta gradient can be calculated by first computing the gradient i,j G i,j,: within each individual layer and then aggregating the values together. This finding lays a foundation for the proposed layer-wise meta gradient approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Layer-wise gradient sampler</head><p>We propose to approximate the total meta gradient by aggregating meta gradients sampled from a few layers. We learn a gradient sampler to accumulate the meta gradient for each layer, and formulate the gradient sampler, denoted as ?(?; ? l ), as follow:</p><formula xml:id="formula_13">r l = ?(? tra l ; ? l ) = ?(Avg-Pool( 1 n n i=1 V tra i (?) ?L tra i (w) ?w l ); ? l ).<label>(8)</label></formula><p>The output to the gradient sampler is the discrete activation status r l ? {0, 1}.</p><p>The input of the gradient sampler is the average gradient? tra l obtained from the Virtual-Train backward step. To be more specific, suppose the gradient tensor for the convolutional kernel has the shape R Dout?Din?K1?K2 where D out and D in are the output/input dimensions; K 1 and K 2 are the kernel sizes. The Avg-Pool operator averages the gradient tensor across all except the first dimensions while leaving the bias term unchanged. Therefore, the dimension for? tra l ? R 1?Dout . The Avg-Pool performs a similar operation for the fully connected layer by setting K 1 = K 2 = 1.</p><p>For efficiency, we adopt a lightweight design for the gradient sampler and implement it by two fully-connected (FC) layers: FC 1 and FC 2 , where the first layer FC 1 is followed by a PReLU layer and FC 2 by the "Gumbel-softmax" operator <ref type="bibr" target="#b16">[17]</ref>. The hidden size of the fully connected layer is fixed to 128 for all experiments.</p><p>Applying the gradient sampler to all layers gives:</p><formula xml:id="formula_14">g ? ?? nm L l=1 1 [r l =1] n i=1 m j=1 G i,j,l ?V tra i (?) ?? ,<label>(9)</label></formula><p>where 1 [r l =1] is the indicator function. As shown in Eq. <ref type="formula" target="#formula_14">(9)</ref>, the meta gradient for the l-th layer is accumulated only if the gradient sampler is turned on (i.e. r l = 1). Finally, we replace g in Eq. (6) with g to update the meta-model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training objective for meta-model</head><p>The proposed gradient samplers are jointly optimized with the meta-model. In addition to the cross-entropy loss described in L val in Eq. (4), we incorporate two auxiliary losses to facilitate learning the gradient samplers.</p><p>The first loss is designed to prevent the gradient samplers from activating too many layers. We introduce a loss L r regularizing the output of the gradient samplers:</p><formula xml:id="formula_15">L r = L l=1 r l ? K 2 2 ,<label>(10)</label></formula><p>where K is the expected number of layers to be activated. Moreover, we add another loss (denoted as L g ) to facilitate learning the meta-model:</p><formula xml:id="formula_16">L g = ? tra L ?? val L 2 2 ,<label>(11)</label></formula><p>where? tra L is the average gradient from training loss discussed in Eq. <ref type="bibr" target="#b7">(8)</ref>.</p><p>Likewise? val L is the average gradient from the validation loss, i.e.? val</p><formula xml:id="formula_17">L = Avg-Pool( 1 m m j=1 ?L val j (?)</formula><p>?? L ). This loss term L g captures the prior knowledge that the distance between validation and training gradient should be close. Notice that we only compute the gradients at the last layer L for efficiency.</p><p>Finally, the total loss to update the meta-model:</p><formula xml:id="formula_18">L val = L c + ? 1 L r + ? 2 L g ,<label>(12)</label></formula><p>where L c is the standard cross-entropy loss in Eq. (4). ? 1 and ? 2 are hyperparameters. We will examine the effectiveness of these loss terms in the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct extensive experiments on the noisy labeled data to verify the efficiency and effectiveness of our method  for learning robust DNN models. Specifically, we show our method improves the efficiency and generalization performance of the meta-learning methods in Section 5.1. Section 5.2 presents ablation studies to verify our design choices. Section 5.3 compares with the state-of-the-art results on synthetic and realistic noisy labels. In addition, we also experiment on the long-tailed recognition task in Section 5.4. The implementation details and more experimental results are presented in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with meta-learning methods</head><p>This subsection shows our method improves the efficiency and generalization performance of three metalearning methods: L2R <ref type="bibr" target="#b39">[40]</ref>, MW-Net <ref type="bibr" target="#b42">[43]</ref>, and MLC <ref type="bibr" target="#b50">[51]</ref>.</p><p>Setups. We apply our method to three meta-learning methods using their official code and train them under the same settings as reported in their papers <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>. This includes using the same clean validation set to learn the meta-model. The experiments are conducted on the standard CIFAR <ref type="bibr" target="#b21">[22]</ref> benchmarks. Following <ref type="bibr" target="#b50">[51]</ref>, we use the symmetric label noise in which a percentage of true labels are randomly replaced with all possible labels, and report the best peak accuracy which is the maximum accuracy on the clean test set during training.</p><p>Implementation details. The proposed gradient samplers are jointly optimized with the meta-model by SGD with a momentum of 0.9. The learning rate is fixed as 0.1 throughout the training. ? 1 and ? 2 are both set to 0.1. K is set to 4. <ref type="table">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> show the results on the CIFAR datasets, where "Time" column lists the average running time (in millisecond) per training iteration on a single NVIDIA V100 GPU. It shows that our method accelerates the training time of the three meta-learning methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>   method improves their generalization performance across all noise rates. To understand the training dynamics, we compare three methods: the best baseline MW-Net <ref type="bibr" target="#b42">[43]</ref>, the MW-Net with our method, and the Random MW-Net in which each layer is randomly sampled to compute the meta gradient. <ref type="figure" target="#fig_1">Figure 3</ref> shows the training curves on the CIFAR-100 dataset with 60% noise. We observe that our method (MW-Net + FaMUS) has the lowest test loss and the highest accuracy throughout the training. We hypothesize that this benefit is related to the low variance in the meta gradient learned by our method. For example, <ref type="figure">Figure 4</ref> visualizes the variance of the meta gradient during training and shows our method yields a low-variance approximation of the meta gradient. These results suggest that FaMUS can learn to select a small number of most informative layers to compute the meta gradient, which hence reduces the noisy learning signals in the corrupted training data. This observation agrees with the finding in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref> that reduction in gradient variance results in faster and more stable optimization. To substantiate this hypothesis, we examine the meta-models by plotting their weight distribution on the clean examples in <ref type="figure">Figure 5</ref>. We find that in both the early (6K step) and late training stages (20K step), the metamodels learned by our method tend to assign larger weights   to more clean examples. The results in <ref type="table">Table 1</ref>, <ref type="table" target="#tab_1">Table 2</ref>, and <ref type="figure" target="#fig_1">Figure 3</ref> demonstrate that our method improves the efficiency and generalization performance of the meta-learning methods. More results can be found in the Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>We conduct the ablation studies using the MW-Net method with the WideResNet-28-10 backbone model <ref type="bibr" target="#b58">[59]</ref>.</p><p>Loss function. <ref type="table" target="#tab_3">Table 3</ref> analyzes the impact of the auxiliary loss components in Eq. (12) on the CIFAR datasets with 60% noise. "L c " denotes the loss of the base meta-learning model (MW-Net). We find that adding "L r " significantly reduces the training time since it limits the number of layers to be activated. When incorporating both "L r " and "L g ", our method achieves the best result and improves both the efficiency and accuracy of the base MW-Net model.</p><p>Sampling strategy. To verify the design of the proposed gradient sampler, we compare with two predefined sampling strategies: pre-specified block and random layers. In the Pre-specified Block, we select a residual block (indexed by b and b ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>), which consists of two convolutional layers and two batch normalization layers, to compute the meta gradients. In the Random Layers, we uniformly select s layers to compute the meta gradients. <ref type="table" target="#tab_4">Table 4</ref> shows the comparison on the CIFAR-100 dataset with 60% noise rate. For the Pre-specified Block, we find that computing the meta gradient from the top residual Method CIFAR-10 CIFAR-100 40% 60% 40% 60% Co-teaching <ref type="bibr" target="#b8">[9]</ref> 74.81 73.06 46.20 35.67 L2R <ref type="bibr" target="#b39">[40]</ref> 86.92 82. <ref type="bibr" target="#b23">24</ref>   <ref type="table">Table 5</ref>: Comparison with the state-of-the-art on CIFAR-10 and CIFAR-100 with 40% and 60% noise rates. ? denotes the results are reported by <ref type="bibr" target="#b17">[18]</ref>.</p><p>block (b = 12) is the most efficient way, which is about 2x faster than using the bottom block (b = 4). As for the Random Layers, the accuracy is improved as the number of layers s increases, while the running time shows a different trend. Our method outperforms all the compared methods both in efficiency and accuracy, suggesting the necessity of the proposed gradient sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison to state-of-the-art</head><p>This subsection compares our method with the state-ofthe-art robust learning methods in overcoming both synthetic and realistic noisy labels.</p><p>Datasets. For the realistic noisy labels, we employ three datasets: (mini) WebVision 1.0 <ref type="bibr" target="#b25">[26]</ref>, Clothing1M <ref type="bibr" target="#b53">[54]</ref>, and Controlled Noisy Web Labels (CNWL) <ref type="bibr" target="#b17">[18]</ref>. WebVision contains 2.4 million images with noisy labels categorized into the same 1,000 classes as in the ImageNet ILSVRC12. Following the previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>, we use the first 50 classes of the Google image subset as the training data. Clothing1M has 1 million noisy labeled clothing images crawled from online shopping websites. CNWL is a recent benchmark of controlled label noise from the web. Uniquely, it allows for comparing methods on various rates of realistic label noises. We use the Red Mini-ImageNet set <ref type="bibr" target="#b48">[49]</ref> that consists of 50K images from 100 classes for training and 5K images for testing. Implementation details. To fairly compare with the thestate-of-art, we use a subset of pseudo labeled training data as the meta-learning validation set. Inspired by <ref type="bibr" target="#b23">[24]</ref>, we employ the Gaussian Mixture Model (GMM) to divide the training data into a pseudo-clean and a pseudo-noisy label set. By doing so, no extra clean labels nor data are used to train the meta-model. We find using the pseudo validation set notably improves the performance because the pseudo validation set is much larger than the clean validation set used in the meta-learning method <ref type="bibr" target="#b42">[43]</ref>. More discussions can be found in the Appendix E.   <ref type="table">Table 7</ref>: Results on Controlled Noisy Web Labels <ref type="bibr" target="#b17">[18]</ref>.</p><p>For experiments on CIFAR-10, CIFAR-100, and CNWL, we employ the PreAct ResNet-18 <ref type="bibr" target="#b13">[14]</ref> as the base DNN. For experiments on Clothing1M and WebVision, we use ResNet-50 <ref type="bibr" target="#b12">[13]</ref> and Inception-ResNet V2 <ref type="bibr" target="#b45">[46]</ref>, respectively.</p><p>Baselines. We briefly introduce the baselines: (1) Coteaching <ref type="bibr" target="#b8">[9]</ref>, Decoupling <ref type="bibr" target="#b32">[33]</ref>, and JoCoR <ref type="bibr" target="#b51">[52]</ref> train two networks to improve each other. (2) F-correction <ref type="bibr" target="#b36">[37]</ref> estimates the noise transition matrix to correct the loss function. (3) D2L <ref type="bibr" target="#b30">[31]</ref> learns to monitor the dimensionality of subspaces and adapts the loss functions accordingly. (4) Iterative-CV <ref type="bibr" target="#b3">[4]</ref> iteratively increases the number of the selected samples to train the networks. (5) MentorNet <ref type="bibr" target="#b19">[20]</ref> is an example-weighting method based on curriculum learning. MentorMix <ref type="bibr" target="#b17">[18]</ref> further combines the MentorNet with the Mixup <ref type="bibr" target="#b60">[61]</ref>. (6) DivideMix <ref type="bibr" target="#b23">[24]</ref> addresses the corrupted labels in a semi-supervised learning fashion. (7) Mcorrection <ref type="bibr" target="#b0">[1]</ref> estimates the probability of a sample being mislabelled and then corrects the loss accordingly. <ref type="table">Table 5</ref> shows the results on the CIFAR-10 and CIFAR-100 datasets with symmetric label noises. For the compared methods, we directly cite the reported numbers in their papers except for MW-Net <ref type="bibr" target="#b42">[43]</ref> and L2R <ref type="bibr" target="#b39">[40]</ref> where we report the reproduced results. For our method, we report the average and standard deviation of over three training trials using different random seeds. The gains over baseline methods are statistically significant at the p-value level of 0.05, according to the one-tailed t-test. These results illustrate the   <ref type="bibr" target="#b1">[2]</ref>. effectiveness of our method on the synthetic noisy labels. <ref type="table" target="#tab_7">Table 6</ref> shows the results on the WebVision dataset. As shown, our method consistently outperforms the baselines, achieving the best accuracy on the validation sets of We-bVision and ImageNet. In particular, our method performs favorably against very recent methods such as Men-torMix <ref type="bibr" target="#b17">[18]</ref> and DivideMix <ref type="bibr" target="#b23">[24]</ref> in the top-1 accuracy. <ref type="table">Table 7</ref> shows the results on the CNWL dataset. We implement several strong baselines using their official codes released on the CIFAR-100 dataset, e.g., MentorMix <ref type="bibr" target="#b17">[18]</ref>. Note that in order to use their implementation, we downsample the images of the CNWL Mini-ImageNet dataset from 84x84 to 32x32. This results in new benchmark numbers to compare our baseline methods, and supplements <ref type="bibr" target="#b17">[18]</ref>'s results on 32x32 images. More details are discussed in the Appendix E. <ref type="table">Table 7</ref> shows that our method outperforms all baseline methods on the realistic web noisy labels. The result is notable because 1) it verifies our method on the challenging CNWL dataset; 2) it demonstrates our consistent improvement across all noise rates as a useful and robust feature since the underlying noise rate is often unknown in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Results on synthetic noisy labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results on realistic noisy labels</head><p>We also apply our method on the Clothing1M dataset, and achieve 74.4% in top-1 accuracy without using extra clean data, which is comparable to recently published methods. The results on the above three datasets demonstrate that our method trained with a noisy validation set is effective for addressing the realistic noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Long-tailed recognition task</head><p>In addition to the noisy training label problem, we also evaluate our method on the long-tailed recognition task.</p><p>Datasets and implementation details. Four imbalanced factors {100, 50, 20, 10} are applied on the long-tailed CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b5">[6]</ref>. The number of training samples for each class is randomly removed by n i ? i , where i indicates the class index, n i is the original number of the training samples for the i-th class, and ? ? (0, 1). The imbalanced factor is the ratio between the largest and the smallest class. Following <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b15">16]</ref>, we do not change the test set and select ten training images per class as the clean validation set. Our method is implemented on the MW-Net model <ref type="bibr" target="#b42">[43]</ref> with the ResNet-32 backbone <ref type="bibr" target="#b12">[13]</ref>.</p><p>From <ref type="table" target="#tab_9">Table 8</ref>, we find our method consistently outperforms previous meta-learning based methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b15">16]</ref>. Moreover, our method accelerates the training of the metalearning model MW-Net by 2.9 times. It is noteworthy that even compared to the very recent approaches (e.g., improved L2R <ref type="bibr" target="#b15">[16]</ref>), our method still obtains a reasonable performance gain, which illustrates the effectiveness of our method on the long-tailed recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we discuss a novel Faster Meta Update Strategy (FaMUS) to efficiently approximate the meta gradients by a layer-wise meta gradient sampling fashion. We empirically show that our method yields not only an accurate but also a low-variance approximation of the meta gradient. The experimental results demonstrate that FaMUS is able to reduce two-thirds of the training time of the metalearning methods, while achieving a better generalization performance. Our method yields the state-of-the-art performance to address the noisy label problem, and obtains competitive performance on the long-tailed recognition task.</p><p>We find meta-model training is considerably influenced by the quantity and quality of the pseudo-clean label set. Future research in this area may include improving the robustness on limited validation data or low-quality pseudo validation data, in addition to further closing the gap in training time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Top-1 accuracy vs. Training time (in hours) on the WebVision dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>by at least 3 times. More importantly, our Test curves under CIFAR-100 with 60% noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Variance of the meta gradient produced by different methods during the training. All models are trained on the CIFAR-100 dataset with 60% noise. Weight distribution over the clean examples in the 6K (left) and 20K (right) training step. All models are trained on the CIFAR-100 dataset with 60% noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Comparison with MLC on CIFAR-10 with four</cell></row><row><cell>different noise rates: {10%, 20%, 30%, 40%}. "Time (ms)"</cell></row><row><cell>denotes the average running time per training iteration on a</cell></row><row><cell>single NVIDIA V100 GPU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Accuracy vs. Training Time on CIFAR-10 and</cell></row><row><cell cols="4">CIFAR-100 with 60% noise. "Time (ms)" denotes the aver-</cell></row><row><cell cols="4">age running time per training iteration on a single NVIDIA</cell></row><row><cell>V100 GPU.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Time (ms) ACC</cell></row><row><cell>MW-Net [43]</cell><cell></cell><cell>933</cell><cell>61.7</cell></row><row><cell></cell><cell>b = 4</cell><cell>718</cell><cell>60.8</cell></row><row><cell>Pre-specified Block</cell><cell>b = 8</cell><cell>569</cell><cell>61.9</cell></row><row><cell></cell><cell>b = 12</cell><cell>326</cell><cell>61.4</cell></row><row><cell></cell><cell>s = 4</cell><cell>764</cell><cell>61.2</cell></row><row><cell>Random Layers</cell><cell>s = 8</cell><cell>826</cell><cell>61.6</cell></row><row><cell></cell><cell>s = 16</cell><cell>899</cell><cell>62.1</cell></row><row><cell cols="2">FaMUS (K = 4)</cell><cell>284</cell><cell>62.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison of sampling strategies on CIFAR-100 with 60% noise. b ? [1, 12] is the index of the residual block. s is the number of randomly selected layers. Our method samples from about 4 layers by setting K = 4 in Eq. (10). "Time (ms)" denotes the average running time per training iteration on a single NVIDIA V100 GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>62.81 50.81 MW-Net [43] 89.60 84.49 68.11 61.71 MentorNet ? [20] 91.20 74.20 66.80 58.80 Mixup</figDesc><table><row><cell>? [61]</cell><cell cols="4">91.50 86.80 66.80 58.80</cell></row><row><cell cols="5">M-correction [1] 92.80 90.30 70.10 59.50</cell></row><row><cell>MentorMix [18]</cell><cell cols="4">94.20 91.30 71.30 64.60</cell></row><row><cell>DivideMix [24]</cell><cell cols="4">94.90 94.30 75.20 72.00</cell></row><row><cell>Ours</cell><cell cols="4">95.37 94.97 75.91 73.58</cell></row><row><cell></cell><cell>?0.15</cell><cell>?0.11</cell><cell>?0.19</cell><cell>?0.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="6">: Comparison with the state-of-the-art on (mini) We-</cell></row><row><cell cols="6">bVision dataset. Numbers denote top-1 (top-5) accuracy on</cell></row><row><cell cols="6">the validation set of WebVision and ImageNet ILSVRC12.</cell></row><row><cell>Method</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell><cell>Mean</cell></row><row><cell>Cross-entropy</cell><cell cols="5">47.36 42.70 37.30 29.76 39.28</cell></row><row><cell>Mixup [61]</cell><cell cols="5">49.10 46.40 40.58 33.58 42.41</cell></row><row><cell>DivideMix [24]</cell><cell cols="5">50.96 46.72 43.14 34.50 43.83</cell></row><row><cell cols="6">MentorMix [18] 51.02 47.14 43.80 33.46 43.85</cell></row><row><cell cols="6">Ours (FaMUS) 51.42 48.06 45.10 35.50 45.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>74.81 82.23 86.39 38.32 43.85 51.14 55.71 Focal Loss ? [28] 70.38 76.71 82.76 86.66 38.41 44.32 51.95 55.78 CB Focal ? [6] 74.57 79.27 84.36 87.49 39.60 45.32 52.59 57.99 [40] with CE loss 74.16 78.93 82.12 85.19 40.23 44.44 51.64 53.73 MW-Net [43] with CE loss 75.21 80.06 84.94 87.84 42.09 46.74 54.37 58.46 [16] with CE loss 76.41 80.51 86.46 88.85 43.35 48.53 55.62 59.58 [16] with LDAM 80.00 82.34 84.37 87.40 44.08 49.16 52.38 58.00 MW-Net with CE loss + FaMUS 79.30 83.15 87.15 89.39 45.60 49.56 56.22 60.42 MW-Net with LDAM loss + FaMUS 80.96 83.32 86.24 87.90 46.03 49.93 55.95 59.03</figDesc><table><row><cell>Method</cell><cell cols="4">Long-Tailed CIFAR-10 100 50 20</cell><cell>10</cell><cell cols="2">Long-Tailed CIFAR-100 100 50 20 10</cell></row><row><cell cols="2">CE loss 70.36 LDAM-DRW [2] 77.03</cell><cell>-</cell><cell>-</cell><cell cols="3">88.16 44.70</cell><cell>-</cell><cell>-</cell><cell>59.59</cell></row><row><cell>BBN [63]</cell><cell cols="2">79.82 82.18</cell><cell>-</cell><cell cols="4">88.32 42.56 47.02</cell><cell>-</cell><cell>59.12</cell></row><row><cell>L2R  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Top-1 test accuracy of ResNet-32 on the long-tailed CIFAR-10 and CIFAR-100 with four imbalanced factors {100, 50, 20, 10}. Methods in the bottom block use extra clean data. The best performance is in bold and the second best is underscored. ? denotes the results are reported by</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The extra validation dataset is not a requirement in meta-learning. As in our experiments, we can use a subset of pseudo-labeled training data as the validation data. In this case, no extra labels or data are used.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Advaug: Robust adversarial augmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking classbalanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferdous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simaug: Learning robust representations from simulation for trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2017</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reducing reparameterization gradient variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Foti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander D&amp;apos;</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adding gradient noise improves learning for very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06807</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Confident learning: Estimating uncertainty in dataset labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Identifying mislabeled data using the area under the margin ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10528</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Meta transition adaptation for robust deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05697</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning soft labels via meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Voice</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09496</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Training noiserobust deep neural networks via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Are anchor points really indispensable in label-noise learning? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Snapshot distillation: Teacher-student optimization in one generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature transfer learning for face recognition with under-represented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Distilling effective supervision from severe label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Inflated episodic memory with region self-attention for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
