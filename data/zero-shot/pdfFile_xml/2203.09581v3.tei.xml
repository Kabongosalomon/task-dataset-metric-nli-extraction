<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SepTr: Separable Transformer for Audio Spectrogram Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolae-C?t?lin</forename><surname>Ristea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Mohamed bin Zayed University of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<email>raducu.ionescu@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Mohamed bin Zayed University of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SepTr: Separable Transformer for Audio Spectrogram Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: separable transformer</term>
					<term>multi-head attention</term>
					<term>au- dio spectrogram processing</term>
					<term>sound recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the successful application of vision transformers in multiple computer vision tasks, these models have drawn the attention of the signal processing community. This is because signals are often represented as spectrograms (e.g. through Discrete Fourier Transform) which can be directly provided as input to vision transformers. However, naively applying transformers to spectrograms is suboptimal. Since the axes represent distinct dimensions, i.e. frequency and time, we argue that a better approach is to separate the attention dedicated to each axis. To this end, we propose the Separable Transformer (SepTr), an architecture that employs two transformer blocks in a sequential manner, the first attending to tokens within the same time interval, and the second attending to tokens within the same frequency bin. We conduct experiments on three benchmark data sets, showing that our separable architecture outperforms conventional vision transformers and other state-of-the-art methods. Unlike standard transformers, SepTr linearly scales the number of trainable parameters with the input size, thus having a lower memory footprint. Our code is available as open source at https://github.com/ristea/septr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision transformers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> have rapidly become the hottest topic in computer vision to date, showing promising results across a broad range of tasks, from object recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> and detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> to medical image segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref> and generation <ref type="bibr" target="#b6">[7]</ref>. Due to the unquestionable success of vision transformers in solving many computer vision tasks, these models have also drawn the attention of the signal processing community <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. For example, ViT <ref type="bibr" target="#b2">[3]</ref> was transferred to the processing of signals by Gong et al. <ref type="bibr" target="#b12">[13]</ref>, by essentially providing visual representations of signals, i.e. spectrograms, as input to the model. Since the axes of a spectrogram represent distinct dimensions, e.g. frequency and time, we argue that naively applying transformers to spectrograms is suboptimal. Indeed, the attention spans across both directions, leading to a quadratic complexity with respect to the number of tokens. While operating over both horizontal and vertical axes seems reasonable for natural or medical images, we conjecture that a better approach for processing spectrograms is to separate the attention for each axis. To this end, we propose an architecture that employs two transformer blocks in a sequential manner. The first block (vertical transformer) attends to tokens within the same time interval, individually processing each time interval. Similarly, the second block (horizontal transformer) attends to tokens within the same frequency * corresponding author bin, independently operating over each frequency bin. To easily implement our transformer, we convert the input spectrogram into a batch of data samples along each axis, where a data sample is alternatively formed of tokens within the same time interval or within the same frequency bin, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. Our approach leads to a quadratic reduction of the number of learnable parameters, which further translates into a more efficient attention mechanism. In a nutshell, our transformer separates the attention for the horizontal and vertical axes of spectrograms, thus bearing the name Separable Transformer (SepTr).</p><p>We conduct experiments on three benchmark data sets, namely Speech Commands V2 <ref type="bibr" target="#b14">[15]</ref>, ESC-50 <ref type="bibr" target="#b15">[16]</ref> and CREMA-D <ref type="bibr" target="#b16">[17]</ref>, showing that our separable architecture attains significantly better results than models inspired by vision transformers <ref type="bibr" target="#b12">[13]</ref>, as well as other state-of-the-art methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, on all benchmarks. Moreover, we demonstrate that SepTr significantly reduces the number of learnable parameters (weights) when compared to ViT.</p><p>In summary, our contribution is twofold:</p><p>? We propose a novel architecture based on separable transformer blocks, which is particularly suitable for the efficient processing of spectrograms. ? We provide empirical evidence to support our claims regarding the high level of effectiveness and the low number of trainable parameters of our approach in relation to competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Due to the recent progress of attention mechanisms <ref type="bibr" target="#b24">[25]</ref>, transformers have become attractive and powerful choices for audio related tasks. On the one hand, some studies took selfattention mechanisms used in natural language processing to process text sequences and adapted them to process audio sequences and solve various audio tasks, e.g. automatic speech recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, speech synthesis <ref type="bibr" target="#b28">[29]</ref>, and speech representation <ref type="bibr" target="#b29">[30]</ref>. On the other hand, a few approaches adopted architectures from computer vision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, mainly due to the similarity between images and time-frequency representations.</p><p>In the preliminary studies adopting transformers for audio tasks, the attention architecture was typically used in conjunction with convolutional neural networks (CNNs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. In <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, the authors stacked a transformer on top of a CNN, while in <ref type="bibr" target="#b32">[33]</ref>, the authors combined the transformer module with a CNN in each block. Our method differs from these studies in that it is convolution-free, being purely based on multihead attention modules.</p><p>The self-attention mechanism was widely adopted in the audio field, providing remarkable results <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. For sound event detection, Miyazaki et al. <ref type="bibr" target="#b33">[34]</ref> utilized a transformer encoder based on BERT, which consists of multiple self-attention mod-    ules, allowing the capture of both local and global contextual information of the input sequence. Koizumi et al. <ref type="bibr" target="#b34">[35]</ref> proposed a similar method based on multi-head self-attention modules, which is used in a multi-task learning setting for the speech enhancement task. They observed that the attention mechanism is able to handle long-term dependencies in speech and noise, obtaining improved results. Instead of introducing attention modules into existing architectures as <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, we design a stand-alone transformer architecture based on separable attention, which is both effective and efficient. Kanda et al. <ref type="bibr" target="#b25">[26]</ref> proposed an end-to-end automatic speech recognition method, which jointly performs speaker counting, speech recognition and speaker identification for monaural multi-talker audio. They replaced a memory-based approach with custom transformers, attaining better word error rates. Similarly, Wang et al. <ref type="bibr" target="#b35">[36]</ref> proposed a hybrid architecture based on transformers for speech recognition. In contrast, we design an efficient transformer based on separable attention, which exploits the data structure of time-frequency representations.</p><p>Other works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> relied on the similarity between digital images and time-frequency representations by applying solutions originally proposed for computer vision tasks. Illium et al. <ref type="bibr" target="#b13">[14]</ref> applied the vision transformer (ViT) <ref type="bibr" target="#b2">[3]</ref> to primates classification and COVID detection. Even if they naively employ the solution proposed in vision, their results surpass the baseline methods. The authors also tried to built more natural tokens for time-frequency representations, considering all the frequency bins from a specific time slot as one token. However, this approach led to inferior results compared with ViT. A more comprehensive study about the utilization of ViT in the audio domain is conducted by Gong et al. <ref type="bibr" target="#b12">[13]</ref>. The authors discussed the utility of vision pre-training in the audio domain, reporting state-of-the-art results on multiple data sets. Unlike other approaches inspired by vision transformers, we propose an efficient transformer with separable attention along the axes, which is more natural for time-frequency representations, e.g. audio spectrograms. Notably, our approach is capable of attaining superior results with a lower number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Data representation. We transform each audio sample into a 2D time-frequency matrix, obtaining an image-like representation. To this end, we compute the discrete Short Time Fourier Transform (STFT), as follows:</p><formula xml:id="formula_0">STFT{x[n]}(m, k) = ? n=?? x[n] ? w[n ? mR] ? e ?j 2? Nx kn , (1) where x[n] is the discrete input signal, w[n]</formula><p>is the window function (in our approach, Hamming), Nx is the STFT length and R is the hop size. Next, we compute the spectrogram as the squared magnitude of the STFT and map the frequency bins onto the Mel scale. Overview of our architecture. We propose SepTr, a Separable Transformer architecture composed of two sequential transformer blocks, each attending to tokens within separate dimensions. The architecture does not impose a certain axis (time or frequency) for the first transformer block, being flexible in this regard. Without loss of generality, in <ref type="figure" target="#fig_1">Figure 1</ref>, we illustrate a model that separates the tokens along the time axis first. Our separable transformer block can be repeated L times to increase the depth of the architecture. The final mean class token (given by the last separable block) is processed by a multi-layer perceptron (MLP), which outputs the final prediction of our model. We next present the building blocks of SepTr. Tokenization and linear projection. Considering a spectrogram with k ? p ? N frequency bins and n ? p ? N time slots as input, in the first stage, we divide the input spectrogram into k ? n square patches (tokens) of size p ? p. In the experiments, we take patches of size 1 ? 1 as individual tokens, since we consider that it is more natural to compute the self-attention at the finest possible level. However, for a better visualization of the architecture shown in <ref type="figure" target="#fig_1">Figure 1</ref>, the tokens are depicted as larger patches. Regardless of their size, the tokens are further fed into a linear projection block, which projects the tokens into d-dimensional vectors. Let T ? R n?k?d represent the output tensor of the linear projection layer, where Ti,j ? R d is a projected token, ?i ? {1, 2, ..., k} and j ? {1, 2, ..., n}, where d ? N is the latent token dimension. Vertical transformer. We separate the projected tokens in the time domain into data sub-samples denoted as T:,j = [T1,j, T2,j, ..., T k,j ] ? R k?d , thus obtaining a batch of n data samples (one sample for each time slot j), where each data sample is composed of k tokens. Moreover, we replicate the class token T [CLS] ? R d n times and add one copy to each data sample T:,j. We also add a learnable positional embedding to each token. The resulting batch of n individual data samples is further processed by the vertical transformer. Horizontal transformer. Let T V :,j denote a data sample given as output by the vertical transformer. Next, we concatenate the processed data samples into a tensor T V ? R n?k?d , while decoupling the class tokens and applying average pooling to obtain a mean class token denoted asT V [CLS] . We proceed by separating the tensor T V in the frequency domain into data sub-samples denoted as T V i,: = T V i,1 , T V i,2 , ..., T V i,n ? R n?d , generating a batch of k data samples (one sample for each frequency bin i), where each sample is formed of n tokens. As for the vertical transformer, we replicate the class tokenT V [CLS] k times and add one copy to each data sample T V i,: . We further append a learnable positional embedding to each token. The resulting batch of k data samples is given as input to the horizontal transformer. Transformer block. The operations performed inside the vertical and horizontal transformers are identical, the only difference being the format of the input data samples. We thus describe the inner operations for the general case. Let X ? R m?d denote a sequence of m tokens (either T:,j or T V i,: ), where m ? {k, n} and d is the embedding dimension of each token. Let f be a multi-head attention layer, g a multi-layer perceptron, norm a normalization layer, and P, R ? R m?d some auxiliary tensors. The transformer block is formally described as follows:</p><formula xml:id="formula_1">P = f (norm(X)) + X,<label>(2)</label></formula><formula xml:id="formula_2">R = g(norm(P)) + P.<label>(3)</label></formula><p>The goal of the transformer block is to capture the interaction among all m entities by encoding each entity in terms of the global contextual information. This is achieved via the multihead attention layer f . This layer comprises three learnable weight matrices (W Q ? R d?dq , W K ? R d?d k , W V ? R d?dv , where dq = d k ) which are used to derive the queries Q, keys K and values V from the input sequence X. Indeed, the input sequence X is first projected onto these weight matrices to get Q = X ? W Q , K = X ? W K , and V = X ? W V , respectively.</p><p>The output Z ? R d?dv of the self-attention is given by:</p><formula xml:id="formula_3">Z = softmax Q ? K dq ? V,<label>(4)</label></formula><p>where K is the transpose of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data sets</head><p>ESC-50. The ESC-50 <ref type="bibr" target="#b15">[16]</ref> data set is a collection of 2,000 samples of 5 seconds each, comprising 50 classes of various common sound events. Samples are recorded at a 44.1 kHz sampling frequency, with a single channel. For evaluation, we followed the 5-fold cross-validation procedure described in <ref type="bibr" target="#b12">[13]</ref>. Speech Commands V2. The Speech Commands V2 (SCV2) <ref type="bibr" target="#b14">[15]</ref> data set is composed of spoken words, being designed to train and evaluate keyword spotting systems. It consists of 105,829 1-second recordings of 35 common speech commands. We used the official training (84,843 samples), validation (9,981 samples) and test (11,005 samples) splits proposed in <ref type="bibr" target="#b14">[15]</ref>. CREMA-D. The CREMA-D multi-modal database <ref type="bibr" target="#b16">[17]</ref> is formed of 7,442 videos of 91 actors (48 male and 43 female) of different ethnic groups. The actors perform various emotions while uttering 12 particular sentences that evoke one of the 6 emotion categories. Following <ref type="bibr" target="#b21">[22]</ref>, we conduct experiments only on the audio modality, dividing the audio samples into 70% for training, 15% for validation and 15% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation setup</head><p>Performance metrics. In all our experiments, we employ the classification accuracy as evaluation measure. We also conduct significance tests to compare SepTr with the top competitor, using a paired McNemar's test <ref type="bibr" target="#b36">[37]</ref> at a significance level of 0.01. Data preprocessing. For CREMA-D, we first standardize all audio clips to a fixed dimension of 4 seconds by padding or clipping the samples. For both CREMA-D and SCV2, we apply the STFT with Nx = 1024, R = 64 and a window size of 512. We use the same Nx value and window length for ESC-50, but we increase the hop size to R = 128. Next, for each STFT, we compute the square root of the magnitude and map the values to 128 Mel bins. The result is converted to a logarithmic scale (decibels) and normalized to the interval [0, 1], generating a single-channel output matrix. In all our experiments, we use the following data augmentation methods: noise perturbation, time shifting, speed perturbation, mix-up and SpecAugment <ref type="bibr" target="#b37">[38]</ref>. Hyperparameter tuning. While using the validation sets to tune the hyperparameters of SepTr and ViT, we found common optimal hyperparameters for all data sets. Thus, all models are optimized with Adam using the cross-entropy loss function. We start with an initial learning rate of 10 ?4 and use a decay factor of 0.5 after every 10 epochs. We train each model for 50 epochs on mini-batches of 4 samples. For SepTr, we set the number of blocks to L = 3 and the token size to d = 256, while for ViT we use a depth of L = 6 and the same token size (d = 256). Since our transformer block comprises two (vertical and horizontal) attention modules, the depths of SepTr and ViT are equivalent. Both SepTr and ViT are designed with 5 attention heads. For the ViT architecture, we divide the spectrograms into 8 ? 8 patches at an overlap of 6 pixels in both directions, following <ref type="bibr" target="#b12">[13]</ref>. We hereby underline that we were not able to train the ViT architecture with 1 ? 1 patches due to out-of-memory issues, while SepTR does not suffer from this problem (see <ref type="figure">Figure 2</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>We first analyze the impact of the vertical and horizontal transformers, when these modules are used alone or jointly. We present ablation results on the CREMA-D data set in <ref type="table" target="#tab_1">Table  1</ref>. We include ablated versions of SepTr performing attention either inside individual time slots (SepTr-V uses only vertical transformer blocks) or frequency bins (SepTr-H uses only horizontal transformer blocks). We observe that computing the attention on a single axis with SepTr-V or SepTr-H leads to suboptimal results, even below the ViT baseline. This shows that performing attention on one axis only is not sufficient. Further, we show results while alternating the order of the vertical and horizontal transformers, i.e. SepTr-HV starts with a horizontal transformer and SepTr-VH starts with a vertical transformer. Regardless of how the separable attention is applied via SepTr-HV or SepTr-VH, we observe important performance boosts (higher than 5%) over the models attending to one of the axes (SepTr-V and SepTr-H). Switching the order of the vertical and horizontal transformers has a marginal influence on the accuracy level, both SepTr-HV or SepTr-VH being able to outperform the state-of-the-art methods by significant margins. As we obtained slightly better results with SepTr-VH, we continued the rest of the experiments with this architecture, which we further simply refer to as SepTr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>Effectiveness. On the CREMA-D data set (see <ref type="table" target="#tab_1">Table 1</ref>), we obtain state-of-the-art results, reaching an accuracy of 70.47% and surpassing the previous best method <ref type="bibr" target="#b21">[22]</ref>, composed of an en- <ref type="bibr" target="#b15">16</ref> 32 <ref type="table" target="#tab_1">64  128  256  512  0   20   40   60   80</ref> 10 6 <ref type="figure">Figure 2</ref>: The number of trainable parameters for SepTr and ViT <ref type="bibr" target="#b12">[13]</ref> with respect to the input dimension.</p><p>semble of ResNet models, by 2.35%. Moreover, we attain a performance boost of 2.66% compared to ViT <ref type="bibr" target="#b12">[13]</ref>. On the SCV2 and ESC-50 data sets (see <ref type="table" target="#tab_2">Table 2</ref>), we consider state-of-the-art methods that are not pre-trained on external data, ensuring a fair comparison with SepTr, which is trained from scratch. We observe that SepTr attains superior results on both data sets. On SCV2, SepTr attains an accuracy of 98.51%, which is 0.40% higher than the previous state-of-the-art accuracy obtained by ViT <ref type="bibr" target="#b12">[13]</ref>. On ESC-50, SepTr yields an accuracy of 91.13%, surpassing ViT [13] by 2.43% and EfficientNet <ref type="bibr" target="#b19">[20]</ref> by 1.63%, respectively. In summary, SepTr surpasses all the state-of-theart methods on each and every data set by significant margins. Efficiency. We compare the performance of SepTr and ViT in terms of the memory footprint (number of learnable parameters), inference time and number of multiply-accumulate operations (MACs), on an Nvidia GeForce GTX 3090 GPU with 24 GB of VRAM. We compare equivalent models having 6 attention layers each, which are fed with 1 ? 1 patches. In terms of GMACs (80.57 for SepTr versus 80.09 for ViT) and inference time (5.72 ms for SepTr versus 5.69 ms for ViT), the differences are negligible, but when we refer to the memory footprint, we observe a large difference between the two models. In <ref type="figure">Figure  2</ref>, we illustrate the number of learnable parameters as a function of the input spectrogram size. We observe that SepTr has an almost constant number of parameters with respect to the input dimension, while the number of weights in ViT exhibits a quadratic growth, leading to comparatively larger models. For instance, for an input size of 512 ? 512, SepTr has 9.4M parameters, while ViT has 75.7M. Unlike ViT, our model is able to handle high resolution spectrograms, an essential advantage leading to high accuracy levels in the audio domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed a novel Separable Transformer (SepTr) architecture composed of two sequential transformers, each computing the attention on a different axis of the input spectrogram, either time or frequency. We obtained state-ofthe-art results on three data sets, surpassing all competing methods by statistically significant margins. Moreover, we showed that SepTr reduces the memory footprint when compared with ViT, without impacting the inference time or MACs. In future work, we aim to employ SepTr for other signal processing tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Our SepTr architecture. The spectrogram is tokenized, projected and further processed by two sequential transformers (vertical and horizontal). Our separable transformer block (comprising both vertical and horizontal transformers) is repeated L times, leading to a SepTr model of L blocks in depth. The final class token is taken as input by an MLP head which decides the final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of SepTr and its ablated versions in comparison with various state-of-the-art methods on CREMA-D. Significantly better results compared to<ref type="bibr" target="#b21">[22]</ref> are marked with ?, using a paired McNemar's test<ref type="bibr" target="#b36">[37]</ref> at a significance level of 0.01. Top scores are highlighted in bold.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>GRU (Shukla et al. [24])</cell><cell>55.01%</cell></row><row><cell>GAN (He et al. [19])</cell><cell>58.71%</cell></row><row><cell>ResNet-18 (Georgescu et al. [18])</cell><cell>65.15%</cell></row><row><cell>ResNet-18 ensemble (Ristea et al. [22])</cell><cell>68.12%</cell></row><row><cell>ViT (Gong et al. [13])</cell><cell>67.81%</cell></row><row><cell>SepTr-V</cell><cell>65.29%</cell></row><row><cell>SepTr-H</cell><cell>65.11%</cell></row><row><cell>SepTr-HV</cell><cell>70.31%  ?</cell></row><row><cell>SepTr-VH (proposed)</cell><cell>70.47%  ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results</figDesc><table><row><cell cols="3">of SepTr versus various state-of-the-art meth-</cell></row><row><cell cols="3">ods on Speech Commands V2 (SCV2) and ESC-50. Significantly</cell></row><row><cell cols="3">better results compared to ViT [13] are marked with  ?, using a</cell></row><row><cell cols="3">paired McNemar's test [37] at a significance level of 0.01. Top</cell></row><row><cell>scores are highlighted in bold.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>SCV2</cell><cell>ESC-50</cell></row><row><cell>RBM (Sailor et al. [23])</cell><cell>-</cell><cell>86.50%</cell></row><row><cell>EfficientNet (Kim et al. [20])</cell><cell>-</cell><cell>89.50%</cell></row><row><cell cols="2">MatchboxNet (Majumdar et al. [21]) 97.40%</cell><cell>-</cell></row><row><cell>ViT (Gong et al. [13])</cell><cell cols="2">98.11% 88.70%</cell></row><row><cell>SepTr (proposed)</cell><cell cols="2">98.51%  ? 91.13%  ?</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Work supported by a grant of the Romanian Ministry of Education and Research, CNCS -UEFISCDI, project no. PN-III-P1-1.1-TE-2019-0235, within PNCDI III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformers in Vision: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">CyTran: Cycle-Consistent Transformers for Non-Contrast to Contrast CT Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-C</forename><surname>Ristea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Savencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06400</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CvT: Introducing Convolutions to Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UNETR: Transformers for 3D Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV, 2022</title>
		<meeting>WACV, 2022</meeting>
		<imprint>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AST: Audio Spectrogram Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2021</title>
		<meeting>INTERSPEECH, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="571" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual Transformers for Primates Classification and Covid Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Illium</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sedlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Popien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2021</title>
		<meeting>INTERSPEECH, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="451" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CREMA-D: Crowd-sourced emotional multimodal actors dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Keutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="390" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Nonlinear Neurons with Human-like Apical Dendrite Activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-C</forename><surname>Ristea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03229</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image2Audio: Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW, 2020</title>
		<meeting>CVPRW, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="912" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Urban sound tagging using multi-channel audio feature with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DCASE</title>
		<meeting>DCASE</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2020</title>
		<meeting>INTERSPEECH, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="3356" to="3360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-paced ensemble learning for speech and audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-C</forename><surname>Ristea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTER-SPEECH</title>
		<meeting>INTER-SPEECH</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2836" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann Machine for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visually guided self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP, 2020</title>
		<meeting>ICASSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="6299" to="6303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-End Speaker-Attributed ASR with Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2021</title>
		<meeting>INTERSPEECH, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="4413" to="4417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lohrenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2021</title>
		<meeting>INTERSPEECH, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2846" to="2850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online Compressive Transformer for End-to-End Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2021</title>
		<meeting>INTERSPEECH, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2082" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transformer-based Acoustic Modeling for Streaming Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kalinli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2021</title>
		<meeting>INTERSPEECH, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="146" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2021</title>
		<meeting>INTERSPEECH, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1169" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolution augmented transformer for semisupervised sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DCASE, 2020</title>
		<meeting>DCASE, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="100" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sound event detection of weakly labelled data with CNN-transformer and automatic threshold optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2450" to="2460" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH, 2020</title>
		<meeting>INTERSPEECH, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly-supervised sound event detection with selfattention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP, 2020</title>
		<meeting>ICASSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Speech enhancement using self-adaptation and multi-head self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yatabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Masuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP, 2020</title>
		<meeting>ICASSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformerbased acoustic modeling for hybrid speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6874" to="6878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
