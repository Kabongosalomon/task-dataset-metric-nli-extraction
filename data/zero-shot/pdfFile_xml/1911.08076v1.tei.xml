<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IFQ-Net: Integrated Fixed-point Quantization Networks for Embedded Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>Gao</surname></persName>
							<email>gaohongxing@canon-ib.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Information Technology (Beijing) Co., LTD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tao</surname></persName>
							<email>taowei@canon-ib.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Information Technology (Beijing) Co., LTD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchao</forename><surname>Wen</surname></persName>
							<email>wendongchao@canon-ib.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Information Technology (Beijing) Co., LTD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tse-Wei</forename><surname>Chen</surname></persName>
							<email>twchen@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="department">Device Technology Development Headquarters</orgName>
								<address>
									<country>Canon Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kinya</forename><surname>Osa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Device Technology Development Headquarters</orgName>
								<address>
									<country>Canon Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masami</forename><surname>Kato</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Device Technology Development Headquarters</orgName>
								<address>
									<country>Canon Inc</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IFQ-Net: Integrated Fixed-point Quantization Networks for Embedded Vision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deploying deep models on embedded devices has been a challenging problem since the great success of deep learning based networks. Fixed-point networks, which represent their data with low bits fixed-point and thus give remarkable savings on memory usage, are generally preferred. Even though current fixed-point networks employ relative low bits (e.g. 8-bits), the memory saving is far from enough for the embedded devices. On the other hand, quantization deep networks, for example XNOR-Net and HWGQ-Net, quantize the data into 1 or 2 bits resulting in more significant memory savings but still contain lots of floatingpoint data. In this paper, we propose a fixed-point network for embedded vision tasks through converting the floatingpoint data in a quantization network into fixed-point. Furthermore, to overcome the data loss caused by the conversion, we propose to compose floating-point data operations across multiple layers (e.g. convolution, batch normalization and quantization layers) and convert them into fixedpoint. We name the fixed-point network obtained through such integrated conversion as Integrated Fixed-point Quantization Networks (IFQ-Net). We demonstrate that our IFQ-Net gives 2.16? and 18? more savings on model size and runtime feature map memory respectively with similar accuracy on ImageNet. Furthermore, based on YOLOv2, we design IFQ-Tinier-YOLO face detector which is a fixed-point network with 256? reduction in model size (246k Bytes) than Tiny-YOLO. We illustrate the promising performance of our face detector in terms of detection rate on Face Detection Data Set and Bencmark (FDDB) and qualitative results of detecting small faces of Wider Face dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the past decade, deep learning models have achieved great success on various machine learning tasks such as image classification, object detection, semantic segmentation, etc. However, applying them on embedded devices remains as a challenging problem due to the enormous resource requirement in terms of memory and computation power. On the other hand, fixed-point data inference yields promising reductions on such requirement for embedded devices <ref type="bibr" target="#b5">[6]</ref>. Thus, fixed-point networks are primarily preferred when deploying deep models for the embedded devices.</p><p>In general, designing a fixed-point CNN network can be fulfilled by two types of approaches: 1)pre-train a floatingpoint deep network and then convert it into a fixed-point network; 2) train a deep CNN model whose data (e.g. weights, feature maps, etc.) is natively fixed-point. In <ref type="bibr" target="#b8">[9]</ref>, a method is introduced to find the optimal bit-width for each layer to convert its floating-point weights and feature maps into their fixed-point counterparts. Given the hardware acceleration for 8-bit integer based computations, <ref type="bibr" target="#b11">[12]</ref> provides optimal thresholds which minimize the data loss during the 32-bits float to 8-bits integer conversion. These works have shown that it is feasible to significantly save memory usage through relatively low bit (e.g. 8-bits) representation yet achieve similar performance. However, such memory saving is far from enough especially for embedded devices. The second approach is to train a network all of whose data is natively fixed-point. Nevertheless, as discussed in <ref type="bibr" target="#b7">[8]</ref>, its training process may suffer from severe unstable weight updating because of the inaccurate gradients. Strategies such as stochastic rounding somehow result in improvement <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref> but a trade-off between low bit data representation and precise gradients still has to be made. Alternatively, BinaryNet <ref type="bibr" target="#b1">[2]</ref> employs binarized weights for forward pass but full precision weights and gradients for stable convergence. Meanwhile, its feature maps are also binarized to {?1, +1} so that its data can be represented as 1-bit fixed-point for less memory usage during inference time. However, a notable performance drop of 30% (Top-1 accuracy) is observed on ImageNet classification. Subsequently, XNOR-Net <ref type="bibr" target="#b12">[13]</ref> employs extra scaling factors on both weights and feature maps so that their "binary" elements are generalized to {??, +?} and {??, +?} respectively. These extra factors enrich the data information thus gains 16% accuracy back on ImageNet. Furthermore, HWGQ-Net <ref type="bibr" target="#b0">[1]</ref> uses a more flexible k-bits quantization on feature maps whose elements can be generalized to {0, ?, 2?, 3?} in the situation of 2-bits uniform quantization. Such k-bits feature maps (k ? 2) give a further 8% improvement making HWGQ-Net to be the state-of-the-art quantization network on ImageNet classification.</p><p>Given a HWGQ-Net, each filter of its quantized convolution layer can be expressed as a multiplication of a floatingpoint ? and a binary fixed-point matrix whose elements are limited to {?1,+1}. Similar representations can also be applied to its feature maps (see <ref type="bibr">Equation 1</ref>). Therefore, to obtain its fixed-point counterpart, it would be only necessary to convert the floating-point ? and ? while other parts of the layer are natively fixed-point. Besides, Batch Normalization (BN) layer, which is usually employed on top of each convolution layer, also contains floating-point parameters and thus requires fixed-point conversion (see Equation 2). One way to do this is to separately convert each of the floating-point data but it usually results in data loss that would be accumulated over the network and cause a notable performance drop.</p><p>In this paper, we propose a novel fixed-point network, IFQ-Net which is obtained through converting a floatingpoint quantization network into its fixed-point counterparts. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, we first divide the quantization network into several substructures, where each substructure is defined as a group of consecutive layers that starts with a convolution layer and ends with a quantization layer. An example of the substructures of AlexNet is listed in <ref type="table" target="#tab_0">Table 1</ref>. Then we convert the floating-point data in each substructure into fixed-point data. Especially for the "quantized substructure", which starts with a quantized convolution layer and ends with a quantization layer, we propose to compose its floating-point data into the thresholds of the quantization layer and then convert the composition result into fixedpoint. As will be presented in section 3.2, our integrated conversion method does not cause any performance drop. At the end, we separately convert each floating-point data in the remaining non-quantized substructures (if any) to fixedpoint resulting in a fixed-point network, IFQ-Net.</p><p>In this paper, the major contributions we made are:</p><p>? proposing IFQ-Net network, obtained through converting a floating-point quantization network into fixedpoint. Due to the relatively low bits of the quantization network, IFQ-Net gives much more savings on model size and runtime feature map memory.</p><p>? proposing an integrated conversion method to convert the floating-point data in the quantized substructures without performance drop. Since its BN operation (if available) is already integrated into the thresholds of the corresponding quantization layer, our IFQ-Net does not require actual BN implementation on target hardware.</p><p>? designing IFQ-Tinier-YOLO face detector, a fixedpoint model with 256? tinier model size (246k Bytes) than Tiny-YOLO.</p><p>? demonstrating the feasibility of quantizing all convolution layers in IFQ-Tinier-YOLO model, which differs from the original HWGQ-Net whose first and last layers are full precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Quantized convolutional neural network</head><p>A CNN network usually consists of a series of layers where the convolution layer monopolizes the inference time of the whole network. However, the weights and features maps were found redundant for most tasks. Consequently, enormous efforts have been done on quantizing the weights and/or the input feature maps into low-bit data for less memory usage and fast computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Quantization network inference</head><p>Embedded devices are usually employed for network inference only because of their limited computation resources. Hence, in this paper, we mainly focus on the inference process of a network. In the following, we take a typical quantized substructure from HWGQ-Net as an example to illustrate the computation details of its inference.</p><p>For the l-th convolution layer of HWGQ-Net, we use W ? R c?h ?w and X l ? R c?h?w to represent one of the filters and its input feature maps respectively, where c, h , w , h, w are the number of channels, the height and width of its filter, and the height and width of the input feature maps respectively. In the case of a 2-bit quantized convolution layer from HWGQ-Net, its filter is binarized into W ? {??, +?} c?h ?w and X l ? {0, ?, 2?, 3?} c?h?w . Then the computation of a convolution layer can be represented as</p><formula xml:id="formula_0">Y conv = W ? X l + b = ?? ? W b ? X l q + b<label>(1)</label></formula><p>where ? represents the convolution operation; W b and X l q are integer part of the quantized filter and feature maps so that W b ? {?1, +1} c?h ?w and X l q ? {0, 1, 2, 3} c?h?w , b is its learned bias.Y conv is its output feature map.</p><p>Typically, a BN layer is applied on top of a convolution layer. It is computed in an element-wise manner as follows,</p><formula xml:id="formula_1">Y BN i,j = Y conv i,j ? ? ?<label>(2)</label></formula><p>where 0 ? i &lt; w, 0 ? j &lt; h, ? and ? are the learned mean and variance of the feature map. At the end, a quantization layer maps its input feature map Y BN into discrete numbers. Taking a 2-bits uniform quantization for instance, its computation can be expressed as</p><formula xml:id="formula_2">X l+1 i,j = ? * ? ? ? ? ? ? ? 0 Y BN i,j ? thr 1 1 thr 1 &lt; Y BN i,j ? thr 2 2 thr 2 &lt; Y BN i,j ? thr 3 3 Y BN i,j &gt; thr 3<label>(3)</label></formula><p>where thr 1 , thr 2 and thr 3 are the thresholds used for quantizing its input Y BN , and ? is the scale factor for its output feature map. The resulting X l+1 is then employed as the input of the (l + 1)-th convolution layer (if available). When max pooling layer appears in the substructure, as discussed in <ref type="bibr" target="#b12">[13]</ref>, it is better to place it between convolution and BN layers for richer data information. In other words,</p><formula xml:id="formula_3">Y pooling i,j = max (m,n)?Ai,j {Y conv m,n }<label>(4)</label></formula><p>where A i,j denotes the local zone employed for pooling operation at location (i, j) of Y conv . Then the input of the BN layer in Equation 2 is accordingly changed to be Y pooling i,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Separated fixed-point conversion</head><p>As illustrated in subsection 2.1, the dominating part of the convolution computation W ? X l can be implemented with native fixed-point data only. However, the network still contains lots of floating-point data these being the scaling factor ? and ? in the convolution layer, ? and ? in the BN layer and also thr i in the quantization layer. Consequently, it is necessary to convert them into fixed-point when designing fixed-point networks for embedded devices.</p><p>A traditional way for the aforementioned conversion is to process them separately. As shown in <ref type="figure" target="#fig_1">Figure 2a</ref>, each floating-point data of the substructure is converted into its fixed-point counterpart. Since directly applying a simple conversion causes significant data loss especially when ? is small (e.g. 0.001), we use a relatively large Q m to scale-up the floating-point data <ref type="bibr" target="#b0">1</ref> . For example, ? can be transformed by ?Q m where ? denotes the flooring operation. At the end, Q m has to be divided back to achieve "equivalent" outputs. Then, fixed-point conversion of a quantized convolution layer can be expressed as</p><formula xml:id="formula_4">Y conv = ?Q m ?Q m ? W b ? X q + bQ 2 m Q 2 m<label>(5)</label></formula><p>To obtain a substructure with fixed-point data only, the same conversion ? is also applied to ?, ?, thr i separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IFQ-Net methodology</head><p>To obtain a fixed-point network for embedded devices, we propose to first train a quantization network and then convert its floating-point data, which has been quantized into extremely low bits (e.g. 1 or 2 bits), into fixed-point data. As demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>, our methodology consists of two steps: first we divide a trained floating-point quantization network into substructures and then we convert each substructure into its fixed-point counterpart. We employ HWGQ-Net algorithm to train a floating-point quantization network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Substructure division</head><p>As mentioned in Section 1, a substructure is defined as a group of consecutive layers that starts with a convolution layer and ends with a quantization layer. Given a quantization network, we search for the quantized substructures in the network as demonstrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Typically, the architecture of a quantized substructure is either {convolution, BN, quantization} or {convolution, pooling, BN, quantization}. The substructures that contain more than one convolution or quantization layer are not considered as quantized substructures. The layers between quantized substructures are defined as non-quantized substructures, which will be treated differently during fixed-point conversion. Generally, BN and/or max pooling layers are placed between convolution layers and quantization layers.</p><p>Taking AlexNet-HWGQ network as an example, we divide it into 7 substructures (see <ref type="table" target="#tab_0">Table 1</ref>). Because the HWGQ network keeps its first and last convolution layer full precision, so the corresponding substructures (substructure1 and substructure7) are non-quantized and thus will be converted differently. Please note that we group all the layers on top of the F C7 layer as one single substructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integrated fixed-point conversion</head><p>A trained quantization network can be divided into substructures that contain lots of floating-point data. To obtain a fixed-point network, it is necessary to convert each of its floating-point substructures into fixed-point. However, converting the floating-point data in a separated manner usually leads to performance drop. Consequently, in the following, we introduce an integrated way to convert a floating-point substructure. Taking 2-bits uniformly quantized substructure from HWGQ-Net as an example, its computations that mentioned in Equation 1, 2 and 3 can be composed as follows</p><formula xml:id="formula_5">Y quant = ? ? ? ? ? ? ? ? ? 0 ???W b ?Xq+b?? ? ? thr 1 ? thr 1 &lt; ???W b ?Xq+b?? ? ? thr 2 2? thr 2 &lt; ???W b ?Xq+b?? ? ? thr 3 3? ???W b ?Xq+b?? ? &gt; thr 3<label>(6)</label></formula><p>Since ? &gt; 0, ? &gt; 0 and also ? &gt; 0, Equation 6 can be transformed to The next step is to convert the new thresholds into fixedpoint data. W b and X q are both integers thus the resulted W b ? X q are also integers. In Equation 7, when thresholding the integers W b ? X q with newly formed floating-point thresholds, theoretically, their fractional parts do not affect the result. Consequently, we can simply discard the fractional part by applying the floor function ? on the new thresholds. Compared to the separated fixed-point conversion method, our method does not require to scale-up the floating-point data with Q m yet gives identical quantization results. Besides, the remaining floating-point data ? can be processed in the next substructure just like how we deal with the ? of Equation 1. Consequently, all the computations of a quantized substructure, as represented in Equation 7, can be casted on fixed-point data after ? is applied on each of the new thresholds.</p><formula xml:id="formula_6">Y quant = ? * ? ? ? ? ? ? ? ? ? 0 W b ? X q ? thr1 * ?+??b ?? 1 thr1 * ?+??b ?? &lt; W b ? X q ? ? * thr2+??b ?? 2 ? * thr2+??b ?? &lt; W b ? X q ? ? * thr3+??b ?? 3 W b ? X q &gt; ? * thr3+??b ??<label>(7)</label></formula><p>The fixed-point implementation of on-device BN computation is challenging for embedded devices. As an alternative solution which differs from the method that merges it into a convolution layer, the proposed integrated fixed-point conversion method transforms the BN computation into the new quantization thresholds. Consequently, our IFQ-Net also does not require actual BN implementation on embedded hardware.</p><p>In the above, we have taken k = 2 as an example of converting a k-bits quantization network. However, when a larger k is employed, it would be necessary to store (2 k ? 1) thresholds. In the uniform quantization scenario, the network's thresholds can be expressed as thr i = i * base + of f set. Thus, one may only need to store base and of f set because all the thresholds can be restored from them. Similarly, denoting base = ? * base ?? and of f set = ??b+? * of f set ?? , our newly formed thresholds can also be represented as thr i = i * base +of f set . Thus, our new thresholds thr can also be represented in an efficient way. Then computations in a k-bits uniformly quantized substructure can be expressed as Equation <ref type="bibr" target="#b7">8</ref> which can be further converted into fixed-point in an integrated manner.</p><formula xml:id="formula_7">Y quant = ? * ? ? ? ? ? ? ? ? ? ? ? ? ? 0 W b ? X q ? thr 1 1 thr 1 &lt; W b ? X q ? thr 2 2 thr 2 &lt; W b ? X q ? thr 3 . . . . . . (2 k ? 1) W b ? X q &gt; thr (2 k ?1)<label>(8)</label></formula><p>In summary, we presented IFQ-Net obtained by dividing a quantization network (e.g. HWGQ-Net) into floatingpoint substructures and then converting each of them into fixed point. For the quantized substructures, we propose an integrated fixed-point conversion method which gives no performance drop. At the end, for the remaining nonquantized substructure (if any), we employ the separated method to convert them into fixed-point.</p><p>It is worth to point out that our IFQ-Net differs from the floating-point data composition method presented in <ref type="bibr" target="#b16">[17]</ref> in many aspects: 1)the paper claims that it combines multiple layers but does not explicitly explain how; 2)the paper applies the floating-point data composition for enabling binary convolution computation leaving other parameters as floating-point while our method is proposed for fixed-point conversion and 3)the paper concentrates on implementing a quantized network on FPGA but the performance (e.g. detection rate, mAP etc.) is not reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In this section, we demonstrate how we convert each substructure of AlexNet into fixed-point to obtain an IFQ-AlexNet. We first test the performance of the integrated conversion method for the quantized substructures. Then, for the non-quantized substructures, we demonstrate how we experimentally set the scale factor Q m for the separated fixed-point conversion. We compare the performance of our IFQ-AlexNet with "Lin et al. <ref type="bibr" target="#b8">[9]</ref>" which is the state-of-the-art AlexNet-based fixed-point network on ImageNet. Furthermore, we also illustrate the performance of IFQ-Tinier-YOLO face detector which is an extremely compact fixedpoint network on both FDDB and Wider Face datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">IFQ-AlexNet network</head><p>To obtain fixed-point networks, we first train floatingpoint quantization networks AlexNet-HWGQ whose weights and feature maps are quantized into 1-bit and k-bits (k ? {2, 3, 4}) respectively. The AlexNet-HWGQ is trained with 320k iterations on ImageNet while the batch size is set to 256. The initial learning rate is set to 0.1 and decreased by a factor of 0.1 every 35k iterations. We inherit other training settings from <ref type="bibr" target="#b0">[1]</ref> and achieve similar performance. As the first step for obtaining the IFQ-AlexNet, we divide a floating-point AlexNet-HWGQ network into 7 substructures <ref type="table" target="#tab_0">( Table 1</ref>). In the table, the superscript q in Conv q 2 and FC q 7 means that their weights are binarized (1-bit) and input feature maps are quantized into k bits by their bottom Quant i layers. We group the layers {FC q 7 , BN 7 , ReLU 7 , FC 8 } as a single non-quantized substructure.</p><p>In the following, we will show how to convert each substructure into fixed-point to obtain an IFQ-AlexNet. In subsection 4.1.1, we show the performance of the proposed integrated conversion method for the quantized substructure while the non-quantized substructures are kept floatingpoint. We then illustrate the way to set a proper scaling factor Q m for converting each of the remaining non-quantized substructures (see subsection 4.1.2). At the end, in subsection 4.1.3, we compare our IFQ-AlexNet with "Lin et al. <ref type="bibr" target="#b8">[9]</ref>" which is the state-of-the-art AlexNet-based fixedpoint network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Integrated conversion for the quantized substructures</head><p>In this subsection, we focus on converting the quantized substructures (substructure2,...,substructure6). The Im-ageNet Top-1 classification accuracy is employed to evaluate the accuracy of the converted networks (see <ref type="table" target="#tab_1">Table 2</ref>). In the table, "separated" refers to the networks obtained by converting the quantized substructures of the corresponding AlexNet-HWGQ (k equals to 2 or 3 or 4) in a separated manner (see section 2.2). In contrast, "integrated" represents the networks obtained by converting their quantized substructures in the proposed integrated way (see section 3.2). Please note that, to compare the performance of different conversion methods on quantized substructures, we keep the non-quantized substructures (substructure1 and substructure7) floating-point.  As shown in <ref type="table" target="#tab_1">Table 2</ref>, the floating-point AlexNet-HWGQ networks achieves competitive classification accuracy. However, "separated" method shows notable performance degradation. The reason is that it separately converts each floating-point data x of a quantized substructures by x * Q m which leads to data loss. To reduce such loss, a large m has to be applied (m = 12) which in turn causes more memory usage. In contrast, for each quantized substructure, our "integrated" method gives identical outputs as its floating-point counterpart in AlexNet-HWGQ while the scaling factor Q m is not required at all (m = 0). Even though we employ the uniform quantization as example, our "integrated" method is also effective for the networks quantized by other strategies as long as their floating-point operations can be composed as in Equation 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Separated conversion for the non-quantized substructures</head><p>In the subsection 4.1.1, we have demonstrated that the proposed integrated method gives lossless fixed-point conversion for quantized substructures. To obtain IFQ-AlexNet all of whose data operations are fixed-point data based, we then convert each of the remaining non-quantized substructures in a "separated" manner. For saving more memory while causing less conversion loss for such substructures, an optimal Q m is required for each of non-quantized substructure: substructure1 and substructure7. Since the substructure7 directly outputs the inferred results for the task, the preciseness of its computation is more critical.</p><p>Consequently, we first find the optimal m for its fixed-point conversion while substructure1 is kept floating-point. As demonstrated in <ref type="figure" target="#fig_5">Figure 4a</ref>), for the networks with different k, a larger m for Q m generally gives better performance. It is because a larger m value gives less data a):subtructure7 b):substructure1 By fixing m = 14 for converting substructure7 into fixed-point, we then optimize the m for substructure1. As shown in <ref type="figure" target="#fig_5">Figure 4b</ref>), m = 9 can be considered as the sufficient scaling factor for the fixed-point conversion of substructure1.</p><p>In summary, to obtain IFQ-AlexNet, we employ the lossless "integrated" conversion method for the quantized substructures and m = 9 and m = 14 for the scaling factor Q m for converting the substructure1 and substructure7 of AlexNet-HWGQ networks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Performance comparison</head><p>In the following, we compare our IFQ-AlexNet with "Lin et al. <ref type="bibr" target="#b8">[9]</ref>" which is the state-of-the-art AlexNet-based fixedpoint network. Lin et al. <ref type="bibr" target="#b8">[9]</ref> employ a ? (? ? 9) as the number of bits for representing each data of the first layer and then introduce an optimal setting on the number of bits for other layers with respect to ? (see <ref type="table">Table 3</ref>). It is worth to point out that "Lin et al. <ref type="bibr" target="#b8">[9]</ref>" is converted from an AlexNetlike network which posses ?2? savings on the number of parameters compared to our IFQ-AlexNet (21.5 million vs. 58.3 million 2 ). <ref type="table">Table 3</ref> compares the number of bits that are employed to represent every fixed-point data of each layer of "Lin et al. <ref type="bibr" target="#b8">[9]</ref>" and our IFQ-AlexNet. As shown in the table, for conv2?conv5 layers, IFQ-AlexNet employs 1-bit for representing their weights which is remarkably lower than "Lin et al. <ref type="bibr" target="#b8">[9]</ref>" . Most importantly, for F C6 and F C7 layers which are parameter intensive and thus dominate the model size, we consistently employ 1-bit weights. Thus, our IFQ-Net gives 6? savings (1-bit vs. 6-bits). On the other hand, regarding to the feature maps, our IFQ-AlexNet networks also generally use lower bits than their competitors (the same bits may happen on conv2 and conv4 layers only if k = 4 and ? = 9). <ref type="table">Table 3</ref>. Comparison on the number of bits employed for each layer of AlexNet-based fixed-point networks.</p><p>Lin et al. <ref type="bibr" target="#b8">[9]</ref> (? ? 9)</p><p>IFQ-AlexNet Weights Feature maps Weights Feature maps conv1 ? ?</p><formula xml:id="formula_8">9 8 conv2 ?-5 ?-5 1 k conv3 ?-4 ?-4 1 k conv4 ?-5 ?-5 1 k conv5 ?-4 ?-4 1 k F C6 6 6 1 k F C7 6 6 1 k</formula><p>For "Lin et al. <ref type="bibr" target="#b8">[9]</ref>" networks, different ? give different preciseness of its fixed-point data. We directly borrow the experimental results from the paper setting ? to 9 and 10. <ref type="table" target="#tab_3">Table 4</ref> illustrates the memory usage of the weights and feature maps of the fixed-point networks in terms of millions of bits (Mbits). As shown in the  Lin et al. <ref type="bibr" target="#b8">[9]</ref> IFQ-AlexNet ? = 9 ? = 10 k = 2 k = 4 To evaluate the memory usage of feature maps during inference time, we measure the maximum memory that consumed by one single layer, which is Conv1 in the case of AlexNet. Such evaluation makes more sense than evaluating the summation of all layers because the feature maps from other un-connected layers is not required thus can be discarded during inference time. Comparing with "Lin et al. <ref type="bibr" target="#b8">[9]</ref>", our IFQ-AlexNet networks output 4? smaller feature maps for Conv1 layer (55?55 vs. 112?112). Furthermore, our IFQ-AlexNet employs less bits to represent each element of the feature maps of Conv1 layer(k = 2 or 3 or 4 vs. ? = 9 or 10). Consequently, when comparing IFQ-AlexNet (k = 2) with "Lin et al. <ref type="bibr" target="#b8">[9]</ref>", our method gives 18? savings on inference memory for feature maps.</p><p>Furthermore, we follow the reference paper <ref type="bibr" target="#b8">[9]</ref> and use Top-5 accuracy to evaluate the performance of the AlexNetbased fixed-point networks. Comparing with "Lin et al. <ref type="bibr" target="#b8">[9]</ref> (? = 9)", IFQ-AlexNet (k = 2) gives 2% improvement accuracy with significant savings on model size and feature maps memory as well. Moreover, comparing the "Lin et al. <ref type="bibr" target="#b8">[9]</ref> (? = 10)" and IFQ-AlexNet (k = 4 ) networks which have higher precision, our method also achieves 2.18? and 10.9? savings on model size and feature maps respectively without performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">IFQ-Tinier-YOLO face detector</head><p>Face detection has various applications in real life and thus emerges many algorithms such as Faster R-CNN <ref type="bibr" target="#b15">[16]</ref>, SSD <ref type="bibr" target="#b10">[11]</ref>, Mask R-CNN <ref type="bibr" target="#b4">[5]</ref> and YOLOv2 <ref type="bibr" target="#b14">[15]</ref>. In this section, we aim to apply our IFQ-Net to face detection task. For the embedded devices, the simple architecture of a deployed network would give great benefit on the hardware design. Consequently, we make use of YOLOv2 detection algorithm as the framework for our face detector.</p><p>We initially employ the Tiny-YOLO <ref type="bibr" target="#b14">[15]</ref> network due to its compact size. Furthermore, we design a more compact network Tinier-YOLO based on Tiny-YOLO by: 1) only using half the number of filters in each convolution layer; 2) replacing the 3 ? 3 filter into 1 ? 1 for the third to last convolution layer; 3)binarizing the weights of all convolution layers by HWGQ. The above three strategies give 4?, 2? and 32? savings respectively and overall 256? savings on model size resulting in a 246k Bytes face detector.  We use the training set of Wider Face <ref type="bibr" target="#b17">[18]</ref> and Darknet deep learning framework <ref type="bibr" target="#b13">[14]</ref> to train the baseline Tiny-YOLO and our Tinier-YOLO networks. Furthermore, to obtain their quantized fixed-point counterparts IFQ-Tiny-YOLO and IFQ-Tinier-YOLO, we first train the quantiza- tion network with our HWGQ implementation on Darknet (k = 2) and then convert each of their substructure into fixed-point. Each network is trained for 100k iterations with batch size 128. The learning rate is initially set to 0.01 and down scaled by 0.1 at 30kth, 60kth and 90kth iteration. Besides, we also inherit the multi-scale training strategy from YOLOv2.</p><p>We compare the trained face detectors on FDDB dataset <ref type="bibr" target="#b6">[7]</ref> which contains 5,171 faces in 2,845 testing images. To evaluate the performance of the face detector, we employ detection rate when false positive rate is 0.1 (1 false positive in 10 test images). It corresponds to the true positive rates (y-axis) when the false positive (x-axis) equals to 0.1 ? 2, 845 = 284 in <ref type="figure" target="#fig_6">Figure 5</ref>. Such evaluation is more meaningful in real applications when low false positive rate is desired. As illustrated in <ref type="table" target="#tab_5">Table 5</ref>, comparing with Tiny-YOLO, IFQ-Tiny-YOLO achieves 32? savings on model size with 3% drop on detection rate (0.89 vs. 0.92). Furthermore, the proposed IFQ-Tinier-YOLO face detector gives a further 8? savings over IFQ-Tiny-YOLO with 5% performance drop. We think its performance is promising in the sense of its extremely compact model size and quite satisfactory detection rate. More importantly, the proposed IFQ-Tinier-YOLO face detector is a fixed-point network which can be easily implemented on embedded devices. The ROC curves of the compared face detectors are illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>. Moreover, the proposed IFQ-Tinier-YOLO is also effective on detecting small faces. We test it on Wider Face val-idation images and show its qualitative results. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, our IFQ-Tinier-YOLO also gives nice detection on small faces in various challenging scenarios such as make-up, out of focus, low-illumination, paintings etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a novel fixed-point network, IFQ-Net, for embedded vision. It divides a quantization network into substructures and then converts each substructure into fixed-point in either separated or the proposed integrated manner. Especially for the quantized substructures, which commonly appear in quantization networks, the integrated conversion method removes on-device batch normalization computation, requires no scaling-up effect (m = 0) yet most importantly does not cause performance drop. We compared our IFQ-Net with the state-of-the-art fixed-point network indicating that our method gives much more savings on model size and feature map memory with similar (or higher in some case) accuracy on ImageNet.</p><p>Furthermore, we also designed a fixed-point face detector IFQ-Tinier-YOLO. Comparing with the Tiny-YOLO detector, our model shows its great benefits on embedded devices in the sense of extremely compact model size (246k Bytes), purely fixed-point data operations and quite satisfactory detection rate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The flowchart of converting a floating-point quantization network into IFQ-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Fixed-point conversion for a substructure: a) separated conversion which separately transforms each floating-point data into fixed-point data through the floor function ? ; b) integrated conversion which employs a composition operation f to compose all the floating-point calculations to quantization layer and then apply the fixed-point conversion for the composed results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Substructure division for a quantized network: a)substructure without max pooling layer; b)substructure with max pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>As illustrated in Equation 7, all the floating-point data of a quantized substructure is composed into the newly formed thresholds (e.g. thr1 * ?+??b ?? ). Such composition process is performed with floating-point data and thus does not impact the output result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>separated(m = 12) 0.5206 0.5296 0.5470 separated(m = 10) 0.5168 0.5292 0.5443 separated(m = 9) 0.5073 0.5230 0.5385 separated(m = 8) 0.4585 0.4678 0.5105 integrated(m = 0) 0.5214 0.5301 0.5471</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Top-1 accuracy on ImageNet of networks with various m for subtructure7 and substructure1 fixed-point conversion. loss during each fixed-point conversion x * 2 m . Nevertheless, when m ? 14, no further performance improvement can be observed for all the three networks indicating m = 14 would be sufficient for fixed-point conversion for the floating-point data in substructure7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Performance of the face detectors on FDDB dataset<ref type="bibr" target="#b6">[7]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative performance of the proposed IFQ-Tiner-YOLO (k = 2) face detector on Wider Face dataset<ref type="bibr" target="#b17">[18]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Substructures of AlexNet-HWGQ network.</figDesc><table><row><cell cols="4">substructure1 substructure2 ... substructure7</cell></row><row><cell>Conv 1</cell><cell>Conv q 2</cell><cell></cell><cell>FC q 7</cell></row><row><cell>Pool 1 BN 1</cell><cell>Pool 2 BN 2</cell><cell>...</cell><cell>BN 7 ReLU 7</cell></row><row><cell>Quant 1</cell><cell>Quant 2</cell><cell></cell><cell>FC 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance of different methods on converting the quantized substructures of AlexNet-HWGQ networks on ImageNet top-1 classification accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table ,</head><label>,</label><figDesc></figDesc><table><row><cell>regarding to</cell></row><row><cell>the model size of the compared fixed-point networks, our</cell></row><row><cell>IFQ-AlexNet networks (k = 2 or 4) give 2.16? savings(58.8</cell></row><row><cell>Mbits vs. 127.3 Mbits) over "Lin et al. [9] (? = 9)".</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Model size (Mbits), inference memory for feature maps (Mbits) and performance comparison of fixed-point networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison on the model size (MB) of the trained face detectors and their detection rate on FDDB dataset<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell></cell><cell>Tiny-</cell><cell>IFQ-Tiny-</cell><cell>Tinier-</cell><cell>IFQ-Tinier-</cell></row><row><cell></cell><cell>YOLO</cell><cell>YOLO (k=2)</cell><cell>YOLO</cell><cell>YOLO (k=2)</cell></row><row><cell>model</cell><cell>63.00</cell><cell>1.97</cell><cell>7.89</cell><cell>0.25</cell></row><row><cell>size (MB)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>detection</cell><cell>0.92</cell><cell>0.89</cell><cell>0.90</cell><cell>0.84</cell></row><row><cell>rate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For fast calculation, Qm is usually set to 2 m so that the multiplication can be implemented by simple m-bit left shift</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To be consistent with the reference paper<ref type="bibr" target="#b8">[9]</ref>, the parameters in F C 8 are not included.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with low precision by half-wave gaussian quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BinaryConnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems, NIPS 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hardware-oriented approximation of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gysel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Motamedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghiasi</surname></persName>
		</author>
		<idno>abs/1604.03168</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computing&apos;s energy problem (and what we can do about it)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-state Circuits Conference Digest of Techinical Papers</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">FDDB: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM- CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Overcoming challenges in fixed point training of deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<idno>abs/1607.02241</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fixed point quantization of deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Annapureddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1510.03009</idno>
		<title level="m">Neural networks with few multiplications. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision, ECCV 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">8-bit inference with TensorRT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Migacz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision, ECCV 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Darknet: Open source neural networks in C</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<ptr target="http://pjreddie.com/darknet/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automated flow for compressing convolution neural networks for efficient edge-computation with FPGA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Vilchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
