<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchopt: Reproducible, efficient and collaborative optimization benchmarks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Moreau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universit? Paris-Saclay</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CEA</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathurin</forename><surname>Massias</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">LIP UMR 5668</orgName>
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">ENS de Lyon</orgName>
								<address>
									<addrLine>UCB Lyon 1</addrLine>
									<postCode>F-69342</postCode>
									<settlement>Lyon</settlement>
									<region>Inria</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universit? Paris-Saclay</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CEA</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ablin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Universit? Paris-Dauphine</orgName>
								<orgName type="institution" key="instit2">PSL University</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Bannier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Charlier</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">IMAG</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Dagr?ou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universit? Paris-Saclay</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CEA</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dupr? La Tour</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghislain</forename><surname>Durif</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">IMAG</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassio</forename><forename type="middle">F</forename><surname>Dantas</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution" key="instit1">TETIS</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">INRAE</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Klopfenstein</surname></persName>
							<affiliation key="aff8">
								<orgName type="institution" key="instit1">University of Luxembourg</orgName>
								<orgName type="institution" key="instit2">LCSB</orgName>
								<address>
									<settlement>Esch-sur-Alzette</settlement>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Larsson</surname></persName>
							<affiliation key="aff9">
								<orgName type="department">The Department of Statistics</orgName>
								<orgName type="institution">Lund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universit? Paris-Saclay</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CEA</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanguy</forename><surname>Lefort</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">IMAG</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Mal?zieux</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universit? Paris-Saclay</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CEA</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badr</forename><surname>Moufad</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">LIP UMR 5668</orgName>
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">ENS de Lyon</orgName>
								<address>
									<addrLine>UCB Lyon 1</addrLine>
									<postCode>F-69342</postCode>
									<settlement>Lyon</settlement>
									<region>Inria</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff10">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">T?l?com Paris</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Rakotomamonjy</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">Criteo AI Lab</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaccharie</forename><surname>Ramzi</surname></persName>
							<affiliation key="aff12">
								<orgName type="laboratory">UMR 8553</orgName>
								<orgName type="institution" key="instit1">ENS Ulm</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Salmon</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">IMAG</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Institut Universitaire de France (IUF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Vaiter</surname></persName>
							<affiliation key="aff13">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Universit? C?te d&apos;Azur</orgName>
								<orgName type="institution" key="instit3">Laboratoire J.A. Dieudonn?</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<settlement>Nice</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Benchopt: Reproducible, efficient and collaborative optimization benchmarks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerical validation is at the core of machine learning research as it allows to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automate, reproduce and publish optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard learning tasks: 2 -regularized logistic regression, Lasso, and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of the state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details. We hope that Benchopt will foster collaborative work in the community hence improving the reproducibility of research findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Numerical experiments have become an essential part of statistics and machine learning (ML). It is now commonly accepted that every new method needs to be validated through comparisons with existing approaches on standard problems. Such validation provides insight into the method's benefits and limitations and thus adds depth to the results. While research aims at advancing knowledge and not just improving the state of the art, experiments ensure that results are reliable and support theoretical claims <ref type="bibr" target="#b129">(Sculley et al., 2018)</ref>. Practical validation also helps the ever-increasing number of ML users in applied sciences to choose the right method for their task. Performing rigorous and extensive experiments is, however, time-consuming <ref type="bibr" target="#b119">(Raff, 2019)</ref>  <ref type="figure" target="#fig_7">Figure 1</ref>: A visual summary of Benchopt. Each Solver is run (in parallel) on each Dataset and each variant of the Objective. Results are exported as a CSV file that is easily shared and can be automatically plotted as interactive HTML visualizations or PDF figures. against existing methods in new settings often requires reimplementing baseline methods from the literature. In addition, ingredients necessary for a proper reimplementation may be missing, such as important algorithmic details, hyperparameter choices, and preprocessing steps <ref type="bibr" target="#b112">(Pineau et al., 2019)</ref>.</p><p>In the past years, the ML community has actively sought to overcome this "reproducibility crisis" <ref type="bibr" target="#b72">(Hutson, 2018)</ref> through collaborative initiatives such as open datasets <ref type="bibr">(OpenML, Vanschoren et al. 2013)</ref>, standardized code sharing <ref type="bibr" target="#b55">(Forde et al., 2018)</ref>, benchmarks (MLPerf, <ref type="bibr" target="#b100">Mattson et al. 2020)</ref>, the NeurIPS and ICLR reproducibility challenges <ref type="bibr" target="#b112">(Pineau et al., 2019;</ref><ref type="bibr" target="#b111">Pineau et al., 2021)</ref> and new journals (e.g., <ref type="bibr" target="#b124">Rougier and Hinsen 2018)</ref>. As useful as these endeavors may be, they do not, however, fully address the problems in optimization for ML since, in this area, there are no clear community guidelines on how to perform, share, and publish benchmarks.</p><p>Optimization algorithms pervade almost every area of ML, from empirical risk minimization, variational inference to reinforcement learning <ref type="bibr" target="#b132">(Sra et al., 2012)</ref>. It is thus crucial to know which methods to use depending on the task and setting <ref type="bibr" target="#b7">(Bartz-Beielstein et al., 2020)</ref>. While some papers in optimization for ML provide extensive validations <ref type="bibr" target="#b96">(Lueckmann et al., 2021)</ref>, many others fall short in this regard due to lack of time and resources, and in turn feature results that are hard to reproduce by other researchers. In addition, both performance and hardware evolve over time, which eventually makes static benchmarks obsolete. An illustration of this is the recent work by <ref type="bibr" target="#b128">Schmidt et al. (2021)</ref>, which extensively evaluates the performances of 15 optimizers across 8 deep-learning tasks. While their benchmark gives an overall assessment of the considered solvers, this assessment is bound to become out-of-date if it is not updated with new solvers and new architectures. Moreover, the benchmark does not reproduce state-of-the-art results on the different datasets, potentially indicating that the considered architectures and optimizers could be improved.</p><p>We firmly believe that this critical task of maintaining an up-to-date benchmark in a field cannot be solved without a collective effort. We want to empower the community to take up this challenge and build a living, reproducible and standardized state of the art that can serve as a foundation for future research. Benchopt provides the tools to structure the optimization for machine learning (Opt-ML) community around standardized benchmarks, and to aggregate individual efforts for reproducibility and results sharing. Benchopt can handle algorithms written in Python, R, Julia or C/C++ via binaries. It offers built-in functionalities to ease the execution of benchmarks: parallel runs, caching, and automatical results archiving. Benchmarks are meant to evolve over time, which is why Benchopt offers a modular structure through which a benchmark can be easily extended with new objective functions, datasets, and solvers by the addition of a single file of code.</p><p>The paper is organized as follows. We first detail the design and usage of Benchopt, before presenting results on three canonical problems:</p><p>? 2 -regularized logistic regression: a convex and smooth problem which is central to the evaluation of many algorithms in the Opt-ML community, and remains of high relevance for practitioners;</p><p>? the Lasso: the prototypical example of non-smooth convex problem in ML;</p><p>? training of ResNet18 architecture for image classification: a large scale non-convex deep learning problem central in the field of computer vision.</p><p>The reported benchmarks, involving dozens of implementations and datasets, shed light on the current state-of-the-art solvers for each problem, across various settings, highlighting that the best algorithm largely depends on the dataset properties (e.g., size, sparsity), the hyperparameters, as well as hardware. A variety of other benchmarks (e.g., <ref type="bibr">MCP,</ref><ref type="bibr">TV1D,</ref><ref type="bibr">etc.)</ref> are also presented in Appendix, with the goal to facilitate contributions from the community.</p><p>By the open source and collaborative design of Benchopt (BSD 3-clause license), we aim to open the way towards community-endorsed and peer-reviewed benchmarks that will improve the tracking of progress in optimization for ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Benchopt library</head><p>The Benchopt library aims to provide a standard toolset and structure to implement benchmarks for optimization in ML, where the problems depend on some input dataset D. The considered problems are of the form ? * ? arg min</p><formula xml:id="formula_0">??? f (?; D, ?) ,<label>(1)</label></formula><p>where f is the objective function, ? are its hyperparameters, and ? is the feasible set for ?. The scope of the library is to evaluate optimization methods in their wide sense by considering the sequence {? t } t produced to approximate ? * . We emphasize than Benchopt does not provide a fixed set of benchmarks, but a framework to create, extend and share benchmarks on any problem of the form (1). To provide a flexible and extendable coding standard, benchmarks are defined as the association of three types of object classes:  Objective: It defines the function f to be minimized as well as the hyperparameters ? or the set ?, and the metrics to track along the iterations (e.g., objective value, gradient norm for smooth problems, or validation loss). Multiple metrics can be registered for each ? t .</p><p>Datasets: The Dataset objects provide the data D to be passed to the Objective class. They control how data is loaded and preprocessed. Datasets are separated from the Objective, making it easy to add new ones, provided they are coherent with the Objective.</p><p>Solvers: The Solver objects define how to run the algorithm. They are provided with the Objective and Dataset objects and output a sequence {? t } t . This sequence can be obtained using a single run of the method, or with multiple runs in case the method only returns its final iterate.</p><p>Each of these objects can have parameters that change their behavior, e.g., the regularization parameters for the Objective, the choice of preprocessing for the Datasets, or the step size for the Solvers. By exposing these parameters in the different objects, Benchopt can evaluate their influence on the benchmark results. The Benchopt library defines an application programming interface (API) for each of these concepts and provides a command line interface (CLI) to make them work together. A benchmark is defined as a folder that contains an Objective as well as subfolders containing the Solvers and Datasets. Appendix B presents a concrete example on Ridge regression of how to construct a Benchopt benchmark while additional design design choices of Benchopt are discussed in Appendix C.</p><p>For each Dataset and Solver, and for each set of parameters, Benchopt retrieves a sequence {? t } t and evaluates the metrics defined in the Objective for each ? t . To ensure fair evaluation, the computation of these metrics is done off-line. The metrics are gathered in a CSV file that can be used to display the benchmark results, either locally or as HTML files published on a website that reference the benchmarks run with Benchopt. This workflow is described in <ref type="figure" target="#fig_7">Figure 1</ref>. As one of the goal of Benchopt is to make benchmarks as simple as possible, it also provides a set of features to make them easy to develop and run. Benchopt is written in Python, but Solvers run with implementations in different languages (e.g., R and Julia, as in Section 4) and frameworks (e.g., PyTorch and TensorFlow, as in Section 5). Moreover, benchmarks can be run in parallel with checkpointing of the results, enabling large scale evaluations on many CPU or GPU nodes. Benchopt also makes it possible to run solvers with many different hyperparameters' values , allowing to assess their sensitivity on the method performance. Benchmark results are also automatically exported as interactive visualizations, helping with the exploration of the many different settings.</p><p>Benchmarks All presented benchmarks are run on 10 cores of an Intel Xeon Gold 6248 CPUs @ 2.50GHz and NVIDIA V100 GPUs (16GB). The results' interactive plots and data are available at https://benchopt.github.io/results/preprint_results.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">First example: 2 -regularized logistic regression</head><p>Logistic regression is a very popular method for binary classification. From a design matrix X ? R n?p with rows X i and a vector of labels y ? {?1, 1} n with corresponding element y i , 2 -regularized logistic regression provides a generalized linear model indexed by ? * ? R p to discriminate the classes by solving</p><formula xml:id="formula_1">? * = arg min ??R p n i=1 log 1 + exp(?y i X i ?) + ? 2 ? 2 2 ,<label>(2)</label></formula><p>where ? &gt; 0 is the regularization hyperparameter. Thanks to the regularization part, Problem <ref type="formula" target="#formula_1">(2)</ref> is strongly convex with a Lipschitz gradient, and thus its solution can be estimated efficiently using many iterative optimization schemes.</p><p>The most classical methods to solve this problem take inspiration from Newton's method <ref type="bibr" target="#b143">(Wright and Nocedal, 1999)</ref>. On the one hand, quasi-Newton methods aim at approximating the Hessian of the cost function with cheap to compute operators. Among these methods, L-BFGS <ref type="bibr" target="#b92">(Liu and Nocedal, 1989)</ref> stands out for its small memory footprint, its robustness and fast convergence in a variety of settings. On the other hand, truncated Newton methods <ref type="bibr" target="#b44">(Dembo et al., 1982)</ref> try to directly approximate Newton's direction by using e.g., the conjugate gradient method <ref type="bibr" target="#b54">(Fletcher and Reeves, 1964)</ref> and Hessian-vector products to solve the associated linear system. Yet, these methods suffer when n is large: each iteration requires a pass on the whole dataset.</p><p>In this context, methods based on stochastic estimates of the gradient have become standard <ref type="bibr" target="#b21">(Bottou, 2010)</ref>, with Stochastic Gradient Descent (SGD) as a main instance. The core idea is to use cheap and noisy estimates of the gradient <ref type="bibr" target="#b122">(Robbins and Monro, 1951;</ref><ref type="bibr" target="#b79">Kiefer and Wolfowitz, 1952)</ref>. While SGD generally converges either slowly due to decreasing step sizes, or to a neighborhood of the solution for constant step sizes, variance-reduced adaptations such as SAG <ref type="bibr" target="#b127">(Schmidt et al., 2017)</ref>, SAGA <ref type="bibr" target="#b43">(Defazio et al., 2014)</ref> and SVRG <ref type="bibr" target="#b75">(Johnson and Zhang, 2013)</ref> make it possible to solve the problem more efficiently and are often considered to be state-of-the-art for large scale problems.</p><p>Finally, methods based on coordinate descent <ref type="bibr" target="#b14">(Bertsekas, 1999)</ref> have also been proposed to solve Problem (2). While these methods are usually less popular, they can be efficient in the context of sparse datasets, where only few samples have non-zero values for a given feature, or when accelerated on distributed systems or GPU <ref type="bibr" target="#b46">(D?nner et al., 2018)</ref>.</p><p>The code for the benchmark is available at https://github.com/benchopt/benchmark_logreg_ l2/. To reflect the diversity of solvers available, we showcase a Benchopt benchmark with 3 datasets, 10 optimization strategies implemented in 5 packages, leveraging GPU hardware when possible. We also consider different scenarios for the objective function: (i) scaling (or not) the features, a recommended data preprocessing step, crucial in practice to have comparable regularization strength on all variables; (ii) fitting (or not) an unregularized intercept term, important in practice and making optimization harder when omitted from the regularization term <ref type="bibr" target="#b81">(Koh et al., 2007)</ref> 10 ?2 10 ?1 10 0 10 1 10 4 10 0 10 ?4 10 ?8 ijcnn1 Raw 10 ?2 10 ?1 10 0 10 1 Scaled 10 ?2 10 ?1 10 0 10 1 Intercept 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 4 10 0 10 ?4 10 ?8 madelon 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 ?1 10 0 10 1 10 2 10 3 10 4 10 0 10 ?4 10 ?8 news20.binary 10 ?1 10 0 10 1 10 2 10 3 10 ?1 10 0 10 1 10 2 10 3 10 0 10 1 10 2 10 3 Time (s) 10 4 10 0 10 ?4 10 ?8 criteo 10 0 10 1 10 2 10 3 Time (s) 10 0 10 1 10 2 10 3 Time (s) <ref type="figure">Figure 3</ref>: Benchmark for the 2 -regularized logistic regression, on 13 solvers, 4 datasets (rows), and 3 variants of the Objective (columns) with ? = 1. The curves display the suboptimality of the iterates, f (? t ) ? f (? * ), as a function of time. The first column corresponds to the objective function detailed in Problem (2). In the second column, datasets were preprocessed by normalizing each feature to unit standard deviation. The third column is for an objective function which includes an unregularized intercept. not) with sparse features, which prevent explicit centering during preprocessing to keep memory usage limited. Details on packages, datasets and additional scenarios are available in Appendix D.</p><p>Results <ref type="figure">Figure D</ref>.1 presents the results of the benchmarks, in terms of suboptimality of the iterates, f (? t ) ? f (? * ), for three datasets and three scenarios. Here, because the problem is convex, ? * is approximated by the best iterate across all runs (see Section C.1). Overall, the benchmark shows the benefit of using GPU solvers (cuML and snapML), even for small scale tasks such as ijcnn1. Note that these two accelerated solvers converge to a higher suboptimality level compared to other solvers, due to operating with 32-bit float precision. Another observation is that data scaling can drastically change the picture. In the case of madelon, most solvers have a hard time converging for the scaled data. For the solvers that converge, we note that the convergence time is one order of magnitude smaller with the scaled dataset compared to the raw one. This stems from the fact that in this case, the scaling improves the conditioning of the dataset. 1 For news20.binary, the stochastic solvers such as SAG and SAGA have degraded performances on scaled data. Here, the scaling makes the problem harder. 2</p><p>On CPU, quasi-Newton solvers are often the most efficient ones, and provide a reasonable choice in most situations. For large scale news20.binary, stochastic solvers such as SAG, SAGA or SVRG -that are often considered as state of the art for such problem-have worst performances for the presented datasets. While this dataset is often used as a test bed for benchmarking new stochastic solvers, we fail to see an improvement over non-stochastic ones for this experimental setup. In contrast, the last row in <ref type="figure">Figure D</ref>.1 displays an experiment with the larger scale criteo dataset, which demonstrates a regime where variance-reduced stochastic gradient methods outperform quasi-Newton methods. For future benchmarking of stochastic solvers, we therefore recommend using such a large dataset.</p><p>Finally, the third column in <ref type="figure">Figure D</ref>.1 illustrates a classical problem when benchmarking different solvers: their specific (and incompatible) definition and resolution of the corresponding optimization problem. Here, the objective function is modified to account for an intercept (bias) in the linear model. In most situations, this intercept is not regularized when it is fitted. However, snapML and liblinear solvers do regularize it, leading to incomparable losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Second example: The Lasso</head><p>The Lasso, <ref type="bibr" target="#b135">(Tibshirani, 1996;</ref><ref type="bibr" target="#b33">Chen et al., 1998)</ref>, is an archetype of non-smooth ML problems, whose impact on ML, statistics and signal processing in the last three decades has been considerable <ref type="bibr" target="#b29">(B?hlmann and van de Geer, 2011;</ref><ref type="bibr" target="#b67">Hastie et al., 2015)</ref>. It consists of solving</p><formula xml:id="formula_2">? * ? arg min ??R p 1 2 y ? X? 2 + ? ? 1 ,<label>(3)</label></formula><p>where X ? R n?p is a design matrix containing p features as columns, y ? R n is the target vector, and ? &gt; 0 is a regularization hyperparameter. The Lasso estimator was popularized for variable selection: when ? is high enough, many entries in ? * are exactly equal to 0. This leads to more interpretable models and reduces overfitting compared to the least-squares estimator.</p><p>Solvers for Problem (3) have evolved since its introduction by <ref type="bibr" target="#b135">Tibshirani (1996)</ref>. After generic quadratic program solvers, new dedicated solvers were proposed based on iterative reweighted leastsquares (IRLS) <ref type="bibr" target="#b62">(Grandvalet, 1998)</ref>, followed by LARS <ref type="bibr" target="#b47">(Efron et al., 2004)</ref>, a homotopy method computing the full Lasso path 3 . The LARS solver helped popularize the Lasso, yet the algorithm suffers from stability issues and can be very slow for worst case situations <ref type="bibr" target="#b97">(Mairal and Yu, 2012)</ref>. General purpose solvers became popular for Lasso-type problems with the introduction of the iterative soft thresholding algorithm (ISTA, <ref type="bibr" target="#b42">Daubechies et al. 2004)</ref>, an instance of forward-backward splitting <ref type="bibr" target="#b37">(Combettes and Wajs, 2005)</ref>. These algorithms became standard in signal and image processing, especially when accelerated (FISTA, Beck and Teboulle 2009).</p><p>In parallel, proximal coordinate descent has proven particularly relevant for the Lasso in statistics. Early theoretical results were proved by <ref type="bibr" target="#b138">Tseng (1993)</ref> and <ref type="bibr" target="#b126">Sardy et al. (2000)</ref>, before it became the standard solver of the widely distributed packages glmnet in R and scikit-learn in Python. For further improvements, some solvers exploit the sparsity of ? * , trying to identify its support to reduce the problem size. Best performing variants of this scheme are screening rules (e.g., El <ref type="bibr" target="#b48">Ghaoui et al., 2012;</ref><ref type="bibr" target="#b20">Bonnefoy et al., 2015;</ref><ref type="bibr" target="#b105">Ndiaye et al., 2017)</ref> and working/active sets (e.g., <ref type="bibr" target="#b76">Johnson and Guestrin 2015;</ref><ref type="bibr" target="#b99">Massias et al. 2018)</ref>, including strong rules <ref type="bibr" target="#b134">(Tibshirani et al., 2012)</ref>.</p><p>While reviews of Lasso solvers have already been performed <ref type="bibr">(Bach et al., 2012, Sec. 8</ref>.1), they are limited to certain implementation and design choices, but also naturally lack comparisons with more recent solvers and modern hardware, hence drawing biased conclusions.</p><p>The code for the benchmark is available at https://github.com/benchopt/benchmark_lasso/.</p><p>Results obtained on 4 datasets, with 9 standard packages and some custom reimplementations, possibly leveraging GPU hardware, and 17 different solvers written in Python/numba/Cython, R, Julia or C++ <ref type="table" target="#tab_9">(Table E</ref>.1) are presented in <ref type="figure" target="#fig_1">Figure 4</ref>. All solvers use efficient numerical implementations, possibly leveraging calls to BLAS, precompiled code in Cython or just-in-time compilation with numba.</p><p>The different parameters influencing the setup are</p><p>? the regularization strength ?, controlling the sparsity of the solution, parameterized as a fraction of ? max = X y ? (the minimal hyperparameter such that ? * = 0),</p><p>? the dataset dimensions: MEG has small n and medium p; rcv1.binary has medium n and p; news20.binary has medium n and very large p while MillionSong has very large n and small p ( 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 1 10 ?2 10 ?5 10 ?8 MEG ? = 0.1? max 10 ?2 10 ?1 10 0 10 1 10 2 10 3 ? = 0.01? max 10 ?2 10 ?1 10 0 10 1 10 2 10 3 ? = 0.001? max 10 ?2 10 ?1 10 0 10 1 10 7 10 4 10 1 10 ?2 10 ?5 MillionSong 10 ?2 10 ?1 10 0 10 1 10 ?2 10 ?1 10 0 10 1 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 0 10 ?3 10 ?6 rcv1.binary 10 ?2 10 ?1 10 0 10 1 10 2 10 ?2 10 ?1 10 0 10 1 10 2 10 ?1 10 0 10 1 10 2 10 3 Time (s) Results <ref type="figure" target="#fig_1">Figure 4</ref> presents the result of the benchmark on the Lasso, in terms of objective suboptimality f (? t ) ? f (? * ) as a function of time.</p><p>Similarly to Section 3, the GPU solvers obtain good performances in most settings, but their advantage is less clear. A consistent finding across all settings is that coordinate descent-based methods outperform full gradient ones (ISTA and FISTA, even restarted), and are improved by the use of working set strategies (blitz, celer, skglm, glmnet). This observation is even more pronounced when the regularization parameter is large, as the solution is sparser.</p><p>When observing the influence of the dataset dimensions, we observe 3 regimes. When n is small (MEG), the support of the solution is small and coordinate descent, LARS and noncvx-pro perform the best. When n is much larger than p (MillionSong), noncvx-pro clearly outperforms other solvers, and working set methods prove useless. Finally, when n and p are large (rcv1.binary, news20.binary), CD and working sets vastly outperforms the rest while noncvx-pro fails, as it requires solving a linear system of size min(n, p). We note that this setting was not tested in the original experiment of <ref type="bibr" target="#b113">Poon and Peyr? (2021)</ref>, which highlights the need for extensive, standard experimental setups.</p><p>When the support of the solution is small (either small ?, either small n since the Lasso solution has at most n nonzero coefficients), LARS is a competitive algorithm. We expect this to degrade when n increases, but as the LARS solver in scikit-learn does not support sparse design matrices we could not include it for news20.binary and rcv1.binary.</p><p>This benchmark is the first to evaluate solvers across languages, showing the competitive behavior of lasso.jl and glmnet compared to Python solvers. Both solvers have a large initialization time, and then converge very fast. To ensure that the benchmark is fair, even though the Benchopt library is implemented in Python, we made sure to ignore conversion overhead, as well as just-in-time compilation cost. We also checked the timing's consistency with native calls to the libraries.</p><p>Since the Lasso is massively used for it feature selection properties, the speed at which the solvers identify the support of the solution is also an important performance measure. Monitoring this with Benchopt is straightforward, and a figure reporting this benchmark is in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Third example: How standard is a benchmark on ResNet18?</head><p>As early successes of deep learning have been focused on computer vision tasks <ref type="bibr" target="#b86">(Krizhevsky et al., 2012)</ref>, image classification has become a de facto standard to validate novel methods in the field. Among the different network architectures, ResNets <ref type="bibr" target="#b69">(He et al., 2016)</ref> are extensively used in the community as they provide strong and versatile baselines <ref type="bibr" target="#b144">(Xie et al., 2017;</ref><ref type="bibr" target="#b133">Tan and Le, 2019;</ref><ref type="bibr" target="#b45">Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b28">Brock et al., 2021;</ref><ref type="bibr" target="#b93">Liu et al., 2022)</ref>. While many papers present results with such model on classical datasets, with sometimes extensive ablation studies <ref type="bibr" target="#b70">(He et al., 2019;</ref><ref type="bibr" target="#b142">Wightman et al., 2021;</ref><ref type="bibr" target="#b9">Bello et al., 2021;</ref><ref type="bibr" target="#b128">Schmidt et al., 2021)</ref>, the lack of standardized codebase and missing implementation details makes it hard to replicate their results.</p><p>The code for the benchmark is available at https://github.com/benchopt/benchmark_resnet_ classif/. We provide a cross-dataset - Another notable difference is that we report the evolution of the test error rather than the training loss.</p><p>Because we chose to monitor the test loss, the Solvers are defined as the combination of an optimization algorithm, its hyperparameters, the learning rate (LR) and weight decay schedules, and the data augmentation strategy. This is in contrast to a case where we would monitor the train loss, and therefore make the LR and weight decay schedules, as well as the data augmentation policy, part of the objective. We focus on 2 standard methods: stochastic gradient descent (SGD) with momentum and Adam <ref type="bibr" target="#b80">(Kingma and Ba, 2015)</ref>, as well as a more recently published one: Lookahead . The LR schedules are chosen among fixed LR, step LR 4 , and cosine annealing <ref type="bibr" target="#b94">(Loshchilov and Hutter, 2017)</ref>. We also consider decoupled weight decay for Adam <ref type="bibr" target="#b95">(Loshchilov and Hutter, 2019)</ref>, and coupled weight decay (i.e., 2 -regularization) for SGD. Regarding data augmentation, we use random cropping for all datasets and add horizontal flipping only for CIFAR-10, as the digits datasets do not exhibit a mirror symmetry. We detail the remaining hyperparameters in <ref type="table" target="#tab_9">Table F</ref>.2, and discuss their selection as well as their sensitivity in Appendix F.</p><p>Aligning cross-framework implementations Due to some design choices, components with the same name in the different frameworks do not have the same behavior. For instance, when it comes to applying weight decay, PyTorch's SGD uses coupled weight decay, while in TensorFlow/Keras weight decay always refers to decoupled weight decay. These two methods lead to significantly different performance and it is not straightforward to apply coupled weight decay in a post-hoc manner in TensorFlow/Keras (see further details in Section F.3). We conducted an extensive effort to align the networks implementation in different frameworks using unit testing to make the conclusions of our benchmarks independent of the chosen framework. We found additional significant differences (reported in <ref type="table" target="#tab_9">Table F</ref>.3) in the initialization, the batch normalization, the convolutional layers and the weight decay scaling. In dashed black is the state of the art for the corresponding datasets with a ResNet18 measured by <ref type="bibr" target="#b146">Zhang et al. (2019)</ref> for CIFAR-10, by <ref type="bibr" target="#b148">Zheng et al. (2021)</ref> for SVHN with a PreAct ResNet18, by PapersWithCode for MNIST with all networks considered. Off-the-shelf ResNet implementations in TensorFlow/Keras do not support images smaller than 32 ? 32 and is hence not shown for MNIST.Curves are exponentially smoothed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results of the benchmark are reported in <ref type="figure" target="#fig_3">Figure 5</ref>. Each graph reports the test error relative to time, with an ablation study on the solvers parameters. Note that we only report selected settings for clarity but that we run every possible combinations. 5 .</p><p>Firstly, reaching the state of the art for a vanilla ResNet18 is not straightforward. On the popular website Papers with code it has been so far underestimated. It can achieve 4.45% and 2.65% test error rates on CIFAR-10 and SVHN respectively (compared to 4.73% and 2.95% -for a PreAct ResNet18before that). Our ablation study shows that a variety of techniques is required to reach it. The most significant one is an appropriate data augmentation strategy, which lowers the error rate on CIFAR-10 from about 18% to about 8%. The second most important one is weight decay, but it has to be used in combination with a proper LR schedule, as well as momentum. While these techniques are not novel, they are regularly overlooked in baselines, resulting in underestimation of their performance level.</p><p>This reproducible benchmark not only allows a researcher to get a clear understanding of how to achieve the best performances for this model and datasets, but also provides a way to reproduce and extend these performances. In particular, we also include in this benchmark the original implementation of Lookahead . We confirm that it slightly accelerates the convergence of the Best SGD, even with a cosine LR schedule -a setting that had not been studied in the original paper.</p><p>Our benchmark also evaluates the relative computational performances of the different frameworks. We observe that PyTorch-Lightning is significantly slower than the other frameworks we tested, in large part due to their callbacks API. We also notice that our TensorFlow/Keras implementation is significantly slower (? 28%) than the PyTorch ones, despite following the best practices and our profiling efforts. Note that we do not imply that TensorFlow is intrinsically slower than PyTorch, but a community effort is needed to ensure that the benchmark performances are framework-agnostic.</p><p>A recurrent criticism of such benchmarks is that only the best test error is reported. In <ref type="figure" target="#fig_4">Figure 6</ref>, we measure the effect of using a train-validation-test split, by keeping a fraction of the training set as a validation set. The splits we use are detailed in <ref type="table" target="#tab_9">Table F</ref>.1. Our finding is that the results of the ablation study do not change significantly when using such procedure, even though their validity is reinforced by the use of multiple trainings. Yet, a possible limitation of our findings is that some of the hyperparameters we used for our study, coming from the PyTorch-CIFAR GitHub repository, may have been tuned while looking at the test set.   <ref type="figure" target="#fig_3">Figure 5</ref> for more details). In addition, we show in colored horizontal dashed lines, the test results for early stopping on the validation and on the test set for the different solvers, the square mark indicating the moment this stopping would happen. The curves for the train-val splits show the exponentially smoothed median results for five different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We have introduced Benchopt, a library that makes it easy to collaboratively develop fair and extensive benchmarks of optimization algorithms, which can then be seamlessly published, reproduced, and extended. In the future, we plan on supporting the creation of new benchmarks, that could become the standards the community builds on. This work is part of a wider effort to improve reproducibility of machine learning results. It aims to contribute to raising the standard of numerical validation for optimization, which is pervasive in the statistics and ML community as well as for the experimental sciences that rely more and more on these tools for research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>It can not be stressed enough how much the Benchopt library relies on contributions from the community and in particular the Python open source ecosystem. In particular, it could not exist without the libraries mentioned in Appendix A.</p><p>This work was granted access to the HPC resources of IDRIS under the allocation 2022-AD011011172R2 and 2022-AD011013570 made by GENCI, which was used to run all the benchmarks. MM also gratefully acknowledges the support of the Centre Blaise Pascal's IT test platform at ENS de Lyon (Lyon, France) for Machine Learning facilities. The platform operates the SIDUS solution <ref type="bibr" target="#b117">(Quemener and Corvellec, 2013)</ref>.</p><p>TL, CFD and JS contributions were supported by the Chaire IA CaMeLOt (ANR-20-CHIA-0001-01). AG, EL and TM contributions were supported by the Chaire IA ANR BrAIN (ANR-20-CHIA-0016). BMa contributions were supported by a grant from Digiteo France. MD contributions were supported by a public grant overseen by the French National Research Agency (ANR) through the program UDOPIA, project funded by the ANR-20-THIA-0013-01 and DATAIA convergence institute (ANR-17-CONV-0003 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Software ecosystem acknowledgement</head><p>The command line interface and API use the click, pyyaml and psutil <ref type="bibr" target="#b123">(Rodola, 2016)</ref> libraries.</p><p>Numerical computations involve numpy  and scipy . For cross-language processing, we used rpy2 for calling R (R Core Team, 2017) libraries and PyJulia for interfacing with Julia <ref type="bibr" target="#b15">(Bezanson et al., 2017)</ref>. The benchmark runs extensively use joblib, loky <ref type="bibr" target="#b103">(Moreau et al., 2017)</ref> and submitit for parallelization.</p><p>The results are stored and processed for visualizations using pandas <ref type="bibr" target="#b102">(McKinney et al., 2010)</ref>, matplotlib <ref type="bibr" target="#b71">(Hunter, 2007)</ref> for static rendering, mako and plotly <ref type="bibr" target="#b73">(Inc., 2015)</ref> for interactive webpages. The participative results website relies partially on pygithub.</p><p>Our documentation is generated by multiple sphinx-based <ref type="bibr" target="#b25">(Brandl, 2010)</ref> libraries (sphinx-bootstrap-theme, sphinx-click, sphinx-gallery <ref type="bibr" target="#b104">(N?jera et al., 2020)</ref> and sphinx-prompt), and also the numpydoc and pillow <ref type="bibr" target="#b36">(Clark, 2015)</ref> libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B A complete Benchmark example: Objective, Dataset and Solver classes for Ridge regression</head><p>Here, we provide code examples for a simple benchmark on Ridge regression. The Ridge regression -also called 2 -regularized least-squares or Tikhonov regression -is a popular method to solve least-square problems in the presence of noisy observations or correlated features. The problem reads: min</p><formula xml:id="formula_3">? 1 2 y ? X? 2 2 + ? 2 ? 2 2 ,<label>(4)</label></formula><p>where X ? R n?p is a design matrix, y ? R n is the target vector and ? is the regularization parameter. This problem is strongly convex and many methods can be used to solve it. Direct computation of the close form solution ? * = (X X + ?Id) ?1 X y can be obtained using matrix factorization methods such as Cholesky decomposition or the SVD <ref type="bibr" target="#b115">(Press et al., 2007)</ref> or iterative linear solver such as Conjugate-Gradient <ref type="bibr" target="#b92">(Liu and Nocedal, 1989)</ref>. One can also resort on first order methods such as gradient descent, coordinate descent (known as the Gauss-Seidel method in this context), or their stochastic variant.</p><p>The code for the benchmark is available at https://github.com/benchopt/benchmark_ridge/.</p><p>The following code snippets are provided in the documentation as a template for new benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Objective class</head><p>The Objective class is the central part of the benchmark, defining the objective function. This class allows us to monitor the quantities of interest along the iterations of the solvers, amongst which the objective function value. An Objective class should define 3 methods:</p><p>? set_data(**data): it allows specifying the nature of the data used in the benchmark. The data is passed as a dictionary of Python variables, so no constraint is enforced to what can be passed here.</p><p>? compute(theta): it allows evaluating the objective function for a given value of the iterate, here called ?. This method should take only one parameter, the output returned by the Solver. All other parameters should be stored in the class with the set_data method. The compute function should return a float (understood as the objective value) or a dictionary. If a dictionary is returned it should contain a key called value (the objective value) and all other keys should correspond to float values allowing tracking more than one quantity of interest (e.g. train and test errors).</p><p>? to_dict(): a method that returns a dictionary to be passed to the set_objective() method of a Solver.</p><p>An Objective class needs to inherit from a base class, benchopt.BaseObjective. Below is the implementation of the Ridge regression Objective class. def set_data ( self , X , y): self .X , self .y = X , y def compute ( self , theta ): res = self .y -self .X @ theta return .5 * res @ res + 0.5 * self . reg * theta @ theta def to_dict ( self ): return dict (X= self .X , y= self .y , reg = self . reg )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Dataset class</head><p>A Dataset class defines data to be passed to the Objective. More specifically, a Dataset class should implement one method:</p><p>? get_data(): A method outputting a dictionary that can be passed as keyword arguments **data to the set_data method of the Objective.</p><p>A Dataset class also needs to inherit from a base class, benchopt.BaseDataset.</p><p>If a Dataset requires some packages to function, Benchopt allows listing some requirements. The necessary packages should be available via conda or pip.</p><p>Below is an example of a Dataset definition using the libsvmdata library, which exposes datasets from libsvm, such as leukemia, bodyfat and gisette -described in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Solver class</head><p>A Solver class must define three methods:</p><p>? set_objective(**objective_dict): This method will be called with the dictionary objective_dict returned by the method to_dict from the Objective. The goal of this method is to provide all necessary information to the Solver so it can optimize the objective function.</p><p>? run(stop_value): This method takes only one parameter that controls the stopping condition of the Solver. Typically this is either a number of iterations n_iter or a tolerance parameter tol.</p><p>Alternatively, a callback function that will be called at each iteration can be passed. The callback should return False once the computation should stop. The parameter stop_value is controlled by the stopping_strategy, see below for details.</p><p>? get_result(): This method returns a variable that can be passed to the compute method from the Objective. This is the output of the Solver.</p><p>If a Python Solver requires some packages such as scikit-learn, Benchopt allows listing some requirements. The necessary packages must be available via conda or pip.</p><p>Below is a simple Solver example using scikit-learn implementation of Ridge regression with different optimization algorithms.</p><p>from benchopt import BaseSolver from benchopt import safe_import_context # This context allow to manipulate the Solver object even if # scikit -learn is not installed . It is used in ' benchopt install '. with safe_import_context () as import_ctx : from sklearn . linear_model import Ridge class Solver ( BaseSolver ): name = " scikit -learn " install_cmd = " conda " requirements = [" scikit -learn "] parameters = { " alg ": [" svd " , " cholesky " , " lsqr " , " sparse_cg " , " saga "] , } def __init__ ( self , alg =" svd "):</p><p>self . alg = alg def set_objective ( self , X , y , reg =1): self .X , self .y = X , y self . clf = Ridge ( fit_intercept = False , alpha = reg , solver = self . alg , tol = 1e -10 )  We also run the solvers on the simulated data described bellow.</p><p>Generation process for simulated dataset We generate a linear regression scenario with decaying correlation for the design matrix, i.e., the ground-truth covariance matrix is a Toeplitz matrix, with each element ? ij = ? |i?j| . As a consequence, the generated features have 0 mean, a variance of 1, and the correlation structure as:</p><formula xml:id="formula_4">E[X i ] = 0 , E[X 2 i ] = 1 and E[X i X j ] = ? |i?j| .<label>(5)</label></formula><p>Our simulation scheme also includes the parameter density = 0.2 that controls the proportion of non-zero elements in ? * . The target vector is generated according to linear relationship with Gaussian noise: y = X? * + ? , such that the signal-to-noise ratio is snr = X? * 2 ? 2 .</p><p>We use a signal-to-noise ratio snr = 3, a correlation ? of 0 or 0.6 with n = 500 samples and p = 1000 features. <ref type="table" target="#tab_9">Table B</ref>.2 describes the different solvers compared in this benchmark. Results <ref type="figure">Figure B</ref>.1 presents the performance of the different methods for different values of the regularization parameter in the benchmark. The algorithms based on the direct computation of the closed-form solution outperform iterative ones in a majority of presented datasets. Among closed-form algorithms, the Cholesky solver converges faster. 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 4 10 0 10 ?4 10 ?8 bodyfat ? = 0.01 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 ? = 0.1 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 ? = 1 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 4 10 0 10 ?4 10 ?8 leukemia 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 4 10 0 10 ?4 10 ?8 gisette 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 4 10 0 10 ?4 10 ?8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description of the solvers</head><p>Simulated 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 ?5 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 Time (s) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Design choices</head><p>Benchopt has made some design choices, while trying as much as possible to leave users free of customizing the behavior on each benchmark. We detail the most important ones in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Estimating ? * for convex problems</head><p>When the problem is convex, many solvers are guaranteed to converge to a global minimizer ? * of the objective function f . To estimate ? * and f (? * ), Benchopt approximates ? * by the iterate achieving the lowest objective_value among all solvers for a given Dataset and Objective. This means that the sub-optimality plot proposed by Benchopt are only valid if at least one solver has converged to the optimal solution. Else, the curves are a lower bound estimate of the sub-optimality. In practice, for most considered convex problems, running the Solver for long enough ensures that f (? * ) is correctly estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Stopping solvers</head><p>Benchopt offers many ways to stop running a solver. The most common is to stop the solver when the objective value does not decrease significantly between iterations. For some convex problems, we also propose to track the duality gap (which upper bounds the suboptimality), as is done for the Lasso. For non convex problems, criteria such as gradient norm or violation of first order conditions can be used, as users do in practice. These criteria can easily be customized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Wall-clock time versus number of iterations</head><p>Measuring time or iteration are two alternatives that make sense in their respective contexts. Practitioners mostly care about the time it takes to solve their problem, while researchers in mathematical optimization may want to abstract away the implementation and hardware details and only consider iteration. The benchmarks we have presented showcase efficient implementations and are also interested in hardware and implementation differences (e.g. CPU vs GPU solvers for ??, torch versus tensorflow for Section F.4), hence our focus on time. However, Benchopt does not impose a choice between the two measures: it is perfectly possible to create plots as a function of the number of iterations as evidenced for example in ??. D 2 -regularized logistic regression D.1 List of solvers and datasets used in the benchmark in Section 3    <ref type="figure">Figure D</ref>.1 presents the performance results for the different solvers on the different datasets using various regularization parameter values, on unscaled raw data. We observe that when the regularization parameter ? increases, the problem tends to become easier and faster to solve for most methods. Also, the relative order of the method does not change significantly for the considered range of regularization. ijcnn1 ? = 0.1 10 ?2 10 ?1 10 0 10 1 ? = 1 10 ?2 10 ?1 10 0 10 1 ? = 10 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 4 10 0 10 ?4 10 ?8 madelon 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?1 10 0 10 1 10 2 10 4 10 0 10 ?4 10 ?8 news20.binary 10 ?1 10 0 10 1 10 2 10 ?1 10 0 10 1 10 2 10 ?2 10 ?1 10 0 10 1 10 4 10 0 10 ?4 10 ?8 rcv1.binary 10 ?2 10 ?1 10 0 10 1 10 ?2 10 ?1 10 0 10 1 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 Time (s)    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Support identification speed benchmark</head><p>Since the Lasso is massively used for its feature selection properties, the speed at which the solvers identify the support of the solution is also an important performance measure. To evaluate the behavior of solvers in this task, it is sufficient to add a single new variable in the Objective, namely the 0 pseudonorm of the iterate, allowing to produce <ref type="figure">Figure E</ref>.1 in addition to <ref type="figure" target="#fig_1">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Convergence in terms of iteration</head><p>?? 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 0 10 ?1 10 ?2 10 ?3 MEG ? = 0.1? max 10 ?2 10 ?1 10 0 10 1 10 2 10 3 ? = 0.01? max 10 ?2 10 ?1 10 0 10 1 10 2 10 3 ? = 0.001? max 10 ?2 10 ?1 10 0 10 1 10 2 10 0 10 ?1 10 ?2 10 ?3 rcv1.binary 10 ?2 10 ?1 10 0 10 1 10 2 10 ?2 10 ?1 10 0 10 1 10 2 10 ?1 10 0 10 1 10 2 10 3 10 0 10 ?1 10 ?2 10 ?3 10 ?4 10 ?5 news20.binary 10 ?1 10 0 10 1 10 2 10 3 10 ?1 10 0 10 1 10 2 10 3 10 ?2 10 ?1 10 0 10 1 Time (s) While practitioners are mainly concerned with the time it takes to solve their optimization problem, one may also be interested in the convergence as a function of the number of iterations. This is particularly relevant to compare theoretical convergence rates with experiments. Benchopt natively supports such functionality. Yet, this makes sense only if one iteration of each algorithm costs the same. <ref type="figure">Figure E</ref>.2 presents such a case on the leukemia dataset, using algorithms for which one iteration costs n ? p. One can observe that cyclic coordinate descent as implemented in Cython in scikit-learn or in Numba lead to identical results, while they outperform proximal gradient methods.  Setting up the benchmark The three currently supported frameworks are TensorFlow/Keras <ref type="bibr">(Abadi et al., 2015;</ref><ref type="bibr" target="#b35">Chollet et al., 2015)</ref>, PyTorch <ref type="bibr" target="#b108">(Paszke et al., 2019)</ref> and PyTorch Lightning <ref type="bibr" target="#b50">(Falcon et al., 2020)</ref>. We report here results for TensorFlow/Keras and PyTorch. To guarantee that the model behaves consistently across the different considered frameworks, we implemented several consistency unit tests. We followed the best practice of each framework to make sure to achieve the optimal computational efficiency. In particular, we tried as much as possible to use official code from the frameworks, and not third-party code. We also optimized and profiled the data pipelines to make sure that our training was not IO-bound. Our benchmarks were run using TensorFlow version 2.8 and PyTorch version 1.10. In <ref type="table" target="#tab_9">Table F</ref>.1, we present some characteristics of the different datasets used for the ResNet18 benchmark. In particular, we specify the size of each splits when using the train-validation-test split strategy. The test split is always fixed, and is the official one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptions of the datasets</head><p>While the datasets are downloaded and preprocessed using the official implementations of the frameworks, we made sure to test that they matched using a unit test.</p><p>ResNet The ResNet18 is the smallest variant of the architecture introduced by <ref type="bibr" target="#b69">He et al. (2016)</ref>. It consists in 3 stages:</p><p>1. A feature extension convolution that goes from 3 channels (RGB, or a repeated grayscale channel in the MNIST case) to 64, followed by a batch normalization and a ReLU.</p><p>2. A series of residual blocks. Residual blocks are grouped by scale, and each individual group starts with a strided convolution to reduce the image scale (except the first one). As the scale increases, so does the number of features <ref type="bibr">(64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512)</ref>. In the ResNet18 case, each scale group has two individual residual blocks and there are four scales. A residual block is comprised of three convolution operations, all followed by a batch normalization layer, and the first two also followed by a ReLU. The input is then added to the output of the third batch normalization layer before being fed to a ReLU.</p><p>3. A classification head that performs global average pooling, before applying a fully connected (i.e. dense) layer to obtain logits.</p><p>Training's hyperparameters   <ref type="table" target="#tab_9">Table F</ref>.2. In dashed black is the state of the art for CIFAR-10 with a ResNet18 measured by <ref type="bibr" target="#b146">Zhang et al. (2019)</ref>. Curves are exponentially smoothed.</p><p>In <ref type="table" target="#tab_9">Table F</ref>.2, we specify the hyperparameters we used for the benchmark. For the SGD, the values were taken from the pytorch-cifar GitHub repository, while for Adam we took the most relevant ones from the work of <ref type="bibr" target="#b142">Wightman et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Hyperparameter sensitivity</head><p>In the benchmark presented in Section 5, we consider fixed hyperparameters chosen from common practices to train ResNet18 models for an image classification task. However, in practice, these hyperparameters must be carefully set, either via a grid search, or via more adapted algorithms such as random search (bergstra2012random) or bayesian optimization <ref type="bibr" target="#b108">(Paszke et al., 2019)</ref>. It is therefore important to evaluate how sensitive an optimizer is to choosing the right parameters, as more sensitive methods will require more exhaustive hyperparameters search. In <ref type="figure">Figure F</ref>.1, we study this issue using Benchopt for image classification on CIFAR-10. Despite achieving the best results in terms of accuracy, SGD is way more sensitive to the choice of hyperparameters than Adam. 6 Another way to look at hyperparameter sensitivity is to evaluate how a given selection of hyperparameters performs for different tasks. <ref type="figure" target="#fig_3">Figure 5</ref> shows that while SGD is sensitive to the choice of learning rate and weight decay, the selected values work very well across 3 different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Aligning TensorFlow and PyTorch ResNet18 training</head><p>We summarized in <ref type="table" target="#tab_9">Table F</ref>.3 the different elements that have to be considered to align the training of a ResNet18 in PyTorch and TensorFlow. Let us detail here some lines of this table:</p><p>? Bias in convolutions: It can be seen in TensorFlow/Keras official implementation, that convolutions operations use a bias. This is in contrast to PyTorch's official implementation in torchvision which does not. Since the convolutions are followed by batch normalization layers, with a mean removal, the convolutions' bias is a spurious parameter, as was noted by <ref type="bibr" target="#b74">Ioffe and Szegedy (2015)</ref>. We therefore chose to use unbiased convolutions.</p><p>? Decoupled weight decay scaling: this led us to scale manually the weight decay used in Ten-sorFlow by the learning rate when setting it. Moreover, because the weight decay is completely decoupled from the learning rate, it is important to update it accordingly when using a learning rate schedule, as noted in the TensorFlow documentation.</p><p>? Batch normalization momentum: an important note here is that the convention used to implement the batch normalization momentum is not the same in the 2 frameworks. Indeed we have the relationship momentum TF = 1 ? momentum PT .  <ref type="bibr" target="#b58">(Glorot and Bengio, 2010)</ref>. PyTorch uses a He normal initialization <ref type="bibr" target="#b68">(He et al., 2015)</ref>. We used TensorFlow's Variance Scaling framework to differentiate the 2.</p><p>? Striding in convolutions: when using a stride of 2 in convolutions on an even-size image, one needs to specify where to start the convolution in order to know which lines (one in every two) in the image will be removed. The decision is different between TensorFlow and PyTorch. This is not expected to have an effect on the final performance, but it makes it more difficult to compare the architectures when unit testing. We therefore decided to align the models on this aspect as well.</p><p>? Variance estimation in batch normalization: in order to estimate the batch variance during training for batch normalization layers, it is possible to chose between the unbiased and the biased variance estimator. The unbiased variance estimator applies a Bessel correction to the biased variance estimator, namely a multiplication by a factor m m?1 , where m is the number of samples used to estimate. It is to be noted that PyTorch does uses the biased estimator in training, but stores the unbiased estimator for use during inference. TensorFlow does not allow for such a behaviour, and the 2 are therefore not reconcilable 7 . Arguably this inconsistency should not play a big role with large batch sizes, but can be significant for smaller batches, especially in deeper layers where the feature map size (and therefore the number of samples used to compute the estimates) is reduced.</p><p>Adapting official ResNet implementations to small images In addition to these elements, it is important to adapt the reference implementations of both frameworks to the small image case. Indeed, for the case of ImageNet, the ResNet applies two downsampling operations (a stride-2 convolution and a max pooling) at the very beginning of the network to make the feature maps size more manageable. In the case of smaller images, it is necessary to do without these downsampling operations (i.e. perform the convolution with stride 1 and get rid of the max pooling).</p><p>Coupled weight decay in TensorFlow In TensorFlow, the SGD implementation does not allow the setting of coupled weight decay. Rather, one has to rely on the equivalence (up to a scale factor of 2) between coupled weight decay and L2 regularization. However, in TensorFlow/Keras, adding L2 regularization on an already built model (which is the case for the official ResNet implementation), is not straightforward and we relied on the workaround of <ref type="bibr" target="#b130">Silva (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 VGG benchmark on CIFAR-10</head><p>In order to show how flexible Benchopt is, we also ran a smaller version of the ResNet benchmark using a VGG16 <ref type="bibr" target="#b131">(Simonyan and Zisserman, 2015)</ref> network instead of a ResNet18. In the Benchopt framework, this amounts to specifying a different model in the objective, while all the other pieces of code in the benchmark remain unchanged. Note that the VGG official implementations also need to be adapted to the CIFAR-10 case by changing the classification head. This was not specified in the original paper, where no experiment was conducted on small-scale datasets, and we relied on available open source implementations (cifar10-vgg16 and cifar-vgg) to make this decision. Importantly, these implementations use batch normalization to make the training of VGG more robust to initialization, which is not the case in the official framework implementations.</p><p>In <ref type="figure">Figure F</ref>.2, we see that for the case of VGG, the application of weight decay is so important that without it, in cases with momentum, the model does not converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G 1 -regularized logistic regression</head><p>This additional benchmark is dedicated to 1 -regularized logistic regression, in the same setting as Problem (2) but this time with an 1 -regularization for the parameters of the model:</p><formula xml:id="formula_5">? * = arg min ??R p n i=1 log 1 + exp(?y i X i ?) + ? ? 1 .<label>(6)</label></formula><p>G.1 List of solvers and datasets used in the 1 -regularized logistic regression benchmark</p><p>The code for the benchmark is available at https://github.com/benchopt/benchmark_logreg_ l1/.   10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 0 10 ?3 10 ?6 gisette ? = 0.1? max 10 ?2 10 ?1 10 0 10 1 10 2 ? = 0.01? max 10 ?2 10 ?1 10 0 10 1 10 2 ? = 0.001? max 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 0 10 ?3 10 ?6 colon-cancer 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 0 10 ?3 10 ?6 rcv1.binary 10 ?2 10 ?1 10 0 10 1 10 2 10 ?2 10 ?1 10 0 10 1 10 2 10 ?1 10 0 10 1 10 2 10 3 Time (s) The first column corresponds to the objective detailed in Problem (6) with ? = 0.1 X y ? /2, the second one with ? = 0.01 X y ? /2 and the third column with ? = 0.001 X y ? /2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Results</head><p>The results of the 1 -regularized logistic regression benchmark are in <ref type="figure">Figure G.</ref>1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Unidimensional total variation</head><p>The use of 1D Total Variation regularization takes its root in the taut-string algorithm <ref type="bibr" target="#b6">(Barlow and Brunk, 1972)</ref> and can be seen as a special case of either the Rudin-Osher-Fatemi model <ref type="bibr" target="#b125">(Rudin et al., 1992)</ref> or the Generalized Lasso <ref type="bibr" target="#b137">(Tibshirani and Taylor, 2011)</ref> for a quadratic data fit term. It reads</p><formula xml:id="formula_6">? * ? arg min ??R p F (y, X?) + ? D? 1 ,<label>(7)</label></formula><p>where F is a data fidelity term, X ? R n?p is a design matrix with n samples and p features, y ? R n is the target vector, ? &gt; 0 is a regularization hyperparameter, and D ? R (p?1)?p is a finite difference operator defined by (D?) k = ? k+1 ? ? k for all 1 ? k ? p ? 1 (it is also possible to use cyclic differences).</p><p>Most often, the data fidelity term is the 2 -loss F (y, z) = 1 2 y ? z 2 2 , following an additive Gaussian noise hypothesis. But the data fit term can also account for other types of noises, such as noises with heavy tails using the Huber loss F (y, z) = |y ? z| ? where | ? | ? is defined coordinate-wise by</p><formula xml:id="formula_7">|t| ? = 1 2 t 2 if |t| ? ? ?|t| ? ? 2 2 otherwise.</formula><p>Problem <ref type="formula" target="#formula_6">(7)</ref> promotes piecewise-constant solutions -alternatively said, solutions such that their gradients is sparse -and was proved to be useful in several applications, in particular for change point detection <ref type="bibr" target="#b16">(Bleakley and Vert, 2011;</ref><ref type="bibr" target="#b136">Tibshirani, 2014)</ref>, for BOLD signal deconvolution in functional MRI <ref type="bibr" target="#b77">(Karahanoglu et al., 2013;</ref><ref type="bibr" target="#b34">Cherkaoui et al., 2019)</ref> or for detrending in oculomotor recordings <ref type="bibr" target="#b87">(Lalanne et al., 2020)</ref>.</p><p>The penalty ? ? D? 1 is convex but non-smooth, and its proximity operator has no closed form. Yet as demonstrated by <ref type="bibr" target="#b39">Condat (2013a)</ref>, the taut-string algorithm allows to compute this proximity operator in O(p 2 ) operations in the worst case, but it enjoys a O(p) complexity in most cases. Other methods do not rely on this proximity operator and directly solve Problem <ref type="formula" target="#formula_6">(7)</ref>, using either primal-dual approaches <ref type="bibr" target="#b32">(Chambolle and Pock, 2011;</ref><ref type="bibr" target="#b40">Condat, 2013b)</ref>, or solving the dual problem <ref type="bibr" target="#b83">(Komodakis and Pesquet, 2015)</ref>. Finally, for 1-dimensional TV regularization, one can also use the synthesis formulation <ref type="bibr" target="#b49">(Elad et al., 2006)</ref> to solve the problem. By setting z = D? and ? = Lz + ? where L ? R p?p?1 is a lower trianglar matrix representing an integral operator (cumulative sum), the problem is equivalent to a Lasso problem, and ? * has a closed-form expression (see e.g., Bleakley and Vert 2011 for a proof). As a consequence, any lasso solver can be used to obtain the solution of the Lasso problem z * and the solution of the original Problem (7) u * is retrieved as u * = Lz * + ? * .</p><p>The code for the benchmark is available at https://github.com/benchopt/benchmark_tv_1d/ and <ref type="table" target="#tab_9">Table H</ref>.1 details the different algorithms used in this benchmark.  10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 n=400, K=50</p><p>10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 n=750, K=10 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 8 10 4 10 0 10 ?4 10 ?8 2 ? = 0.1? max 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 8 10 4 10 0 10 ?4 10 ?8</p><p>Huber[? = 0.9] ? = 0.5? max 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 Time (s) Simulated dataset We use here simulated data, as applications based on fMRI and EOG signals require access to open and preprocessed data that we will make available on OpenML <ref type="bibr" target="#b139">Vanschoren et al., 2013</ref> in the future. The data are generated as follows: a block signal? ? R p is generated by sampling first a sparse random vector z ? R p with K non-zero coefficients positioned randomly, and taking random values following a N (0, 1) distribution. Finally,? is obtained by discrete integration as? i = i k=1 z k for 1 ? i ? p. The design matrix X ? R n?p is a Gaussian random design with X ij <ref type="figure" target="#fig_7">? N (0, 1)</ref>. The observations y are obtained as y = X? + , with ? N (0, 0.01) a Gaussian white noise. For all experiments, we used p = 500 and vary the number of non-zero coefficient K, and the number of rows n of the matrix X.</p><p>Results <ref type="figure">Figure H</ref>.1 shows that the solvers using the synthesis formulation and coordinate descentbased solvers for the Lasso ( 2 data fit term) work best on this type of problem. For the Huber data fit term, the solver using the analysis formulation and the taut-string algorithm for the proximal operator are faster. An interesting observation from this benchmark is the behavior of the solvers based on primal-dual splitting or dual formulation. We observe that for all these solvers, the objective starts by increasing. This is probably due to a sub-optimal initialization of the dual variable compared to the primal one. While this initialization is seldom described in the literature, it seems to have a large</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Linear regression with minimax concave penalty (MCP)</head><p>The Lasso problem <ref type="bibr" target="#b135">(Tibshirani, 1996)</ref> is a least-squares regression problem with a convex non-smooth penalty that induces sparsity in its solution. However, despite its success and large adoption by the machine learning and signal processing communities, it is plagued with some statistical drawbacks, such as bias for large coefficients. To overcome these issues, the standard approach is to consider non-convex sparsity-inducing penalties. Several penalties have been proposed: Smoothly Clipped Absolute Deviation (SCAD, <ref type="bibr" target="#b51">Fan and Li 2001)</ref>, the Log Sum penalty <ref type="bibr" target="#b30">(Cand?s et al., 2008)</ref>, the capped-1 penalty <ref type="bibr" target="#b147">(Zhang, 2010b)</ref> or the Minimax Concave Penalty (MCP, Zhang 2010a).</p><p>This benchmark is devoted to least-squares regression with the latter, namely the problem:</p><formula xml:id="formula_8">? * ? arg min ??R p 1 2n y ? X? 2 + p j=1 ? ?,? (? j ) ,<label>(8)</label></formula><p>where X ? R n?p is a design matrix containing p features as columns, y ? R n is the target vector, and ? ?,? the penalty function that reads as:</p><formula xml:id="formula_9">? ?,? (t) = ?|t| ? t 2 2? , if |t| ? ?? , ? 2 ? 2 , if |t| &gt; ?? .</formula><p>Similarly to the Lasso, Problem (8) promotes sparse solutions but the optimization problem raises some difficulties due to the non-convexity and non-smoothness of the penalty. Nonetheless, several efficient algorithms have been derived for solving it. The ones we use in the benchmark are listed in <ref type="table" target="#tab_9">Table I</ref>.1. The code for the benchmark is available at https://github.com/benchopt/benchmark_mcp/. For this benchmark, we run the solvers on the colon-cancer dataset and on the simulated dataset described in <ref type="table" target="#tab_9">Table B</ref>.4. We use a signal-to-noise ratio snr = 3, a correlation ? = 0.6 with n = 500 observations and p = 2000 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 Results</head><p>The result of the benchmark is presented in <ref type="figure" target="#fig_7">Figure I.1</ref> The problem is non-convex and solvers are only guaranteed to converge to local minima; hence in <ref type="figure">Figure I</ref>.1 we monitor the distance of the negative gradient to the Fr?chet subdifferential of the MCP, representing the violation of the first order optimality condition. Other metrics, such as objective of iterates sparsity, are monitored in the full benchmark, allowing to compare the different limit points obtained by the solvers.</p><p>coordinate descent PGD accelerated GIST skglm PGD WorkSetCD 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 2 10 ?1 10 ?4 10 ?7 10 ?10 colon-cancer ? = 0.1? max 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 ? = 0.01? max 10 ?4 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 ? = 0.001? max 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 ?1 10 ?4 10 ?7 10 ?10 simulated (scaled) 10 ?3 10 ?2 10 ?1 10 0 10 1 10 ?3 10 ?2 10 ?1 10 0 10 1 10 ?3 10 ?2 10 ?1 10 0 10 1 Time (s) 10 2 10 ?1 10 ?4 10 ?7 10 ?10 simulated 10 ?3 10 ?2 10 ?1 10 0 10 1 Time (s) 10 ?3 10 ?2 10 ?1 10 0 10 1 Time (s) <ref type="figure">Figure I</ref>.1: Benchmark for the MCP regression on variants of the Objective (columns). The curves display the violation of optimality conditions, dist(?X (X? t ? y)/n, ?? ?,? (? t )), as a function of time. ? is set to 3, and ? is parameterized as a fraction of the Lasso's ? max , X y ? /n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Zero-order optimization on standard functions</head><p>Zero-order optimization refers to scenarios where only calls to the function to minimize are possible. This is in contrast with first-order optimization where gradient information is available. Grid search, random search, evolution strategies (ES) or Bayesian optimization (BO) are popular methods to tackle such a problem and are most commonly employed for hyperparameter optimization. This setting is also known as black-box optimization.</p><p>This benchmark demonstrates the usability of Benchopt for zero-order optimization considering functions classically used in the literature <ref type="bibr" target="#b64">(Hansen et al., 2021)</ref>. The functions are available in the PyBenchFCN package https://github.com/Y1fanHE/PyBenchFCN/. In particular, among the 61 functions of interest we present here (see <ref type="figure">Figure J</ref>.1) a benchmark for three functions, defined for any x = (x 1 , . . . , x N ) ? R N :</p><formula xml:id="formula_10">f (x) = N ?1 i=1 [100(x i+1 ? x 2 i ) 2 + (1 ? x i ) 2 ] (Rosenbrock) f (x) = 10 ? N + N i=1</formula><p>[(x 2 i ? 10 ? cos(2?x i )]</p><p>(Rastrigin)</p><formula xml:id="formula_11">f (x) = ?20 ? exp ? ? ?0.2 1 d N i=1 x 2 i ? ? ? exp 1 d N i=1</formula><p>cos(2?x i ) + e + 20 (Ackley) .</p><p>For each function, the domain is restricted to a box: x ? ? 32 for Ackley, x ? ? 30 for Rosenbrock and x ? ? 5.12 for Rastrigin. The algorithms considered in the benchmark are listed in <ref type="table" target="#tab_9">Table J</ref>.1. As BFGS requires first-order information, gradients are approximated with finitedifferences. Basin-hopping <ref type="bibr" target="#b141">Wales and Doye (1997)</ref> and <ref type="bibr" target="#b140">Virtanen et al. (2020)</ref> Two-phase method: global step + local min.</p><p>Nevergrad-RandomSearch <ref type="bibr" target="#b120">Rapin and Teytaud (2018)</ref> and <ref type="bibr" target="#b10">Bergstra and Bengio (2012)</ref> Sampler by random search Nevergrad-CMA <ref type="bibr" target="#b120">Rapin and Teytaud (2018)</ref> and <ref type="bibr" target="#b65">Hansen and Ostermeier (2001)</ref> CMA evolutionary strategy The code for the benchmark is available at https://github.com/benchopt/benchmark_zero_ order/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1 Results</head><p>The results of the benchmark are presented in <ref type="figure">Figure J</ref>.1. The functions are non-convex and solvers are only guaranteed to converge to local minima; hence in <ref type="figure">Figure J</ref>.1 we monitor the value of the function. The functions are designed such that the global minimum of the function is always 0. One nevergrad <ref type="bibr">[CMA]</ref> nevergrad <ref type="bibr">[ScrHammersleySearch]</ref> optuna <ref type="bibr">[TPE]</ref> scipy <ref type="bibr">[Powell]</ref> nevergrad <ref type="bibr">[NGOpt]</ref> nevergrad <ref type="bibr">[TwoPointsDE]</ref> scipy <ref type="bibr">[BFGS]</ref> nevergrad <ref type="bibr">[RandomSearch]</ref> optuna <ref type="bibr">[CMA]</ref> scipy <ref type="bibr">[Nelder-Mead]</ref> 10 ?4 10 ?2 10 <ref type="formula">0</ref>  can observe that the CMA and TwoPointsDE implementations from nevergrad consistently reaches the global minimum. In addition, the CMA implementation from optuna is a bit slower than the one from nevergrad. Also one can notice that random search offers reasonable results. The TPE method seems to suffer from the curse of dimensionality, as most kernel methods in non-parametric estimation. Finally regarding the scipy solvers, Powell can be competitive, while Nelder-Mead and BFGS suffer a lot from local minima.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 2: Standard benchmark structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Benchmark for the Lasso, on 17 solvers, 4 datasets (rows), and 3 variants of the Objective (columns) with decreasing regularization ?. The curves display the suboptimality of the objective function, f (? t ) ? f (? * ), as a function of time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>SVHN, Netzer et al. (2011);MNIST, LeCun et al. (2010)  and CIFAR-10, Krizhevsky (2009)-and cross-framework -TensorFlow/Keras,Abadi et al. (2015)  and<ref type="bibr" target="#b35">Chollet et al. (2015)</ref>; PyTorch, Paszke et al.(2019)-evaluation of the training strategies for image classification with ResNet18 (see Appendix F for details on architecture and datasets). We train the network by minimizing the cross entropy loss relatively to the weights ? of the model. Contrary to logistic regression and the Lasso, this problem is non-convex due to the non-linearity of the model f ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>ResNet18 image classification benchmark with PyTorch Solvers. The best SGD configuration features data augmentation, momentum, cosine learning rate schedule and weight decay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>ResNet18 image classification benchmark with a validation split. In dashed black is the state of the art (see caption of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>class</head><label></label><figDesc>Dataset ( BaseDataset ): name = " libsvm " install_cmd = " conda " requirements = [" libsvmdata "] parameters = { " dataset ": [" bodyfat " , " leukemia " , " gisette "] } def __init__ ( self , dataset =" bodyfat "): self . dataset = dataset def get_data ( self ): X , y = fetch_libsvm ( self . dataset ) return dict (X= self .X , y= self .y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure B. 1 :</head><label>1</label><figDesc>Benchmark for the Ridge regression, on 10 solvers, 5 datasets (rows), and 3 variants of the Objective (columns) each with a different regularization value ? ? {0.01, 0.1, 1}. The curves display the suboptimality of the iterates, f (? t ) ? f (? * ), as a function of time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure D. 1 :</head><label>1</label><figDesc>Additional benchmark for the 2 -regularized logistic regression on variants of the Objective (columns) with fit_intercept=False. The curves display the suboptimality of the iterates, f (? t ) ? f (? * ), as a function of time. The columns correspond to the objective detailed in Problem (2) with different value of ?: (first) ? = 0.1, (second) ? = 1 and (third) ? = 10. E Lasso E.1 List of solvers and datasets used in the Lasso benchmark in Section 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure E. 1 :</head><label>1</label><figDesc>Additional benchmark for the Lasso on variants of the Objective (columns). The curves display the fraction of non-zero coefficients in iterates ? t ( ? t 0 /p), as a function of time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure E. 2 :</head><label>2</label><figDesc>Convergence speed with respect to the number of iterations for some solvers of the Lasso benchmark on the leukemia dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure F. 1 :</head><label>1</label><figDesc>ResNet18 image classification benchmark on CIFAR-10 for different values of learning rate and weight decay for SGD and Adam. The default values are that reported in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure F. 2 :</head><label>2</label><figDesc>VGG16 image classification benchmark with PyTorch solvers. The best SGD configuration features data augmentation, momentum, step learning rate schedule and weight decay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure G. 1 :</head><label>1</label><figDesc>Benchmark for the 1 -regularized logistic regression on variants of the Objective (columns). The curves display the suboptimality of the iterates, f (? t ) ? f (? * ), as a function of time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>10 ?2 10 ?1 10 0 10 1 10 2 10 3 Time (s) 10 ?3 10 ?2 10 ?1 10 0 10 1 10 2 10 3 Time (s)Figure H.1: Benchmark for the T V -regularized regression regression, on 13 solvers, 4 variants of the Objective (rows), and 3 configurations for a simulated dataset (columns). The curves display the suboptimality of the iterates, f (? t ) ? f (? * ), as a function of time. The solvers in this benchmark showcase the three resolution approaches with the Analysis (A), Dual (D) and Synthesis (S) formulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure J. 1 :</head><label>1</label><figDesc>Benchmark for the zero-order optimization on the Ackley, Rosenbrock and Rastrigin functions in dimension N = 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, particularly because comparisons 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.13424v3 [cs.LG] 28 Oct 2022</figDesc><table><row><cell>.PDF</cell></row><row><cell>.CSV</cell></row><row><cell>.HTML</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Finally, by supporting multiple metrics -e.g., training and testing losses, error on parameter estimates, sparsity of the estimate -the Objective class offers the flexibility to define the concurrent evaluation, which can be extended to track extra metrics on a per benchmark basis, depending on the problem at hand.</figDesc><table /><note>This modular and standardized organization for benchmarks empowers the optimization community by making numerical experiments easily reproducible, shareable, flexible and extendable. The benchmark can be shared as a git repository or a folder containing the different definitions for the Objective, Datasets and Solvers and it can be run with the Benchopt CLI, hence becoming a convenient reference for future comparisons. This ensures fair evaluation of baselines in follow-up experiments, as implementations validated by the community are available. Moreover, benchmarks can be extended easily as one can add a Dataset or a Solver to the comparison by adding a single file.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table E.2).</figDesc><table><row><cell>blitz</cell><cell>celer</cell><cell>coordinate descent</cell><cell>cuML[cd]</cell></row><row><cell>cuML[qn]</cell><cell>FISTA</cell><cell>FISTA[adaptive-1]</cell><cell>FISTA[greedy]</cell></row><row><cell>glmnet</cell><cell>ISTA</cell><cell>LARS</cell><cell>lasso jl</cell></row><row><cell>noncvx-pro</cell><cell>scikit-learn</cell><cell>skglm</cell><cell>snapML[cpu]</cell></row><row><cell>snapML[gpu]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Best SGD with step LR schedule (no val test)Best SGD with step LR schedule (val test)Best SGD with cosine LR schedule (no val test)Best SGD with cosine LR schedule (val test)</figDesc><table><row><cell>Test/Val error (%)</cell><cell>6 8 10 4</cell><cell>1000</cell><cell>1500 CIFAR-10 2000</cell><cell>2500</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Time (s)</cell><cell></cell><cell></cell><cell>Time (s)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>). BN work was supported by the T?l?com Paris's Chaire DSAIDIS (Data Science &amp; Artificial Intelligence for Digitalized Industry Services). BMo contributions were supported by a grant from the Labex MILYON. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [Yes] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] These are specified in Appendix, on a per-benchmark basis. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Error bars are not reported for clarity, but Benchopt allows this in particular in html versions of the plots that can be found in https://benchopt.github.io/results/preprint_results.html. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] In the introduction. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects...</figDesc><table><row><cell>(a) Did you state the full set of assumptions of all theoretical results? [N/A]</cell></row><row><cell>(b) Did you include complete proofs of all theoretical results? [N/A]</cell></row><row><cell>3. If you ran experiments...</cell></row><row><cell>(a) Did you include the code, data, and instructions needed to reproduce the main experi-</cell></row><row><cell>mental results (either in the supplemental material or as a URL)? [Yes]</cell></row><row><cell>(b)</cell></row></table><note>(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table B .</head><label>B</label><figDesc></figDesc><table><row><cell cols="2">Datasets References</cell><cell cols="2">Samples (n) Features (p)</cell></row><row><cell cols="2">leukemia Golub et al. (1999)</cell><cell>38</cell><cell>7129</cell></row><row><cell>bodyfat</cell><cell>Guyon et al. (2004)</cell><cell>252</cell><cell>8</cell></row><row><cell>gisette</cell><cell>Guyon et al. (2004)</cell><cell>6000</cell><cell>5000</cell></row></table><note>1: List of the datasets used in Ridge regression in Appendix B</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table B .</head><label>B</label><figDesc>2: List of solvers used in the Ridge benchmark in Appendix B</figDesc><table><row><cell>Solver</cell><cell>References</cell><cell>Description</cell><cell></cell><cell>Language</cell></row><row><cell>GD</cell><cell cols="3">Boyd and Vandenberghe (2004) Gradient Descent</cell><cell>Python</cell></row><row><cell>Accelerated GD</cell><cell>Nesterov (1983)</cell><cell cols="2">Gradient Descent + ac-</cell><cell>Python</cell></row><row><cell></cell><cell></cell><cell>celeration</cell><cell></cell><cell></cell></row><row><cell>scikit-learn[svd]</cell><cell>Pedregosa et al. (2011)</cell><cell cols="2">SVD (Singular Value</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell cols="2">Decomposition)</cell><cell></cell></row><row><cell cols="2">scikit-learn[cholesky] Pedregosa et al. (2011)</cell><cell cols="2">Cholesky decomposi-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>tion</cell><cell></cell><cell></cell></row><row><cell>scikit-learn[lsqr]</cell><cell>Pedregosa et al. (2011)</cell><cell>regularized</cell><cell>least-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>squares</cell><cell></cell><cell></cell></row><row><cell>scikit-learn[saga]</cell><cell>Pedregosa et al. (2011)</cell><cell>SAGA</cell><cell>(Varianced</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>reduced</cell><cell>stochastic</cell><cell></cell></row><row><cell></cell><cell></cell><cell>method)</cell><cell></cell><cell></cell></row><row><cell>scikit-learn[cg]</cell><cell>Pedregosa et al. (2011)</cell><cell cols="2">Conjugate Gradient</cell><cell>Python (Cython)</cell></row><row><cell>CD</cell><cell>Bertsekas (1999)</cell><cell cols="2">Cyclic Coordinate De-</cell><cell>Python (Numba)</cell></row><row><cell></cell><cell></cell><cell>scent</cell><cell></cell><cell></cell></row><row><cell>lightning[cd]</cell><cell>Blondel and Pedregosa (2016)</cell><cell cols="2">Cyclic Coordinate De-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>scent</cell><cell></cell><cell></cell></row><row><cell>snapML[cpu]</cell><cell>D?nner et al. (2018)</cell><cell>CD</cell><cell></cell><cell>Python, C++</cell></row><row><cell>snapML[gpu]</cell><cell>D?nner et al. (2018)</cell><cell>CD + GPU</cell><cell></cell><cell>Python, C++</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table D.1 and Table D.2 respectively present the Solvers and Datasets used in this benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table D .</head><label>D</label><figDesc>1: List of solvers used in the 2 -regularized logistic regression benchmark in Section 3</figDesc><table><row><cell>Solver</cell><cell>References</cell><cell>Description</cell><cell>Language</cell></row><row><cell>lightning[sag]</cell><cell>Blondel and Pedregosa (2016)</cell><cell>SAG</cell><cell>Python (Cython)</cell></row><row><cell>lightning[saga]</cell><cell>Blondel and Pedregosa (2016)</cell><cell>SAGA</cell><cell>Python (Cython)</cell></row><row><cell>lightning[cd]</cell><cell>Blondel and Pedregosa (2016)</cell><cell>Cyclic Coordi-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>nate Descent</cell><cell></cell></row><row><cell>Tick[svrg]</cell><cell>Bacry et al. (2017)</cell><cell>Stochastic Vari-</cell><cell>Python, C++</cell></row><row><cell></cell><cell></cell><cell>ance Reduced</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Gradient</cell><cell></cell></row><row><cell>scikit-learn[sgd]</cell><cell>Pedregosa et al. (2011)</cell><cell>Stochastic Gra-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>dient Descent</cell><cell></cell></row><row><cell>scikit-learn[sag]</cell><cell>Pedregosa et al. (2011)</cell><cell>SAG</cell><cell>Python (Cython)</cell></row><row><cell>scikit-learn[saga]</cell><cell>Pedregosa et al. (2011)</cell><cell>SAGA</cell><cell>Python (Cython)</cell></row><row><cell cols="2">scikit-learn[liblinear] Pedregosa et al. (2011), Fan et al.</cell><cell>Truncated New-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell>(2008)</cell><cell>ton Conjugate-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Gradient</cell><cell></cell></row><row><cell>scikit-learn[lbfgs]</cell><cell>Pedregosa et al. (2011), Virtanen</cell><cell>L-BFGS</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell>et al. (2020)</cell><cell>(Quasi-Newton</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Method)</cell><cell></cell></row><row><cell cols="2">scikit-learn[newton-cg] Pedregosa et al. (2011), Virtanen</cell><cell>Truncated New-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell>et al. (2020)</cell><cell>ton Conjugate-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Gradient</cell><cell></cell></row><row><cell>snapml[cpu]</cell><cell>D?nner et al. (2018)</cell><cell>CD</cell><cell>Python, C++</cell></row><row><cell>snapml[gpu]</cell><cell>D?nner et al. (2018)</cell><cell>CD + GPU</cell><cell>Python, C++</cell></row><row><cell>cuML[gpu]</cell><cell>Raschka et al. (2020)</cell><cell cols="2">L-BFGS + GPU Python, C++</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table D .</head><label>D</label><figDesc>2: List of the datasets used in 2 -regularized logistic regression in Section 3</figDesc><table><row><cell>Datasets</cell><cell>References</cell><cell cols="2">Samples (n) Features (p)</cell><cell>Density</cell></row><row><cell cols="2">ijcnn1 madelon news20.binary Keerthi et al. (2005) Prokhorov (2001) Guyon et al. (2004) criteo Criteo-Labs (2015)</cell><cell>49 990 2000 19 996 45 840 617</cell><cell>22 500 1 355 191 1 000 000</cell><cell>4.5 ? 10 ?2 2.0 ? 10 ?3 3.4 ? 10 ?4 3.9 ? 10 ?5</cell></row><row><cell>D.2 Results</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Table E.1 and Table E.2 respectively present the Solvers and Datasets used in this benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table E .</head><label>E</label><figDesc>1: List of solvers used in the Lasso benchmark in Section 4</figDesc><table><row><cell>Solver</cell><cell>References</cell><cell>Description</cell><cell>Language</cell></row><row><cell>blitz</cell><cell>Johnson and Guestrin (2015)</cell><cell>CD + working set</cell><cell>Python, C++</cell></row><row><cell cols="2">coordinate descent Friedman et al. (2010)</cell><cell>(Cyclic) Minimization</cell><cell>Python (Numba)</cell></row><row><cell></cell><cell></cell><cell>along coordinates</cell><cell></cell></row><row><cell>celer</cell><cell>Massias et al. (2018)</cell><cell>CD + working set + dual</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>extrapolation</cell><cell></cell></row><row><cell>cuML[cd]</cell><cell>Raschka et al. (2020)</cell><cell>(Cyclic) Minimization</cell><cell>Python, C++</cell></row><row><cell></cell><cell></cell><cell>along coordinates</cell><cell></cell></row><row><cell>cuML[qn]</cell><cell>Raschka et al. (2020)</cell><cell>Orthant-Wise Limited</cell><cell>Python, C++</cell></row><row><cell></cell><cell></cell><cell>Memory Quasi-Newton</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(OWL-QN)</cell><cell></cell></row><row><cell>FISTA</cell><cell>Beck and Teboulle (2009)</cell><cell>ISTA + acceleration</cell><cell>Python</cell></row><row><cell>glmnet</cell><cell>Friedman et al. (2010)</cell><cell>CD + working set +</cell><cell>R, C++</cell></row><row><cell></cell><cell></cell><cell>strong rules</cell><cell></cell></row><row><cell>ISTA</cell><cell>Daubechies et al. (2004)</cell><cell>ISTA (Proximal GD)</cell><cell>Python</cell></row><row><cell>LARS</cell><cell>Efron et al. (2004)</cell><cell>Least-Angle Regression</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>algorithm (LARS)</cell><cell></cell></row><row><cell>FISTA[adaptive-1]</cell><cell>Liang et al. (2022, Algo 4), Far-</cell><cell cols="2">FISTA + adaptive restart Python</cell></row><row><cell></cell><cell>rens et al. (2020)</cell><cell></cell><cell></cell></row><row><cell>FISTA[greedy]</cell><cell>Liang et al. (2022, Algo 5), Far-</cell><cell cols="2">FISTA + greedy restart Python</cell></row><row><cell></cell><cell>rens et al. (2020)</cell><cell></cell><cell></cell></row><row><cell>noncvx-pro</cell><cell>Poon and Peyr? (2021)</cell><cell>Bilevel optim + L-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>BFGS</cell><cell></cell></row><row><cell>skglm</cell><cell>Bertrand et al. (2022)</cell><cell>CD + working set + pri-</cell><cell>Python (Numba)</cell></row><row><cell></cell><cell></cell><cell>mal extrapolation</cell><cell></cell></row><row><cell>scikit-learn</cell><cell>Pedregosa et al. (2011)</cell><cell>CD</cell><cell>Python (Cython)</cell></row><row><cell>snapML[gpu]</cell><cell>D?nner et al. (2018)</cell><cell>CD + GPU</cell><cell>Python, C++</cell></row><row><cell>snapML[cpu]</cell><cell>D?nner et al. (2018)</cell><cell>CD</cell><cell>Python, C++</cell></row><row><cell>lasso.jl</cell><cell>Kornblith (2021)</cell><cell>CD</cell><cell>Julia</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table E</head><label>E</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">.2: List of datasets used in the Lasso benchmark in Section 4</cell></row><row><cell>Dataset</cell><cell>References</cell><cell cols="2">Samples (n) Features (p)</cell><cell>Density</cell></row><row><cell>MEG</cell><cell>Gramfort et al. (2014)</cell><cell>305</cell><cell>7498</cell><cell>1.0</cell></row><row><cell cols="2">news20 rcv1 MillionSong Bertin-Mahieux et al. (2011) Keerthi et al. (2005) Lewis et al. (2004)</cell><cell>19 996 20 242 463 715</cell><cell>1 355 191 47 236 90</cell><cell>3.4 ? 10 ?4 3.6 ? 10 ?3 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table F .</head><label>F</label><figDesc>1: Description of the datasets used in the ResNet18 image classification benchmark</figDesc><table><row><cell>Dataset</cell><cell>Content</cell><cell>References</cell><cell cols="5">Classes Train Size Val. Size Test Size Image Size RGB</cell></row><row><cell cols="2">CIFAR-10 natural images</cell><cell>Krizhevsky (2009)</cell><cell>10</cell><cell>40k</cell><cell>10k</cell><cell>10k</cell><cell>32</cell></row><row><cell>SVHN</cell><cell cols="2">digits in natural images Netzer et al. (2011)</cell><cell>10</cell><cell>58.6k</cell><cell>14.6k</cell><cell>26k</cell><cell>32</cell></row><row><cell>MNIST</cell><cell>handwritten digits</cell><cell>LeCun et al. (2010)</cell><cell>10</cell><cell>50k</cell><cell>10k</cell><cell>10k</cell><cell>28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table F .</head><label>F</label><figDesc>2: Hyperparameters used for each solver. If a hyperparameter's value is not specified in the table, it was set as the default of the implementation (checked to be consistent across frameworks).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">SGD -Learning rate</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SGD -Weight decay</cell></row><row><cell></cell><cell>20</cell><cell>1.0e-03</cell><cell>1.0e-02</cell><cell>1.0e-01</cell><cell>5.0e-01</cell><cell>20</cell><cell>5.0e-05</cell><cell>5.0e-04</cell><cell>1.0e-03</cell><cell>5.0e-03</cell></row><row><cell>Test error (%)</cell><cell>10 20</cell><cell>0 1.0e-04</cell><cell cols="2">1000 1.0e-03 Adam -Learning rate 2000 5.0e-03</cell><cell>1.0e-02</cell><cell>10 20</cell><cell>0 2.0e-03</cell><cell cols="2">1000 2.0e-02 Adam -Weight decay 2000 2.0e-01</cell><cell>5.0e-01</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>1000</cell><cell>2000</cell><cell cols="2">Time (s)</cell><cell>0</cell><cell>1000</cell><cell>2000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Hyperparameter</cell><cell>SGD</cell><cell cols="2">Adam</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Learning Rate</cell><cell></cell><cell>0.1</cell><cell cols="2">0.001</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Momentum</cell><cell></cell><cell>0.9</cell><cell></cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Weight Decay Batch Size</cell><cell></cell><cell cols="2">5 ? 10 ?4 128</cell><cell>0.02 128</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table F .</head><label>F</label><figDesc>3: Differences in off-the-shelf implementations of various components when training ResNet18 for image classification in PyTorch and TensorFlow. The selected versions are put in bold font for components that we were able to reconcile. This highlights the numerous details to consider when comparing experimental results.</figDesc><table><row><cell>Component</cell><cell>PyTorch</cell><cell>TensorFlow/Keras</cell></row><row><cell>Bias in convolutions</cell><cell></cell><cell></cell></row><row><cell>Decoupled weight decay scaling</cell><cell>Multiplied by learning rate</cell><cell>Completely decoupled</cell></row><row><cell>Batch normalization momentum</cell><cell>0.9</cell><cell>0.99</cell></row><row><cell>Conv2D weights init.</cell><cell>Fan out, normal</cell><cell>Fan average, uniform</cell></row><row><cell>Classification head init. (weights)</cell><cell>Fan in, uniform</cell><cell>Fan average, uniform</cell></row><row><cell>Classification head init. (bias)</cell><cell>Fan in, uniform</cell><cell>Zeros</cell></row><row><cell>Striding in convolutions</cell><cell>Starts one off</cell><cell>Ends one off</cell></row><row><cell cols="2">Variance estimation in batch norm unbiased (eval)/biased (training)</cell><cell>biased</cell></row><row><cell cols="3">? Conv2D weights intialization: TensorFlow/Keras uses the default intialization which is a Glorot</cell></row><row><cell>uniform intialization</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>Table G.1 and Table G.2 present the solvers and datasets used in this benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table G</head><label>G</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">.1: List of solvers used in the 1 -regularized logistic regression benchmark</cell></row><row><cell>Solver</cell><cell></cell><cell>References</cell><cell>Description</cell><cell></cell><cell>Language</cell></row><row><cell>blitz</cell><cell></cell><cell>Johnson and Guestrin</cell><cell cols="2">CD + working set</cell><cell>Python, C++</cell></row><row><cell></cell><cell></cell><cell>(2015)</cell><cell></cell><cell></cell></row><row><cell cols="2">coordinate descent</cell><cell>Friedman et al. (2010)</cell><cell cols="2">(Cyclic) Minimization</cell><cell>Python (Numba)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">along coordinates</cell></row><row><cell>coordinate</cell><cell>descent</cell><cell>Friedman et al. (2010)</cell><cell cols="2">CD + Newton</cell><cell>Python (Numba)</cell></row><row><cell>(Newton)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>celer</cell><cell></cell><cell>Massias et al. (2018)</cell><cell cols="2">CD + working set + dual</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>extrapolation</cell><cell></cell></row><row><cell>copt[FISTA</cell><cell>line</cell><cell>Pedregosa et al. (2020),</cell><cell cols="2">FISTA (ISTA + acceler-</cell><cell>Python (Cython)</cell></row><row><cell>search]</cell><cell></cell><cell>Beck and Teboulle (2009)</cell><cell cols="2">ation) + line search</cell></row><row><cell>copt[PGD]</cell><cell></cell><cell>Pedregosa et al. (2020),</cell><cell cols="2">Proximal Gradient De-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>Combettes and Wajs</cell><cell>scent</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(2005)</cell><cell></cell><cell></cell></row><row><cell cols="3">copt[PGD linesearch] Pedregosa et al. (2020),</cell><cell cols="2">Proximal Gradient De-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>Combettes and Wajs</cell><cell cols="2">scent + linesearch</cell></row><row><cell></cell><cell></cell><cell>(2005)</cell><cell></cell><cell></cell></row><row><cell>copt[saga]</cell><cell></cell><cell>Pedregosa et al. (2020)</cell><cell>SAGA</cell><cell>(Variance</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>reduced</cell><cell>stochastic</cell></row><row><cell></cell><cell></cell><cell></cell><cell>method)</cell><cell></cell></row><row><cell>copt[svrg]</cell><cell></cell><cell>Pedregosa et al. (2020)</cell><cell>SVRG</cell><cell>(Variance</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>reduced</cell><cell>stochastic</cell></row><row><cell></cell><cell></cell><cell></cell><cell>method)</cell><cell></cell></row><row><cell>cuML[gpu]</cell><cell></cell><cell>Raschka et al. (2020)</cell><cell cols="2">L-BFGS + GPU</cell><cell>Python, C++</cell></row><row><cell>cuML[qn]</cell><cell></cell><cell>Raschka et al. (2020)</cell><cell cols="2">Orthant-Wise Limited</cell><cell>Python, C++</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Memory Quasi-Newton</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(OWL-QN)</cell><cell></cell></row><row><cell>cyanure</cell><cell></cell><cell>Mairal (2019)</cell><cell>Proximal</cell><cell>Minimiza-</cell><cell>Python, C++</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">tion by Incremental</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Surrogate Optimization</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(MISO)</cell><cell></cell></row><row><cell>lightning</cell><cell></cell><cell>Blondel and Pedregosa</cell><cell cols="2">(Cyclic) Coordinate De-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>(2016)</cell><cell>scent</cell><cell></cell></row><row><cell cols="3">scikit-learn[liblinear]Pedregosa et al. (2011),</cell><cell>Truncated</cell><cell>Newton</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>Fan et al. (2008)</cell><cell cols="2">Conjugate-Gradient</cell></row><row><cell cols="2">scikit-learn[lbfgs]</cell><cell>Pedregosa et al. (2011),</cell><cell>L-BFGS</cell><cell>(Quasi-</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>Virtanen et al. (2020)</cell><cell cols="2">Newton Method)</cell></row><row><cell cols="3">scikit-learn[newton-cg]Pedregosa et al. (2011),</cell><cell>Truncated</cell><cell>Newton</cell><cell>Python (Cython)</cell></row><row><cell></cell><cell></cell><cell>Virtanen et al. (2020)</cell><cell cols="2">Conjugate-Gradient</cell></row><row><cell cols="2">snapml[gpu=True]</cell><cell>D?nner et al. (2018)</cell><cell>CD + GPU</cell><cell></cell><cell>Python, C++</cell></row><row><cell cols="2">snapml[gpu=False]</cell><cell>D?nner et al. (2018)</cell><cell>CD</cell><cell></cell><cell>Python, C++</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>Table G.2: List of the datasets used in the 1 -regularized logistic regression benchmark</figDesc><table><row><cell>Datasets</cell><cell cols="2">References</cell><cell cols="2">Samples (n) Features (p)</cell><cell>Density</cell></row><row><cell>gisette colon-cancer</cell><cell cols="2">Guyon et al. (2004) Guyon et al., 2004</cell><cell>6000 62</cell><cell>5000 2000</cell><cell>9.9 ? 10 ?1 1.0</cell></row><row><cell cols="3">news20.binary Keerthi et al. (2005) rcv1.binary Guyon et al., 2004</cell><cell>19 996 20 242</cell><cell>1 355 191 19 959</cell><cell>3.4 ? 10 ?4 3.6 ? 10 ?3</cell></row><row><cell>blitz</cell><cell></cell><cell>celer</cell><cell>coordinate descent</cell><cell cols="2">coordinate descent (Newton)</cell></row><row><cell cols="2">copt (FISTA line search)</cell><cell>copt (FISTA)</cell><cell>copt (PGD line search)</cell><cell>copt (PGD)</cell></row><row><cell>copt (SAGA)</cell><cell></cell><cell>copt (SVRG)</cell><cell>cuML[qn]</cell><cell>cyanure</cell></row><row><cell>liblinear</cell><cell></cell><cell>lightning</cell><cell>snapML[cpu]</cell><cell>snapML[gpu]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table H .</head><label>H</label><figDesc>1: List of solvers used in the 1D Total Variation benchmarks</figDesc><table><row><cell>Solver</cell><cell>References</cell><cell cols="2">Formulation Description</cell><cell></cell><cell></cell></row><row><cell>ADMM</cell><cell>Boyd et al. (2011)</cell><cell>Analysis</cell><cell>Primal-Dual</cell><cell>Augmented</cell><cell>La-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>grangian</cell><cell></cell><cell></cell></row><row><cell cols="2">ChambollePock Chambolle and Pock (2011)</cell><cell>Analysis</cell><cell cols="2">Primal-Dual Hybrid Gradient</cell><cell></cell></row><row><cell>CondatVu</cell><cell>Condat (2013b)</cell><cell>Analysis</cell><cell cols="2">Primal-Dual Hybrid Gradient</cell><cell></cell></row><row><cell>DPGD</cell><cell cols="2">Komodakis and Pesquet (2015) Analysis</cell><cell cols="3">Dual proximal GD (+ acceleration)</cell></row><row><cell>PGD</cell><cell>Condat (2013a)</cell><cell>Analysis</cell><cell cols="2">Proximal GD + taut-string</cell><cell></cell></row><row><cell></cell><cell>Barbero and Sra (2018)</cell><cell></cell><cell cols="2">ProxTV (+ acceleration)</cell><cell></cell></row><row><cell>celer</cell><cell>Massias et al. (2018)</cell><cell>Synthesis</cell><cell cols="2">CD + working set (lasso)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">only for 2 data-fit</cell><cell></cell></row><row><cell>FP</cell><cell>Combettes and Glaudin (2021)</cell><cell>Synthesis</cell><cell cols="2">Fixed point with block updates</cell><cell></cell></row><row><cell>ISTA</cell><cell>Daubechies et al. (2004)</cell><cell>Synthesis</cell><cell cols="2">Proximal GD (+ acceleration)</cell><cell></cell></row><row><cell>skglm</cell><cell>Bertrand et al. (2022)</cell><cell>Synthesis</cell><cell cols="2">CD + working set</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table I .</head><label>I</label><figDesc>1: List of solvers used in the MCP benchmark</figDesc><table><row><cell>Solver</cell><cell>References</cell><cell>Short Description</cell></row><row><cell>CD</cell><cell>Breheny and Huang (2011), Mazumder</cell><cell>Proximal coordinate descent</cell></row><row><cell></cell><cell>et al. (2011)</cell><cell></cell></row><row><cell>PGD</cell><cell>Bolte et al. (2014)</cell><cell>Proximal gradient descent</cell></row><row><cell>GIST</cell><cell>Gong et al. (2013)</cell><cell>Proximal gradient + Barzilai-Borwein rule</cell></row><row><cell cols="2">WorkSet CD Boisbunon et al. (2014)</cell><cell>Coordinate descent + working set</cell></row><row><cell>skglm</cell><cell>Bertrand et al. (2022)</cell><cell>Accelerated coordinate descent + Working set</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table J .</head><label>J</label><figDesc>1: List of solvers used in the zero-order benchmark</figDesc><table><row><cell>Solver</cell><cell>References</cell><cell>Description</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The condition number of the dataset is divided by 5.9 after scaling.2  The condition number is multiplied by 407 after scaling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The Lasso path is the set of solutions of Problem (3) as ? varies in (0, ?).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">decreasing the learning rate by a factor 10 at mid-training, and again at 3/4 of the training</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The results are available online as a user-friendly interactive HTML file</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We ran the same experiment on two other datasets obtaining similar figures.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">It is possible to use the unbiased estimator in TensorFlow for the batch normalization, even if not documented, but its application is consistent between training and inference unlike PyTorch.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optuna: A Next-generation Hyperparameter Optimization Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimization with sparsity-inducing penalties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends in Machine Learning 4.1</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bacry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bompaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ga?ffas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poulsen</surname></persName>
		</author>
		<title level="m">Tick: A Python Library for Statistical Learning, with a Particular Emphasis on Time-Dependent Modeling</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In: ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modular Proximal Optimization for Multidimensional Total-Variation Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="82" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Isotonic Regression Problem and Its Dual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Brunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="140" to="147" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Benchmarking in optimization: Best practice and open issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bartz-Beielstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bossek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eftimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">La</forename><surname>Kerschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Ibanez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03488</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Revisiting ResNets: Improved Training and Scaling Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Million Song Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Music Information Retrieval</title>
		<meeting>the 12th International Conference on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Klopfenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Bannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Massias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07826</idno>
		<title level="m">Beyond L1: Faster and Better Sparse Models with skglm</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonlinear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Julia: A fresh approach to numerical computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bezanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karpinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Review 59.1</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The group fused Lasso for multiple change-point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bleakley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lightning: Large-Scale Linear Classification, Regression and Ranking in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Active set strategy for high-dimensional non-convex sparse optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boisbunon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICASSP. IEEE</publisher>
			<biblScope unit="page" from="1517" to="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Proximal alternating linearized minimization for nonconvex and nonsmooth problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="459" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic screening: accelerating first-order algorithms for the Lasso and Group-Lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Emiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-Scale Machine Learning with Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COMP-STAT</title>
		<imprint>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2010" />
			<publisher>Physica-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends in Machine Learning 3.1</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sphinx documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brandl</surname></persName>
		</author>
		<ptr target="http://sphinx-doc.org/sphinx.pdf" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kunisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="526" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Breheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat. 5</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">232</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<editor>ICML. PMLR</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1059" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistics for high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van De Geer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Series in Statistics. Methods, theory and applications</title>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhancing Sparsity by Reweighted l 1 Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fourier Anal. Applicat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="877" to="905" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image recovery via total variation minimization and related problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Lions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="167" to="188" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A first-order primal-dual algorithm for convex problems with applications to imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput. 20</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparsity-Based Semi-Blind Deconvolution of Neural Activation Signal in fMRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cherkaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ciuciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pillow (PIL Fork) Documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Signal recovery by proximal forward-backward splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wajs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiscale modeling &amp;</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1168" to="1200" />
		</imprint>
	</monogr>
	<note>simulation 4.4</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Solving Composite Fixed Point Problems with Block Updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Glaudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Nonlinear Analysis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A Direct Algorithm for 1-D Total Variation Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Condat</surname></persName>
		</author>
		<idno>IEEE SIGNAL PROCESSING LETTERS 20.12</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A primal-dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Condat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Criteo releases industry&apos;s largest-ever dataset for machine learning to academic community</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Criteo-Labs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An iterative thresholding algorithm for linear inverse problems with a sparsity constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defrise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Mol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1413" to="1457" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Dembo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Eisenstat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Steihaug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inexact Newton Methods&quot;. In: SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="400" to="408" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Snap ML: A hierarchical framework for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>D?nner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sarigiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anghel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pozidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">With discussion, and a rejoinder by the authors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Least angle regression</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Safe feature elimination in sparse supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Viallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rabbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pacific Optim</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="667" to="698" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Analysis versus synthesis in signal priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>W?lchli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Skafte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ir1dxd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bereznyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Murrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Praesius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Addair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lipin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bapat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schr?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karnachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">B</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Schiratti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eyzaguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>PyTorchLightning/pytorchlightning: 0.7.6 release. Version 0.7.6</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Variable selection via nonconcave penalized likelihood and its oracle properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc. 96</title>
		<imprint>
			<biblScope unit="volume">456</biblScope>
			<biblScope unit="page" from="1348" to="1360" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">PySAP: Python Sparse Data Analysis Package for multidisciplinary image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grigis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">El</forename><surname>Gueddari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ramzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaithya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sarthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cherkaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ciuciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy and Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">100402</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Function Minimization by Conjugate Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="154" />
			<date type="published" when="1964-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Reproducible research environments with repo2docker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holdgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nalvarete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sundell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw. 33</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Implementing the Nelder-Mead simplex algorithm with adaptive parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="259" to="277" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AISTATS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaasenbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Mesirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Coller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Caligiuri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="531" to="537" />
		</imprint>
	</monogr>
	<note>In: science 286.5439</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">MNE software for processing MEG and EEG data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luessi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Engemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Strohmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brodbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Parkkonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>H?m?l?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="446" to="460" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Least absolute shrinkage is equivalent to quadratic penalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Result analysis of the nips 2003 feature selection challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems 17</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Auger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mersmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tu?ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brockhoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08785</idno>
	</analytic>
	<monogr>
		<title level="m">Optimization Methods and Software 36.1. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="114" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Completely derandomized self-adaptation in evolution strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ostermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary computation 9</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="159" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Array programming with NumPy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<idno>Nature 585.7825</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<title level="m">Statistical Learning with Sparsity: The Lasso and Generalizations</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks&quot;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="558" to="567" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Matplotlib: A 2D graphics environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing in Science &amp; Engineering 9</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Artificial intelligence faces reproducibility crisis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Science 359</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6377</biblScope>
			<biblScope unit="page" from="725" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Collaborative data science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://plot.ly" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1171" to="1179" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Total Activation: fMRI Deconvolution through Spatio-Temporal Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Karahanoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caballero-Gaudes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lazeyras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A modified finite Newton method for fast solution of large scale linear SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Stochastic estimation of the maximum of a regression function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wolfowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="page" from="462" to="466" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An interior-point method for large-scale l1-regularized logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: J. Mach. Learn. Res. 8</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1519" to="1555" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In: IEEE Transactions on Pattern Analysis and Machine Intelligence 26</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="147" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Playing with Duality: An overview of recent primal-dual approaches for solving large-scale optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Pesquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>IEEE Signal Processing Magazine, Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10-28" />
		</imprint>
	</monogr>
	<note>Lasso.jl. Version 0.6.3. JuliaStats</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>South Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Extraction of Nystagmus Patterns from Eye-Tracker Data with Convolutional Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rateaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Montreal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="928" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist2" />
	</analytic>
	<monogr>
		<title level="m">ATT Labs</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell-Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Improving &quot;Fast Iterative Shrinkage-Thresholding Algorithm&quot;: Faster, Smarter, and Greedier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-B</forename><surname>Sch?enlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1069" to="1091" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">On the Limited Memory BFGS Method for Large Scale Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">A ConvNet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Benchmarking Simulation-Based Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Lueckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boelts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="343" to="351" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Complexity analysis of the Lasso regularization path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Cyanure: An open-source toolbox for empirical risk minimization for python, c++, and soon more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08165</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Celer: a fast solver for the lasso with dual extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Massias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3315" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">MLPerf: An industry standard benchmark suite for machine learning performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schmuelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8" to="16" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Sparsenet: Coordinate descent with nonconvex penalties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc. 106</title>
		<imprint>
			<biblScope unit="volume">495</biblScope>
			<biblScope unit="page" from="1125" to="1138" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Data structures for statistical computing in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mckinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Python in Science Conference</title>
		<meeting>the 9th Python in Science Conference<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">445</biblScope>
			<biblScope unit="page" from="51" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yurchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<ptr target="https://github.com/joblib/loky" />
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note type="report_type">Loky. Version 3.0</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>N?jera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Est?ve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holdgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouillart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luessi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robitaille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Appelhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kunzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sunden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Sta?czak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>sphinx-gallery/sphinx-gallery: Release v0.7.0. Version v0.7.0</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Gap Safe screening rules for sparsity enforcing penalties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ndiaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fercoq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res. 18</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. akad. nauk Sssr</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">copt: composite optimization in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Negiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dresdner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Improving reproducibility in machine learning research: a report from the NeurIPS 2019 reproducibility program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent-Lamarre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Larivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">ICLR reproducibility challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ReScience C 5</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Smooth Bilevel Programming for Sparse Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1543" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">An efficient method for finding the minimum of a function of several variables without calculating derivatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Numerical recipes 3rd edition: The art of scientific computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">IJCNN 2001 neural network competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Slide presentation in IJCNN 1.97</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">SIDUS-the solution for extreme deduplication of an operating system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Quemener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corvellec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linux Journal</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R Core Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">A step toward quantifying independently reproducible machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5486" to="5496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Nevergrad -A gradient-free optimization platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<ptr target="https://GitHub.com/FacebookResearch/Nevergrad" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nolet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">193</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Psutil package: a cross-platform library for retrieving information on running processes and system utilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rodola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Google Scholar</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">ReScience C: a journal for reproducible replications in computational science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hinsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Reproducible Research in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note>1-4</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Block coordinate relaxation methods for nonparametric wavelet denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="379" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Minimizing Finite Sums with the Stochastic Average Gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.2388</idno>
	</analytic>
	<monogr>
		<title level="m">Math. Program. 162</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="83" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Descending through a Crowded Valley -Benchmarking Deep Learning Optimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9367" to="9376" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Winner&apos;s curse? On pace, progress, and empirical rigor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">How to Add Regularization to Keras Pre-trained Models the Right Way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Silva</surname></persName>
		</author>
		<ptr target="https://sthalles.github.io" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Optimization for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Strong rules for discarding predictors in lasso-type problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="245" to="266" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B Stat. Methodol. 58</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Adaptive Piecewise Polynomial Estimation via Trend Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">The solution path of the generalized lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1335" to="1371" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Dual coordinate ascent methods for non-strictly convex minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="231" to="247" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">OpenML: networked science in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD Explorations 15</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Mulbregt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
	<note>and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python&quot;. In: Nature Methods 17</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Global optimization by basin-hopping and the lowest energy structures of Lennard-Jones clusters containing up to 110 atoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Doye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry A</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5111" to="5116" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">ResNet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Springer</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Nearly unbiased variable selection under minimax concave penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="894" to="942" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Analysis of multi-stage convex relaxation for sparse regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1081" to="1107" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Regularizing neural networks via adversarial model perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="8156" to="8165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">This shows how Benchopt allows to reveal such behavior, and could lead to practical guidelines on how to select this dual initialization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">2D TV problems for images -or higher order TV regularization, such as Total Generalized Variation Bredies et al., 2010 or inf-convolution of TV functionals Chambolle and Lions, 1997 -used of instance for change point detection (Tibshirani, 2014). Yet, for 2D or higher dimensional problems, we can no longer use the synthesis formulation. It is however possible to apply the taut-string method of Condat (2013a) and graph-cut methods of Boykov et al. (2001) and Kolmogorov and Zabin (2004) for anisotropic TV, and dual or primal-dual methods for isotropic</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Extensions We plan to extend this benchmark in the future to consider higher dimensional problems. such as Primal-Dual Hybrid Gradient algorithm (Chambolle and Pock</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
