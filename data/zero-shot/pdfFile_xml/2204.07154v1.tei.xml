<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MiniViT: Compressing Vision Transformers with Weight Multiplexing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinnian</forename><surname>Zhang</surname></persName>
							<email>v-jinnizhang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<email>houwen.peng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud+AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud+AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<email>luyuan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud+AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MiniViT: Compressing Vision Transformers with Weight Multiplexing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformer (ViT) models have recently drawn much attention in computer vision due to their high model capability. However, ViT models suffer from huge number of parameters, restricting their applicability on devices with limited memory. To alleviate this problem, we propose MiniViT, a new compression framework, which achieves parameter reduction in vision transformers while retaining the same performance. The central idea of MiniViT is to multiplex the weights of consecutive transformer blocks. More specifically, we make the weights shared across layers, while imposing a transformation on the weights to increase diversity. Weight distillation over self-attention is also applied to transfer knowledge from large-scale ViT models to weight-multiplexed compact models. Comprehensive experiments demonstrate the efficacy of MiniViT, showing that it can reduce the size of the pre-trained Swin-B transformer by 48%, while achieving an increase of 1.0% in Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters, MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters, without seriously compromising the performance. Finally, we verify the transferability of MiniViT by reporting its performance on downstream benchmarks. Code and models are available at here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>"Only Mini Can Do It." -BMW Mini Cooper Large-scale pre-trained vision transformers, such as ViT <ref type="bibr" target="#b17">[18]</ref>, CvT <ref type="bibr" target="#b57">[58]</ref>, and Swin <ref type="bibr" target="#b35">[36]</ref>, have recently drawn a great deal of attention due to their high model capabilities and superior performance on downstream tasks. However, they generally involve giant model sizes and large amounts of pre-training data. For example, ViT uses 300 million images to train a huge model with 632 million parameters, achieving state-of-the-art performance on image classification <ref type="bibr" target="#b17">[18]</ref>. Meanwhile, the Swin transformer uses 200-300 million parameters, and is pre-trained on ImageNet- <ref type="bibr">Figure 1</ref>. Comparisons between MiniViTs and popular vision transformers, such as DeiT <ref type="bibr" target="#b51">[52]</ref> and Swin Transformers <ref type="bibr" target="#b35">[36]</ref> 22K <ref type="bibr" target="#b15">[16]</ref>, to attain promising results on downstream detection and segmentation tasks <ref type="bibr" target="#b35">[36]</ref>.</p><p>Hundreds of millions of parameters consume considerable storage and memory, making these models unsuitable for applications involving limited computational resources, such as edge and IoT devices, or in which real-time predictions are needed. Recent studies reveal that the large-scale pre-trained models are over-parametrized <ref type="bibr" target="#b29">[30]</ref> . Therefore, it is necessary and feasible to eliminate redundant parameters and the computational overhead of these pre-trained models without compromising their performance.</p><p>Weight sharing is a simple, but effective, technique to reduce model sizes. The original idea of weight sharing in neural networks was proposed in the 1990s by LeCun and Hinton <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>, and recently reinvented for transformer model compression in natural language processing (NLP) <ref type="bibr" target="#b31">[32]</ref>. The most representative work, ALBERT <ref type="bibr" target="#b31">[32]</ref>, introduces a cross-layer parameter sharing method to prevent the number of parameters from growing with network depth. Such technique can significantly reduce the model size without seriously hurting performance, thus improving parameter efficiency. However, the efficacy of weight sharing in vision transformer compression is not well explored.</p><p>To examine this, we perform the cross-layer weight sharing <ref type="bibr" target="#b31">[32]</ref> on DeiT-S <ref type="bibr" target="#b51">[52]</ref> and Swin-B <ref type="bibr" target="#b51">[52]</ref> transformers. Unexpectedly, this straightforward usage of weight sharing brings two severe issues: <ref type="bibr" target="#b0">(1)</ref> Training instability. We ob-served that weight sharing across transformer layers makes the training become unstable, and even causes training collapse as the number of shared layers increases, as visualized in <ref type="figure" target="#fig_2">Fig. 4.</ref> (2) Performance degradation. The performance of weight-shared vision transformers drops significantly compared to the original models. For example, it leads to a 5.6% degradation in accuracy for Swin-S, although weight sharing can reduce the number of model parameters by fourfold.</p><p>To investigate the underlying reasons for these observations, we analyze the ? 2 -norm of gradients during training and the similarities between intermediate feature representations from the model before and after weight sharing (cf. Sec. <ref type="bibr">4.2)</ref>. We found that strictly identical weights across different layers is the main cause of the issues. In particular, the layer normalization <ref type="bibr" target="#b4">[5]</ref> in different transformer blocks should not be identical during parameter sharing, because the features of different layers have various scales and statistics. Meanwhile, the ? 2 -norm of the gradient becomes large and fluctuates across different layers after weight sharing, leading to training instability. Finally, the Central Kernel Alignment (CKA) <ref type="bibr" target="#b28">[29]</ref> values, a popular similarity metric, drop significantly in the last few layers, indicating that feature maps generated by the model before and after weight sharing become less correlated, which can be the reason of performance degradation.</p><p>In this paper, we propose a new technique, called weight multiplexing, to address the above issues. It consists of two components, weight transformation and weight distillation, to jointly compress pre-trained vision transformers. The key idea of weight transformation is to impose transformations on the shared weights, such that different layers have slightly different weights, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. This operation can not only promote parameter diversity, but also improve training stability. More concretely, we impose simple linear transformations on the multi-head self-attention (MSA) module and the multilayer perceptron (MLP) module for each weight-shared transformer layer. Each layer includes separate transformation matrices, so the corresponding attention weights and outputs of MLP are different across layers. The layer normalization for different layers is also separated, in contrast to sharing the same parameters. As such, the optimization of weight sharing transformer networks becomes more stable, as demonstrated in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>To mitigate performance degradation, we further equip weight multiplexing with weight distillation, such that the information embedded in the pre-trained models can be transferred into the weight-shared small ones, which are much more compact and lightweight. In contrast to previous works that only rely on prediction-level distillation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">52]</ref>, our method additionally considers both attentionlevel and hidden-state distillation, allowing the smaller model to closely mimic the behavior of the original pretrained large teacher model.</p><p>The experiments demonstrate that our weight multiplexing method achieves clear improvements in accuracy over the baselines and compresses pre-trained vision transformers by 2 times while transferring well to downstream tasks. For instance, with the proposed weight multiplexing, the Mini-Swin-B model with 12-layer parameters obtains 0.8% higher accuracy than the 24-layer Swin-B. Moreover, Mini-DeiT-B with 9M parameters achieves 79.8% top-1 accuracy on ImageNet, being 9.7 times smaller than DeiT-B (with 86 parameters and 81.8% accuracy). The 12M tiny model compressed by our approach transfers well to downstream object detection, achieving an AP of 48.6 on the COCO validation set, which is on par with the original Swin-T using 28M parameters.</p><p>We summarize our contributions as follows:</p><p>? We systematically investigate the efficacy of weight sharing in vision transformers, and analyze the underlying reasons of issues brought by weight sharing.</p><p>? We propose a novel compression framework termed MiniViT for general vision transformers. Experimental results demonstrate that MiniViT can achieve a large compression ratio without losing accuracy. Furthermore, the performance of MiniViT transfers well to downstream benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Before presenting our method, we first briefly review some background on vision transformers and parameter sharing, which are fundamental to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vision Transformers</head><p>Transformers, though originally designed for NLP <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54]</ref>, have recently demonstrated their great potentials in computer vision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52]</ref>. Vision transformers first split an input image into a sequence of 2D patches known as tokens. They then flatten and transform these patches to D-dimensional vectors using a linear projection <ref type="bibr" target="#b17">[18]</ref> or stacked CNN layers <ref type="bibr" target="#b60">[61]</ref>, also known as patch embeddings. To retain positional information, positional embeddings are added to patch embeddings. The combined embeddings are then fed to a transformer encoder (described below). Lastly, a linear layer is used to produce the final classification.</p><p>A transformer encoder consists of alternating blocks of multihead self-attention (MSA) and multi-layer perceptron (MLP) blocks. Layer normalization (LN) <ref type="bibr" target="#b4">[5]</ref> and residual connections are applied before and after each block, respectively. We elaborate on the MSA and MLP blocks as below. MSA: Let M be the number of heads, also known as selfattention modules. Given the input sequence Z 0 ? R N ?D , in the k th head, we generate queries, keys, and values by linear projections, denoted by Q k , K k , and V k ? R N ?d respectively, where N is the number of tokens. D and d are the dimensions of patch embeddings and Q-K-V matrices, respectively. We then compute a weighted sum over all values for each position in the sequence. The weights, called attentions and denoted by A k , are based on the pairwise similarity between two elements in the sequence, namely</p><formula xml:id="formula_0">h k = A k V k , and<label>(1)</label></formula><formula xml:id="formula_1">A k = softmax Q k K T k ? d ,<label>(2)</label></formula><p>where softmax(?) is conducted on each row of the input matrix. Finally, a fully-connected layer is applied to the concatenation of the outputs of all heads. MLP: The MLP block comprises two fully-connected layers with an activation function denoted by ?(?), usually GELU <ref type="bibr" target="#b23">[24]</ref>. Let Y ? R N ?d be the input of MLP. The output of MLP can be expressed as</p><formula xml:id="formula_2">H = ?(YW (1) + b (1) )W (2) + b (2) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">W (1) ? R d?d ? , b (1) ? R d ? , W (2) ? R d ? ?d</formula><p>, and b (2) ? R d are the weights and biases for the first and second layers, respectively. Notably, we usually set d ? &gt; d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Weight Sharing</head><p>Weight sharing is a simple but effective way to improve parameter efficiency. The core idea is to share parameters across layers, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>(a). Mathematically, weight sharing can be formulated as a recursive update of one transformer block f (i.e., one shared layer):</p><formula xml:id="formula_4">Z i+1 = f (Z i ; ?), i = 0, . . . , L ? 1,<label>(4)</label></formula><p>where Z i denotes the feature embedding of the sequence in layer i, L is the total number of layers, and ? represents the shared weights of the transformer block across all layers. The efficacy of weight sharing has been explored and proved in natural language transformer models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32]</ref>. It can prevent the number of parameters from growing with the depth of the network without seriously hurting the performance, thus improving parameter-efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe our proposed weight multiplexing strategy for vision transformer compression. It consists of two key components, weight transformation and weight distillation, to improve training stability and model performance during weight sharing. Finally, we depict the pipeline of model compression with weight multiplexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weight Multiplexing</head><p>The potential of weight sharing has been demonstrated in NLP <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32]</ref>; however, its efficacy is still unclear in vision transformers. To examine this, we directly apply the cross-layer weight sharing in Eq. (4) on the DeiT-S <ref type="bibr" target="#b51">[52]</ref> and Swin-B <ref type="bibr" target="#b35">[36]</ref> transformer models, and observe two issues: training instability and performance degradation. Based on our analysis in Sec. 4.2, the strict identity of weights across different layers is the main cause of the issues. In particular, the ? 2 -norm of the gradients after weight sharing becomes large and fluctuates in different transformer blocks, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Furthermore, the CKA values indicate that feature maps generated by the model after weight sharing are less correlated with the original model, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. To solve these issues, inspired by the multiplexing technologies in telecommunications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46]</ref>, we propose a new technique called weight multiplexing for transformer compression. It combines multi-layer weights into a single weight over a shared part, while involving transformation and distillation to increase parameter diversity.</p><p>More concretely, as shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>, the proposed weight multiplexing method consists of (1) sharing weights across multiple transformer blocks, which can be considered as a combination process in multiplexing <ref type="bibr" target="#b45">[46]</ref>; <ref type="bibr" target="#b1">(2)</ref> introducing transformations in each layer to mimic demultiplexing <ref type="bibr" target="#b45">[46]</ref>; and (3) applying knowledge distillation to increase the similarity of feature representations between the models before and after compression. Following Eq. (4), we can re-formulate our weight multiplexing as follows:</p><formula xml:id="formula_5">Z i+1 = f (Z i ; ?, ? ? i ), i = 0, . . . , L ? 1,<label>(5)</label></formula><p>where ? ? i represents the weights of transformation blocks in the i th transformer layer. Note that the number of parameters in ? ? i is far fewer than ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Weight Transformation</head><p>The transformations in our method are imposed on both attention matrices and feed-forward networks. Such transformations allow each layer to be different, thus elevating parameter diversity and model representation capability. As illustrated in <ref type="figure">Fig. 3</ref> (Right), the parameters of transformation kernels are not shared across layers, while all the other blocks in the original transformer are shared except Layer-Norm <ref type="bibr" target="#b4">[5]</ref>. Since the shared blocks occupy the vast majority of the model parameters, the model size only increases slightly after weight multiplexing. Transformation for MSA. To improve parameter diversity, we insert two linear transformations before and after <ref type="bibr">Figure 3</ref>. Left: The overall framework of MiniViT. Note that the number of stages are configurable, instead of being fixed in Swin transformers <ref type="bibr" target="#b35">[36]</ref>. The transformer layers in each stage of the original models to be compressed should have identical structures and dimension. Right: The detailed transformer block in a MiniViT. We share weights of MSA and MLP in each stage, and add two transformation blocks to increase the parameter diversity. The transformation blocks and normalization layers are not shared. the softmax self-attention module, respectively. Formally, different from the original self-attention in Eq. (1-2), the transformation-equipped attention is defined as</p><formula xml:id="formula_6">h k = A ? k V k = M n=1 F (1) kn A n V k ,<label>(6)</label></formula><formula xml:id="formula_7">A n = softmax M m=1 F (2) nm Q m K T m ? d ,<label>(7)</label></formula><p>where F <ref type="bibr" target="#b0">(1)</ref> , F (2) ? R M ?M are the linear transformation kernels before and after the softmax, respectively. Such linear transformations can make each attention matrix A n different, while combining information across attention heads to increase parameter variance. Transformation for MLP. On the other hand, we further impose a lightweight transformation for MLP to elevate parameter diversity. In particular, let the input be Y = [y 1 , . . . , y d ], where y l denotes the l th position of embedding vectors of all tokens. We then introduce d linear trans-</p><formula xml:id="formula_8">formations to convert Y into Y ? = [C (1) y 1 , . . . , C (d) y d ], where C (1) , . . . , C (d) ? R N ?N are independent weight matrices of linear layers. Then Eq. (3) is re-formulated as H = ?(Y ? W (1) + b (1) )W (2) + b (2) .<label>(8)</label></formula><p>To reduce the number of parameters and introduce locality in the transformations, we resort to depth-wise convolution <ref type="bibr" target="#b12">[13]</ref> to sparsify and share weights in each weight matrix, leading to only K 2 d parameters compared to N 2 d parameters (K &lt;&lt; N ), where K is the kernel size of convolution. After the transformations, the outputs of MLP become more diverse, improving the parameter efficacy. Theoretically, with these transformations, the weightshared layers can restore the behaviors of the pre-trained models, similar to a demultiplexing process. Then the training instability and performance drop issues can be alleviated, because these issues are not observed in the original models. Similar transformations are also applied to improve the performance of transformers without sharing blocks, such as talking-heads attention <ref type="bibr" target="#b48">[49]</ref> and CeiT <ref type="bibr" target="#b59">[60]</ref>. However, we extend the ability of transformations to circumvent the drawbacks of weight-sharing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Weight Distillation</head><p>To compress the large pre-trained models and address the performance degradation issues induced by weight sharing, we further resort to weight distillation to transfer knowledge from the large models to the small and compact models. We consider three types of distillation for transformer blocks, i.e., prediction-logit distillation, self-attention distillation, and hidden-state distillation.</p><p>Prediction-Logit Distillation. Hinton et al. <ref type="bibr" target="#b24">[25]</ref> firstly demonstrated that deep learning models can achieve better performance by imitating the output behavior of wellperforming teacher models during training. We leverage this idea to introduce a prediction loss, as follows:</p><formula xml:id="formula_9">L pred = CE softmax z s T , softmax z t T ,<label>(9)</label></formula><p>where z s and z t are the logits predicted by the student and teacher models, respectively, and T is a temperature value which controls the smoothness of logits. In our experiments, we set T = 1. CE denotes the cross-entropy loss. Self-Attention Distillation. Recent literature has shown that utilizing attention maps in transformer layers to guide the training of student models is beneficial <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50]</ref>. To solve the dimension inconsistency between the student and teacher models due to differing numbers of heads, and inspired by <ref type="bibr" target="#b54">[55]</ref>, we apply cross-entropy losses on relations among queries, keys, and values in MSA.</p><p>In particular, we first append matrices over all heads. For example, we define Q = [Q 1 , . . . , Q M ] ? R N ?M d , and K, V ? R N ?M d in the same way. For notational simplic-ity, we write S 1 , S 2 , and S 3 to denote Q, K, and V respectively. Then we can generate nine different relation matrices defined by R ij = softmax(S i S T j / ? M d). Note that R 12 is the attention matrix A. The self-attention distillation loss can be expressed as</p><formula xml:id="formula_10">L attn = 1 9N N n=1 i,j? {1,2,3} CE(R s ij,n , R t ij,n ),<label>(10)</label></formula><p>where R ij,n represents the n th row of R ij .</p><p>Hidden-State Distillation. Similarly, we can generate relation matrices for hidden states, i.e., the features output by MLP. Denoting the hidden states of a transformer layer by H ? R N ?d , the hidden-state distillation loss based on relation matrices is defined as</p><formula xml:id="formula_11">L hddn = 1 N N n=1 CE(R s H,n , R t H,n ),<label>(11)</label></formula><p>where R H,n indicates the n th row of R H , which is computed by R H = softmax(HH T / ? d). Based on our observation that only using prediction soft labels can yield better performance than using both the prediction and ground truth labels (see the ablation in Sec. 4.2), thus the final distillation objective function is formulated as</p><formula xml:id="formula_12">L train = L pred + ?L attn + ?L hddn ,<label>(12)</label></formula><p>where ? and ? are hyperparameters with default values of 1 and 0.1 respectively, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Compression Pipeline</head><p>Our compression pipeline includes two phases: Phase 1: Generating compact architectures with weight transformation. Given a large pre-trained vision transformer model, we first share the parameters across every-K adjacent transformer layers except the LayerNorm <ref type="bibr" target="#b4">[5]</ref>. Then we apply weight transformation to each layer by inserting a tiny linear layer before and after the softmax layer, as defined in Eq. (6-7). Furthermore, we introduce a depthwise convolutional layer for MLP. These linear layers and transformation blocks are not shared.</p><p>Phase 2: Training the compressed models with weight distillation. In this step, we apply the proposed weight distillation method to transfer knowledge from the large pretrained models to small ones, using the objective function defined in Eq. <ref type="bibr" target="#b11">(12)</ref>. Such distillation inside the transformer modules allows the student network to reproduce the behaviors of the teacher network <ref type="bibr" target="#b27">[28]</ref>, thus extracting more useful knowledge from the large-scale pre-trained models. Note that it is only performed when both the teacher and student models are transformer architectures. In other cases where the architectures of the student and teacher are heterogeneous, we only preserve the prediction-logit distillation.  <ref type="table">1-1  1-2  2-1  2-2  3-1  3-2  3-3  3-4  3-5  3-6  3-7  3-8  3-9  3-10  3-11  3-12  3-13  3-14  3-15  3-16  3-17  3-</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first provide an analysis of weight sharing in vision transformers, followed by experiments on the effects of the proposed weight transformation and weight distillation methods. Next, we show the parameter efficiency of our method by comparing with other state-ofthe-art models. Finally, we demonstrate the transferability of the compression models on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Architectures for Compression. In the case of hierarchical models such as Swin <ref type="bibr" target="#b35">[36]</ref>, we only share parameters of transformer layers in each stage, due to the unaligned parameter dimension in different stages (caused by feature down-sampling), and generate a series of compact models named Mini-Swins. For DeiT, one of the popular isomorphic models, we consider it as a single-stage vision transformer and create Mini-DeiTs. We make several modifications on DeiT: First, we remove the [class] token. The model is attached with a global average pooling layer and a fully-connected layer for image classification. We also utilize relative position encoding to introduce inductive bias to boost the model convergence <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b58">59]</ref>. Finally, based on our observation that transformation for FFN only brings limited performance gains in DeiT, we remove the block to speed up both training and inference. The results of MiniViT based on the original DeiT <ref type="bibr" target="#b51">[52]</ref> are presented in the supplementary materials for comprehensive comparisons.</p><p>Training Settings. We train our models from scratch on ImageNet-1K <ref type="bibr" target="#b15">[16]</ref> by directly inheriting the hyperparameters from DeiT <ref type="bibr" target="#b51">[52]</ref> or Swin transformers <ref type="bibr" target="#b35">[36]</ref> except for the drop path rate, which are set to be 0.0/0.0/0.1 for DeiT-Ti/S/B and 0.0/0.1/0.2 for Swin-T/S/B, respectively. The data augmentation techniques include RandAugment <ref type="bibr" target="#b13">[14]</ref>, Cutmix <ref type="bibr" target="#b61">[62]</ref>, Mixup <ref type="bibr" target="#b62">[63]</ref>, and random erasing. RepeatAug <ref type="bibr" target="#b25">[26]</ref> is only applied in DeiT <ref type="bibr" target="#b51">[52]</ref>. In the down- stream tasks, we fine-tune the models for 30 epochs at 384 2 resolution. Specifically, the position encoding is resized with bicubic interpolation. The AdamW <ref type="bibr" target="#b36">[37]</ref> optimizer is applied with weight decay 10 ?8 and a cosine scheduler, batch size 256. The learning rates are 2.5 ? 10 ?6 and 10 ?5 for DeiT and Swin, respectively. All models are implemented using PyTorch <ref type="bibr" target="#b42">[43]</ref> and Timm library <ref type="bibr" target="#b56">[57]</ref>, and trained for 300 epochs with 8 NVIDIA Tesla V100 GPUs. For compressing Swin <ref type="bibr" target="#b35">[36]</ref>, we adopt the ImageNet-22k <ref type="bibr" target="#b15">[16]</ref> pre-trained Swin-B with 88M parameters as the teacher model. Thus, our compressed models can still learn the knowledge of the large-scale ImageNet-22k data by distillation without requiring the access to the dataset. Similarly, for DeiT <ref type="bibr" target="#b51">[52]</ref>, we use RegNet-16GF <ref type="bibr" target="#b43">[44]</ref> with 84M parameters as the teacher, and only perform prediction-logit distillation due to heterogenous architectures between CNN and ViT. We share the consecutive two layers except Swin-T where all layers in each stage are shared. For the depth-wise convolution in weight transformation, we set the kernel size to be the same as the window size in Swin <ref type="bibr" target="#b35">[36]</ref> and the stride as 1. Besides, same-padding is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis and Ablation</head><p>Analysis of Weight Sharing (WS). When directly applying weight sharing as in Eq. (4), the training process collapses, and the model suffers from severe performance drop.</p><p>First, we investigate the training stability. We share every two layers in DeiT-S while sharing all layers in each stage of Swin-B. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the ? 2 -norm of gradients in DeiT-S and Swin-B after weight sharing becomes much larger, indicating a fast change of magnitude of weights. Furthermore, weight sharing causes fluctuations in the gradient norm across different layers. This may lead to different optimization paces of layers. In particular, some layers  <ref type="table">Table 3</ref>. Ablation study on different distillation losses on ImageNet-1K <ref type="bibr" target="#b15">[16]</ref>. We use Swin-B <ref type="bibr" target="#b35">[36]</ref> pre-trained on ImageNet-22K <ref type="bibr" target="#b15">[16]</ref> as the teacher model. The weights for all losses are 1 except using both GT and L pred , where both weights are 0.5.</p><p>are updated quickly, whereas the other parts are hardly optimized, making the model likely to converge to a bad local optimum or even diverge in training. Therefore, strictly identical weights shared across layers lead to training instability. However, our weight multiplexing method can both reduce the gradient norm and increase the smoothness across layers by introducing transformations to improve parameter diversity, promoting a more stable training process.</p><p>As for performance analysis, we present a comparison of feature similarities with CKA between weight sharing and weight multiplexing in <ref type="figure" target="#fig_3">Fig. 5</ref>. Higher CKA values indicate more similar feature representations between two models <ref type="bibr" target="#b28">[29]</ref>, thus achieving similar performance. We observe that both DeiT and Swin suffer from large deviations of feature representations after applying weight sharing, especially in the last several layers, which may be one of the reasons for performance drops brought about by weight sharing. Nevertheless, our proposed weight multiplexing method can improve the similarities.</p><p>Component-wise Analysis. We evaluate the effects of different components in our proposed weight multiplexing method on ImageNet-1K <ref type="bibr" target="#b15">[16]</ref>, and report the results in Tab. 1. Our baselines are the official Swin-T <ref type="bibr" target="#b35">[36]</ref> and DeiT-S [52] models. After applying the weight-sharing method, the numbers of parameters of both models are halved, whereas the accuracy also decreases by 2% (#1 vs. #2, #6 vs. #7). The performance can be improved by either applying weight distillation (#2 vs. #3, #7 vs. #8) or weight transformation (#2 vs. #4, #7 vs. #9), which demonstrates their individual effectiveness. It is notable that weight transformation only introduces a few parameters. Furthermore, when combining all three components, MiniViTs can achieve the best accuracy (#5 and #10), which even outperforms the original Swin-T and DeiT-S models.</p><p>Number of Sharing Blocks. We study the performance of MiniViTs in different sharing settings. In Tab. 2, sharing every two blocks in each stage of Swin-T or DeiT-B can significantly reduce the number of parameters from 28M to 16M, and 86M to 44M, respectively, whereas the Top-1 accuracy becomes 1% higher. Note that we consider DeiT as a single-stage vision transformer. In the extreme case where all blocks in each stage are shared, Mini-Swin-T can still outperform the original model with only 43% of the parameters. Mini-DeiT-B can achieve 90% parameter reduction  <ref type="table">Table 4</ref>. MiniViT Top-1 accuracy on ImageNet-1K <ref type="bibr" target="#b15">[16]</ref>, Real <ref type="bibr" target="#b6">[7]</ref> and V2 <ref type="bibr" target="#b46">[47]</ref> with comparisons to state-of-the-art models. Our MiniViTs consistently outperform existing transformer-based visual models and CNNs with fewer parameters. ? denotes finetuning with 384 2 resolution. with only a 2% performance drop. The results indicate that parameter redundancy exists in vision transformers and our proposed MiniViT is effective in improving parameter efficiency. Moreover, MiniViT is configurable to satisfy various requirements on model size and performance.</p><p>Distillation Losses. We investigate the effectiveness of different types of distillation in MiniViT. When using both ground truth (GT) and prediction soft labels as loss, we set the trade-off weights to 0.5. We use Swin-T as our baseline. As shown in Tab. 3, compared to only using the prediction loss, the extra GT labels leads to a 0.3% performance drop for Swin, which is due to the degradation of learning ability brought by weight sharing. Furthermore, we also observe around a 0.2% improvement in accuracy after applying the attention-level and hidden-state distillation, indicating the effectiveness of our proposed distillation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on ImageNet</head><p>We compare our proposed MiniViT models for diverse parameter sizes with the state-of-the-art ones on ImageNet-1K <ref type="bibr" target="#b15">[16]</ref>, Real <ref type="bibr" target="#b6">[7]</ref> and V2 <ref type="bibr" target="#b46">[47]</ref>. The top-1 accuracy is reported in Tab. 4. Note that our MiniViTs are trained from  <ref type="bibr" target="#b11">[12]</ref>, and VTP (40%) (48M) <ref type="bibr" target="#b63">[64]</ref>, respectively.</p><p>Our Mini-DeiTs can also outperform automatical nearul architecture search methods. In particular, Mini-DeiT-B uses only 44M parameters to achieve 0.8% better accuracy than AutoFormer-B (54M) <ref type="bibr" target="#b10">[11]</ref>. Our tiny model, Mini-DeiT-Ti, still has comparable performance to AutoFormer-T, while the model size is merely 3M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Transfer Learning Results</head><p>Image Classification. We transfer MiniViT to a collection of commonly used recognition datasets: (1) CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b30">[31]</ref>; (2) fine-grained classification: Flowers <ref type="bibr" target="#b38">[39]</ref>, Stanford Cars <ref type="bibr" target="#b1">[2]</ref>, and Oxford-IIIT Pets <ref type="bibr" target="#b41">[42]</ref>. Following the fine-tuning settings of DeiT <ref type="bibr" target="#b51">[52]</ref>, an SGD optimizer is adopted with learning rate 5 ? 10 ?3 , batch size 256, weight decay 10 ?4 , and disabled random erase, except Cars <ref type="bibr" target="#b1">[2]</ref>, using AdamW <ref type="bibr" target="#b36">[37]</ref> with learning rate 10 ?3 , weight decay 5 ? 10 ?2 and random erase. We train models for 300 epochs on Flowers and Cars, and 1000 on others. Tab. 5 shows the results of Top-1 accuracy. Compared to the state-of-the-art ConvNets and transformer-based models, Mini-DeiT-B?384 achieves comparable or even better results on all datasets, only using 44M parameters.</p><p>Object Detection. We also investigate the transferabil-  <ref type="table">Table 6</ref>. Comparison on COCO <ref type="bibr" target="#b34">[35]</ref> object detection using Cascade Mask R-CNN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>. We replace the original backbone with our compressed models, and report the number of parameters of the backbone. We train detectors for 12 epochs.</p><p>ity of MiniViT to the COCO 2017 detection dataset <ref type="bibr" target="#b34">[35]</ref>. We use Cascade R-CNN <ref type="bibr" target="#b7">[8]</ref> with Swin-T <ref type="bibr" target="#b35">[36]</ref> as our baseline. For distillation during fine-tuning (Det KD), Cascade R-CNN with Swin-B is adopted as the teacher model, and a mean square error (MSE) loss on the backbone outputs of the student and teacher models is utilized with weight 0.1. We follow the same training techniques with Swin-based Cascade R-CNN <ref type="bibr" target="#b35">[36]</ref>. As shown in Tab. 6, (1) weightsharing can cause a 1.6 AP decrease, although it reduces the number of parameter; (2) models with weight transformation can transfer well, with only a 0.6 AP decrease but fewer parameters; (3) MiniViT, combining weight sharing, transformation, and distillation, can achieve comparable AP performance to the baseline with 57% parameter reduction on the backbone, and can achieve the best result after coupled with detection distillation in the fine-tuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Vision Transformer. Transformers were originally proposed for language modeling <ref type="bibr" target="#b53">[54]</ref>, and recently applied in computer vision. It has shown promising potential on a variety of tasks, such as recognition, detection, and segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>. Dosovitskiy et al. first introduced the ViT model <ref type="bibr" target="#b17">[18]</ref>, a pure transformer architecture for visual recognition pre-trained on large-scale data. This work inspired a large amount of follow-up approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b60">61]</ref>. Among them, DeiT <ref type="bibr" target="#b51">[52]</ref> and the Swin transformer <ref type="bibr" target="#b35">[36]</ref> are two representative ones. DeiT <ref type="bibr" target="#b51">[52]</ref> demonstrates that largescale data is not necessary when training a ViT model. Swin transformers <ref type="bibr" target="#b35">[36]</ref> introduce a hierarchical structure to the ViT regime, mimicking traditional convolutional networks. Equipped with shifted windows, they have shown promising results on visual recognition and downstream tasks.</p><p>ViT models are becoming increasingly heavy and expensive, so several recent works have proposed methods for compression. One primitive way is pruning, such as removing redundant tokens <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45]</ref>, attention heads <ref type="bibr" target="#b11">[12]</ref>, or hidden dimensions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b63">64]</ref>. A recent work <ref type="bibr" target="#b3">[4]</ref> combines pruning, skipping, and distillation together and proposes a unified compression framework for vision transformers. However, most existing compression literature focuses on isomorphic vision transformers, whereas their efficacy on hierarchical vision transformers remains unclear. Our method is more general, aiming at the compression of both isomorphic and hierarchical vision transformers.</p><p>Weight Sharing. The history of weight sharing can be traced back to the 1990s <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48]</ref>. Due to its good capability in improving parameter efficiency, weight sharing has been adopted in transformers for language tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>. Universal transformers <ref type="bibr" target="#b14">[15]</ref> propose a weight sharing mechanism across both positions and time steps, yielding better performance than standard transformers <ref type="bibr" target="#b53">[54]</ref> on a variety of sequence-to-sequence tasks. After observing that the output of weight-shared models converges to a fixed point, Deep Equilibrium Models <ref type="bibr" target="#b5">[6]</ref> design a Quasi-Newton method utilizing equilibrium states to fit transformers, and have been demonstrated to outperform other deep sequence models on language tasks. Different from the previous work, Albert <ref type="bibr" target="#b31">[32]</ref> found a performance drop in NLP benchmarks after sharing all weights of transformers in BERT <ref type="bibr" target="#b16">[17]</ref>. By contrast, our work investigates the efficacy of weight sharing in vision transformers, while equipping it with transformation and distillation to further enhance the method.</p><p>Knowledge Distillation. Distillation in a teacher-student framework, is widely used to reduce model sizes. It has been extensively studied in convolutional networks <ref type="bibr" target="#b18">[19]</ref>. However, in vision transformers, it is still under-explored. A few relevant recent works include Touvron et al. <ref type="bibr" target="#b51">[52]</ref> who introduce a distillation token to allow the transformer to learn from a ConvNet teacher, and Jia et al. <ref type="bibr" target="#b26">[27]</ref> propose to excavate knowledge from the teacher transformer via the connection between images and patches. Distillation inside the transformer (e.g., MSA and MLP) has been verified to be effective in transformers for NLP <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. In contrast, our work provides an initial design for attention-level and hidden-state distillation, and explores their effectiveness in vision transformer compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a new compression framework, i.e. MiniViT, for vision transformers. Our method combines weight sharing, transformation, and distillation to reduce the number of parameters while achieving even better performance compared to the original models. Limitations. One limitation of MiniViT is that, despite improving the parameter efficiency, the computational cost is slightly increased compared to the classical weight sharing strategy, due to the introduced weight transformation blocks. Second, we observe that MiniViT suffers from moderate performance degradation as the compression ratio increases. In future work, we are interested in further improving both the parameter and computational efficiency. Broader Impacts. Presented in the supplementary materials.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Classical weight sharing versus weight multiplexing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Comparisons of ?2-norm of gradients during training among the models with weight sharing (WS), and with both weight sharing and multiplexing (WS+MUX), and the original one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparisons of feature similarity with respect to the original model by CKA<ref type="bibr" target="#b28">[29]</ref> between the models with weight sharing (WS), and with weight sharing and multiplexing (WS+MUX).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to thank Microsoft NNI and OpenPAI v-team for kind helps and supports. Thanks to Prof. Po-Ling Loh for the final proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multiplexing</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Multiplexing.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">3d object representations for fine-grained categorization</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nvit: Vision transformer compression and parameter redistribution</title>
	</analytic>
	<monogr>
		<title level="m">OpenView, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Anonymous authors</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unified visual transformer compression</title>
	</analytic>
	<monogr>
		<title level="m">OpenView</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Anonymous authors</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xiaohua Zhai, and A?ron van den Oord</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
	</analytic>
	<monogr>
		<title level="m">Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chasing sparsity in vision transformers: An end-to-end exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Universal transformers. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Knowledge distillation: A survey. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Levenshtein transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient vision transformers via fine-grained manifold distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01378</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tinybert: Distilling bert for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generalization and network design strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Connectionism in perspective</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A tensorized transformer for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simplifying neural networks by soft weight-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ia-red2: Interpretability-aware redundancy reduction for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Bowen Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Wireless communications: principles and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theodore S Rappaport</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<pubPlace>New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning representations by back-propagating errors. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02436,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Talking-heads attention. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mobilebert: a compact taskagnostic bert for resource-limited devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<editor>ICML. PMLR</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Grafit: Learning finegrained image representations with coarse labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>2021. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rethinking and improving relative position encoding for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno>2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Vision transformer pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop on Model Mining</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
