<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CorrI2P: Deep Image-to-Point Cloud Registration via Dense Correspondence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Junhui</forename><forename type="middle">Hou</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">CorrI2P: Deep Image-to-Point Cloud Registration via Dense Correspondence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Point cloud</term>
					<term>registration</term>
					<term>cross-modality</term>
					<term>corre- spondence</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the intuition that the critical step of localizing a 2D image in the corresponding 3D point cloud is establishing 2D-3D correspondence between them, we propose the first feature-based dense correspondence framework for addressing the challenging problem of 2D image-to-3D point cloud registration, dubbed CorrI2P. CorrI2P is mainly composed of three modules, i.e., feature embedding, symmetric overlapping region detection, and pose estimation through the established correspondence. Specifically, given a pair of a 2D image and a 3D point cloud, we first transform them into high-dimensional feature spaces and feed the resulting features into a symmetric overlapping region detector to determine the region where the image and point cloud overlap. Then we use the features of the overlapping regions to establish dense 2D-3D correspondence, on which EPnP within RANSAC is performed to estimate the camera pose, i.e., translation and rotation matrices. Experimental results on KITTI and NuScenes datasets show that our CorrI2P outperforms state-of-the-art image-to-point cloud registration methods significantly. The code will be publicly available at https://github.com/rsy6318/CorrI2P.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D-3D correspondences are same-modal. Cross-modal data registration, i.e., image-to-point cloud (I2P) registration <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, can remedy the disadvantages of these two same-modal techniques. However, it relies on cross-modal correspondence, i.e., 2D-3D correspondence, which is more challenging to estimate. Previous works for I2I and P2P registration cannot be simply extended to establishing 2D-3D correspondence in I2P registration because they establish 2D-2D or 3D-3D correspondence from same-modal data. SfM <ref type="bibr" target="#b5">[6]</ref> is a wellknown approach for obtaining 2D-3D correspondence, which reconstructs 3D point clouds from a series of images and obtains correspondence based on image features. However, the reconstructed point cloud from images has low accuracy and suffers from the limitation that image features are easily influenced by external environments. 2D3D-MatchNet <ref type="bibr" target="#b25">[26]</ref> is the first feature-based registration method, which seeks 2D-3D correspondence directly. However, it focuses on the correspondence of key points detected based on the hand-crafted features by SIFT <ref type="bibr" target="#b26">[27]</ref> and ISS <ref type="bibr" target="#b27">[28]</ref>. The above-mentioned methods are feature-based, meaning that these methods use features of pixels and points to establish 2D-3D correspondence according to the nearest neighborhood principle. The recent DeepI2P <ref type="bibr" target="#b18">[19]</ref> casts the correspondence problem to point-wise classification without utilizing pixel-wise or point-wise features to establish 2D-3D correspondence. However, the points at the frustum boundary are prone to be wrongly classified, thus limiting registration accuracy.</p><p>In this paper, we propose CorrI2P, the first learnable paradigm for building dense 2D-3D correspondence. It is a two-branch neural network with a symmetric cross-attention fusion module identifying overlap and extracting dense features from the image and point cloud. Based on the extracted overlap features, it builds the 2D-3D correspondence and estimates the camera's posture. We design a descriptor loss and a detector loss to drive the training of CorrI2P. Experimental results show that our CorrI2P achieves state-of-the-art performance on KITTI <ref type="bibr" target="#b28">[29]</ref> and NuScenes <ref type="bibr" target="#b29">[30]</ref> datasets.</p><p>In summary, the main contributions of this paper are threefold: 1) we propose the first feature-based dense correspondence framework for image-to-point cloud registration; 2) we design a novel symmetric overlapping region detector for the cross-modal data, i.e., images and point clouds; and 3) we propose a joint loss function to drive the learning process of the cross-modal overlap detection and dense correspondence.  Then the resulting features are fed into an overlapping region detector to detect the overlapping regions on both the image and point cloud, and the features of pixels and points located in the detected overlapping regions are adopted to establish 2D-3D correspondence. Finally, the camera pose can be obtained by applying EPnP within RANSAC on the dense correspondence.</p><p>The rest of this paper is organized as follows. Sec. II briefly reviews existing works about visual pose estimation and 2D-3D correspondence for registration. Sec. III presents the proposed CorrI2P, followed by extensive experiments and analysis in Sec. IV. Finally, Sec. V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visual Pose Estimation</head><p>Given a query image, estimating the 6DOF camera pose in a 3D scene model, usually presented as a point cloud, is crucial for visual navigation. The critical step in this progress is to build 2D-3D correspondence between the image and point cloud. SfM <ref type="bibr" target="#b5">[6]</ref> is a traditional method of recovering the point cloud from a sequence of images while using the handcrafted image feature (SIFT <ref type="bibr" target="#b26">[27]</ref>, ORB <ref type="bibr" target="#b30">[31]</ref>, or FAST <ref type="bibr" target="#b31">[32]</ref>) to generate the 2D-3D correspondence. It utilizes pixelwise features and the recovered 3D points to establish the 2D-3D correspondence. However, the reconstructed point cloud is sparse, and the imaging environment can affect pixel-wise features.</p><p>Some learnable approaches based on same-modal data, i.e., I2I or P2P, have been proposed with the emergence of deep learning. As for the I2I registration methods, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> collect the images from different environments and train CNNs to extract robust features to establish correspondence. Furthermore, <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref> use CNNs to regress camera pose directly. They benefit from the easy availability of image data but are susceptible to environmental conditions. P2P registration methods, on the other hand, obtain accurate point cloud data from Lidar or RGBD cameras.</p><p>With the growth of deep learning on the point cloud, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b41">[42]</ref> use neural networks to extract point-wise or global features from the point cloud directly and combine some traditional methods, such as RPM <ref type="bibr" target="#b42">[43]</ref> and RANSAC <ref type="bibr" target="#b43">[44]</ref>, to estimate the rigid transformation. The above samemodal methods cannot be readily extended to I2P registration, which relies on learned 2D-3D correspondence from crossmodal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2D-3D Correspondence for Registration</head><p>Unlike 2D images represented with regular dense grids, 3D point clouds are irregular and unordered, posing substantial challenges to learning correspondence between these two modalities. 2D3D-MatchNet <ref type="bibr" target="#b25">[26]</ref> selects key points from the image and point cloud using SIFT <ref type="bibr" target="#b26">[27]</ref> and ISS <ref type="bibr" target="#b27">[28]</ref>, respectively, then feeds the patches around these pixels and points to CNNs <ref type="bibr" target="#b44">[45]</ref> and PointNet <ref type="bibr" target="#b45">[46]</ref> to extract features for creating 2D-3D correspondence. However, independent key point detection for different modalities will reduce the inlier ratio and registration accuracy. DeepI2P <ref type="bibr" target="#b18">[19]</ref> employs a feature-free technique in which a network is trained to classify whether each point of the point cloud is located in the visual frustum, i.e., the area where the points could be projected on the image, then inverse camera projection is used to optimize the camera pose until the points identified in the image fall within the frustum. However, the points near the border of the frustum are easy to get erroneous classification results. Besides, DeepI2P also divides the image into square regions and uses the network to classify which region the points of the point could be projected onto. The classification result only indicates coarse 2D-3D correspondence, thus limiting the low registration accuracy.</p><p>Yu et al. <ref type="bibr" target="#b46">[47]</ref> utilized the 2D-3D correspondence established from the linear shapes to estimate the camera pose. But, it requires accurate initialization, which is different from ours. Liu et al. <ref type="bibr" target="#b47">[48]</ref> projected the point cloud onto the image and built the correspondence between the projected points and the point cloud. Their 2D-3D correspondence is different from ours, where the image is taken from the camera directly. Agarwal et al. <ref type="bibr" target="#b5">[6]</ref>, Chen et al. <ref type="bibr" target="#b32">[33]</ref> and Mulfellner et al. <ref type="bibr" target="#b33">[34]</ref> reconstructed the point cloud from a series of images at different locations. During the reconstruction, they used 2D-3D correspondence to localize each image, but the point cloud features are from the image not the point cloud itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Let I ? R W ?H?3 and P ? R N ?3 be a pair of a 2D image and a 3D point cloud, where W and H are the width and height of the image, respectively, and N is the number of points. The objective of I2P registration is to estimate the camera pose in . For I2PAF (resp. P2IAF), the point cloud (resp. image) feature is concatenated with the image (resp. point cloud) global feature and fed into an MLP (resp. a CNN), followed by softmax to generate the attention weight. Then the attention weight is applied to the image (resp. point cloud) feature of the same hierarchy to generate the weighted image (resp. point cloud) feature.</p><p>the space of P, denoted as T = [R|t] with R ? SO(3) and t ? R 3 , given a query image I. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, our method consists of three modules, i.e., feature embedding, symmetric overlapping region detection, and pose estimation with dense correspondence. Specifically, given I and P, we first embed them into highdimensional feature spaces separately, then feed the resulting features into the symmetric overlapping region detector to predict the overlapping region and build the dense 2D-3D correspondence, on which we finally run EPnP <ref type="bibr" target="#b6">[7]</ref> with RANSAC <ref type="bibr" target="#b43">[44]</ref> to estimate the camera pose. In what follows, we will provide the detail of each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Embedding</head><p>Due to the different structures and characteristics of 2D images and 3D point clouds, it is impossible to deal with them using the same network architecture. Thus, following DeepI2P <ref type="bibr" target="#b18">[19]</ref>, we utilize ResNet <ref type="bibr" target="#b48">[49]</ref> and SO-Net <ref type="bibr" target="#b49">[50]</ref> to embed I and P to high-dimensional feature spaces in a hierarchical manner, respectively, generating the pixel-wise feature embedding F l I ? R W l ?H l ?d l and the point-wise feature embedding F l P ? R N l ?d l at the l-th layer (l = 1, 2). Then we perform the max-pooling operation on features F 2 I and F 2 P to obtain the global features of I and P, denoted as g I ? R d and g P ? R d , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Symmetric Overlapping Region Detection</head><p>We design a novel symmetric detector to select the overlapping pixels and points where 2D-3D correspondence is built. As shown in <ref type="figure">Fig. 2</ref>, we first pass the 2D-3D features into a cross-attention fusion module, composed of two components, namely image-to-point cloud attention fusion (I2PAF) and point cloud-to-image attention fusion (P2IAF), generating weighted features of the image and point cloud. The underlying intuition is to map the image and point cloud features to each other's space. Then we feed the weighted features with the 2D-3D features into the image and point cloud decoders to predict the overlapping regions.</p><p>Cross-attention fusion module. As shown in <ref type="figure">Fig. 3</ref>, this module, which aims to fuse the image and point cloud information to detect the overlapping regions, consists of I2PAF and P2IAF that are symmetric share a similar structure.</p><p>For I2PAF, we concatenate the image global feature g I and point cloud local feature F l P and feed them into an MLP followed by the Softmax operator to learn the attention</p><formula xml:id="formula_0">weight W l I2P ? R N l ?(H l ?W l )</formula><p>. Then we multiply the image local features F l I by attention weights W l I2P , producing the weighted image feature F l WI ? R N l ?d l . Similar to I2PAF, we can get the weighted point cloud feature F l WP ? R H l ?W l ?d l using the symmetric module P2IAF.</p><p>Image and point cloud decoders. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the image and point cloud decoders share a similar design. Our intuitions are to recover the spatial dimension using the pixel/point upsampling layers and decrease (or fuse) the feature channels using the ResNet/PointNet layers hierarchically.</p><p>For the image decoder shown in <ref type="figure" target="#fig_2">Fig. 4a</ref>, we first concatenate (F 2 WP , F 2 I ) and then feed them into a ResNet followed by a pixel upsampling operation (Res&amp;pixelUp) to obtain feature map F 1 I ? R H1?W1?d 1 . Then we further concatenate (F 1 WP , F 1 I , F 1 I ) and feed them into another two sets of such operators (Res&amp;pixelUp) to obtain the fused feature map F I ? R H ?W ?d . And F I will be passed into two CNNs for generating the pixel-wise scores S I ? R H ?W ?1 and pixelwise features H I ? R H ?W ?c . Finally, we determine that the pixels whose scores are higher than a threshold ? belong to the overlapping regions. Let O I ? R KI?2 be the set of the coordinates of K I pixels detected in the overlapping regions, and F OI ? R KI?c their features collected from H I .</p><p>The point cloud decoder shown in <ref type="figure" target="#fig_2">Fig. 4b</ref> shares the same procedure as the image decoder, except that ResNet is replaced with PointNet and the pixelUp with pointUp realized by PointNet++ <ref type="bibr" target="#b50">[51]</ref>. The features are fed into a PointNet followed by a pointUp to generate the fused feature map F P ? R N ?d . Also, the CNNs are replaced with PointNet to generate the point-wise scores S P ? R N ?1 and features H P ? R N ?c . We use the same threshold ? to filter them and obtain the estimated overlapping region O P ? R KP?3 and the corresponding features F OP ? R KP?c , where K P is the number of overlapping points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dense Correspondence-based Pose Estimation</head><p>Under the assumption that the matched pixel-point pairs have similar features, whereas non-matched pairs have distinct features, we apply the nearest neighbor principle in the feature space of the detected overlapping region to establish 2D-3D correspondence.</p><p>Specifically, considering that multiple points cloud may be projected onto an identical pixel due to occlusions in the scene and the limited image resolution, we build the 2D-3D correspondence by finding the most similar pixel for each 3D point, i.e., for each point P i ? O P , i = 1, 2, ..., K P , we select the pixel in the O I whose feature is the nearest to that of P i as its correspondence. Formally, let I p(i) be the corresponding pairs of pixels and points, where p(i) is the index of the pixel in O I , obtained by optimizing</p><formula xml:id="formula_1">p(i) = arg min j=1,2,...,KI F OP,i ? F OI,j .<label>(1)</label></formula><p>It is inevitable that the above method will generate wrong correspondence, and directly applying the EPnP to them may decrease the registration accuracy, and even produce the wrong transformation. Similar to I2I and P2P registration using 2D or 3D correspondence, we run EPnP within RANSAC to optimize the camera pose and reject the outliers simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>To drive the learning process of overlapping region detection and feature matching, we design a joint loss function consisting of a descriptor loss and a detector loss. Generally, the descriptor loss promotes the network to produce similar features for matched pixel-point pairs and different features for non-matched pairs. The detector loss encourages that the network can reliably identify overlap, producing higher scores for pixels and points inside the overlapping regions and lower scores for those outside the overlapping regions.</p><p>Descriptor loss. We use the cosine distance to compute the distance in the feature space, i.e.,</p><formula xml:id="formula_2">d(F 1 , F 2 ) = 1 ? F 1 , F 2 F 1 F 2 ,<label>(2)</label></formula><p>where F 1 and F 2 represent image and point cloud feature vectors, and ?, ? is their inner product. Given I and P, we use ground truth information to sample n pairs of 2D-3D correspondence {I i , P i }, i = 1, 2, ..., n from the overlapping region of the image and point cloud, and their corresponding feature pairs are {F OI,i , F OP,i }. The feature distance of a positive pair is defined as</p><formula xml:id="formula_3">d pos (i) = d(F OI,i , F OP,i ).<label>(3)</label></formula><p>If the distance between a pixel and the projection of a point onto the image is larger than a safe radius, denoted as R, they could be thought of as a negative pair. For a typical 3D point, many 2D pixels could be adopted to form a feasible negative pair. Instead of using all these negative pairs, we only select the pair with the smallest feature distance and define the distance of a negative pair as</p><formula xml:id="formula_4">d neg (i) = min j=1,2,...,n {d(F OI,i , F OP,j )} s.t. I j ? I i &gt; R. (4)</formula><p>In order to accelerate the training process, we also introduce two margins, i.e., positive margin M pos and negative margin M neg , and define the descriptor loss as</p><formula xml:id="formula_5">L desc = 1 n n i=1 [max(0, d pos (i) ? M pos )+ max(0, M neg ? d neg (i))].<label>(5)</label></formula><p>Detector loss. According to the ground truth transformation, we sample n pairs of pixels and points from the overlapping regions with their scores, Such a loss function promotes the pixels and points within the overlapping regions have high scores, while those beyond the overlapping regions have low scores during training.</p><p>In total, our loss function is</p><formula xml:id="formula_6">L = L desc + ?L det ,<label>(7)</label></formula><p>where ? is a hyperparameter for balancing the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset</head><p>We adopted two commonly used datasets, i.e., KITTI <ref type="bibr" target="#b28">[29]</ref> and NuScenes <ref type="bibr" target="#b29">[30]</ref>, to evaluate the proposed method.</p><p>? KITTI Odometry <ref type="bibr" target="#b28">[29]</ref>. The image-point cloud pairs were selected from the same data frame, i.e., the images and point clouds were captured simultaneously through a 2D camera and a 3D LiDAR with fixed relative positions. We followed the common practice <ref type="bibr" target="#b18">[19]</ref> of utilizing the first 8 sequences for training and the last 2 for testing. The transformation of the point cloud consists of a rotation around the up-axis and a 2D translation on the ground within the range of 10 m. During training, the image size was set to 160 ? 512, and the number of points to 40960. ? NuScenes <ref type="bibr" target="#b29">[30]</ref>. Point clouds were acquired from a Li-DAR with a 360-degree field of view. We used the official SDK to get the image-point cloud pairs, where the point cloud was accumulated from the nearby frames, and the image from the current data frame. We followed the official data split of NuScenes, where 850 scenes were used for training, and 150 for testing, and the transformation range was similar to the KITTI dataset. The image size was set to 160 ? 320, and the number of points was the same as the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Training. During training, we established the ground truth 2D-3D correspondence to supervise the network. The transformation of a 3D point P i ? R 3 to the image coordinate of the camera p i ? R 2 is given b?</p><formula xml:id="formula_7">p i = ? ? x i y i z i ? ? = K(R gt P i + t gt ),<label>(8)</label></formula><formula xml:id="formula_8">p i = p xi p yi = x i /z i y i /z i ,<label>(9)</label></formula><p>where K ? R 3?3 is the camera intrinsic matrix and T gt = [R gt |t gt ] is the ground truth camera pose. For image features, F 1 I is a 1/16 scale of the original image while F 2 I is 1/32, and H I is 1/4, i.e., W = 16W 1 = 32W 2 = 4W and H = 16H 1 = 32H 2 = 4H , as shown in <ref type="figure">Fig. 2</ref>. As for point cloud features, N 1 = N 2 = 256, and the number of the k-NN in SO-Net <ref type="bibr" target="#b49">[50]</ref> is k = 32. The feature channels are d 1 = d 1 = 256, d 2 = d 3 = 512 and c = 128. We trained our network for 25 epochs on each dataset, with a batch size of 24. We used the Adam <ref type="bibr" target="#b51">[52]</ref> to optimize the network, and the initial learning rate was 0.001 and multiplied by 0.25 every five epochs until it reached 0.00001. During training, we set the safe radius R to 1 pixel, the value of ? involved in the loss function to 0.5, the positive margin to M pos = 0.2, and the negative margin to M neg = 1.8.</p><p>Testing. Based on the experimental observation, we set ? = 0.9 to determine the overlapping regions. For RANSAC, we set the number of iterations to 500 and the threshold for inlier reprojection error to 1 pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compared Methods</head><p>We compared our CorrI2P with the setting called Grid Cls. + PnP. and Frus. Cls. + Inv.Proj.:</p><p>? Grid Cls. + PnP. proposed in recent DeepI2P <ref type="bibr" target="#b18">[19]</ref> divides the image into 32?32 grids and uses the network to classify which grid the points in the point cloud would be projected into. It establishes 2D-3D correspondence based on the classification result and then uses EPnP with RANSAC to estimate the camera pose. ? Frus. Cls. + Inv.Proj. uses the frustum classification, and inverse camera projection in DeepI2P <ref type="bibr" target="#b18">[19]</ref> to obtain the camera pose. We used the same network setting as their paper and tried the 2D and 3D inverse camera projection to optimize the pose, namely DeepI2P (2D) and DeepI2P (3D), respectively. We used a 60-fold random initialization to search for the initial pose of the camera.</p><p>Evaluation metric. Similar to P2P registration <ref type="bibr" target="#b52">[53]</ref>, we used Relative Translational Error (RTE) E t and Relative Rotational Error (RRE) E R to evaluate our registration result, respectively computed as</p><formula xml:id="formula_9">E R = 3 i=1 |?(i)|,<label>(10)</label></formula><formula xml:id="formula_10">E t = t gt ? t E ,<label>(11)</label></formula><p>where ? is the Euler angle of the matrix R ?1 gt R E , R gt and t gt are the ground-truth transformation, and R E and t E are the estimated transformation.</p><p>In the ablation study, we also conducted a feature matching experiment to show the quality of the correspondence estimator. Inspired by P2P registration <ref type="bibr" target="#b53">[54]</ref>, we designed two kinds of recall to evaluate the feature matching. Pair recall R pair is the ratio of the correct correspondences, while fragment recall R frag is the ratio of the fragments with higher proportion of correct 2D-3D correspondences than a pre-set threshold. They are calculated as</p><formula xml:id="formula_11">R pair = 1 M M s=1 ? ? 1 |O s P | i?O s P 1 p s p(i) ? ?(T s gt P s i ) &lt; ? 1 ? ? ,<label>(12)</label></formula><formula xml:id="formula_12">R frag = 1 M M s=1 1 ? ? ? ? 1 |O s P | i?O s P 1 p s p(i) ? ?(T s gt P s i ) &lt; ? 1 &gt; ? 2 ,<label>(13)</label></formula><p>where ? 1 and ? 2 are the inlier distance and inlier ratio threshold, M is the number of the ground truth matching image-point cloud pairs, ?(?) is the projection from the point cloud to image coordinate according to Eqs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>Registration accuracy. Since some failed registration results may cause dramatically large RRE and RTE, showing unreliable error metrics, similar to P2P registration <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, we calculated the average RTE and RRE only for those with     <ref type="figure">Fig. 7</ref>. Comparison of 2D-3D correspondence and registration accuracy by different methods. We aligned the point cloud through ground-truth transformation for visualization purposes, the color of points represents the depth, and the lines represent the 2D-3D correspondence. Here we only show the correct correspondence. It can be seen that our method produces the densest correspondence and the highest registration accuracy. (a) Original. (b) Random. (c) SIFT-ISS <ref type="bibr" target="#b25">[26]</ref>. (d) Grid Cls <ref type="bibr" target="#b18">[19]</ref>. (e) Our method.</p><p>RTE lower than 5m and RRE lower than 10 ? . The registration accuracy is illustrated in <ref type="table" target="#tab_1">Table I</ref>, where it can be seen that our method outperforms all compared methods by a noticeable margin on both datasets. Besides, for a more detailed comparison of the registration performance, we showed the registration recall with different RTE and RRE thresholds on two datasets in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>As listed in <ref type="table" target="#tab_1">Table II</ref>, although the recall, precision, and F2-Score values of the frustum classification of DeepI2P <ref type="bibr" target="#b18">[19]</ref> achieve 0.935, 0.946, and 0.938, respectively, it is still worse than our CorrI2P because the points located in the boundary of the frustum are prone to be wrongly classified, as shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, which has an adverse influence on the inverse camera projection and eventually gets the wrong camera pose. Grid Cls. + EPnP has worse registration accuracy because the 32 ? 32 grid size is too coarse to get an accurate pose, and in this way, it can only get sparse and coarse 2D-3D correspondence, as shown in <ref type="figure">Fig. 7d</ref>, which decreases the registration accuracy although the grid classification accuracy is higher than 0.50. By contrast, our CorrI2P estimates the camera pose according to dense correspondence, as shown in <ref type="figure">Fig. 7e</ref>, which is beneficial to the final registration accuracy.</p><p>Error distribution. The distributions of the registration error, i.e., RTE and RRE, are shown in <ref type="figure" target="#fig_10">Fig. 9</ref>, where it can be seen that the performance is better on KITTI than NuScenes. The mode of RTE/RRE is ?0.5m/2?on KITTI and ?1.5m/2?on NuScenes. The RTE and RRE variances are also smaller on KITTI.</p><p>Accuracy of overlapping region detection. Overlapping region detection is critical for our method to select the corresponding pixels and points, and the accurate overlapping region detection would increase the registration accuracy. As visualized in <ref type="figure" target="#fig_6">Fig. 6</ref>, the overlapping region predicted by our method is the most accurate. Furthermore, we conducted experiments to quantitatively compare the accuracy of overlapping region detection on the KITTI dataset. As overlapping region detection on the image and point cloud can be regarded as pixel-wise and point-wise binary classification, we used recall, precision, and F2-score as metrics to evaluate the performance of our overlapping region detection. We adopted random sampling on the image and point cloud as a baseline, where 2048 pixels and 8192 points were sampled from the image and point cloud, respectively. Besides, we also used SIFT <ref type="bibr" target="#b26">[27]</ref> and ISS <ref type="bibr" target="#b27">[28]</ref> to extract the keypoints in the image and point cloud and regarded them as overlapping regions, just like 2D3D-MatchNet <ref type="bibr" target="#b25">[26]</ref>. DeepI2P uses pointwise classification to select the points within the frustum, i.e., the overlapping region of the point cloud, so we used it as a comparison of the overlapping region detection on the point cloud.</p><p>The results are listed in <ref type="table" target="#tab_1">Table II</ref>, where it can be seen that the detection accuracy of Random, SIFT, and ISS is much worse than ours. The reason is that these methods select pixels and points in the whole image and point cloud, but their ground truth overlapping regions only take up a small proportion, resulting in less correct correspondence and lower registration accuracy. The ablation study would show the accuracy of registration based on these overlapping region detection methods. And it can also be seen that our overlapping region detection method for the point cloud is better than DeepI2P. The precision of our overlapping region detection is higher than 0.9 on both image and point cloud, ensuring registration accuracy.</p><p>KITTI vs. NuScenes. Our registration accuracy is higher on KITTI than that on NuScenes. The main reason is that the point clouds of the scene in the two datasets are acquired through different methods. The point cloud in the KITTI dataset is dense enough to use directly. As for the NuScenes dataset, every point cloud frame is so sparse that we need to splice it with the adjacent frames. However, the point cloud is collected on the street, and some components in the scenes are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>In this section, we conducted ablation studies for our CorrI2P on the KITTI dataset.</p><p>Cross-attention fusion module. The cross-attention module can fuse the information from the image and point cloud with each other, which facilitates overlapping region detection, feature extraction, and correspondence estimation. Thus, we conducted an ablation study to identify its necessity. Similar to P2P registration <ref type="bibr" target="#b56">[57]</ref>, we trained a network without the cross attention fusion module and detection loss and used pair recall R pair and fragment recall R frag respectively defined in Eqs. <ref type="bibr" target="#b11">(12)</ref> and <ref type="bibr" target="#b12">(13)</ref> to evaluate the 2D-3D feature matching. <ref type="figure" target="#fig_9">Fig. 8a</ref> shows the recall of feature matching through varying ? 1 and ? 2 , convincingly demonstrating that the cross-attention fusion module is beneficial to feature matching and the establishment of the 2D-3D correspondence.</p><p>Overlapping region detection. Our CorrI2P can detect the overlapping region of the image and point cloud, where dense 2D-3D correspondence is built. This can increase the inlier ratio of correspondence, thus achieving higher registration accuracy. To verify this, we also set baselines by employing different sampling strategies during the registration, including random and keypoint selection. We only used the descriptor loss to train the network for these experiments. For the random selection strategy, we kept the same number of sampled pixels and points, i.e., we randomly selected 2048 pixels and 8192 points from the image and point cloud, respectively, as well as their features to do the registration. For the keypoint selection strategy, we imitated 2D3D-MatchNet, which uses SIFT and ISS to detect the key points from the image and point cloud to do registration, leading to sparse 2D-3D correspondence. We used 'Score' to represent our overlapping region detector because we used a confidence score to select the pixels and points. Besides, we also mixed these sampling strategies with ours, such as 'IMG Random' and 'PC Score'. The result is shown in <ref type="figure" target="#fig_9">Fig. 8b</ref> and <ref type="table" target="#tab_1">Table III</ref>. It can be seen that without our overlapping region detection, the registration accuracy would significantly decrease. However, only by removing the overlap detection for image, i.e., 'IMG Random + PC Score' or 'IMG SIFT + PC Score', would the accuracy decrease     only a little while removing that for point cloud, i.e. 'PC Random' or 'PC ISS' the registration performance would decrease significantly. The points that can be projected onto the image only take up a small part of the whole point cloud, while the overlapping region on the image accounts for a large proportion as shown in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p><p>3D point cloud density. Considering that the density of 3D point clouds is a key factor in feature extraction, we carried out the ablation experiment on it. For a scene of the same size, we changed the density of the point cloud by downsampling different numbers of points. To keep the same receptive fields for point clouds with different point densities, we scaled the numbers of knn searching for different densities, i.e., k = 32 for 40960 points, k = 16 for 20480 points, and k = 8 for 10240 and 5120 points. The result is shown in <ref type="table" target="#tab_1">Table IV</ref>, the registration accuracy decreases with reducing 3D point density because point cloud in low density would omit some structure information, and the features extracted would be less descriptive, resulting in wrong 2D-3D correspondence and thus low registration accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Efficiency Analysis</head><p>We evaluated the efficiency of our CorrI2P on the KITTI dataset. We used an NVIDIA Geforce RTX 3090 GPU for neural network inference and Intel(R) Xeon(R) Gold 6346 CPU for pose estimation. We fed data with a batch size of 8 to the neural network and got the average FLOPs, GPU memory usage, and inference time. The results are shown in <ref type="table" target="#tab_6">Table V</ref>, where the classification-based methods, i.e., Grid Cls. and DeepI2P, require less GPU resource and are faster than ours during inference, because our method needs to perform feature extraction and overlapping region detection on both images and point clouds, i.e., using image and point cloud decoders to produce pixel-wise and point-wise features and scores, rather than only classifying the points of the point cloud. As for pose estimation, Grid Cls. + EPnP is the fastest because the image grid is 32?32, resulting in a higher inlier ratio than ours, and the RANSAC only needs fewer iterations. DeepI2P is much slower than other methods because it needs 60-fold pose initialization before the optimization to enhance the robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a new learning-based framework named CorrI2P for 2D image-to-3D point cloud registration by estimating the dense correspondence between the two data modalities. Technically, we designed a symmetric overlapping region detector for both images and point clouds to estimate the overlapping regions, where dense 2D-3D correspondence is estimated based on their features. We demonstrated the significant advantages of our CorrI2P over state-of-the-art ones by conducting extensive experiments on the KITTI and NuScenes datasets, as well as comprehensive ablation studies. We believe our methods will benefit other tasks, such as distillation on image or point cloud and semantic segmentation for cross-modality data, which usually transform the different kinds of data to the same feature space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the overall pipeline of the proposed CorrI2P for image-to-point registration. Taking a pair of a 2D RGB image and a 3D point cloud as input, CorrI2P first performs feature embedding, producing pixel-wise and point-wise features of two scales, as well as global features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Illustration of the network architecture of the symmetric overlapping region detector. It is intended to realize the interaction between the features of the image and the point cloud. We feed the images and point cloud features of various scales and their global features into the cross-attention fusion module to map the image and point cloud features into each other's space. Two decoders then fuse the original and mapped features to determine the overlapping regions. Illustration of the network architectures of the proposed cross-attention fusion module. (a) Image-to-Point Cloud Attention Fusion Module (I2PAF) (b)Point Cloud-to-Image Attention Fusion Module (P2IAF)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>{I u , S I,u }, I u ? O I , u = 1, ..., n and {P v , S P,v }, P v ? O P , v = 1, ..., n. Besides, we also collect n pixels and points in the non-overlapping region, {I k , S I,k }, I k / ? O I , k = 1, .., n and {P m , S P,m }, P m / ? O P , m = 1, ..., n. We define the detector loss as L det = 1 Illustration of the network architectures of the decoder module. The features of the image and point cloud are fed into two detectors, and the coordinates and features of the overlapping regions are produced. (a) Image Decoder. (b) Point Cloud Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(8)-(9), 1(?) is the indicator function, T s gt is the ground truth transformation of the s-th image-point cloud pair, p is the pixel coordinate, and P is the point cloud coordinate. p(i) is the index of i-th point in the point cloud's corresponding pixel using Eq. (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of the Registration recall of different methods with various RTE and RRE thresholds on KITTI and NuScenes datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Visual illustration of overlapping region detection on the image and point cloud. The green pixels and points are detected in the overlapping region of the image and point cloud, and the red ones are wrongly detected. Note that DeepI2P conducts overlapping region detection only on point clouds. It is obvious that our method has better performance on both image and point cloud overlapping region detection over other methods. (a) and (f): Original. (b) and (g): Ground Truth. (c) and (h): Random. (d) and (i): SIFT-ISS [26]. (e) and (j): Ours. (k): DeepI2P [19].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) RTE(m)/RRE( ? ) (b) 2.90/9.16 (c) 2.76/5.73 (d) 1.63/8.02 (e) 0.66/0.49</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Ablation study results on the KITTI dataset. (a) Feature matching recall in relation to inlier ratio threshold ? 1 (left) and inlier ratio threshold ? 2 (right). (b) Registration recall with different RTE and RRE thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Histograms of RTE and RRE on the KITTI and NuScenes datasets.x-axis is RTE(m) and RRE(?), and y-axis is the percentage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Point Cloud P Image I Feature Embedding Point Cloud Feature Image Feature Overlapping Region Detec?or Pose Es?ma?on with Correspondence</head><label></label><figDesc></figDesc><table><row><cell>(</cell><cell>)</cell></row><row><cell></cell><cell>Camera Pose</cell></row><row><cell></cell><cell>T=[R|t]</cell></row><row><cell>(</cell><cell>)</cell></row><row><cell cols="2">Overlap Region</cell></row><row><cell cols="2">and Feature</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF THE REGISTRATION ACCURACY (MEAN ? STD) OF DIFFERENT METHODS ON THE KITTI AND NUSCENES DATASETS. "?" MEANS THAT THE SMALLER, THE BETTER. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.</figDesc><table><row><cell></cell><cell>KITTI</cell><cell></cell><cell cols="2">NuScenes</cell></row><row><cell></cell><cell>RTE ? (m)</cell><cell>RRE ? ( ? )</cell><cell>RTE ? (m)</cell><cell>RRE ? ( ? )</cell></row><row><cell>Grid Cls. + EPnP [19]</cell><cell>1.07 ? 0.61</cell><cell>6.48 ? 1.66</cell><cell>2.35 ? 1.12</cell><cell>7.20 ? 1.65</cell></row><row><cell>DeepI2P (3D) [19]</cell><cell>1.27 ? 0.80</cell><cell>6.26 ? 2.29</cell><cell>2.00 ? 1.08</cell><cell>7.18 ? 1.92</cell></row><row><cell>DeepI2P (2D) [19]</cell><cell>1.46 ? 0.96</cell><cell>4.27 ? 2.74</cell><cell>2.19 ? 1.16</cell><cell>3.54 ? 2.51</cell></row><row><cell>Ours</cell><cell>0.74 ? 0.65 0.74 ? 0.65 0.74 ? 0.65</cell><cell>2.07 ? 1.64 2.07 ? 1.64 2.07 ? 1.64</cell><cell>1.83 ? 1.06 1.83 ? 1.06 1.83 ? 1.06</cell><cell>2.65 ? 1.93 2.65 ? 1.93 2.65 ? 1.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II THE</head><label>II</label><figDesc>PERFORMANCE OF THE OVERLAP DETECTION ON THE KITTI DATASET. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Recall</cell><cell>Precision</cell><cell>F2-Score</cell></row><row><cell></cell><cell>ISS [28]</cell><cell>0.044</cell><cell>0.268</cell><cell>0.076</cell></row><row><cell>PC</cell><cell>Random</cell><cell>0.199</cell><cell>0.196</cell><cell>0.197</cell></row><row><cell></cell><cell>DeepI2P [19]</cell><cell>0.935</cell><cell>0.946</cell><cell>0.938</cell></row><row><cell></cell><cell>Ours</cell><cell>0.975</cell><cell>0.911</cell><cell>0.941 0.941 0.941</cell></row><row><cell></cell><cell>SIFT [27]</cell><cell>0.091</cell><cell>0.585</cell><cell>0.156</cell></row><row><cell>IMG</cell><cell>Random</cell><cell>0.329</cell><cell>0.599</cell><cell>0.424</cell></row><row><cell></cell><cell>Ours</cell><cell>0.783</cell><cell>0.903</cell><cell>0.838 0.838 0.838</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF THE REGISTRATION ACCURACY OF DIFFERENT SAMPLING STRATEGIES ON THE KITTI DATASET. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.</figDesc><table><row><cell>IMG</cell><cell>PC</cell><cell>RTE (m)</cell><cell>RRE ( ? )</cell></row><row><cell>Random</cell><cell>Random</cell><cell>1.57 ? 1.01</cell><cell>3.53 ? 2.22</cell></row><row><cell>Random</cell><cell>Score</cell><cell>0.94 ? 0.71</cell><cell>3.06 ? 1.85</cell></row><row><cell>Score</cell><cell>Random</cell><cell>1.79 ? 1.14</cell><cell>3.67 ? 2.19</cell></row><row><cell>Score</cell><cell>ISS</cell><cell>1.67 ? 1.08</cell><cell>3.40 ? 2.12</cell></row><row><cell>SIFT</cell><cell>Score</cell><cell>0.94 ? 0.71</cell><cell>2.89 ? 1.88</cell></row><row><cell>SIFT</cell><cell>ISS</cell><cell>1.91 ? 1.15</cell><cell>4.13 ? 2.25</cell></row><row><cell>Score</cell><cell>Score</cell><cell>0.74 ? 0.65 0.74 ? 0.65 0.74 ? 0.65</cell><cell>2.07 ? 1.64 2.07 ? 1.64 2.07 ? 1.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF THE REGISTRATION ACCURACY OF DIFFERENT 3D POINT DENSITY.</figDesc><table><row><cell># points</cell><cell>RTE (m)</cell><cell>RRE ( ? )</cell></row><row><cell>5120</cell><cell>1.19 ? 0.87</cell><cell>2.72 ? 1.91</cell></row><row><cell>10240</cell><cell>1.08 ? 0.83</cell><cell>2.59 ? 1.91</cell></row><row><cell>20480</cell><cell>0.93 ? 0.77</cell><cell>2.26 ? 1.77</cell></row><row><cell>40960</cell><cell>0.74 ? 0.65</cell><cell>2.07 ? 1.64</cell></row><row><cell cols="3">dynamic, such as cars or pedestrians, making the point cloud</cell></row><row><cell cols="3">not aligned completely, which would cause trouble extracting</cell></row><row><cell>point cloud features.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF THE EFFICIENCY OF DIFFERENT METHODS ON THE KITTI DATASET.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Method</cell><cell></cell><cell cols="3">Network size (MB)</cell><cell></cell><cell></cell><cell cols="2">FLOPs (G)</cell><cell>GPU Memory (GB)</cell><cell>Inference (ms)</cell><cell>Pose Estimation (s)</cell></row><row><cell></cell><cell></cell><cell cols="5">Grid Cls. + EPnP</cell><cell></cell><cell>100.75</cell><cell></cell><cell></cell><cell></cell><cell>20.75</cell><cell>2.39</cell><cell>11.20</cell><cell>0.04</cell></row><row><cell></cell><cell></cell><cell cols="5">DeepI2P (3D)</cell><cell></cell><cell>100.12</cell><cell></cell><cell></cell><cell></cell><cell>13.99</cell><cell>2.01</cell><cell>7.55</cell><cell>16.58</cell></row><row><cell></cell><cell></cell><cell cols="5">DeepI2P (2D)</cell><cell></cell><cell>100.12</cell><cell></cell><cell></cell><cell></cell><cell>13.99</cell><cell>2.01</cell><cell>7.58</cell><cell>9.38</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>141.07</cell><cell></cell><cell></cell><cell></cell><cell>30.84</cell><cell>2.88</cell><cell>13.75</cell><cell>2.97</cell></row><row><cell>5.0% 10.0% 15.0% 20.0% 25.0% 30.0% 35.0% 40.0%</cell><cell>KITTI RTE Histogram</cell><cell cols="2">5.0% 10.0% 15.0% 20.0% 25.0% 30.0%</cell><cell></cell><cell></cell><cell>KITTI RRE Histogram</cell><cell>2.5% 5.0% 7.5% 10.0% 12.5% 15.0% 17.5% 20.0%</cell><cell>NuScenes RTE Histogram</cell><cell cols="2">5.0% 10.0% 15.0% 20.0% 25.0% 30.0%</cell><cell></cell><cell>NuScenes RRE Histogram</cell></row><row><cell>0.0%</cell><cell cols="2">0.0 2.5 5.0 7.5 10.0 12.5 15.0</cell><cell>0.0%</cell><cell>0</cell><cell>5</cell><cell>10 15 20 25 30</cell><cell>0.0%</cell><cell cols="2">0.0 2.5 5.0 7.5 10.0 12.5 15.0</cell><cell>0.0%</cell><cell>0</cell><cell>5</cell><cell>10 15 20 25 30</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is the author of 2 books, more than 180 articles, and more than 7 inventions. His research interests include photoelectric detection technology and instrument, image processing and machine vision detection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time slam based on image stitching for autonomous navigation of uavs in gnss-denied regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rizk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mroue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Summary maps for lifelong visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?hlfellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>B?rki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Derendarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Philippsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Furgale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="561" to="590" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Virtual, augmented, and mixed reality applications in orthopedic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Verhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Haglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Verhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Hartigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Medical Robotics and Computer Assisted Surgery</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2067</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simultaneous localization and mapping: part i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Durrant-Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE robotics &amp; automation magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="110" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual slam location methods based on complex scenes: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="487" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deformable medical image registration: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1153" to="1190" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An fft-based technique for translation, rotation, and scale-invariant image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Chatterji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1266" to="1271" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parametric estimation of affine deformations of binary images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Francos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="889" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate registration using adaptive block processing for multispectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1491" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A lowcomplexity image registration algorithm for global motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Frater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="426" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page" from="586" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Go-icp: Solving 3d registration efficiently and globally optimally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3dmatch: Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The perfect match: 3d point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5545" to="5554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predator: Registration of 3d point clouds with low overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usvyatsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4267" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepi2p: Image-to-point cloud registration via deep classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="960" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepicp: An end-to-end deep neural network for 3d point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A coarse-to-fine algorithm for matching and registration in 3d cross-source point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2965" to="2977" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward efficient and robust metrics for ransac hypotheses and 3d rigid registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="893" to="906" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aligning 2.5 d scene fragments with distinctive local geometric features and voting-based correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="714" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corrnet3d: Unsupervised end-to-end learning of dense correspondence for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6052" to="6061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent multi-view alignment network for unsupervised surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2d3d-matchnet: Learning to match keypoints across 2d image and 3d point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4790" to="4796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
		<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Intrinsic shape signatures: A shape descriptor for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th international conference on computer vision workshops, ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364913491297</idno>
		<ptr target="https://doi.org/10.1177/0278364913491297" />
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Machine learning for high-speed corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning features at scale for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3223" to="3230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Summary maps for lifelong visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?hlfellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>B?rki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Derendarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Philippsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Furgale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="561" to="590" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vidloc: A deep spatio-temporal model for 6-dof video-clip relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6856" to="6864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image-based localization using lstms for structured feature correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Walch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hilsenbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="627" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3523" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rpm-net: Robust point matching using learned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">D3feat: Joint learning of dense detection and description of 3d local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6359" to="6367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pointnetlk: Robust &amp; efficient point cloud registration using pointnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7163" to="7172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pcrnet: Point cloud registration network using pointnet encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sarode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07906</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust registration of 2d and 3d point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0262885603001835" />
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1145" to="1153" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>british Machine Vision Computing</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular camera localization in prior lidar maps with 2d-3d line correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4588" to="4594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning 2d-3d correspondences to solve the blind perspective-n-point problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06752</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast and accurate registration of structured point clouds with small overlaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pointdsc: Robust point cloud registration using deep spatial consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hregnet: A hierarchical network for large-scale outdoor lidar point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ppfnet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
